{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:31:17.652307Z",
     "start_time": "2019-01-07T01:30:55.995799Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold;\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:31:17.717355Z",
     "start_time": "2019-01-07T01:31:17.655903Z"
    }
   },
   "outputs": [],
   "source": [
    "df_house = pd.read_table('../TFLearn/df_house.txt', low_memory=False, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:31:17.733609Z",
     "start_time": "2019-01-07T01:31:17.720082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Float64Index: 27785 entries, 4553.0 to 9435.0\n",
      "Data columns (total 5 columns):\n",
      "SBJE        27785 non-null float64\n",
      "EXCEL_ID    27785 non-null float64\n",
      "NL          27785 non-null float64\n",
      "GJJJE       27785 non-null float64\n",
      "Y           27785 non-null float64\n",
      "dtypes: float64(5)\n",
      "memory usage: 1.3 MB\n"
     ]
    }
   ],
   "source": [
    "df_house.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:31:17.742932Z",
     "start_time": "2019-01-07T01:31:17.737836Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_house_x = df_house.iloc[:, [0, 2, 3]]\n",
    "df_house_x = df_house[[\"SBJE\", \"NL\", \"GJJJE\"]]\n",
    "df_house_y = df_house['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:31:17.751114Z",
     "start_time": "2019-01-07T01:31:17.746902Z"
    }
   },
   "outputs": [],
   "source": [
    "# 独热编码\n",
    "np_house_y_oh = (np.arange(2) == df_house_y[:,None]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:31:17.768523Z",
     "start_time": "2019-01-07T01:31:17.753847Z"
    }
   },
   "outputs": [],
   "source": [
    "# np_x_train, np_x_test, np_y_train, np_y_test = train_test_split(\n",
    "#     df_house_x, np_house_y_oh, test_size=0.7)\n",
    "np_x_train, np_x_test, np_y_train, np_y_test = train_test_split(\n",
    "    df_house_x, df_house_y, test_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:31:17.783361Z",
     "start_time": "2019-01-07T01:31:17.772049Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8335, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:31:17.799666Z",
     "start_time": "2019-01-07T01:31:17.786429Z"
    }
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "np_x_train = ss.fit_transform(np_x_train)\n",
    "np_x_test = ss.fit_transform(np_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:32:13.504766Z",
     "start_time": "2019-01-07T01:32:13.501949Z"
    }
   },
   "outputs": [],
   "source": [
    "# g = sns.pairplot(df_house[[\"Y\",\"SBJE\", \"NL\", \"GJJJE\"]],\n",
    "# hue=\"Y\", palette = \"seismic\",size=1.2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:32:13.878573Z",
     "start_time": "2019-01-07T01:32:13.876063Z"
    }
   },
   "outputs": [],
   "source": [
    "# colormap = plt.cm.viridis\n",
    "# plt.figure(figsize=(12,12))\n",
    "# plt.title('HOUSE', y=1.05, size=15)\n",
    "# sns.heatmap(df_house[[\"Y\",\"SBJE\", \"NL\", \"GJJJE\"]].astype(float).corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True, xticklabels=True, yticklabels=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:31:25.230664Z",
     "start_time": "2019-01-07T01:31:25.220843Z"
    }
   },
   "outputs": [],
   "source": [
    "# Some useful parameters which will come in handy later on\n",
    "ntrain = np_x_train.shape[0]\n",
    "ntest = np_x_test.shape[0]\n",
    "SEED = 0 # for reproducibility\n",
    "NFOLDS = 5 # set folds for out-of-fold prediction\n",
    "kf = KFold(n_splits = NFOLDS,shuffle=False,random_state=0)\n",
    "\n",
    "# Class to extend the Sklearn classifier\n",
    "class SklearnHelper(object):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)\n",
    "    \n",
    "    def fit(self,x,y):\n",
    "        return self.clf.fit(x,y)\n",
    "    \n",
    "    def feature_importances(self,x,y):\n",
    "        print(self.clf.fit(x,y).feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:31:25.242993Z",
     "start_time": "2019-01-07T01:31:25.234435Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_oof(clf, x_train, y_train, x_test):\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(np_x_train)):\n",
    "#         print('train_index',train_index)\n",
    "#         print('test_index',test_index.shape)\n",
    "#         print('x_train',type(x_train))\n",
    "        \n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "\n",
    "        clf.train(x_tr, y_tr)\n",
    "#         print('x_te',x_te.shape)\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i, :] = clf.predict(x_test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:31:25.255125Z",
     "start_time": "2019-01-07T01:31:25.246726Z"
    }
   },
   "outputs": [],
   "source": [
    "# Put in our parameters for said classifiers\n",
    "# Random Forest parameters\n",
    "rf_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 500,\n",
    "     'warm_start': True, \n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 6,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features' : 'sqrt',\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Extra Trees Parameters\n",
    "et_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators':500,\n",
    "    #'max_features': 0.5,\n",
    "    'max_depth': 8,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# AdaBoost parameters\n",
    "ada_params = {\n",
    "    'n_estimators': 500,\n",
    "    'learning_rate' : 0.75\n",
    "}\n",
    "\n",
    "# Gradient Boosting parameters\n",
    "gb_params = {\n",
    "    'n_estimators': 500,\n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Support Vector Classifier parameters \n",
    "svc_params = {\n",
    "    'kernel' : 'linear',\n",
    "    'C' : 0.025\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:31:25.267618Z",
     "start_time": "2019-01-07T01:31:25.258195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8335, 3)\n",
      "(8335,)\n",
      "(19450, 3)\n"
     ]
    }
   ],
   "source": [
    "np_x_train = np.array(np_x_train)\n",
    "np_y_train = np.array(np_y_train)\n",
    "np_x_test = np.array(np_x_test)\n",
    "\n",
    "\n",
    "print(np_x_train.shape)\n",
    "print(np_y_train.shape)\n",
    "print(np_x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:31:25.512685Z",
     "start_time": "2019-01-07T01:31:25.271480Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'et' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2c8e87bad6e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0met_oof_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0met_oof_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_oof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0met\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_x_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_y_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_x_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Extra Trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# rf_oof_train, rf_oof_test = get_oof(rf,np_x_train, np_y_train, np_x_test) # Random Forest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mada_oof_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mada_oof_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_oof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mada\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_x_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_y_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_x_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# AdaBoost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgb_oof_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgb_oof_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_oof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp_x_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_y_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_x_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Gradient Boost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msvc_oof_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvc_oof_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_oof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp_x_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_y_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_x_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Support Vector Classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'et' is not defined"
     ]
    }
   ],
   "source": [
    "et_oof_train, et_oof_test = get_oof(et, np_x_train, np_y_train, np_x_test) # Extra Trees\n",
    "# rf_oof_train, rf_oof_test = get_oof(rf,np_x_train, np_y_train, np_x_test) # Random Forest\n",
    "ada_oof_train, ada_oof_test = get_oof(ada, np_x_train, np_y_train, np_x_test) # AdaBoost \n",
    "gb_oof_train, gb_oof_test = get_oof(gb,np_x_train, np_y_train, np_x_test) # Gradient Boost\n",
    "svc_oof_train, svc_oof_test = get_oof(svc,np_x_train, np_y_train, np_x_test) # Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T05:29:26.089753Z",
     "start_time": "2019-01-05T05:29:26.084962Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:31:25.518114Z",
     "start_time": "2019-01-07T01:31:05.017Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create 5 objects that represent our 4 models\n",
    "rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\n",
    "et = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\n",
    "ada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\n",
    "gb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\n",
    "svc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:31:25.519950Z",
     "start_time": "2019-01-07T01:31:05.199Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_feature = rf.feature_importances(np_x_train,np_y_train)\n",
    "et_feature = et.feature_importances(np_x_train, np_y_train)\n",
    "ada_feature = ada.feature_importances(np_x_train, np_y_train)\n",
    "gb_feature = gb.feature_importances(np_x_train,np_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:31:25.521522Z",
     "start_time": "2019-01-07T01:31:05.360Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_features = [0.16675993,0.57758664,0.25565343]\n",
    "et_features = [0.06074514,0.66660879,0.27264607]\n",
    "ada_features = [0.416,0.082,0.502]\n",
    "gb_features = [0.21774065,0.53160719,0.25065216]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:31:25.523326Z",
     "start_time": "2019-01-07T01:31:05.529Z"
    }
   },
   "outputs": [],
   "source": [
    "cols = df_house_x.columns.values\n",
    "# Create a dataframe with features\n",
    "feature_dataframe = pd.DataFrame( {'features': cols,\n",
    "     'Random Forest feature importances': rf_features,\n",
    "     'Extra Trees  feature importances': et_features,\n",
    "      'AdaBoost feature importances': ada_features,\n",
    "    'Gradient Boost feature importances': gb_features\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:31:25.524957Z",
     "start_time": "2019-01-07T01:31:05.742Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 散点图 \n",
    "trace = go.Scatter(\n",
    "    y = feature_dataframe['Random Forest feature importances'].values,\n",
    "    x = feature_dataframe['features'].values,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        sizemode = 'diameter',\n",
    "        sizeref = 1,\n",
    "        size = 25,\n",
    "#       size= feature_dataframe['AdaBoost feature importances'].values,\n",
    "        #color = np.random.randn(500), #set color equal to a variable\n",
    "        color = feature_dataframe['Random Forest feature importances'].values,\n",
    "        colorscale='Portland',\n",
    "        showscale=True\n",
    "    ),\n",
    "    text = feature_dataframe['features'].values\n",
    ")\n",
    "data = [trace]\n",
    "\n",
    "layout= go.Layout(\n",
    "    autosize= True,\n",
    "    title= '随机森林特征重要度',\n",
    "    hovermode= 'closest',\n",
    "#     xaxis= dict(\n",
    "#         title= 'Pop',\n",
    "#         ticklen= 5,\n",
    "#         zeroline= False,\n",
    "#         gridwidth= 2,\n",
    "#     ),\n",
    "    yaxis=dict(\n",
    "        title= '特征重要度',\n",
    "        ticklen= 5,\n",
    "        gridwidth= 2\n",
    "    ),\n",
    "    showlegend= False\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig,filename='scatter2010')\n",
    "\n",
    "# Scatter plot \n",
    "trace = go.Scatter(\n",
    "    y = feature_dataframe['Extra Trees  feature importances'].values,\n",
    "    x = feature_dataframe['features'].values,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        sizemode = 'diameter',\n",
    "        sizeref = 1,\n",
    "        size = 25,\n",
    "#       size= feature_dataframe['AdaBoost feature importances'].values,\n",
    "        #color = np.random.randn(500), #set color equal to a variable\n",
    "        color = feature_dataframe['Extra Trees  feature importances'].values,\n",
    "        colorscale='Portland',\n",
    "        showscale=True\n",
    "    ),\n",
    "    text = feature_dataframe['features'].values\n",
    ")\n",
    "data = [trace]\n",
    "\n",
    "layout= go.Layout(\n",
    "    autosize= True,\n",
    "    title= 'Extra Trees Feature Importance',\n",
    "    hovermode= 'closest',\n",
    "#     xaxis= dict(\n",
    "#         title= 'Pop',\n",
    "#         ticklen= 5,\n",
    "#         zeroline= False,\n",
    "#         gridwidth= 2,\n",
    "#     ),\n",
    "    yaxis=dict(\n",
    "        title= '特征重要度',\n",
    "        ticklen= 5,\n",
    "        gridwidth= 2\n",
    "    ),\n",
    "    showlegend= False\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig,filename='scatter2010')\n",
    "\n",
    "# Scatter plot \n",
    "trace = go.Scatter(\n",
    "    y = feature_dataframe['AdaBoost feature importances'].values,\n",
    "    x = feature_dataframe['features'].values,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        sizemode = 'diameter',\n",
    "        sizeref = 1,\n",
    "        size = 25,\n",
    "#       size= feature_dataframe['AdaBoost feature importances'].values,\n",
    "        #color = np.random.randn(500), #set color equal to a variable\n",
    "        color = feature_dataframe['AdaBoost feature importances'].values,\n",
    "        colorscale='Portland',\n",
    "        showscale=True\n",
    "    ),\n",
    "    text = feature_dataframe['features'].values\n",
    ")\n",
    "data = [trace]\n",
    "\n",
    "layout= go.Layout(\n",
    "    autosize= True,\n",
    "    title= 'AdaBoost Feature Importance',\n",
    "    hovermode= 'closest',\n",
    "#     xaxis= dict(\n",
    "#         title= 'Pop',\n",
    "#         ticklen= 5,\n",
    "#         zeroline= False,\n",
    "#         gridwidth= 2,\n",
    "#     ),\n",
    "    yaxis=dict(\n",
    "        title= '特征重要度',\n",
    "        ticklen= 5,\n",
    "        gridwidth= 2\n",
    "    ),\n",
    "    showlegend= False\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig,filename='scatter2010')\n",
    "\n",
    "# Scatter plot \n",
    "trace = go.Scatter(\n",
    "    y = feature_dataframe['Gradient Boost feature importances'].values,\n",
    "    x = feature_dataframe['features'].values,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        sizemode = 'diameter',\n",
    "        sizeref = 1,\n",
    "        size = 25,\n",
    "#       size= feature_dataframe['AdaBoost feature importances'].values,\n",
    "        #color = np.random.randn(500), #set color equal to a variable\n",
    "        color = feature_dataframe['Gradient Boost feature importances'].values,\n",
    "        colorscale='Portland',\n",
    "        showscale=True\n",
    "    ),\n",
    "    text = feature_dataframe['features'].values\n",
    ")\n",
    "data = [trace]\n",
    "\n",
    "layout= go.Layout(\n",
    "    autosize= True,\n",
    "    title= 'Gradient Boosting Feature Importance',\n",
    "    hovermode= 'closest',\n",
    "#     xaxis= dict(\n",
    "#         title= 'Pop',\n",
    "#         ticklen= 5,\n",
    "#         zeroline= False,\n",
    "#         gridwidth= 2,\n",
    "#     ),\n",
    "    yaxis=dict(\n",
    "        title= 'AdaBoost',\n",
    "        ticklen= 5,\n",
    "        gridwidth= 2\n",
    "    ),\n",
    "    showlegend= False\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig,filename='scatter2010')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:31:25.526573Z",
     "start_time": "2019-01-07T01:31:05.964Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1表示操作横轴 computes the mean row-wise\n",
    "feature_dataframe.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:31:25.528101Z",
     "start_time": "2019-01-07T01:31:06.546Z"
    }
   },
   "outputs": [],
   "source": [
    "base_predictions_train = pd.DataFrame( {\n",
    "     'ExtraTrees': et_oof_train.ravel(),\n",
    "     'AdaBoost': ada_oof_train.ravel(),\n",
    "      'GradientBoost': gb_oof_train.ravel()\n",
    "    })\n",
    "base_predictions_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:31:25.529834Z",
     "start_time": "2019-01-07T01:31:07.101Z"
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    go.Heatmap(\n",
    "        z= base_predictions_train.astype(float).corr().values ,\n",
    "        x=base_predictions_train.columns.values,\n",
    "        y= base_predictions_train.columns.values,\n",
    "          colorscale='Portland',\n",
    "            showscale=True,\n",
    "            reversescale = True\n",
    "    )\n",
    "]\n",
    "py.iplot(data, filename='labelled-heatmap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:31:25.531489Z",
     "start_time": "2019-01-07T01:31:07.337Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = np.concatenate(( et_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\n",
    "x_test = np.concatenate(( et_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:31:25.533300Z",
     "start_time": "2019-01-07T01:31:07.581Z"
    }
   },
   "outputs": [],
   "source": [
    "gbm = xgb.XGBClassifier(\n",
    "    #learning_rate = 0.02,\n",
    " n_estimators= 2000,\n",
    " max_depth= 4,\n",
    " min_child_weight= 2,\n",
    " #gamma=1,\n",
    " gamma=0.9,                        \n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread= -1,\n",
    " scale_pos_weight=1).fit(x_train, np_y_train)\n",
    "predictions = gbm.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:31:25.535160Z",
     "start_time": "2019-01-07T01:31:07.741Z"
    }
   },
   "outputs": [],
   "source": [
    "print((x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:31:25.536970Z",
     "start_time": "2019-01-07T01:31:07.900Z"
    }
   },
   "outputs": [],
   "source": [
    "eq = predictions == np_y_test\n",
    "eq = np.array(np.where(eq == True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:31:25.538228Z",
     "start_time": "2019-01-07T01:31:08.180Z"
    }
   },
   "outputs": [],
   "source": [
    "print(eq.shape[1]/x_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:30:49.508401Z",
     "start_time": "2019-01-07T01:30:23.329113Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e040b9cdb85d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mn_folds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "#模型尝试\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC,LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "n_folds = 10\n",
    "\n",
    "def rmsle_cv(model,train_x_head=train_x_head):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train_x_head)\n",
    "    rmse= -cross_val_score(model, train_x_head, Y, scoring=\"neg_mean_squared_error\", cv = kf)\n",
    "    return(rmse)\n",
    "    \n",
    "svr = make_pipeline( SVR(kernel='linear'))\n",
    " \n",
    "line = make_pipeline( LinearRegression())\n",
    "lasso = make_pipeline( Lasso(alpha =0.0005, random_state=1))\n",
    "ENet = make_pipeline( ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
    "KRR1 = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "KRR2 = KernelRidge(alpha=1.5, kernel='linear', degree=2, coef0=2.5)\n",
    "#KRR3 = KernelRidge(alpha=0.6, kernel='rbf', degree=2, coef0=2.5)\n",
    "# =============================================================================\n",
    "# GBoost = GradientBoostingRegressor(n_estimators=5000, learning_rate=0.02,\n",
    "#                                    max_depth=5, max_features=7,\n",
    "#                                    min_samples_leaf=15, min_samples_split=10, \n",
    "#                                    loss='huber', random_state =5)\n",
    "# =============================================================================\n",
    "\n",
    "model_xgb = xgb.XGBRegressor(booster='gbtree',colsample_bytree=0.8, gamma=0.1, \n",
    "                             learning_rate=0.02, max_depth=5, \n",
    "                             n_estimators=500,min_child_weight=0.8,\n",
    "                             reg_alpha=0, reg_lambda=1,\n",
    "                             subsample=0.8, silent=1,\n",
    "                             random_state =42, nthread = 2)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# cv_params = {'min_child_weight': [0.05,0.1,0.15,0.2,0.25],\n",
    "#              'learning_rate': [0.01, 0.02, 0.05, 0.1],\n",
    "#              'max_depth': [3,5,7,9]}\n",
    "# \n",
    "# other_params = {'learning_rate': 0.02, 'n_estimators': 400, 'max_depth': 5, 'min_child_weight': 0.8, 'seed': 0,\n",
    "#                 'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0.5, 'reg_alpha': 0, 'reg_lambda': 1}\n",
    "# \n",
    "# model = xgb.XGBRegressor(**other_params)\n",
    "# optimized_GBM = GridSearchCV(estimator=model, param_grid=cv_params, scoring='neg_mean_squared_error', cv=5, verbose=1, n_jobs=4)\n",
    "# optimized_GBM.fit(train_x, Y)\n",
    "# evalute_result = optimized_GBM.grid_scores_\n",
    "# print('每轮迭代运行结果:{0}'.format(evalute_result))\n",
    "# print('参数的最佳取值：{0}'.format(optimized_GBM.best_params_))\n",
    "# print('最佳模型得分:{0}'.format(optimized_GBM.best_score_))\n",
    "# model_xgb = xgb.XGBRegressor(optimized_GBM.best_params_)\n",
    "# =============================================================================\n",
    "# =============================================================================\n",
    "# model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "#                               learning_rate=0.05, n_estimators=720,\n",
    "#                               max_bin = 55, bagging_fraction = 0.8,\n",
    "#                               bagging_freq = 5, feature_fraction = 0.2319,\n",
    "#                               feature_fraction_seed=9, bagging_seed=9,\n",
    "#                               min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# parameters = { \n",
    "#             'n_estimators':[300,600,900,1500,2500],\n",
    "#             #'boosting':'dart',\n",
    "#             'max_bin':[55,75,95],\n",
    "#             'num_iterations':[50,100,250,400],\n",
    "#              # 'max_features':[7,9,11,13],\n",
    "#               'min_samples_leaf': [15, 25, 35, 45],\n",
    "#               'learning_rate': [0.01, 0.03, 0.05, 0.1],\n",
    "#               'num_leaves':[15,31,63],\n",
    "#             \n",
    "#               'lambda_l2':[0,1]}  # 定义要优化的参数信息\n",
    "# clf = GridSearchCV( model_lgb, parameters, n_jobs=3,scoring = 'neg_mean_squared_error' )\n",
    "# clf.fit(train_x,Y)\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "#print('best n_estimators:', clf.best_params_)\n",
    "#print('best cv score:', clf.score_)\n",
    "\n",
    "\n",
    "score = rmsle_cv(svr)\n",
    "print(\"\\nSVR 得分: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "svr.fit(train_x_head,Y)\n",
    "score = rmsle_cv(line)\n",
    "print(\"\\nLine 得分: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "score = rmsle_cv(lasso)\n",
    "print(\"\\nLasso 得分: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "score = rmsle_cv(ENet)\n",
    "print(\"ElasticNet 得分: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "\n",
    "score = rmsle_cv(KRR2)\n",
    "print(\"Kernel Ridge2 得分: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "KRR2.fit(train_x_head,Y)\n",
    "#score = rmsle_cv(KRR3)\n",
    "#print(\"Kernel Ridge3 得分: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "# =============================================================================\n",
    "head_feature_num = 18\n",
    "feat_scored_headnum = feature_scoring.sort_values('score', ascending=False).head(head_feature_num)['feature']\n",
    "train_x_head2 = train_x[train_x.columns[train_x.columns.isin(feat_scored_headnum)]]\n",
    "X_scaled = pd.DataFrame(preprocessing.scale(train_x),columns = train_x.columns)\n",
    "score = rmsle_cv(KRR1,train_x_head2)\n",
    "print(\"Kernel Ridge1 得分: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "# score = rmsle_cv(GBoost)\n",
    "# print(\"Gradient Boosting 得分: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "# =============================================================================\n",
    "head_feature_num = 22\n",
    "feat_scored_headnum = feature_scoring.sort_values('score', ascending=False).head(head_feature_num)['feature']\n",
    "train_x_head3 = train_x[train_x.columns[train_x.columns.isin(feat_scored_headnum)]]\n",
    "X_scaled = pd.DataFrame(preprocessing.scale(train_x),columns = train_x.columns)\n",
    "score = rmsle_cv(model_xgb,train_x_head3)\n",
    "print(\"Xgboost 得分: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "model_xgb.fit(train_x_head,Y)\n",
    "# =============================================================================\n",
    "# score = rmsle_cv(model_lgb)\n",
    "# print(\"LGBM 得分: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
