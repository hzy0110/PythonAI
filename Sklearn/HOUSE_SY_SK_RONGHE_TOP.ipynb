{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:47:07.047448Z",
     "start_time": "2019-01-07T05:46:38.312917Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "import seaborn as sns\n",
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:47:07.157160Z",
     "start_time": "2019-01-07T05:47:07.120009Z"
    }
   },
   "outputs": [],
   "source": [
    "df_house = pd.read_table('../TFLearn/df_house.txt', low_memory=False, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:47:07.162105Z",
     "start_time": "2019-01-07T05:47:07.159082Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_house.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:47:07.177752Z",
     "start_time": "2019-01-07T05:47:07.173279Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_house.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:47:07.189284Z",
     "start_time": "2019-01-07T05:47:07.181761Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_house_x = df_house.iloc[:, [0, 2, 3]]\n",
    "df_house_x = df_house[[\"SBJE\", \"NL\", \"GJJJE\"]]\n",
    "df_house_y = df_house['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:47:07.197979Z",
     "start_time": "2019-01-07T05:47:07.192463Z"
    }
   },
   "outputs": [],
   "source": [
    "# 独热编码\n",
    "np_house_y_oh = (np.arange(2) == df_house_y[:,None]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:47:07.210525Z",
     "start_time": "2019-01-07T05:47:07.201215Z"
    }
   },
   "outputs": [],
   "source": [
    "# np_x_train, np_x_test, np_y_train, np_y_test = train_test_split(\n",
    "#     df_house_x, np_house_y_oh, test_size=0.7)\n",
    "np_x_train, np_x_test, np_y_train, np_y_test = train_test_split(\n",
    "    df_house_x, df_house_y, test_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:47:07.220559Z",
     "start_time": "2019-01-07T05:47:07.213823Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8335, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:47:07.234007Z",
     "start_time": "2019-01-07T05:47:07.223164Z"
    }
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "np_x_train = ss.fit_transform(np_x_train)\n",
    "np_x_test = ss.fit_transform(np_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:47:07.239221Z",
     "start_time": "2019-01-07T05:47:07.236240Z"
    }
   },
   "outputs": [],
   "source": [
    "combine = [np_x_train, np_x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:47:07.281088Z",
     "start_time": "2019-01-07T05:47:07.242528Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.67\n",
      "0.6761953727506427\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "logreg = LogisticRegression(solver='lbfgs')\n",
    "logreg.fit(np_x_train, np_y_train)\n",
    "Y_pred = logreg.predict(np_x_test)\n",
    "acc_log = round(logreg.score(np_x_train, np_y_train) * 100, 2)\n",
    "\n",
    "\n",
    "print(acc_log)\n",
    "print(accuracy_score(np_y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:47:11.982132Z",
     "start_time": "2019-01-07T05:47:07.284589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.27\n",
      "0.6932647814910026\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "svc = SVC()\n",
    "svc.fit(np_x_train, np_y_train)\n",
    "Y_pred = svc.predict(np_x_test)\n",
    "acc_svc = round(svc.score(np_x_train, np_y_train) * 100, 2)\n",
    "\n",
    "\n",
    "print(acc_svc)\n",
    "print(accuracy_score(np_y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:47:12.168130Z",
     "start_time": "2019-01-07T05:47:11.987341Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.56\n",
      "0.6495629820051414\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn.fit(np_x_train, np_y_train)\n",
    "Y_pred = knn.predict(np_x_test)\n",
    "acc_knn = round(knn.score(np_x_train, np_y_train) * 100, 2)\n",
    "\n",
    "\n",
    "print(acc_knn)\n",
    "print(accuracy_score(np_y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:47:12.190321Z",
     "start_time": "2019-01-07T05:47:12.171499Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.34\n",
      "0.6630848329048843\n"
     ]
    }
   ],
   "source": [
    "# Gaussian Naive Bayes\n",
    "\n",
    "gaussian = GaussianNB()\n",
    "gaussian.fit(np_x_train, np_y_train)\n",
    "Y_pred = gaussian.predict(np_x_test)\n",
    "acc_gaussian = round(gaussian.score(np_x_train, np_y_train) * 100, 2)\n",
    "\n",
    "\n",
    "print(acc_gaussian)\n",
    "print(accuracy_score(np_y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:47:12.216605Z",
     "start_time": "2019-01-07T05:47:12.193673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.41\n",
      "0.6016966580976864\n"
     ]
    }
   ],
   "source": [
    "# Perceptron\n",
    "\n",
    "perceptron = Perceptron(max_iter=10,tol=1000)\n",
    "perceptron.fit(np_x_train, np_y_train)\n",
    "Y_pred = perceptron.predict(np_x_test)\n",
    "acc_perceptron = round(perceptron.score(np_x_train, np_y_train) * 100, 2)\n",
    "\n",
    "print(acc_perceptron)\n",
    "print(accuracy_score(np_y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:47:13.370008Z",
     "start_time": "2019-01-07T05:47:12.219760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.61\n",
      "0.6762982005141388\n"
     ]
    }
   ],
   "source": [
    "# Linear SVC\n",
    "\n",
    "linear_svc = LinearSVC(max_iter=10000)\n",
    "linear_svc.fit(np_x_train, np_y_train)\n",
    "Y_pred = linear_svc.predict(np_x_test)\n",
    "acc_linear_svc = round(linear_svc.score(np_x_train, np_y_train) * 100, 2)\n",
    "\n",
    "print(acc_linear_svc)\n",
    "print(accuracy_score(np_y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:47:13.897495Z",
     "start_time": "2019-01-07T05:47:13.372967Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.41\n",
      "0.6807712082262211\n"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "sgd = SGDClassifier(max_iter=1000,tol=None)\n",
    "sgd.fit(np_x_train, np_y_train)\n",
    "Y_pred = sgd.predict(np_x_test)\n",
    "acc_sgd = round(sgd.score(np_x_train, np_y_train) * 100, 2)\n",
    "\n",
    "print(acc_sgd)\n",
    "print(accuracy_score(np_y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:47:13.939731Z",
     "start_time": "2019-01-07T05:47:13.899894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.83\n",
      "0.6324935732647815\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(np_x_train, np_y_train)\n",
    "Y_pred = decision_tree.predict(np_x_test)\n",
    "acc_decision_tree = round(decision_tree.score(np_x_train, np_y_train) * 100, 2)\n",
    "print(acc_decision_tree)\n",
    "print(accuracy_score(np_y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:47:15.168793Z",
     "start_time": "2019-01-07T05:47:13.942818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.83\n",
      "0.6644730077120823\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "random_forest.fit(np_x_train, np_y_train)\n",
    "Y_pred = random_forest.predict(np_x_test)\n",
    "random_forest.score(np_x_train, np_y_train)\n",
    "acc_random_forest = round(random_forest.score(np_x_train, np_y_train) * 100, 2)\n",
    "print(acc_random_forest)\n",
    "print(accuracy_score(np_y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:47:15.199500Z",
     "start_time": "2019-01-07T05:47:15.171205Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>88.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>88.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN</td>\n",
       "      <td>77.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Support Vector Machines</td>\n",
       "      <td>69.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Stochastic Gradient Decent</td>\n",
       "      <td>67.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>66.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>66.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>65.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>59.41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Score\n",
       "3               Random Forest  88.83\n",
       "8               Decision Tree  88.83\n",
       "1                         KNN  77.56\n",
       "0     Support Vector Machines  69.27\n",
       "6  Stochastic Gradient Decent  67.41\n",
       "2         Logistic Regression  66.67\n",
       "7                  Linear SVC  66.61\n",
       "4                 Naive Bayes  65.34\n",
       "5                  Perceptron  59.41"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = pd.DataFrame({\n",
    "    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n",
    "              'Random Forest', 'Naive Bayes', 'Perceptron', \n",
    "              'Stochastic Gradient Decent', 'Linear SVC', \n",
    "              'Decision Tree'],\n",
    "    'Score': [acc_svc, acc_knn, acc_log, \n",
    "              acc_random_forest, acc_gaussian, acc_perceptron, \n",
    "              acc_sgd, acc_linear_svc, acc_decision_tree]})\n",
    "models.sort_values(by='Score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:47:15.213388Z",
     "start_time": "2019-01-07T05:47:15.202413Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6644730077120823"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np_y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:47:30.189489Z",
     "start_time": "2019-01-07T05:47:30.180225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "#单变量\n",
    "X_scored = SelectKBest(score_func=f_regression, k='all').fit(df_house_x, df_house_y)\n",
    "feature_scoring = pd.DataFrame({\n",
    "        'feature': df_house_x.columns,\n",
    "        'score': X_scored.scores_\n",
    "    })\n",
    "head_feature_num = 3\n",
    "feat_scored_headnum = feature_scoring.sort_values('score', ascending=False).head(head_feature_num)['feature']\n",
    "print(type(feat_scored_headnum))\n",
    "# train_x_head = train_x[train_x.columns[train_x.columns.isin(feat_scored_headnum)]]\n",
    "# X_scaled = pd.DataFrame(preprocessing.scale(train_x),columns = train_x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T05:49:25.032Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_9.4.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#模型尝试\n",
    "from sklearn.linear_model import ElasticNet, Lasso, BayesianRidge, LassoLarsIC, LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "n_folds = 10\n",
    "\n",
    "\n",
    "def rmsle_cv(model, df_house_x=df_house_x):\n",
    "    kf = KFold(\n",
    "        n_folds, shuffle=True, random_state=42).get_n_splits(df_house_x)\n",
    "    rmse = -cross_val_score(\n",
    "        model, df_house_x, df_house_y, scoring=\"neg_mean_squared_error\", cv=kf)\n",
    "    return (rmse)\n",
    "\n",
    "\n",
    "svr = make_pipeline(SVR(kernel='linear'))\n",
    "\n",
    "line = make_pipeline(LinearRegression())\n",
    "lasso = make_pipeline(Lasso(alpha=0.0005, random_state=1))\n",
    "ENet = make_pipeline(ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
    "KRR1 = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "KRR2 = KernelRidge(alpha=1.5, kernel='linear', degree=2, coef0=2.5)\n",
    "#KRR3 = KernelRidge(alpha=0.6, kernel='rbf', degree=2, coef0=2.5)\n",
    "# =============================================================================\n",
    "# GBoost = GradientBoostingRegressor(n_estimators=5000, learning_rate=0.02,\n",
    "#                                    max_depth=5, max_features=7,\n",
    "#                                    min_samples_leaf=15, min_samples_split=10,\n",
    "#                                    loss='huber', random_state =5)\n",
    "# =============================================================================\n",
    "\n",
    "model_xgb = xgb.XGBRegressor(\n",
    "    booster='gbtree',\n",
    "    colsample_bytree=0.8,\n",
    "    gamma=0.1,\n",
    "    learning_rate=0.02,\n",
    "    max_depth=5,\n",
    "    n_estimators=500,\n",
    "    min_child_weight=0.8,\n",
    "    reg_alpha=0,\n",
    "    reg_lambda=1,\n",
    "    subsample=0.8,\n",
    "    silent=1,\n",
    "    random_state=42,\n",
    "    nthread=2)\n",
    "\n",
    "# =============================================================================\n",
    "# cv_params = {'min_child_weight': [0.05,0.1,0.15,0.2,0.25],\n",
    "#              'learning_rate': [0.01, 0.02, 0.05, 0.1],\n",
    "#              'max_depth': [3,5,7,9]}\n",
    "#\n",
    "# other_params = {'learning_rate': 0.02, 'n_estimators': 400, 'max_depth': 5, 'min_child_weight': 0.8, 'seed': 0,\n",
    "#                 'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0.5, 'reg_alpha': 0, 'reg_lambda': 1}\n",
    "#\n",
    "# model = xgb.XGBRegressor(**other_params)\n",
    "# optimized_GBM = GridSearchCV(estimator=model, param_grid=cv_params, scoring='neg_mean_squared_error', cv=5, verbose=1, n_jobs=4)\n",
    "# optimized_GBM.fit(train_x, Y)\n",
    "# evalute_result = optimized_GBM.grid_scores_\n",
    "# print('每轮迭代运行结果:{0}'.format(evalute_result))\n",
    "# print('参数的最佳取值：{0}'.format(optimized_GBM.best_params_))\n",
    "# print('最佳模型得分:{0}'.format(optimized_GBM.best_score_))\n",
    "# model_xgb = xgb.XGBRegressor(optimized_GBM.best_params_)\n",
    "# =============================================================================\n",
    "# =============================================================================\n",
    "# model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "#                               learning_rate=0.05, n_estimators=720,\n",
    "#                               max_bin = 55, bagging_fraction = 0.8,\n",
    "#                               bagging_freq = 5, feature_fraction = 0.2319,\n",
    "#                               feature_fraction_seed=9, bagging_seed=9,\n",
    "#                               min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# parameters = {\n",
    "#             'n_estimators':[300,600,900,1500,2500],\n",
    "#             #'boosting':'dart',\n",
    "#             'max_bin':[55,75,95],\n",
    "#             'num_iterations':[50,100,250,400],\n",
    "#              # 'max_features':[7,9,11,13],\n",
    "#               'min_samples_leaf': [15, 25, 35, 45],\n",
    "#               'learning_rate': [0.01, 0.03, 0.05, 0.1],\n",
    "#               'num_leaves':[15,31,63],\n",
    "#\n",
    "#               'lambda_l2':[0,1]}  # 定义要优化的参数信息\n",
    "# clf = GridSearchCV( model_lgb, parameters, n_jobs=3,scoring = 'neg_mean_squared_error' )\n",
    "# clf.fit(train_x,Y)\n",
    "# =============================================================================\n",
    "\n",
    "#print('best n_estimators:', clf.best_params_)\n",
    "#print('best cv score:', clf.score_)\n",
    "\n",
    "score = rmsle_cv(svr)\n",
    "print(\"\\nSVR 得分: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "svr.fit(np_x_train, np_y_train)\n",
    "score = rmsle_cv(line)\n",
    "print(\"\\nLine 得分: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "score = rmsle_cv(lasso)\n",
    "print(\"\\nLasso 得分: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "score = rmsle_cv(ENet)\n",
    "print(\"ElasticNet 得分: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "\n",
    "score = rmsle_cv(KRR2)\n",
    "print(\"Kernel Ridge2 得分: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "KRR2.fit(df_house_x, df_house_y)\n",
    "#score = rmsle_cv(KRR3)\n",
    "#print(\"Kernel Ridge3 得分: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "# =============================================================================\n",
    "head_feature_num = 18\n",
    "feat_scored_headnum = feature_scoring.sort_values(\n",
    "    'score', ascending=False).head(head_feature_num)['feature']\n",
    "train_x_head2 = df_house_x[df_house_x.columns[df_house_x.columns.isin(\n",
    "    feat_scored_headnum)]]\n",
    "X_scaled = pd.DataFrame(preprocessing.scale(df_house_x), columns=df_house_x.columns)\n",
    "score = rmsle_cv(KRR1, train_x_head2)\n",
    "print(\"Kernel Ridge1 得分: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "# score = rmsle_cv(GBoost)\n",
    "# print(\"Gradient Boosting 得分: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "# =============================================================================\n",
    "head_feature_num = 22\n",
    "feat_scored_headnum = feature_scoring.sort_values(\n",
    "    'score', ascending=False).head(head_feature_num)['feature']\n",
    "train_x_head3 = df_house_x[df_house_x.columns[df_house_x.columns.isin(\n",
    "    feat_scored_headnum)]]\n",
    "X_scaled = pd.DataFrame(preprocessing.scale(df_house_x), columns=df_house_x.columns)\n",
    "score = rmsle_cv(model_xgb, train_x_head3)\n",
    "print(\"Xgboost 得分: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "model_xgb.fit(df_house_x, df_house_y)\n",
    "# =============================================================================\n",
    "# score = rmsle_cv(model_lgb)\n",
    "# print(\"LGBM 得分: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T03:07:12.756418Z",
     "start_time": "2019-01-07T03:05:58.930017Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 对基模型集成后的得分: 0.2271 (0.0080)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#简单模型融合\n",
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # 遍历所有模型，你和数据\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    # 预估，并对预估结果值做average\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        #return 0.85*predictions[:,0]+0.15*predictions[:,1]\n",
    "        #return 0.7*predictions[:,0]+0.15*predictions[:,1]+0.15*predictions[:,2]\n",
    "        return np.mean(predictions, axis=1)   \n",
    "#averaged_models = AveragingModels(models = (lasso,KRR))    \n",
    "averaged_models = AveragingModels(models = (svr,KRR2,model_xgb))\n",
    "\n",
    "score = rmsle_cv(averaged_models)\n",
    "print(\" 对基模型集成后的得分: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
