{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "任务 4\n",
    "------------\n",
    "\n",
    "根据先前的`2_fullyconnected.ipynb` 和 `3_regularization.ipynb`，我们训练了[notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html)字母分类的全连接网络\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb` and `3_regularization.ipynb`, we trained fully connected networks to classify [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) characters.\n",
    "\n",
    "这次任务的目标是创建一个卷积神经网络\n",
    "\n",
    "The goal of this assignment is make the neural network convolutional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11948,
     "status": "ok",
     "timestamp": 1446658914837,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "016b1a51-0290-4b08-efdb-8c95ffc3cd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "重新格式化TensorFlow-friendly shape\n",
    "- 卷积需要立方体格式的图像数据\n",
    "- 标签是独热编码\n",
    "\n",
    "Reformat into a TensorFlow-friendly shape:\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11952,
     "status": "ok",
     "timestamp": 1446658914857,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集 (200000, 28, 28) (200000,)\n",
      "验证集 (10000, 28, 28) (10000,)\n",
      "测试集 (10000, 28, 28) (10000,)\n",
      "训练集 (200000, 28, 28, 1) (200000, 10)\n",
      "验证集 (10000, 28, 28, 1) (10000, 10)\n",
      "测试集 (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "print('训练集', train_dataset.shape, train_labels.shape)\n",
    "print('验证集', valid_dataset.shape, valid_labels.shape)\n",
    "print('测试集', test_dataset.shape, test_labels.shape)\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('训练集', train_dataset.shape, train_labels.shape)\n",
    "print('验证集', valid_dataset.shape, valid_labels.shape)\n",
    "print('测试集', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "AgQDIREv02p1"
   },
   "outputs": [],
   "source": [
    "# 计算精准度，np.argmax是取数组里最大值，横轴 100*sum(训练结果=测试结果的数量)/总行数\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conv模型\n",
    "def model_conv(data):\n",
    "    # tf.nn.conv2d是TensorFlow里面实现卷积的函数\n",
    "    # tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, name=None)\n",
    "    # 第一个参数input：指需要做卷积的输入图像，它要求是一个Tensor，具有[batch, in_height, in_width, in_channels]这样的shape，\n",
    "    # 具体含义是[训练时一个batch的图片数量, 图片高度, 图片宽度, 图像通道数]，注意这是一个4维的Tensor，要求类型为float32和float64其中之一\n",
    "    # 第二个参数filter：相当于CNN中的卷积核，二维的滤波器矩阵，也叫权重矩阵\n",
    "    # 它要求是一个Tensor，具有[filter_height, filter_width, in_channels, out_channels]这样的shape，\n",
    "    # 具体含义是[卷积核的高度，卷积核的宽度，图像通道数，卷积核个数]，要求类型与参数input相同，\n",
    "    # 有一个地方需要注意，第三维in_channels，就是参数input的第四维\n",
    "    # 最后的out_channels是输出几个图的结果（深度）\n",
    "    # 第三个参数strides：卷积时在图像每一维的步长，这是一个一维的向量，长度4\n",
    "    # strides[0]和strides[3]的两个1是默认值，中间两个1代表padding时在x方向运动一步，y方向运动一步\n",
    "    # 第一个是批处理（batch），最后一个是卷积的深度（depth）\n",
    "    # 第四个参数padding：string类型的量，只能是\"SAME\",\"VALID\"其中之一，这个值决定了不同的卷积方式\n",
    "    # V是不可超越边界是图片尺寸-核尺寸+1/步长，S是可以超越的，核心位置可以贴变，特征结果是图片尺寸／步长\n",
    "    # 第五个参数：use_cudnn_on_gpu:bool类型，是否使用cudnn加速，默认为true\n",
    "    # 输出结果[0]是批处理（batch）[1]和[2]是图片经过过滤器后的长宽结果，[3]是卷积的深度（depth）\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    print(shape)\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    print(shape)\n",
    "    # reshape重新转成2维格式用来变成以前的格式计算\n",
    "    # 新的二维数组A的维度是[filter_height＊filter_width＊in_channels, out_channels].\n",
    "    # 其次，input数组依然为4维数组，但是维度发生了变化。\n",
    "    # input产生新的4维数组B的维度是[batch, out_height, out_width,  filter_height * filter_width * in_channels]。\n",
    "    # 最后进行乘法B＊A。\n",
    "    # 所以在设置filter时应注意filter的维度以及input的维度的设置，否则conv2d无法进行运算。\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    print(reshape)# 16, 784\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KedKkn4EutIK"
   },
   "source": [
    "---\n",
    "问题 1\n",
    "---------\n",
    "上面的卷积模型使用步幅为2的卷积来降低维数。通过池化操作取代步幅步幅2和内核大小2的卷积。\n",
    "\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# maxpool模型\n",
    "def model_maxpool(data):\n",
    "    # tf.nn.conv2d是TensorFlow里面实现卷积的函数\n",
    "    # tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, name=None)\n",
    "    # 第一个参数input：指需要做卷积的输入图像，它要求是一个Tensor，具有[batch, in_height, in_width, in_channels]这样的shape，\n",
    "    #                具体含义是[训练时一个batch的图片数量, 图片高度, 图片宽度, 图像通道数]，\n",
    "    #                注意这是一个4维的Tensor，要求类型为float32和float64其中之一\n",
    "    # 第二个参数filter：相当于CNN中的卷积核，二维的滤波器矩阵，也叫权重矩阵\n",
    "    #                 要求是一个Tensor，具有[filter_height, filter_width, in_channels, out_channels]这样的shape，\n",
    "    #                 具体含义是[卷积核的高度，卷积核的宽度，图像通道数，卷积核个数]，要求类型与参数input相同，\n",
    "    #                 有一个地方需要注意，第三维in_channels，就是参数input的第四维\n",
    "    #                 最后的out_channels是输出几个图的结果（深度）\n",
    "    # 第三个参数strides：卷积时在图像每一维的步长，这是一个一维的向量，长度4\n",
    "    #                  strides[0]和strides[3]的两个1是默认值，中间两个1代表padding时在x方向运动一步，y方向运动一步\n",
    "    #                  第一个是批处理（batch），最后一个是卷积的深度（depth）\n",
    "    # 第四个参数padding：string类型的量，只能是\"SAME\",\"VALID\"其中之一，这个值决定了不同的卷积方式\n",
    "    #                  V是不可超越边界是图片尺寸-核尺寸+1/步长，S是可以超越的，核心位置可以贴变，特征结果是图片尺寸／步长\n",
    "    # 第五个参数：use_cudnn_on_gpu:bool类型，是否使用cudnn加速，默认为true\n",
    "    #           输出结果[0]是批处理（batch）[1]和[2]是图片经过过滤器后的长宽结果，[3]是卷积的深度（depth）\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "#     print(shape)\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    # 池化操作 参数：\n",
    "    # value：需要池化的输入。\n",
    "    #       一般池化层接在卷积层后面，所以输入通常是feature map，依然是[batch, height, width, channels]这样的shape.\n",
    "    # ksize：池化窗口的大小。\n",
    "    #       取一个四维向量，一般是[1, height, width, 1]，因为我们不想在batch和channels上做池化，所以这两个维度设为了1.\n",
    "    # strides：和卷积类似，窗口在每一个维度上滑动的步长，一般也是[1, stride,stride, 1]\n",
    "    # padding：和卷积类似，可以取'VALID' 或者'SAME'\n",
    "    # data_format：字符串. 目前支持 'NHWC' 和 'NCHW'.\n",
    "    conv = tf.nn.max_pool(conv, [1,2,2,1], [1,2,2,1], padding='SAME') # 逐步改变尺寸\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "#     print(shape)\n",
    "    # reshape重新转成2维格式用来变成以前的格式计算\n",
    "    # 新的二维数组A的维度是[filter_height＊filter_width＊in_channels, out_channels].\n",
    "    # 其次，input数组依然为4维数组，但是维度发生了变化。\n",
    "    # input产生新的4维数组B的维度是[batch, out_height, out_width,  filter_height * filter_width * in_channels]。\n",
    "    # 最后进行乘法B＊A。\n",
    "    # 所以在设置filter时应注意filter的维度以及input的维度的设置，否则conv2d无法进行运算。\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    print(\"hidden.shape\", shape[0] , shape[1] , shape[2] , shape[3])\n",
    "    print(\"reshape.shape\", reshape.shape)\n",
    "    print(\"layer3_weights.shape\", layer3_weights.shape)\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(data):  \n",
    "        # data (batch, 28, 28, 1)          \n",
    "        # weights reshaped to (patch_size*patch_size*num_channels, depth)  \n",
    "        # data reshaped to (batch, 14, 14,  patch_size*patch_size*num_channels)  \n",
    "        # conv shape (batch, 14, 14, depth)  \n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME') # convolution  \n",
    "        hidden = tf.nn.relu(conv + layer1_biases)  \n",
    "        # weights shape (patch_size, patch_size, depth, depth)  \n",
    "        # weights reshaped into (patch_size*patch_size* depth, depth)  \n",
    "        # hidden reshaped into (batch, 7, 7, patch_size*patch_size* depth)  \n",
    "        # conv shape (batch, 7, 7, depth)  \n",
    "        conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME') # convolution  \n",
    "        # conv shape (batch, 7, 7, depth)  \n",
    "        #print('conv1 shape', conv.get_shape().as_list())  \n",
    "        conv = tf.nn.max_pool(conv, [1,2,2,1], [1,2,2,1], padding='SAME') # strides change dimensions  \n",
    "        #print('conv2 shape', conv.get_shape().as_list())  \n",
    "        hidden = tf.nn.relu(conv + layer2_biases)  \n",
    "        #  hidden shape (batch, 4, 4, depth)  \n",
    "         \n",
    "        shape = hidden.get_shape().as_list()  \n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])   \n",
    "        # reshape (batch,4*4*depth)  \n",
    "        # weights shape( 4 * 4*depth, num_hidden)  \n",
    "        # hidden shape(batch, num_hidden)  \n",
    "        #print('reshape shape', reshape.get_shape().as_list())  \n",
    "        #print('layer3_weights', layer3_weights.get_shape().as_list())  \n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)   \n",
    "        #  return tensor  (batch, num_labels)  \n",
    "        return tf.matmul(hidden, layer4_weights) + layer4_biases  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rhgjmROXu2O"
   },
   "source": [
    "让我们用两个卷积层建设一个小型网络，跟在一个全连接层之后。卷积网络是更加昂贵的计算，所以我们限制全连接节点的深度和数量\n",
    "\n",
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "IZYv70SvvOan"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_train_dataset.shape= (16, 28, 28, 1)\n",
      "tf_valid_dataset.shape= (10000, 28, 28, 1)\n",
      "hidden.shape 16 4 4 16\n",
      "reshape.shape (16, 256)\n",
      "layer3_weights.shape (256, 64)\n",
      "hidden.shape 10000 4 4 16\n",
      "reshape.shape (10000, 256)\n",
      "layer3_weights.shape (256, 64)\n",
      "hidden.shape 10000 4 4 16\n",
      "reshape.shape (10000, 256)\n",
      "layer3_weights.shape (256, 64)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # 输入数据\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    print(\"tf_train_dataset.shape=\",tf_train_dataset.shape)\n",
    "    print(\"tf_valid_dataset.shape=\",tf_valid_dataset.shape)\n",
    "    \n",
    "\n",
    "    # 变量，在这里是过滤器用\n",
    "    # truncated_normal按照正态分布初始化权重\n",
    "    # mean是正态分布的平均值\n",
    "    # stddev是正态分布的标准差（standard deviation）\n",
    "    # seed是作为分布的random seed（随机种子，我百度了一下，跟什么伪随机数发生器还有关，就是产生随机数的）\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal([image_size // 7 * image_size // 7 * depth, num_hidden], stddev=0.1))\n",
    "\n",
    "#     print(layer3_weights.shape)\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "\n",
    "\n",
    "    # 训练计算\n",
    "    # 损失函数（loss function）是用来估量你模型的预测值f(x)与真实值Y的不一致程度\n",
    "    # 它是一个非负实值函数，通常使用L(Y, f(x))来表示，损失函数越小，模型的可能指就越好。\n",
    "    logits = model_maxpool(tf_train_dataset)\n",
    "    \n",
    "#     print(logits.get_shape())# (16, 10)\n",
    "#     print(tf_train_labels.get_shape()) # (16, 10)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "    # 优化器\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "    # 对训练，验证和测试数据集进行预测\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model_maxpool(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model_maxpool(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klf21gpbAgb-"
   },
   "source": [
    "---\n",
    "问题 2\n",
    "---------\n",
    "试着用卷积网来获得最好的性能。以经典的[LeNet5](http://yann.lecun.com/exdb/lenet/)架构为例，添加Dropout和/或添加学习速率衰减。\n",
    "\n",
    "Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "IZYv70SvvOan"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 14, 14, 16]\n",
      "[16, 4, 4, 16]\n",
      "Tensor(\"Reshape:0\", shape=(16, 256), dtype=float32)\n",
      "(16, 10)\n",
      "(16, 10)\n",
      "[10000, 14, 14, 16]\n",
      "[10000, 4, 4, 16]\n",
      "Tensor(\"Reshape_4:0\", shape=(10000, 256), dtype=float32)\n",
      "[10000, 14, 14, 16]\n",
      "[10000, 4, 4, 16]\n",
      "Tensor(\"Reshape_5:0\", shape=(10000, 256), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # 输入数据\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # 变量，在这里是过滤器用\n",
    "    \n",
    "    # 初始的学习速率\n",
    "    starter_learning_rate = 0.1 \n",
    "    # 全局的step，与 decay_step 和 decay_rate一起决定了 learning rate的变化\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    # 衰减速度\n",
    "    decay_steps = 100\n",
    "    # 衰减系数\n",
    "    decay_rate = 0.5\n",
    "    # 如果staircase=True，那就表明每decay_steps次计算学习速率变化，更新原始学习速率.\n",
    "    # 如果是False，那就是每一步都更新学习速率\n",
    "    staircase = True\n",
    "    # 指数衰减:法通过这个函数，可以先使用较大的学习率来快速得到一个比较优的解，然后随着迭代的继续逐步减小学习率，使得模型在训练后期更加稳定\n",
    "    # 87.7% 仅仅指数衰减\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate,global_step,decay_steps,decay_rate,staircase)\n",
    "    \n",
    "    # truncated_normal按照正态分布初始化权重\n",
    "    # mean是正态分布的平均值\n",
    "    # stddev是正态分布的标准差（standard deviation）\n",
    "    # seed是作为分布的random seed（随机种子，我百度了一下，跟什么伪随机数发生器还有关，就是产生随机数的）\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal([image_size // 7 * image_size // 7 * depth, num_hidden], stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "\n",
    "\n",
    "    # 训练计算\n",
    "    # 损失函数（loss function）是用来估量你模型的预测值f(x)与真实值Y的不一致程度\n",
    "    # 它是一个非负实值函数，通常使用L(Y, f(x))来表示，损失函数越小，模型的可能指就越好。\n",
    "    logits = model_maxpool(tf_train_dataset)\n",
    "    \n",
    "    print(logits.get_shape())# (16, 10)\n",
    "    print(tf_train_labels.get_shape()) # (16, 10)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "    # 优化器\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # 对训练，验证和测试数据集进行预测\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model_maxpool1(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model_maxpool1(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "tf_train_dataset Tensor(\"Placeholder:0\", shape=(16, 28, 28, 1), dtype=float32)\n",
      "tf_train_labels Tensor(\"Placeholder_1:0\", shape=(16, 10), dtype=float32)\n",
      "batch_labels.shape (16, 10)\n",
      "Minibatch loss at step 0: 3.771455\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 10.0%\n",
      "Test accuracy: 10.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        print(\"tf_train_dataset\",tf_train_dataset)\n",
    "        print(\"tf_train_labels\",tf_train_labels)\n",
    "        \n",
    "        print(\"batch_labels.shape\",batch_labels.shape)\n",
    "        \n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 50 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
