{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow deep NN\n",
    "#### 用MNIST数据和TensorFlow包来深度学习的高级教程 \n",
    "#### A high-level tutorial into Deep Learning using MNIST data and TensorFlow library.\n",
    "by [@kakauandme](https://twitter.com/KaKaUandME) and [@thekoshkina](https://twitter.com/thekoshkina)\n",
    "\n",
    "Accuracy: 0.99\n",
    "\n",
    "**必备知识** 基础编码技巧，一点线性代数，特别是矩阵操作，可能也要明白如何图片是如何存储到计算机内存。和开始机器学习一样，我们建议学习吴恩达的课程\n",
    "\n",
    "**Prerequisites:** fundamental coding skills, a bit of linear algebra, especially matrix operations and perhaps understanding how images are stored in computer memory. To start with machine learning, we suggest [coursera course](https://www.coursera.org/learn/machine-learning) by Andrew Ng.\n",
    "\n",
    "笔记:\n",
    "Note: \n",
    "通过随意分叉和调整常量，来调整网络行为和探索如何去改变算法表现和精准度。此外用TensorFlow 图部分也能调整学习目标\n",
    "\n",
    "*Feel free to fork and adjust* CONSTANTS *to tweak network behaviour and explore how it changes algorithm performance and accuracy. Besides **TensorFlow graph** section can also be modified for learning purposes.*\n",
    "\n",
    "非常推荐输出每个你不是100%了解的变量。也能使用你的本地环境让她可视化和调试\n",
    "\n",
    "*It is highly recommended printing every variable that isn’t 100% clear for you. Also, [tensorboard](https://www.tensorflow.org/versions/master/how_tos/summaries_and_tensorboard/index.html) can be used on a local environment for visualisation and debugging.*\n",
    "\n",
    "## 包和设置 Libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# settings\n",
    "LEARNING_RATE = 1e-4\n",
    "# set to 20000 on local environment to get 0.99 accuracy\n",
    "TRAINING_ITERATIONS = 25       \n",
    "    \n",
    "DROPOUT = 0.5\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "# set to 0 to train on all available data\n",
    "VALIDATION_SIZE = 2000\n",
    "\n",
    "# image number to output\n",
    "IMAGE_TO_DISPLAY = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理 Data preparation\n",
    "开始，我们读取数据train.csv有42000行和785列。每一行代表一个手写的数字图片和一个标签用于表示这个图片是几的数字\n",
    "\n",
    "To start, we read provided data. The *train.csv* file contains 42000 rows and 785 columns. Each row represents an image of a handwritten digit and a label with the value of this digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data(42000,785)\n",
      "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
      "0      1       0       0       0       0       0       0       0       0   \n",
      "1      0       0       0       0       0       0       0       0       0   \n",
      "2      1       0       0       0       0       0       0       0       0   \n",
      "3      4       0       0       0       0       0       0       0       0   \n",
      "4      0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
      "0       0    ...            0         0         0         0         0   \n",
      "1       0    ...            0         0         0         0         0   \n",
      "2       0    ...            0         0         0         0         0   \n",
      "3       0    ...            0         0         0         0         0   \n",
      "4       0    ...            0         0         0         0         0   \n",
      "\n",
      "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
      "0         0         0         0         0         0  \n",
      "1         0         0         0         0         0  \n",
      "2         0         0         0         0         0  \n",
      "3         0         0         0         0         0  \n",
      "4         0         0         0         0         0  \n",
      "\n",
      "[5 rows x 785 columns]\n"
     ]
    }
   ],
   "source": [
    "# read training data from CSV file \n",
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "print('data({0[0]},{0[1]})'.format(data.shape))\n",
    "print (data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个图片是一个拉伸的数组的像素值\n",
    "\n",
    "Every image is a \"stretched\" array of pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images(42000,784)\n"
     ]
    }
   ],
   "source": [
    "images = data.iloc[:,1:].values\n",
    "images = images.astype(np.float)\n",
    "\n",
    "# 数据乘以1/255，0-255->0-1\n",
    "# convert from [0:255] => [0.0:1.0]\n",
    "images = np.multiply(images, 1.0 / 255.0)\n",
    "\n",
    "print('images({0[0]},{0[1]})'.format(images.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "吧784像素值转求出28*28像素\n",
    "\n",
    "In this case it's 784 pixels => 28 * 28px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_size => 784\n",
      "image_width => 28\n",
      "image_height => 28\n"
     ]
    }
   ],
   "source": [
    "image_size = images.shape[1]\n",
    "print ('image_size => {0}'.format(image_size))\n",
    "\n",
    "# in this case all images are square\n",
    "image_width = image_height = np.ceil(np.sqrt(image_size)).astype(np.uint8)\n",
    "\n",
    "print ('image_width => {0}\\nimage_height => {1}'.format(image_width,image_height))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出一个图片，我们改造像素值向量变成二维数组，这基本是一个灰度图像\n",
    "\n",
    "To output one of the images, we reshape this long string of pixels into a 2-dimensional array, which is basically a grayscale image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112d54a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 显示图片\n",
    "def display(img):\n",
    "    \n",
    "    # (784) => (28,28)\n",
    "    one_image = img.reshape(image_width,image_height)\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.imshow(one_image, cmap=cm.binary)\n",
    "\n",
    "# 输出图像，参数是第几行数据     \n",
    "display(images[IMAGE_TO_DISPLAY])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相应的标签是0-9，描述每个图像是哪个数字\n",
    "\n",
    "The corresponding labels are numbers between 0 and 9, describing which digit a given image is of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_flat(42000)\n",
      "labels_flat[10] => 8\n"
     ]
    }
   ],
   "source": [
    "# print(data)\n",
    "\n",
    "# labels_flat = data[[0]].values.ravel()\n",
    "# 获取标签列\n",
    "labels_flat = data.iloc[:,0].values\n",
    "# 打印标签列长度\n",
    "print('labels_flat({0})'.format(len(labels_flat)))\n",
    "# 打印第X行的图片对应的标签\n",
    "print ('labels_flat[{0}] => {1}'.format(IMAGE_TO_DISPLAY,labels_flat[IMAGE_TO_DISPLAY]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个案例里，有10个不同的数字，标签，分类\n",
    "\n",
    "In this case, there are ten different digits/labels/classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_count => 10\n"
     ]
    }
   ],
   "source": [
    "# unique是返回数组的唯一元素，然后获取形状位置0\n",
    "labels_count = np.unique(labels_flat).shape[0]\n",
    "# 打印总数\n",
    "print('labels_count => {0}'.format(labels_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大多数分类程序使用独热向量（独热编码）。独热向量是一个包含1个1和剩余都是0的向量.在这个案例，数字几就是第几个是1的独热向量。\n",
    "\n",
    "For most classification problems \"one-hot vectors\" are used. A one-hot vector is a vector that contains a single element equal to 1 and the rest of the elements equal to 0. In this case, the *nth* digit is represented as a zero vector with 1 in the *nth* position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels(42000,10)\n",
      "labels[10] => [0 0 0 0 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# 转换类标签标量到独热编码\n",
    "# convert class labels from scalars to one-hot vectors\n",
    "# 0 => [1 0 0 0 0 0 0 0 0 0]\n",
    "# 1 => [0 1 0 0 0 0 0 0 0 0]\n",
    "# ...\n",
    "# 9 => [0 0 0 0 0 0 0 0 0 1]\n",
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "#    labels_dense = [1 0 1 ..., 7 6 9]\n",
    "#    num_classes = 10\n",
    "    num_labels = labels_dense.shape[0]\n",
    "#    num_labels = 42000\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "#    index_offset = [     0     10     20 ..., 419970 419980 419990]\n",
    "#    index_offset.shape = (42000,)\n",
    "#    创建一个全0数组\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "#     print(labels_one_hot)\n",
    "#    labels_one_hot.shape = (42000,10)\n",
    "#    flat是一个循环迭代器,通过乘以10，来给每一行的数字打上one hot的1\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "#     print(labels_one_hot.flat[22])\n",
    "    return labels_one_hot\n",
    "# print(labels_one_hot.flat[21])\n",
    "labels = dense_to_one_hot(labels_flat, labels_count)\n",
    "labels = labels.astype(np.uint8)\n",
    "\n",
    "print('labels({0[0]},{0[1]})'.format(labels.shape))\n",
    "print ('labels[{0}] => {1}'.format(IMAGE_TO_DISPLAY,labels[IMAGE_TO_DISPLAY]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后我们留出部分数据作为验证.在机器学习中，必须有一个独立的数据集，它不参与训练，用来确保我们训练的数据可以验证。\n",
    "\n",
    "Lastly we set aside data for validation. It's essential in machine learning to have a separate dataset which doesn't take part in the training and is used to make sure that what we've learned can actually be generalised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images(40000,784)\n",
      "validation_images(2000,784)\n"
     ]
    }
   ],
   "source": [
    "# 分割数据，分成验证集和训练集\n",
    "validation_images = images[:VALIDATION_SIZE]\n",
    "validation_labels = labels[:VALIDATION_SIZE]\n",
    "\n",
    "train_images = images[VALIDATION_SIZE:]\n",
    "train_labels = labels[VALIDATION_SIZE:]\n",
    "\n",
    "\n",
    "print('train_images({0[0]},{0[1]})'.format(train_images.shape))\n",
    "print('validation_images({0[0]},{0[1]})'.format(validation_images.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_images(28000,784)\n",
      "predicted_lables(28000)\n",
      "predicted_lables[10] => 3.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10751f358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read test data from CSV file \n",
    "test_images = pd.read_csv('test.csv').values\n",
    "test_images = test_images.astype(np.float)\n",
    "\n",
    "# convert from [0:255] => [0.0:1.0]\n",
    "test_images = np.multiply(test_images, 1.0 / 255.0)\n",
    "\n",
    "print('test_images({0[0]},{0[1]})'.format(test_images.shape))\n",
    "\n",
    "\n",
    "# predict test set\n",
    "#predicted_lables = predict.eval(feed_dict={x: test_images, keep_prob: 1.0})\n",
    "\n",
    "# using batches is more resource efficient\n",
    "predicted_lables = np.zeros(test_images.shape[0])\n",
    "for i in range(0,test_images.shape[0]//BATCH_SIZE):\n",
    "    predicted_lables[i*BATCH_SIZE : (i+1)*BATCH_SIZE] = predict.eval(feed_dict={x: test_images[i*BATCH_SIZE : (i+1)*BATCH_SIZE], \n",
    "                                                                                keep_prob: 1.0})\n",
    "\n",
    "\n",
    "print('predicted_lables({0})'.format(len(predicted_lables)))\n",
    "\n",
    "# output test image and prediction\n",
    "display(test_images[IMAGE_TO_DISPLAY])\n",
    "print ('predicted_lables[{0}] => {1}'.format(IMAGE_TO_DISPLAY,predicted_lables[IMAGE_TO_DISPLAY]))\n",
    "\n",
    "# save results\n",
    "np.savetxt('submission_softmax.csv', \n",
    "           np.c_[range(1,len(test_images)+1),predicted_lables], \n",
    "           delimiter=',', \n",
    "           header = 'ImageId,Label', \n",
    "           comments = '', \n",
    "           fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算精准度\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 保存结果\n",
    "def savecsv(test_prediction_np):\n",
    "    np.savetxt('submission_uda_dr.csv', \n",
    "           np.c_[range(1,len(test_images)+1),test_prediction_np], \n",
    "           delimiter=',', \n",
    "           header = 'ImageId,Label', \n",
    "           comments = '', \n",
    "           fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_size=784\n",
      "Tensor(\"Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 0.90029\n",
    "batch_size = 128  \n",
    "regularation_param = 0.0001  \n",
    "  \n",
    "graph = tf.Graph()  \n",
    "with graph.as_default():\n",
    "    # -----------------------------------------1\n",
    "    # 输入 \n",
    "    # placeholder 插入一个待初始化的张量占位符\n",
    "    # 重要事项：这个张量被求值时会产生错误。 \n",
    "    # 它的值必须在Session.run(), Tensor.eval() 或 Operation.run() 中使用feed_dict的这个可选参数来填充。\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, labels_count))\n",
    "    # 创建一个常量张量\n",
    "    # tf_valid_dataset = Tensor(\"Const:0\", shape=(10000, 784), dtype=float32)\n",
    "\n",
    "    \n",
    "    tf_valid_dataset = tf.constant(validation_images)\n",
    "    tf_test_dataset = tf.constant(test_images)\n",
    "    # ------------------------------------------2\n",
    "    # 变量\n",
    "    # 当你训练一个模型的时候，你使用变量去保存和更新参数。\n",
    "    # 在Tensorflow中变量是内存缓冲区中保存的张量（tensor）。\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size, labels_count]))\n",
    "    biases = tf.Variable(tf.zeros([labels_count]))\n",
    "    print(\"image_size=%d\" % image_size)\n",
    "    # -------------------------------------------3\n",
    "    # 训练计算.\n",
    "\n",
    "    \n",
    "    # 转换数据类型\n",
    "    tf_valid_dataset = tf.to_float(tf_valid_dataset)\n",
    "    tf_test_dataset = tf.to_float(tf_test_dataset)\n",
    "    \n",
    "    train_logits = tf.matmul(tf_train_dataset, weights) +  biases\n",
    "    \n",
    "#     print(type(tf_train_dataset))\n",
    "#     print(type(tf_valid_dataset))\n",
    "#     print(type(tf_test_dataset))\n",
    "#     print(tf_train_dataset.dtype)\n",
    "#     print(tf_valid_dataset.dtype)\n",
    "#     print(tf_test_dataset.dtype)\n",
    "    \n",
    "#     print(tf_valid_dataset.shape)\n",
    "#     print(weights.shape)\n",
    "    \n",
    "    \n",
    "    valid_logits = tf.matmul(tf_valid_dataset, weights) + biases\n",
    "    test_logits = tf.matmul(tf_test_dataset, weights) +  biases\n",
    "    # reduce_mean可跨越维度的计算张量各元素的平均值\n",
    "    # 计算loss是代价值，也就是我们要最小化的值\n",
    "    # 第一个参数logits：就是神经网络最后一层的输出，\n",
    "    # 如果有batch的话，它的大小就是[batchsize，num_classes]，单样本的话，大小就是num_classes\n",
    "    # 第二个参数labels：实际的标签，大小同上\n",
    "    # 第一步是先对网络最后一层的输出做一个softmax，这一步通常是求取输出属于某一类的概率\n",
    "    # 第二步是softmax的输出向量[Y1，Y2,Y3...]和样本的实际标签做一个交叉熵\n",
    "    # 注意！！！这个函数的返回值并不是一个数，而是一个向量.\n",
    "    # 如果要求交叉熵，我们要再做一步tf.reduce_sum操作,就是对向量里面所有元素求和，最后才得到. \n",
    "    # 如果求loss，则要做一步tf.reduce_mean操作，对向量求均值！\n",
    "    \n",
    "    # 如果直接将l2_loss加到train_loss上，每次的train_loss都特别大，几乎只取决于l2_loss\n",
    "    # 为了让原本的train_loss与l2_loss都能较好地对参数调整方向起作用，它们应当至少在同一个量级\n",
    "    # 这里还有一个重要的点，Hyper Parameter: β\n",
    "    # 我觉得这是一个拍脑袋参数，取什么值都行，但效果会不同，我这里解释一下我取β=0.001的理由\n",
    "    # 为了让原本的train_loss与l2_loss都能较好地对参数调整方向起作用，它们应当至少在同一个量级\n",
    "    # 观察不加l2_loss，step 0 时，train_loss在300左右\n",
    "    # 加l2_loss后， step 0 时，train_loss在300000左右\n",
    "    # 因此给l2_loss乘0.0001使之降到同一个量级\n",
    "    # 原始 loss=18\n",
    "    # loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=train_logits, labels=tf_train_labels))\n",
    "    # 原始L2后loss=58000\n",
    "    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=train_logits)) * tf.nn.l2_loss(weights)  \n",
    "    # 使用Hyper Parameter 0.0001 相乘来降低L2的影响，使用后loss=16\n",
    "    hpl2 = regularation_param * tf.nn.l2_loss(weights)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=train_logits))\n",
    "    print(loss)\n",
    "    loss = tf.add(loss, hpl2)\n",
    "\n",
    "    \n",
    "    #loss = tf.add(loss,\n",
    "    # -------------------------------------------4\n",
    "    # 最优化.因为深度学习常见的是对于梯度的优化，也就是说，优化器最后其实就是各种对于梯度下降算法的优化。 \n",
    "    # 函数training()通过梯度下降法为最小化损失函数增加了相关的优化操作，在训练过程中，\n",
    "    # 先实例化一个优化函数，比如 tf.train.GradientDescentOptimizer，并基于一定的学习率进行梯度优化训练：\n",
    "    # learning_rate参数：要使用的学习率 \n",
    "    # minimize：非常常用的一个函数 通过更新var_list来减小loss，这个函数就是前面compute_gradients() 和apply_gradients().的结合\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    \n",
    "    #--------------------------------------------5\n",
    "    # 预测训练, 验证,和测试数据.\n",
    "    # tf.nn.softmax仅产生将softmax function应用于输入张量的结果。 \n",
    "    # softmax 压扁输入，使sum(输入)= 1;这是一种正常化的方式。 \n",
    "    # softmax的输出形状与输入相同 – 它只是对值进行归一化。 \n",
    "    # softmax的输出可以解释为概率。\n",
    "    # a = tf.constant(np.array([[.1, .3, .5, .9]]))\n",
    "    # print s.run(tf.nn.softmax(a))\n",
    "    # [[ 0.16838508  0.205666    0.25120102  0.37474789]]\n",
    "    train_prediction = tf.nn.softmax(train_logits)\n",
    "    valid_prediction = tf.nn.softmax(valid_logits)\n",
    "    test_prediction = tf.nn.softmax(test_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 0.94586\n",
    "batch_size = 128  \n",
    "hidden_nodes = 1024\n",
    "regularation_param = 0.0001  \n",
    "keep_prob = 0.8\n",
    "  \n",
    "graph = tf.Graph()  \n",
    "with graph.as_default():\n",
    "    # -----------------------------------------1\n",
    "    # 输入 \n",
    "    # placeholder 插入一个待初始化的张量占位符\n",
    "    # 重要事项：这个张量被求值时会产生错误。 \n",
    "    # 它的值必须在Session.run(), Tensor.eval() 或 Operation.run() 中使用feed_dict的这个可选参数来填充。\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, labels_count))\n",
    "    # 创建一个常量张量\n",
    "    # tf_valid_dataset = Tensor(\"Const:0\", shape=(10000, 784), dtype=float32)\n",
    "\n",
    "    \n",
    "    tf_valid_dataset = tf.constant(validation_images)\n",
    "    tf_test_dataset = tf.constant(test_images)\n",
    "    # ------------------------------------------2\n",
    "    # 变量\n",
    "    # 当你训练一个模型的时候，你使用变量去保存和更新参数。\n",
    "    # 在Tensorflow中变量是内存缓冲区中保存的张量（tensor）。\n",
    "    #weights = tf.Variable(tf.truncated_normal([image_size, labels_count]))\n",
    "    #biases = tf.Variable(tf.zeros([labels_count]))\n",
    "    \n",
    "    \n",
    "    # 第一层\n",
    "    # truncated_normal 从一个正态分布片段中输出随机数值,\n",
    "    # 生成的值会遵循一个指定了平均值和标准差的正态分布，只保留两个标准差以内的值，超出的值会被弃掉重新生成。\n",
    "    # 返回 一个指定形状并用正态分布片段的随机值填充的张量\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size, hidden_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "    \n",
    "    # 第二层\n",
    "    weights2 = tf.Variable(tf.truncated_normal([hidden_nodes, labels_count]))\n",
    "    biases2 = tf.Variable(tf.zeros([labels_count]))\n",
    "    \n",
    "    \n",
    "    #print(\"image_size=%d\" % image_size)\n",
    "    # -------------------------------------------3\n",
    "    # 训练计算.\n",
    "\n",
    "    \n",
    "    # 转换数据类型\n",
    "    tf_valid_dataset = tf.to_float(tf_valid_dataset)\n",
    "    tf_test_dataset = tf.to_float(tf_test_dataset)\n",
    "    \n",
    "    #使用dropout\n",
    "    drop = tf.nn.dropout(tf_train_dataset, keep_prob=keep_prob)\n",
    "    \n",
    "    # train_logits = tf.matmul(tf_train_dataset, weights) +  biases\n",
    "    # valid_logits = tf.matmul(tf_valid_dataset, weights) + biases\n",
    "    # test_logits = tf.matmul(tf_test_dataset, weights) +  biases\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_logits = tf.add(tf.matmul(drop, weights1), biases1)\n",
    "    train_logits = tf.nn.relu(train_logits)\n",
    "    train_logits = tf.add(tf.matmul(train_logits,weights2),biases2)\n",
    "\n",
    "    valid_logits = tf.add(tf.matmul(tf_valid_dataset, weights1),biases1)\n",
    "    valid_logits = tf.nn.relu(valid_logits)\n",
    "    valid_logits = tf.add(tf.matmul(valid_logits,weights2),biases2)\n",
    "\n",
    "    test_logits = tf.add(tf.matmul(tf_test_dataset, weights1),biases1)\n",
    "    test_logits = tf.nn.relu(test_logits)\n",
    "    test_logits = tf.add(tf.matmul(test_logits,weights2),biases2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # reduce_mean可跨越维度的计算张量各元素的平均值\n",
    "    # 计算loss是代价值，也就是我们要最小化的值\n",
    "    # 第一个参数logits：就是神经网络最后一层的输出，\n",
    "    # 如果有batch的话，它的大小就是[batchsize，num_classes]，单样本的话，大小就是num_classes\n",
    "    # 第二个参数labels：实际的标签，大小同上\n",
    "    # 第一步是先对网络最后一层的输出做一个softmax，这一步通常是求取输出属于某一类的概率\n",
    "    # 第二步是softmax的输出向量[Y1，Y2,Y3...]和样本的实际标签做一个交叉熵\n",
    "    # 注意！！！这个函数的返回值并不是一个数，而是一个向量.\n",
    "    # 如果要求交叉熵，我们要再做一步tf.reduce_sum操作,就是对向量里面所有元素求和，最后才得到. \n",
    "    # 如果求loss，则要做一步tf.reduce_mean操作，对向量求均值！\n",
    "    \n",
    "    # 如果直接将l2_loss加到train_loss上，每次的train_loss都特别大，几乎只取决于l2_loss\n",
    "    # 为了让原本的train_loss与l2_loss都能较好地对参数调整方向起作用，它们应当至少在同一个量级\n",
    "    # 这里还有一个重要的点，Hyper Parameter: β\n",
    "    # 我觉得这是一个拍脑袋参数，取什么值都行，但效果会不同，我这里解释一下我取β=0.001的理由\n",
    "    # 为了让原本的train_loss与l2_loss都能较好地对参数调整方向起作用，它们应当至少在同一个量级\n",
    "    # 观察不加l2_loss，step 0 时，train_loss在300左右\n",
    "    # 加l2_loss后， step 0 时，train_loss在300000左右\n",
    "    # 因此给l2_loss乘0.0001使之降到同一个量级\n",
    "    # 原始 loss=18\n",
    "    # loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=train_logits, labels=tf_train_labels))\n",
    "    # 原始L2后loss=58000\n",
    "    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=train_logits)) * tf.nn.l2_loss(weights)  \n",
    "    # 使用Hyper Parameter 0.0001 相乘来降低L2的影响，使用后loss=16\n",
    "    hpl2 = regularation_param * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=train_logits))\n",
    "    loss = tf.add(loss , hpl2)\n",
    "    \n",
    "\n",
    "    \n",
    "    #loss = tf.add(loss,\n",
    "    # -------------------------------------------4\n",
    "    # 最优化.因为深度学习常见的是对于梯度的优化，也就是说，优化器最后其实就是各种对于梯度下降算法的优化。 \n",
    "    # 函数training()通过梯度下降法为最小化损失函数增加了相关的优化操作，在训练过程中，\n",
    "    # 先实例化一个优化函数，比如 tf.train.GradientDescentOptimizer，并基于一定的学习率进行梯度优化训练：\n",
    "    # learning_rate参数：要使用的学习率 \n",
    "    # minimize：非常常用的一个函数 通过更新var_list来减小loss，这个函数就是前面compute_gradients() 和apply_gradients().的结合\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    \n",
    "    #--------------------------------------------5\n",
    "    # 预测训练, 验证,和测试数据.\n",
    "    # tf.nn.softmax仅产生将softmax function应用于输入张量的结果。 \n",
    "    # softmax 压扁输入，使sum(输入)= 1;这是一种正常化的方式。 \n",
    "    # softmax的输出形状与输入相同 – 它只是对值进行归一化。 \n",
    "    # softmax的输出可以解释为概率。\n",
    "    # a = tf.constant(np.array([[.1, .3, .5, .9]]))\n",
    "    # print s.run(tf.nn.softmax(a))\n",
    "    # [[ 0.16838508  0.205666    0.25120102  0.37474789]]\n",
    "    train_prediction = tf.nn.softmax(train_logits)\n",
    "    valid_prediction = tf.nn.softmax(valid_logits)\n",
    "    test_prediction = tf.nn.softmax(test_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 259.307404\n",
      "Minibatch accuracy: 14.1%\n",
      "Validation accuracy: 26.1%\n",
      "Minibatch loss at step 500: 31.825253\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 93.0%\n",
      "Minibatch loss at step 1000: 28.782307\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 93.7%\n",
      "Minibatch loss at step 1500: 27.505709\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 93.0%\n",
      "Minibatch loss at step 2000: 26.266083\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 93.9%\n",
      "Minibatch loss at step 2500: 24.772188\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 93.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001  \n",
    "print (test_images.shape[0]//batch_size)\n",
    "with tf.Session(graph=graph) as session:  \n",
    "    tf.initialize_all_variables().run()  \n",
    "    print(\"Initialized\")  \n",
    "    for step in range(num_steps):  \n",
    "        # 在训练数据中选择一个已被随机化的偏移量.\n",
    "        # 提醒: 我们能使用更好的随机化穿过所有数据.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)  \n",
    "        # 生成一个小批量数据\n",
    "        batch_data = train_images[offset:(offset + batch_size), :]  \n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]  \n",
    "        # feed_dict的作用是给使用placeholder创建出来的tensor赋值。\n",
    "        # 其实，他的作用更加广泛：feed 使用一个 值临时替换一个 op 的输出结果. \n",
    "        # 你可以提供 feed 数据作为 run() 调用的参数. feed 只在调用它的方法内有效, 方法结束, feed 就会消失.\n",
    "        #  传递值到tf的命名空间  \n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}  \n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)  \n",
    "        if (step % 500 == 0):  \n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))  \n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))  \n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), validation_labels))  \n",
    "    # 获取结果，用于保存\n",
    "    test_prediction_np = test_prediction.eval()\n",
    "    test_prediction_np = np.argmax(test_prediction_np, 1)\n",
    "    savecsv(test_prediction_np)\n",
    "    #predicted_lables = np.zeros(test_images.shape[0])\n",
    "    #for i in range(0,test_images.shape[0]//BATCH_SIZE):\n",
    "    #    predicted_lables[i*BATCH_SIZE : (i+1)*BATCH_SIZE] = test_prediction.eval(feed_dict={x: test_images[i*BATCH_SIZE : (i+1)*BATCH_SIZE], keep_prob: 1.0})\n",
    "\n",
    "    #print(test_prediction.eval().shape)\n",
    "\n",
    "\n",
    "#     predictions = np.argmax(sess.run(y_pred, feed_dict={X: X_test}), 1)  \n",
    "    # print(\"Test accuracy: %.1f%% with regularation_param = %f \" %( accuracy(test_prediction.eval(), test_labels), regularation_param)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 10)\n"
     ]
    }
   ],
   "source": [
    "# 0.98000\n",
    "batch_size = 128\n",
    "# 隐藏节点\n",
    "hidden_nodes = 2048\n",
    "# L2缩放率\n",
    "regularation_param = 0.0001  \n",
    "\n",
    "graph = tf.Graph()  \n",
    "def compute_logits(data, weightss, biasess, dropout_vals=None):  \n",
    "    temp = data  \n",
    "    if dropout_vals:  \n",
    "        for w,b,d in zip(weightss[:-1], biasess[:-1], dropout_vals[:-1]):  \n",
    "            temp = tf.nn.relu_layer(tf.nn.dropout(temp, d), w, b)  \n",
    "        temp = tf.matmul(temp, weightss[-1]) + biasess[-1]  \n",
    "    else:  \n",
    "        for w,b in zip(weightss[:-1], biasess[:-1]):\n",
    "            temp = tf.nn.relu_layer(temp, w, b)  \n",
    "        temp = tf.matmul(temp, weightss[-1]) + biasess[-1]  \n",
    "    return temp  \n",
    "with graph.as_default():\n",
    "    # -----------------------------------------1\n",
    "    # 输入 \n",
    "    # placeholder 插入一个待初始化的张量占位符\n",
    "    # 重要事项：这个张量被求值时会产生错误。 \n",
    "    # 它的值必须在Session.run(), Tensor.eval() 或 Operation.run() 中使用feed_dict的这个可选参数来填充。\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, labels_count))\n",
    "    \n",
    "\n",
    "    \n",
    "    # 创建一个常量张量\n",
    "    # tf_valid_dataset = Tensor(\"Const:0\", shape=(10000, 784), dtype=float32)\n",
    "    tf_valid_dataset = tf.constant(validation_images)\n",
    "    tf_test_dataset = tf.constant(test_images)\n",
    "    \n",
    "    # 转换数据类型\n",
    "    tf_valid_dataset = tf.to_float(tf_valid_dataset)\n",
    "    tf_test_dataset = tf.to_float(tf_test_dataset)\n",
    "    # ------------------------------------------2\n",
    "    # 变量\n",
    "    # #该函数返回以下结果\n",
    "    # decayed_learning_rate = learning_rate *\n",
    "    # decay_rate ^ (global_step / decay_steps)\n",
    "    # 初始的学习速率\n",
    "    starter_learning_rate = 0.3\n",
    "    # 全局的step，与 decay_step 和 decay_rate一起决定了 learning rate的变化\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    # 衰减速度\n",
    "    decay_steps = 200\n",
    "    # 衰减系数\n",
    "    decay_rate = 0.3\n",
    "    # 如果staircase=True，那就表明每decay_steps次计算学习速率变化，更新原始学习速率.\n",
    "    # 如果是False，那就是每一步都更新学习速率\n",
    "    staircase = True\n",
    "    # 指数衰减:法通过这个函数，可以先使用较大的学习率来快速得到一个比较优的解，然后随着迭代的继续逐步减小学习率，使得模型在训练后期更加稳定\n",
    "    # 87.7% 仅仅指数衰减\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate,global_step,decay_steps,decay_rate,staircase)\n",
    "    \n",
    "    # 当你训练一个模型的时候，你使用变量去保存和更新参数。\n",
    "    # 在Tensorflow中变量是内存缓冲区中保存的张量（tensor）。\n",
    "\n",
    "    # 第一层\n",
    "    # truncated_normal 从一个正态分布片段中输出随机数值,\n",
    "    # 生成的值会遵循一个指定了平均值和标准差的正态分布，只保留两个标准差以内的值，超出的值会被弃掉重新生成。\n",
    "    # 返回 一个指定形状并用正态分布片段的随机值填充的张量\n",
    "    # 数字平方根\n",
    "    x = 2.0\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size, hidden_nodes], stddev = np.sqrt(x / hidden_nodes)))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "    \n",
    "    # 第二层\n",
    "    weights2 = tf.Variable(tf.truncated_normal([hidden_nodes, int(hidden_nodes / 2)], stddev = np.sqrt(x / hidden_nodes / 2)))\n",
    "    biases2 = tf.Variable(tf.zeros([hidden_nodes / 2]))\n",
    "    hidden_nodes = int(hidden_nodes / 2)\n",
    "    \n",
    "    # 第三层\n",
    "    weights3 = tf.Variable(tf.truncated_normal([hidden_nodes, int(hidden_nodes / 2)], stddev = np.sqrt(x / hidden_nodes / 2)))\n",
    "    biases3 = tf.Variable(tf.zeros([hidden_nodes / 2]))\n",
    "    hidden_nodes = int(hidden_nodes / 2)\n",
    "    \n",
    "    # 第四层 94.5\n",
    "    weights4 = tf.Variable(tf.truncated_normal([hidden_nodes, int(hidden_nodes / 2)], stddev = np.sqrt(x / hidden_nodes / 2)))\n",
    "    biases4 = tf.Variable(tf.zeros([hidden_nodes / 2]))\n",
    "    hidden_nodes = int(hidden_nodes / 2)\n",
    "    \n",
    "    # 第五层 94.5\n",
    "    weights5 = tf.Variable(tf.truncated_normal([hidden_nodes, int(hidden_nodes / 2)], stddev = np.sqrt(x / hidden_nodes / 2)))\n",
    "    biases5 = tf.Variable(tf.zeros([hidden_nodes / 2]))\n",
    "    hidden_nodes = int(hidden_nodes / 2)\n",
    "    \n",
    "    # 最后一层\n",
    "    weights_end = tf.Variable(tf.truncated_normal([hidden_nodes, labels_count], stddev = np.sqrt(x / labels_count)))\n",
    "    biases_end  = tf.Variable(tf.zeros([labels_count]))\n",
    "    \n",
    "    # -------------------------------------------3\n",
    "    # 训练计算.\n",
    "    # logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    # DropDown的保留概率\n",
    "    keep_prob = 0.8\n",
    "    drop = tf.nn.dropout(tf_train_dataset, keep_prob=keep_prob)\n",
    "    \n",
    "    train_logits = compute_logits(tf_train_dataset, [weights1, weights2, weights3,weights4,weights5, weights_end], \n",
    "                                  [biases1,biases2,biases3,biases4,biases5, biases_end],  \n",
    "                            dropout_vals=(1.0,0.95,0.95,0.95,0.95,1.0))\n",
    "    \n",
    "    # train_logits = tf.add(tf.matmul(drop, weights1),biases1)\n",
    "    # train_logits = tf.nn.relu(train_logits)\n",
    "    # train_logits = tf.add(tf.matmul(train_logits,weights2),biases2)\n",
    "\n",
    "    valid_logits = compute_logits(tf_valid_dataset, [weights1, weights2, weights3,weights4,weights5, weights_end], \n",
    "                              [biases1,biases2,biases3,biases4,biases5, biases_end])\n",
    "    \n",
    "    # valid_logits = tf.add(tf.matmul(tf_valid_dataset, weights1),biases1)\n",
    "    # valid_logits = tf.nn.relu(valid_logits)\n",
    "    # valid_logits = tf.add(tf.matmul(valid_logits,weights2),biases2)\n",
    "\n",
    "    test_logits = compute_logits(tf_test_dataset, [weights1, weights2, weights3,weights4,weights5, weights_end], \n",
    "                              [biases1,biases2,biases3,biases4,biases5, biases_end])\n",
    "    \n",
    "    # test_logits = tf.add(tf.matmul(tf_test_dataset, weights1),biases1)\n",
    "    # test_logits = tf.nn.relu(test_logits)\n",
    "    # test_logits = tf.add(tf.matmul(test_logits,weights2),biases2)\n",
    "    \n",
    "    # reduce_mean可跨越维度的计算张量各元素的平均值\n",
    "    # 计算loss是代价值，也就是我们要最小化的值\n",
    "    # 第一个参数logits：就是神经网络最后一层的输出，\n",
    "    # 如果有batch的话，它的大小就是[batchsize，num_classes]，单样本的话，大小就是num_classes\n",
    "    # 第二个参数labels：实际的标签，大小同上\n",
    "    # 第一步是先对网络最后一层的输出做一个softmax，这一步通常是求取输出属于某一类的概率\n",
    "    # 第二步是softmax的输出向量[Y1，Y2,Y3...]和样本的实际标签做一个交叉熵\n",
    "    # 注意！！！这个函数的返回值并不是一个数，而是一个向量.\n",
    "    # 如果要求交叉熵，我们要再做一步tf.reduce_sum操作,就是对向量里面所有元素求和，最后才得到. \n",
    "    # 如果求loss，则要做一步tf.reduce_mean操作，对向量求均值！\n",
    "    \n",
    "    # 如果直接将l2_loss加到train_loss上，每次的train_loss都特别大，几乎只取决于l2_loss\n",
    "    # 为了让原本的train_loss与l2_loss都能较好地对参数调整方向起作用，它们应当至少在同一个量级\n",
    "    # 这里还有一个重要的点，Hyper Parameter: β\n",
    "    # 我觉得这是一个拍脑袋参数，取什么值都行，但效果会不同，我这里解释一下我取β=0.001的理由\n",
    "    # 为了让原本的train_loss与l2_loss都能较好地对参数调整方向起作用，它们应当至少在同一个量级\n",
    "    # 观察不加l2_loss，step 0 时，train_loss在300左右\n",
    "    # 加l2_loss后， step 0 时，train_loss在300000左右\n",
    "    # 因此给l2_loss乘0.0001使之降到同一个量级\n",
    "    # 原始 loss=18\n",
    "    # loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=train_logits, labels=tf_train_labels))\n",
    "    # 原始L2后loss=58000\n",
    "    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=train_logits)) * tf.nn.l2_loss(weights)  \n",
    "    # 使用Hyper Parameter 0.0001 相乘来降低L2的影响，使用后loss=16\n",
    "    #hpl2 = regularation_param * tf.nn.l2_loss(weights)\n",
    "    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=train_logits))\n",
    "    #loss = tf.add(loss, hpl2)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=train_logits)) + regularation_param * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))  \n",
    "\n",
    "    \n",
    "    #loss = tf.add(loss,\n",
    "    # -------------------------------------------4\n",
    "    # 最优化.因为深度学习常见的是对于梯度的优化，也就是说，优化器最后其实就是各种对于梯度下降算法的优化。 \n",
    "    # 函数training()通过梯度下降法为最小化损失函数增加了相关的优化操作，在训练过程中，\n",
    "    # 先实例化一个优化函数，比如 tf.train.GradientDescentOptimizer，并基于一定的学习率进行梯度优化训练：\n",
    "    # learning_rate参数：要使用的学习率 \n",
    "    # minimize：非常常用的一个函数 通过更新var_list来减小loss，这个函数就是前面compute_gradients() 和apply_gradients().的结合\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.03).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    \n",
    "    #--------------------------------------------5\n",
    "    # 预测训练, 验证,和测试数据.\n",
    "    # tf.nn.softmax仅产生将softmax function应用于输入张量的结果。 \n",
    "    # softmax 压扁输入，使sum(输入)= 1;这是一种正常化的方式。 \n",
    "    # softmax的输出形状与输入相同 – 它只是对值进行归一化。 \n",
    "    # softmax的输出可以解释为概率。\n",
    "    # a = tf.constant(np.array([[.1, .3, .5, .9]]))\n",
    "    # print s.run(tf.nn.softmax(a))\n",
    "    # [[ 0.16838508  0.205666    0.25120102  0.37474789]]\n",
    "    print(train_logits.get_shape())\n",
    "    train_prediction = tf.nn.softmax(train_logits)\n",
    "    valid_prediction = tf.nn.softmax(valid_logits)\n",
    "    test_prediction = tf.nn.softmax(test_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 2.404335\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 15.2%\n",
      "Minibatch loss at step 500: 0.325492\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 94.0%\n",
      "Minibatch loss at step 1000: 0.186428\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 96.2%\n",
      "Minibatch loss at step 1500: 0.168846\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 96.8%\n",
      "Minibatch loss at step 2000: 0.198202\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 97.5%\n",
      "Minibatch loss at step 2500: 0.136462\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 97.7%\n",
      "Minibatch loss at step 3000: 0.152722\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 97.8%\n",
      "Minibatch loss at step 3500: 0.117042\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 97.8%\n",
      "Minibatch loss at step 4000: 0.128496\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 97.9%\n",
      "Minibatch loss at step 4500: 0.125471\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 97.8%\n",
      "Minibatch loss at step 5000: 0.112604\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.2%\n",
      "Minibatch loss at step 5500: 0.107188\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.0%\n",
      "Minibatch loss at step 6000: 0.102120\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.2%\n",
      "Minibatch loss at step 6500: 0.124123\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 98.0%\n",
      "Minibatch loss at step 7000: 0.102726\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.0%\n",
      "Minibatch loss at step 7500: 0.100608\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.3%\n",
      "Minibatch loss at step 8000: 0.107891\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.2%\n",
      "Minibatch loss at step 8500: 0.100422\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.5%\n",
      "Minibatch loss at step 9000: 0.103369\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.0%\n",
      "Minibatch loss at step 9500: 0.098528\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.2%\n",
      "Minibatch loss at step 10000: 0.098841\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001  \n",
    "  \n",
    "with tf.Session(graph=graph) as session:  \n",
    "    tf.initialize_all_variables().run()  \n",
    "    print(\"Initialized\")  \n",
    "    for step in range(num_steps):  \n",
    "        # 在训练数据中选择一个已被随机化的偏移量.\n",
    "        # 提醒: 我们能使用更好的随机化穿过所有数据.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)  \n",
    "        # 生成一个小批量数据\n",
    "        batch_data = train_images[offset:(offset + batch_size), :]  \n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]  \n",
    "        # feed_dict的作用是给使用placeholder创建出来的tensor赋值。\n",
    "        # 其实，他的作用更加广泛：feed 使用一个 值临时替换一个 op 的输出结果. \n",
    "        # 你可以提供 feed 数据作为 run() 调用的参数. feed 只在调用它的方法内有效, 方法结束, feed 就会消失.\n",
    "        #  传递值到tf的命名空间  \n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}  \n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)  \n",
    "        if (step % 500 == 0):  \n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))  \n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))  \n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), validation_labels))  \n",
    "    # 获取结果，用于保存\n",
    "    test_prediction_np = test_prediction.eval()\n",
    "    test_prediction_np = np.argmax(test_prediction_np, 1)\n",
    "    savecsv(test_prediction_np)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "- [Deep MNIST for Experts](https://www.tensorflow.org/versions/master/tutorials/mnist/pros/index.html#deep-mnist-for-experts)\n",
    "- [A Convolutional Network implementation example using TensorFlow library](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3%20-%20Neural%20Networks/convolutional_network.ipynb)\n",
    "- [Digit recognizer in Python using CNN](https://www.kaggle.com/kobakhit/digit-recognizer/digit-recognizer-in-python-using-cnn)\n",
    "- [Deep Learning in a Nutshell: Core Concepts](http://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-core-concepts/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
