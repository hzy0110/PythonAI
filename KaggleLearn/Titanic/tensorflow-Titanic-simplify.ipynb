{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 介绍 Introduction\n",
    "\n",
    "这个笔记是非常基础和简单的介绍融合模型的入门方式，特别是堆叠融合的变体。简而言之，堆叠用作第一级（基础），一些基本的机器学习模型（分类器）的预测，然后在第二级使用另一个模型来预测早期一级预测的输出。\n",
    "\n",
    "This notebook is a very basic and simple introductory primer to the method of ensembling models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic machine learning models (classifiers) and then uses another model at the second-level to predict the output from the earlier first-level predictions.\n",
    "\n",
    "泰坦尼克号数据集是引入这一概念的主要方案，许多新手到Kaggle是从这里开始的。此外，即使堆叠技术已经为许多团队在Kaggle比赛获胜的方案，似乎在这个话题上缺少核心，所以我希望这款笔记本可以填补一些空白。\n",
    "\n",
    "The Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning Kaggle competitions there seems to be a dearth of kernels on this topic so I hope this notebook can fill somewhat of that void.\n",
    "\n",
    "我自己也是Kaggle的新手，我设法学习的第一个合适的融合/堆叠脚本是由伟大的Faron在AllState Severity索赔比赛中写的。这笔记的教材大量借鉴了Faron的脚本，尽管我们是融合分类，他的是融合回归方案。但无论如何，请在这里查看他的脚本：\n",
    "\n",
    "I myself am quite a newcomer to the Kaggle scene as well and the first proper ensembling/stacking script that I managed to chance upon and study was one written in the AllState Severity Claims competition by the great Faron. The material in this notebook borrows heavily from Faron's script although ported to factor in ensembles of classifiers whilst his was ensembles of regressors. Anyway please check out his script here:\n",
    "\n",
    "[Stacking Starter](https://www.kaggle.com/mmueller/allstate-claims-severity/stacking-starter/run/390867) : by Faron\n",
    "\n",
    "现在在笔记本上我希望它能以一种直观和简洁的方式，做到公正并且传达出融合的概念。我的另一个独立的Kaggle脚本实现了完全相同的组合步骤（尽管有不同的参数），下面讨论的公共LB分数为0.808，这足以达到前9％，运行时间不到4分钟。 因此，我很确定有很多改进的空间，并添加到该脚本。 无论如何，请随时给我留下任何关于我如何改善的意见\n",
    "\n",
    "Now onto the notebook at hand and I hope that it manages to do justice and convey the concept of ensembling in an intuitive and concise manner. My other standalone Kaggle [script](https://www.kaggle.com/arthurtok/titanic/simple-stacking-with-xgboost-0-808) which implements exactly the same ensembling steps (albeit with different parameters) discussed below gives a Public LB score of 0.808 which is good enough to get to the top 9% and runs just under 4 minutes. Therefore I am pretty sure there is a lot of room to improve and add on to that script. Anyways please feel free to leave me any comments with regards to how I can improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T14:53:14.103233Z",
     "start_time": "2018-11-02T14:52:40.967717Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load in our libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sklearn\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Going to use these 5 base models for the stacking\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import KFold;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征探索，工程和清洁 Feature Exploration, Engineering and Cleaning\n",
    "\n",
    "现在我们将进行常规的工作，就像大多数要点的结构一样，首先是探索手头的数据，找出可能的特征工程，以及对任何分类特征进行数字编码。\n",
    "\n",
    "Now we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:12:55.967615Z",
     "start_time": "2018-11-02T15:12:55.952850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 12)\n",
      "(418, 11)\n"
     ]
    }
   ],
   "source": [
    "# Load in the train and test datasets\n",
    "train = pd.read_csv('./input/train.csv')\n",
    "test = pd.read_csv('./input/test.csv')\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "# Store our passenger ID for easy access\n",
    "PassengerId = test['PassengerId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:12:56.419952Z",
     "start_time": "2018-11-02T15:12:56.404041Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "毫无疑问，我们的任务是以某种方式从分类变量中提取信息。\n",
    "\n",
    "Well it is no surprise that our task is to somehow extract the information out of the categorical variables\n",
    "\n",
    "**特征工程**\n",
    "\n",
    "**Feature Engineering**\n",
    "\n",
    "在这里，必须扩展到Sina的特征工程理念，这是非常全面和周全的笔记，所以请看看他的工作\n",
    "\n",
    "Here, credit must be extended to Sina's very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work\n",
    "\n",
    "[Titanic Best Working Classfier](https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier): by Sina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:12:57.419233Z",
     "start_time": "2018-11-02T15:12:56.744272Z"
    }
   },
   "outputs": [],
   "source": [
    "full_data = [train, test]\n",
    "\n",
    "# Some features of my own that I have added in\n",
    "# Gives the length of the name\n",
    "train['Name_length'] = train['Name'].apply(len)\n",
    "test['Name_length'] = test['Name'].apply(len)\n",
    "# Feature that tells whether a passenger had a cabin on the Titanic\n",
    "train['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n",
    "test['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n",
    "\n",
    "# Feature engineering steps taken from Sina\n",
    "# Create new feature FamilySize as a combination of SibSp and Parch\n",
    "for dataset in full_data:\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "# Create new feature IsAlone from FamilySize\n",
    "for dataset in full_data:\n",
    "    dataset['IsAlone'] = 0\n",
    "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "# Remove all NULLS in the Embarked column\n",
    "for dataset in full_data:\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n",
    "# Remove all NULLS in the Fare column and create a new feature CategoricalFare\n",
    "for dataset in full_data:\n",
    "    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\n",
    "train['CategoricalFare'] = pd.qcut(train['Fare'], 4)\n",
    "# Create a New feature CategoricalAge\n",
    "for dataset in full_data:\n",
    "    age_avg = dataset['Age'].mean()\n",
    "    age_std = dataset['Age'].std()\n",
    "    age_null_count = dataset['Age'].isnull().sum()\n",
    "    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n",
    "    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n",
    "    dataset['Age'] = dataset['Age'].astype(int)\n",
    "train['CategoricalAge'] = pd.cut(train['Age'], 5)\n",
    "# Define function to extract titles from passenger names\n",
    "def get_title(name):\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "# Create a new feature Title, containing the titles of passenger names\n",
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Name'].apply(get_title)\n",
    "# Group all non-common titles into one single grouping \"Rare\"\n",
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
    "\n",
    "for dataset in full_data:\n",
    "    # Mapping Sex\n",
    "    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "    \n",
    "    # Mapping titles\n",
    "    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
    "    dataset['Title'] = dataset['Title'].map(title_mapping)\n",
    "    dataset['Title'] = dataset['Title'].fillna(0)\n",
    "    \n",
    "    # Mapping Embarked\n",
    "    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n",
    "    \n",
    "    # Mapping Fare\n",
    "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n",
    "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
    "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n",
    "    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n",
    "    dataset['Fare'] = dataset['Fare'].astype(int)\n",
    "    \n",
    "    # Mapping Age\n",
    "    dataset.loc[ dataset['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\n",
    "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n",
    "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n",
    "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n",
    "    dataset.loc[ dataset['Age'] > 64, 'Age'] ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:12:57.439449Z",
     "start_time": "2018-11-02T15:12:57.421195Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Name_length</th>\n",
       "      <th>Has_Cabin</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>IsAlone</th>\n",
       "      <th>CategoricalFare</th>\n",
       "      <th>CategoricalAge</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>(-0.001, 7.91]</td>\n",
       "      <td>(16.0, 32.0]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>3</td>\n",
       "      <td>C85</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>(31.0, 512.329]</td>\n",
       "      <td>(32.0, 48.0]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "\n",
       "                                                Name  Sex  Age  SibSp  Parch  \\\n",
       "0                            Braund, Mr. Owen Harris    1    1      1      0   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...    0    2      1      0   \n",
       "\n",
       "      Ticket  Fare Cabin  Embarked  Name_length  Has_Cabin  FamilySize  \\\n",
       "0  A/5 21171     0   NaN         0           23          0           2   \n",
       "1   PC 17599     3   C85         1           51          1           2   \n",
       "\n",
       "   IsAlone  CategoricalFare CategoricalAge  Title  \n",
       "0        0   (-0.001, 7.91]   (16.0, 32.0]      1  \n",
       "1        0  (31.0, 512.329]   (32.0, 48.0]      3  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:18:07.302693Z",
     "start_time": "2018-11-02T15:18:07.280766Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 特征选择train[[\"Survived\"\n",
    "# # 准备训练和测试数据\n",
    "from sklearn.feature_selection import SelectKBest,f_classif\n",
    "# print(train.shape)\n",
    "# y = train[\"Survived\"]\n",
    "# print(\"y.shape\",y.shape)\n",
    "\n",
    "# # print(train.shape)\n",
    "# tf_train = train.drop([\"Survived\"], axis = 1)\n",
    "# drop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\n",
    "# tf_train = tf_train.drop(drop_elements, axis = 1)\n",
    "# tf_train = tf_train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\n",
    "# print(X_train.shape)\n",
    "# # X_train_ss_new = SelectKBest(f_classif, k=16).fit_transform(X_train, y)\n",
    "# # print(X_train_ss_new.shape)\n",
    "# X_train.head(2)\n",
    "\n",
    "# # X_train.to_csv('./out1.csv')\n",
    "\n",
    "tf_train_new = SelectKBest(f_classif, k=9).fit_transform(tf_train, y)\n",
    "# tf_train_new_pd = pd.DataFrame(tf_train_new)\n",
    "# tf_train_new_pd.to_csv('./out2.csv')\n",
    "tf_test = test[['Pclass','Sex','Parch','Fare','Embarked','Name_length','Has_Cabin','IsAlone','Title']]\n",
    "# # 独热编码\n",
    "# y = (np.arange(2) == y[:,None]).astype(np.float32)\n",
    "# print(\"y.shape\",y.shape)\n",
    "# print(\"tf_train_new.shape\",tf_train_new.shape)\n",
    "# print(\"tf_labels.shape\",tf_labels.shape)\n",
    "# # 'Pclass','Sex','Parch','Fare','Embarked','Name_length','Has_Cabin','IsAlone','Title'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:12:57.452945Z",
     "start_time": "2018-11-02T15:12:57.450513Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(tf_labels.shape)\n",
    "# print(type(tf_labels))\n",
    "# tf_labels.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:12:57.469131Z",
     "start_time": "2018-11-02T15:12:57.455002Z"
    }
   },
   "outputs": [],
   "source": [
    "# 特征选择\n",
    "y = train[\"Survived\"]\n",
    "\n",
    "tf_train = train.drop([\"Survived\"], axis = 1)\n",
    "drop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\n",
    "tf_train = tf_train.drop(drop_elements, axis = 1)\n",
    "tf_train = tf_train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\n",
    "\n",
    "train = train.drop(drop_elements, axis = 1)\n",
    "train = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\n",
    "test  = test.drop(drop_elements, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好了，现在我们已经清理了这些特性并提取了相关的信息并删除了分类列，现在我们的特性现在应该是数字的，这种格式适合于我们的机器学习模型。\n",
    "\n",
    "All right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. \n",
    "\n",
    "然而，在进行之前，让我们生成一些简单的相关和分布图，以转换我们的数据集来观察\n",
    "\n",
    "However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:12:59.163010Z",
     "start_time": "2018-11-02T15:12:59.157526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 12)\n"
     ]
    }
   ],
   "source": [
    "train.head(3)\n",
    "\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 皮尔逊相关热图（Pearson Correlation Heatmap）\n",
    "\n",
    "让我们产生一些特征的相关图，看看下一个特征是如何相关的。为了做到这一点，我们将利用Seaborn绘图软件，让我们很方便地绘制热图，如下所示\n",
    "\n",
    "let us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:13:00.054137Z",
     "start_time": "2018-11-02T15:13:00.051452Z"
    }
   },
   "outputs": [],
   "source": [
    "# colormap = plt.cm.viridis\n",
    "# plt.figure(figsize=(12,12))\n",
    "# plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "# sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Takeaway from the Plots**\n",
    "\n",
    "皮尔森相关图可以告诉我们的一点是，特征之间没有太多的强关联。\n",
    "\n",
    "One thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another.\n",
    "\n",
    "这对我们的模型来说是一个好消息，因为这意味着训练数据中没有太多的冗余信息，我们每一个特征都可以提供一个独立的信息\n",
    "\n",
    "This is good from a point of view of feeding these features into your learning model because this means that there isn't much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. \n",
    "\n",
    "这两个最相关的特征是家庭大小和父母（父母和孩子）的特征。\n",
    "\n",
    "Here are two most correlated features are that of Family size and Parch (Parents and Children). \n",
    "\n",
    "为了本练习的目的，我仍然会留下这两个功能\n",
    "\n",
    "I'll still leave both features in for the purposes of this exercise.\n",
    "\n",
    "**配对图 Pair plots**\n",
    "\n",
    "最后，我们生成一些配对图来观察数据从一个特征到另一个特征的分布。再次，我们用Seaborn帮助我们。\n",
    "\n",
    "Finally let us generate some pairplots to observe the distribution of data from one feature to the other. Once again we use Seaborn to help us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:13:01.436343Z",
     "start_time": "2018-11-02T15:13:01.433615Z"
    }
   },
   "outputs": [],
   "source": [
    "# g = sns.pairplot(train[[\"Survived\", \"Pclass\", \"Sex\", \"Age\", \"Parch\", \"Fare\", \"Embarked\",\"FamilySize\", \"Title\"]],\n",
    "# hue=\"Survived\", palette = \"seismic\",size=1.2 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，由于在OOP框架中提到了Object和类，现在让我们通过我们前面定义的Helper Sklearn类创建5个代表我们5个学习模型的对象。\n",
    "\n",
    "Furthermore, since having mentioned about Objects and classes within the OOP framework, let us now create 5 objects that represent our 5 learning models via our Helper Sklearn Class we defined earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**从我们的训练集和测试集生成NumPy数组**\n",
    "\n",
    "**Creating NumPy arrays out of our train and test sets**\n",
    "\n",
    "我们已经准备好了我们的第一层基础模型, 现在我们可以通过从原始 dataframes 中生成 NumPy 阵列, 输入到我们的分类器中准备培训和测试数据, 如下所示:\n",
    "\n",
    "Great. Having prepared our first layer base models as such, we can now ready the training and test test data for input into our classifiers by generating NumPy arrays out of their original dataframes as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:13:02.876594Z",
     "start_time": "2018-11-02T15:13:02.865296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape (891, 11)\n",
      "y_train.shape (891,)\n",
      "train.shape (891, 11)\n",
      "x_test.shape (418, 11)\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "# 用训练，测试，目标创建的Numpy数组提供给模型\n",
    "# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\n",
    "\n",
    "y_train = train['Survived'].ravel()\n",
    "train = train.drop(['Survived'], axis=1)\n",
    "x_train = train.values # Creates an array of the train data\n",
    "x_test = test.values # Creats an array of the test data\n",
    "\n",
    "print(\"x_train.shape\",x_train.shape)\n",
    "print(\"y_train.shape\",y_train.shape)\n",
    "print(\"train.shape\",train.shape)\n",
    "print(\"x_test.shape\",x_test.shape)\n",
    "print(type(x_train.shape))\n",
    "print(type(train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:09:30.476818Z",
     "start_time": "2018-11-02T15:09:30.473683Z"
    }
   },
   "outputs": [],
   "source": [
    "# x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\n",
    "# x_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在连接和加入第一级训练和测试预测集是x_train和x_test，我们现在能放入第二级训练模型\n",
    "\n",
    "Having now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:13:07.641239Z",
     "start_time": "2018-11-02T15:13:07.632916Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import model_selection\n",
    "# settings\n",
    "LEARNING_RATE = 1e-4\n",
    "# set to 20000 on local environment to get 0.99 accuracy\n",
    "TRAINING_ITERATIONS = 25       \n",
    "    \n",
    "DROPOUT = 0.5\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "# set to 0 to train on all available data\n",
    "VALIDATION_SIZE = 2000\n",
    "\n",
    "# image number to output\n",
    "IMAGE_TO_DISPLAY = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:13:08.484386Z",
     "start_time": "2018-11-02T15:13:08.474010Z"
    }
   },
   "outputs": [],
   "source": [
    "# 计算精准度\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "# 保存结果\n",
    "def savecsv(test_prediction_np, filename=\"submission_tf_titanic.csv\"):\n",
    "#     np.savetxt('submission_tf_titanic.csv', \n",
    "#            np.c_[range(1,len(x_test)+1),test_prediction_np], \n",
    "#            delimiter=',', \n",
    "#            header = 'PassengerId,Survived', \n",
    "#            comments = '', \n",
    "#            fmt='%d')\n",
    "    StackingSubmission = pd.DataFrame({'PassengerId': PassengerId,'Survived': test_prediction_np })\n",
    "    StackingSubmission.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:13:09.260523Z",
     "start_time": "2018-11-02T15:13:09.245142Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 11)\n",
      "(891,)\n",
      "(891, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Name_length</th>\n",
       "      <th>Has_Cabin</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>IsAlone</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass  Sex  Age  Parch  Fare  Embarked  Name_length  Has_Cabin  \\\n",
       "0       3    1    1      0     0         0           23          0   \n",
       "1       1    0    2      0     3         1           51          1   \n",
       "\n",
       "   FamilySize  IsAlone  Title  \n",
       "0           2        0      1  \n",
       "1           2        0      3  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(train.shape)\n",
    "train.head(2)\n",
    "# y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:13:12.608638Z",
     "start_time": "2018-11-02T15:13:12.603695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 2)\n"
     ]
    }
   ],
   "source": [
    "# 独热编码\n",
    "y_train = (np.arange(2) == y_train[:,None]).astype(np.float32)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:13:13.613539Z",
     "start_time": "2018-11-02T15:13:13.603990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(623, 11)\n",
      "(268, 11)\n",
      "(623,)\n",
      "(268,)\n"
     ]
    }
   ],
   "source": [
    "#形成验证数据\n",
    "train_dataset, valid_dataset = model_selection.train_test_split(train, test_size=0.3, random_state=0)\n",
    "train_labels, valid_labels = model_selection.train_test_split(y, test_size=0.3, random_state=0)\n",
    "print(train_dataset.shape)\n",
    "print(valid_dataset.shape)\n",
    "print(train_labels.shape)\n",
    "print(valid_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:13:19.509578Z",
     "start_time": "2018-11-02T15:13:18.717509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(623, 11)\n",
      "weights_end.shape (32, 2)\n",
      "biases_end.shape (2,)\n",
      "(623, 11)\n"
     ]
    }
   ],
   "source": [
    "# 根据tf任务3.1加入l2 0.77990\n",
    "# 加入DropOut 0.78947\n",
    "# 加入梯度学习率，原学习率0.5  0.78469\n",
    "# 加入5层隐藏层 0.78469\n",
    "# 减少到3层，初始化节点减小到256\n",
    "\n",
    "batch_size = 64  \n",
    "regularation_param = 0.0001  \n",
    "keep_prob = 0.8\n",
    "graph = tf.Graph()  \n",
    "num_labels = 2\n",
    "hidden_nodes = 512\n",
    "\n",
    "print(train_dataset.shape)\n",
    "\n",
    "def compute_logits(data, weightss, biasess, dropout_vals=None):  \n",
    "    temp = data  \n",
    "    if dropout_vals:  \n",
    "        for w,b,d in zip(weightss[:-1], biasess[:-1], dropout_vals[:-1]):  \n",
    "            temp = tf.nn.relu_layer(tf.nn.dropout(temp, d), w, b)  \n",
    "        temp = tf.matmul(temp, weightss[-1]) + biasess[-1]  \n",
    "    else:  \n",
    "        for w,b in zip(weightss[:-1], biasess[:-1]):  \n",
    "            temp = tf.nn.relu_layer(temp, w, b)  \n",
    "        temp = tf.matmul(temp, weightss[-1]) + biasess[-1]  \n",
    "    return temp\n",
    "\n",
    "with graph.as_default():\n",
    "    # -----------------------------------------1\n",
    "    # 输入 \n",
    "    # placeholder 插入一个待初始化的张量占位符\n",
    "    # 重要事项：这个张量被求值时会产生错误。 \n",
    "    # 它的值必须在Session.run(), Tensor.eval() 或 Operation.run() 中使用feed_dict的这个可选参数来填充。\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, train_dataset.shape[1]))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    \n",
    "    # 创建一个常量张量\n",
    "    # tf_valid_dataset = Tensor(\"Const:0\", shape=(10000, 784), dtype=float32)\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(x_test)\n",
    "    \n",
    "    #转数据类型，f64->f32\n",
    "    tf_train_dataset = tf.to_float(tf_train_dataset)\n",
    "    tf_valid_dataset = tf.to_float(tf_valid_dataset)\n",
    "    tf_test_dataset = tf.to_float(tf_test_dataset)\n",
    "    \n",
    "    # 变量\n",
    "    # 梯度学习率\n",
    "    # 初始的学习速率\n",
    "    starter_learning_rate = 0.03 \n",
    "    # 全局的step，与 decay_step 和 decay_rate一起决定了 learning rate的变化\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    # 衰减速度\n",
    "    decay_steps = 50\n",
    "    # 衰减系数\n",
    "    decay_rate = 0.8\n",
    "    # 如果staircase=True，那就表明每decay_steps次计算学习速率变化，更新原始学习速率.\n",
    "    # 如果是False，那就是每一步都更新学习速率\n",
    "    staircase = False\n",
    "    # 指数衰减:法通过这个函数，可以先使用较大的学习率来快速得到一个比较优的解，然后随着迭代的继续逐步减小学习率，使得模型在训练后期更加稳定\n",
    "    # 87.7% 仅仅指数衰减\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate,global_step,decay_steps,decay_rate,staircase)\n",
    "    \n",
    "    \n",
    "    # 当你训练一个模型的时候，你使用变量去保存和更新参数。\n",
    "    # 在Tensorflow中变量是内存缓冲区中保存的张量（tensor）\n",
    "    \n",
    "    # 第一层\n",
    "    # truncated_normal 从一个正态分布片段中输出随机数值,\n",
    "    # 生成的值会遵循一个指定了平均值和标准差的正态分布，只保留两个标准差以内的值，超出的值会被弃掉重新生成。\n",
    "    # 返回 一个指定形状并用正态分布片段的随机值填充的张量\n",
    "    # 数字平方根\n",
    "    x = 2.0\n",
    "    weights1 = tf.Variable(tf.truncated_normal([train_dataset.shape[1], hidden_nodes], stddev = np.sqrt(x / hidden_nodes)))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "    \n",
    "    # 第二层\n",
    "    weights2 = tf.Variable(tf.truncated_normal([hidden_nodes, int(hidden_nodes / 2)], stddev = np.sqrt(x / hidden_nodes / 2)))\n",
    "    biases2 = tf.Variable(tf.zeros([hidden_nodes / 2]))\n",
    "    hidden_nodes = int(hidden_nodes / 2)\n",
    "    \n",
    "    # 第三层\n",
    "    weights3 = tf.Variable(tf.truncated_normal([hidden_nodes, int(hidden_nodes / 2)], stddev = np.sqrt(x / hidden_nodes / 2)))\n",
    "    biases3 = tf.Variable(tf.zeros([hidden_nodes / 2]))\n",
    "    hidden_nodes = int(hidden_nodes / 2)\n",
    "    \n",
    "    # 第四层 94.5\n",
    "    weights4 = tf.Variable(tf.truncated_normal([hidden_nodes, int(hidden_nodes / 2)], stddev = np.sqrt(x / hidden_nodes / 2)))\n",
    "    biases4 = tf.Variable(tf.zeros([hidden_nodes / 2]))\n",
    "    hidden_nodes = int(hidden_nodes / 2)\n",
    "    \n",
    "    # 第五层 94.5\n",
    "    weights5 = tf.Variable(tf.truncated_normal([hidden_nodes, int(hidden_nodes / 2)], stddev = np.sqrt(x / hidden_nodes / 2)))\n",
    "    biases5 = tf.Variable(tf.zeros([hidden_nodes / 2]))\n",
    "    hidden_nodes = int(hidden_nodes / 2)\n",
    "    \n",
    "    \n",
    "    weights_end = tf.Variable(tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "    biases_end = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    print(\"weights_end.shape\", weights_end.shape)\n",
    "    print(\"biases_end.shape\", biases_end.shape)\n",
    "    print(train_dataset.shape)\n",
    "    \n",
    "    #DropOut\n",
    "    drop = tf.nn.dropout(tf_train_dataset, keep_prob=keep_prob)\n",
    "    \n",
    "    # 训练计算.\n",
    "#     train_logits = tf.add(tf.matmul(drop, weights), biases)\n",
    "#     valid_logits = tf.add(tf.matmul(tf_valid_dataset, weights), biases)\n",
    "#     test_logits = tf.add(tf.matmul(tf_test_dataset, weights), biases)\n",
    "    train_logits = compute_logits(tf_train_dataset, [weights1, weights2, weights3, weights4, weights5, weights_end], \n",
    "                            [biases1,biases2,biases3,biases4,biases5, biases_end],  \n",
    "                            dropout_vals=(1.0,1,1,1,1,1.0))\n",
    "    valid_logits = compute_logits(tf_valid_dataset, [weights1, weights2, weights3, weights4, weights5, weights_end], \n",
    "                          [biases1,biases2,biases3,biases4,biases5, biases_end])\n",
    "    test_logits = compute_logits(tf_test_dataset, [weights1, weights2, weights3, weights4, weights5, weights_end], \n",
    "                          [biases1,biases2,biases3,biases4,biases5, biases_end])\n",
    "    \n",
    "    \n",
    "    #加l2_loss\n",
    "    hpl2 =  regularation_param * (tf.nn.l2_loss(weights1)\n",
    "                                  + tf.nn.l2_loss(weights2)\n",
    "                                  + tf.nn.l2_loss(weights3)\n",
    "                                  + tf.nn.l2_loss(weights4)\n",
    "                                  + tf.nn.l2_loss(weights5)\n",
    "                                 )\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=train_logits))\n",
    "    loss = tf.add(loss, hpl2)\n",
    "\n",
    "\n",
    "    # 最优化.因为深度学习常见的是对于梯度的优化，也就是说，优化器最后其实就是各种对于梯度下降算法的优化。 \n",
    "#     optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.02).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    \n",
    "    train_prediction = tf.nn.softmax(train_logits)\n",
    "    valid_prediction = tf.nn.softmax(valid_logits)\n",
    "    test_prediction = tf.nn.softmax(test_logits)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:16:02.139363Z",
     "start_time": "2018-11-02T15:16:02.071880Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'shpae'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-1932caa37dff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_dataset.shpae'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshpae\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_labels.shpae'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshpae\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3612\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3613\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3614\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3616\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'shpae'"
     ]
    }
   ],
   "source": [
    "num_steps = 10000  \n",
    "print(type(train_dataset))\n",
    "print(type(train_labels))\n",
    "print('train_dataset.shpae',train_dataset.shpae)\n",
    "print('train_labels.shpae',train_labels.shpae)\n",
    "\n",
    "train_dataset_np = train_dataset.values\n",
    "with tf.Session(graph=graph) as session:  \n",
    "    tf.global_variables_initializer().run()  \n",
    "    print(\"Initialized\")  \n",
    "    for step in range(num_steps):  \n",
    "        # 在训练数据中选择一个已被随机化的偏移量.\n",
    "        # 提醒: 我们能使用更好的随机化穿过所有数据.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "#         print(train_dataset.shape)\n",
    "#         print(train_labels.shape)\n",
    "        # 生成一个小批量数据\n",
    "        batch_data = train_dataset_np[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]  \n",
    "#         print(\"batch_data.shape\",batch_data.shape)\n",
    "#         print(\"batch_labels.shape\",batch_labels.shape)\n",
    "        # feed_dict的作用是给使用placeholder创建出来的tensor赋值。\n",
    "        # 其实，他的作用更加广泛：feed 使用一个 值临时替换一个 op 的输出结果. \n",
    "        # 你可以提供 feed 数据作为 run() 调用的参数. feed 只在调用它的方法内有效, 方法结束, feed 就会消失.\n",
    "        #  传递值到tf的命名空间  \n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}  \n",
    "        summary, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):  \n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))  \n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))  \n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))  \n",
    "            \n",
    "    # 获取结果，用于保存\n",
    "    test_prediction_np = test_prediction.eval()\n",
    "    test_prediction_np = np.argmax(test_prediction_np, 1)\n",
    "#     print(\"test_prediction_np.shape\",test_prediction_np.shape)\n",
    "#     print(\"test_prediction_np\",test_prediction_np)\n",
    "    savecsv(test_prediction_np,\"submission_tf_2lnn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:16:59.932559Z",
     "start_time": "2018-11-02T15:16:59.914040Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 变厚矩阵\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "# 偏置\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "# 卷积处理 变厚过程\n",
    "def conv2d(x, W):\n",
    "    # stride [1, x_movement, y_movement, 1] x_movement、y_movement就是步长\n",
    "    # Must have strides[0] = strides[3] = 1 padding='SAME'表示卷积后长宽不变\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "# pool 长宽缩小一倍\n",
    "def max_pool_2x2(x):\n",
    "    # stride [1, x_movement, y_movement, 1]\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:17:03.130525Z",
     "start_time": "2018-11-02T15:17:02.764201Z"
    }
   },
   "outputs": [],
   "source": [
    "graphCNN = tf.Graph()  \n",
    "\n",
    "with graphCNN.as_default():\n",
    "    # define placeholder for inputs to network\n",
    "    xs = tf.placeholder(tf.float32, [None, 9])  # 原始数据的维度：9\n",
    "    ys = tf.placeholder(tf.float32, [None, 2])  # 输出数据为维度：2\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32)  # dropout的比例\n",
    "\n",
    "    x_image = tf.reshape(xs, [-1, 3, 3, 1])  # 原始数据9变成二维图片3*3\n",
    "    ## conv1 layer ##第一卷积层\n",
    "    W_conv1 = weight_variable([2, 2, 1, 32])  # patch 2x2, in size 1, out size 32,每个像素变成32个像素，就是变厚的过程\n",
    "    b_conv1 = bias_variable([32])\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)  # output size 2x2x32，长宽不变，高度为32的三维图像\n",
    "    # h_pool1 = max_pool_2x2(h_conv1)     # output size 2x2x32 长宽缩小一倍\n",
    "\n",
    "    ## conv2 layer ##第二卷积层\n",
    "    W_conv2 = weight_variable([2, 2, 32, 64])  # patch 2x2, in size 32, out size 64\n",
    "    b_conv2 = bias_variable([64])\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_conv1, W_conv2) + b_conv2)  # 输入第一层的处理结果 输出shape 4*4*64\n",
    "\n",
    "    ## fc1 layer ##  full connection 全连接层\n",
    "    W_fc1 = weight_variable([3 * 3 * 64, 512])  # 4x4 ，高度为64的三维图片，然后把它拉成512长的一维数组\n",
    "    b_fc1 = bias_variable([512])\n",
    "\n",
    "    h_pool2_flat = tf.reshape(h_conv2, [-1, 3 * 3 * 64])  # 把4*4，高度为64的三维图片拉成一维数组 降维处理\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)  # 把数组中扔掉比例为keep_prob的元素\n",
    "    ## fc2 layer ## full connection\n",
    "    W_fc2 = weight_variable([512, 2])  # 512长的一维数组压缩为长度为2的数组\n",
    "    b_fc2 = bias_variable([2])  # 偏置\n",
    "    # 最后的计算结果\n",
    "    prediction = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "    # prediction = tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "    # 计算 predition与y 差距 所用方法很简单就是用 suare()平方,sum()求和,mean()平均值\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=ys, logits=prediction))\n",
    "    # 0.01学习效率,minimize(loss)减小loss误差\n",
    "    train_step = tf.train.AdamOptimizer(0.03).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:20:29.045862Z",
     "start_time": "2018-11-02T15:20:28.779416Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_train_new.shape (891, 9)\n",
      "y.shape (891,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (891,) for Tensor 'Placeholder_1:0', which has shape '(?, 2)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-c3cdaf206498>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#     feed_dict_test = {tf_train_dataset : X_test_new_ss}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m#     feed_dict = {tf_train_dataset : X_tr_tf, tf_train_labels : train_labels, keep_prob_s: keep_prob }\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf_train_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf_train_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# 输出loss值\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1074\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[0;32m-> 1076\u001b[0;31m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1077\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (891,) for Tensor 'Placeholder_1:0', which has shape '(?, 2)'"
     ]
    }
   ],
   "source": [
    "print(\"tf_train_new.shape\",tf_train_new.shape)\n",
    "# print(\"tf_labels.shape\",tf_labels.shape)\n",
    "print(\"y.shape\",y.shape)\n",
    "\n",
    "print(type(tf_train_new))\n",
    "print(type(y))\n",
    "with tf.Session(graph=graphCNN) as sess:\n",
    "\n",
    "    # important step\n",
    "    # tf.initialize_all_variables() no long valid from\n",
    "    # 2017-03-02 if using tensorflow >= 0.12\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # 训练500次\n",
    "    for i in range(2001):\n",
    "\n",
    "    #     feed_dict_test = {tf_train_dataset : X_test_new_ss}  \n",
    "    #     feed_dict = {tf_train_dataset : X_tr_tf, tf_train_labels : train_labels, keep_prob_s: keep_prob }  \n",
    "        sess.run(train_step, feed_dict={xs: tf_train_new, ys: y, keep_prob: 0.7})\n",
    "        loss = sess.run(cross_entropy, feed_dict={xs: tf_train_new, ys: y, keep_prob: 1.0})\n",
    "        # 输出loss值\n",
    "        if (i % 100 == 0): \n",
    "            print(i, '误差=',loss)  \n",
    "    # 可视化\n",
    "    prediction_value = sess.run(prediction, feed_dict={xs: tf_test, keep_prob: 1.0})\n",
    "#     print(prediction_value)\n",
    "    prediction_value = np.argmax(prediction_value, 1)\n",
    "    print(prediction_value.shape)\n",
    "    #     print(\"test_prediction_np\",test_prediction_np[0])\n",
    "    #     print(\"test_prediction_np.shpe\",test_prediction_np.shape)\n",
    "    #     print(\"x.shpe\",x.shape)\n",
    "    #     print(\"x\",x)\n",
    "    #     print(\"test_prediction_np.shape\",test_prediction_np.shape)\n",
    "    savecsv(prediction_value,\"submission_tf_2lcnn\")\n",
    "\n",
    "    print(\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
