{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f1e716ff-fb54-4a26-96e9-38ed9d0b3ce5",
    "_execution_state": "idle",
    "_uuid": "317c28cf62b5cdb145f09aed91a5f4100b32ba23"
   },
   "source": [
    "## Blending regression models\n",
    "*August 2017*\n",
    "\n",
    "I've read lots of notebooks here and I think the best is this one:\n",
    "\n",
    "1. [Stacked Regressions : Top 4% on LeaderBoard](https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard)   by **Serigne**: great feature engineering and regression models with tuned parameters.\n",
    "2. [A study on Regression applied to the Ames dataset](https://www.kaggle.com/juliencs/a-study-on-regression-applied-to-the-ames-dataset) by juliencsA: excellent regression techinques analysis.\n",
    "\n",
    "\n",
    "In this notebook I won't be tuning those parameters or spend much time on feature engineering, the main purpose is to show how you can use tensorflow when doing blending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_cell_guid": "9a38d225-b235-4fb7-8e33-7455a044d6e3",
    "_execution_state": "idle",
    "_uuid": "9999d88e1b311ebb7566d888ba3448ddea9c6faf",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import some libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "_cell_guid": "885f5f2e-bb0b-4b9f-bd61-1a2488f3e54e",
    "_execution_state": "idle",
    "_uuid": "05a71b3f0d59a415044230d839792fbacde4df3d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def submit(ids, price):\n",
    "    \"\"\"\n",
    "    Writes predicted prices to .csv file.\n",
    "    \n",
    "    Arguments:\n",
    "        ids -- ID values\n",
    "        price -- predicted price\n",
    "    \"\"\"\n",
    "    subm = pd.DataFrame({'Id': ids,\n",
    "                        'SalePrice': price})\n",
    "    subm.to_csv('submissionLB.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_cell_guid": "712cbb69-26bd-4b9b-8560-b5f4de492059",
    "_execution_state": "idle",
    "_uuid": "6d4d224672b05dd5905d7acf993213dfc6d78fc1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read train and test datasets\n",
    "train = pd.read_csv('input/train.csv')\n",
    "test = pd.read_csv('input/test.csv')\n",
    "\n",
    "# Save the ID column and then drop it from datasets\n",
    "train_ID = train['Id']\n",
    "test_ID = test['Id']\n",
    "\n",
    "train.drop(\"Id\", axis = 1, inplace = True)\n",
    "test.drop(\"Id\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_cell_guid": "aa8d73ec-a40d-4442-9b01-1990ad8803f2",
    "_execution_state": "idle",
    "_uuid": "975e3c208d90e4a32fcd3534bf58f8d303c51775",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deleting outliers\n",
    "train = train.drop(train[(train['GrLivArea'] > 4000) & ((train['SalePrice'] < 300000))].index)\n",
    "\n",
    "# Take log(1+x) of target variable\n",
    "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
    "\n",
    "# Concat train and test\n",
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "y_train = train.SalePrice.values\n",
    "all_data = pd.concat((train, test)).reset_index(drop=True)\n",
    "all_data.drop(['SalePrice'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_cell_guid": "140faca2-c8b2-4390-9d98-6a12d4e45d3a",
    "_execution_state": "idle",
    "_uuid": "c14ce12246c608a3e6f51f26985ef2bcd9cf0dcf",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inplacing missing values\n",
    "all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\n",
    "all_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\n",
    "all_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\n",
    "all_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\n",
    "all_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\n",
    "\n",
    "all_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n",
    "    all_data[col] = all_data[col].fillna('None')\n",
    "for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n",
    "    all_data[col] = all_data[col].fillna(0)\n",
    "for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n",
    "    all_data[col] = all_data[col].fillna(0)\n",
    "for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n",
    "    all_data[col] = all_data[col].fillna('None')\n",
    "    \n",
    "all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\n",
    "all_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\n",
    "all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\n",
    "all_data = all_data.drop(['Utilities'], axis=1)\n",
    "all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\n",
    "all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\n",
    "all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\n",
    "all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\n",
    "all_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\n",
    "all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\n",
    "all_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_cell_guid": "ce76a102-b03f-4e55-a692-c36be6c841c9",
    "_execution_state": "idle",
    "_uuid": "a319c624ceb9aec75186558be3ddd7e6123dee6a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n",
    "\n",
    "# Changing OverallCond into a categorical variable\n",
    "all_data['OverallCond'] = all_data['OverallCond'].astype(str)\n",
    "\n",
    "# Year and month sold are transformed into categorical features.\n",
    "all_data['YrSold'] = all_data['YrSold'].astype(str)\n",
    "all_data['MoSold'] = all_data['MoSold'].astype(str)\n",
    "\n",
    "cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n",
    "        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n",
    "        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n",
    "        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n",
    "        'YrSold', 'MoSold')\n",
    "# Applying LabelEncoder to categorical features\n",
    "for c in cols:\n",
    "    lbl = LabelEncoder() \n",
    "    lbl.fit(list(all_data[c].values)) \n",
    "    all_data[c] = lbl.transform(list(all_data[c].values))\n",
    "\n",
    "# Adding total sqfootage feature \n",
    "all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_cell_guid": "e065a0ed-42b6-4601-a867-71ead84c16d9",
    "_execution_state": "idle",
    "_uuid": "be01205eecb8be52f5f2c528cd66dc7e81d8acfe",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "\n",
    "# The skew of all numerical features\n",
    "skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "\n",
    "skewness = skewness[abs(skewness) > 0.75]\n",
    "\n",
    "# Applying Box Cox transformation\n",
    "from scipy.special import boxcox1p\n",
    "skewed_features = skewness.index\n",
    "lam = 0.15\n",
    "for feat in skewed_features:\n",
    "    all_data[feat] = boxcox1p(all_data[feat], lam)\n",
    "    all_data[feat] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_cell_guid": "502848eb-d871-4adc-9d27-8c4c16f06297",
    "_execution_state": "idle",
    "_uuid": "51ff377f3b40045e90a5b5e85b018039bd16a270"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2917, 220)\n",
      "(1458, 220)\n",
      "(1459, 220)\n"
     ]
    }
   ],
   "source": [
    "# Get dummy categorical features\n",
    "all_data = pd.get_dummies(all_data)\n",
    "print(all_data.shape)\n",
    "\n",
    "train = all_data[:ntrain]\n",
    "test = all_data[ntrain:]\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "492ed995-6c71-4029-9f18-4e34b9c506b7",
    "_uuid": "1596101a542abba6276f4f3bd9f09c6db8a02cbe"
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_cell_guid": "8cd36954-596f-4ed2-9702-4933a4c57f11",
    "_execution_state": "idle",
    "_uuid": "af7e9fbec256de5cbed162979d65710131329012",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries with regression models\n",
    "from sklearn.linear_model import ElasticNet,  BayesianRidge, LassoLarsIC, LassoCV\n",
    "from sklearn.ensemble import  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_cell_guid": "3eaa60a5-5da6-40fa-ba8b-80631cfd825c",
    "_execution_state": "idle",
    "_uuid": "1e4f1c4d4511d81dc064e25f61cbb60a57480a54",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# So these are 6 models with tuned parameters\n",
    "lasso = make_pipeline(RobustScaler(), LassoCV(eps =1e-8))\n",
    "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=0.9, random_state=3))\n",
    "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.2, gamma=0.0, \n",
    "                             learning_rate=0.05, max_depth=6, \n",
    "                             min_child_weight=1.5, n_estimators=7200,\n",
    "                             reg_alpha=0.9, reg_lambda=0.6,\n",
    "                             subsample=0.2,seed=42, silent=1) #0.12105 LB\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ac1be71e-0fd4-4386-a379-353a686ad0af",
    "_uuid": "3c94a213d44bb161b3861dc67dff4a66b4bfe1fe",
    "collapsed": true
   },
   "source": [
    "### Blending \n",
    "\n",
    "Let's assume we have $m$ trained regression models. Let's get $m$ vectors of predicted prices and stack them together in matrix $P$ of shape $(n, m)$, where $n$ is the size of one dataset element. Here's what matrix $P$ will look like:\n",
    "\n",
    "$$ P = \\begin{bmatrix}\n",
    "    :  & : & : \\\\\n",
    "    p1  & p2 & pm \\\\\n",
    "    :  & : & : \n",
    "\\end{bmatrix}\\;\\;\\;\n",
    "$$\n",
    "\n",
    "With given vector $Y$ of actual prices we want to combine our $m$ predictions in weighted average prediction (WAP) to get the lowest RMSE as follows:\n",
    "\n",
    "\n",
    "\n",
    "$$ WAP_m = \\frac{\\sum_{i=1}^m p_i A_i}{\\sum_{i=1}^m A_i}\n",
    "$$\n",
    "\n",
    "If you are familiar with linear algebra maybe you've noticed that previous formula can be vectorized:\n",
    "$$ WAP_m = P A ,\n",
    "$$\n",
    "$$ A = (A_1 , ..., A_m)^T\n",
    "$$\n",
    "\n",
    "To tune parameters A we will now implement computational graph in tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "_cell_guid": "26f50ea8-eb6d-41c8-8379-0cf3a17558e6",
    "_execution_state": "idle",
    "_uuid": "04deb25d97eabae70ad561edd4eb4b696a37ee08",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, m):\n",
    "    \"\"\"\n",
    "    Creates placeholdes for P and Y.\n",
    "    \n",
    "    Arguments:\n",
    "        n_x -- size of one sample element\n",
    "        m -- number of regression models\n",
    "    Returns:\n",
    "        P -- placeholder for P\n",
    "        Y -- placeholder for Y\n",
    "        \"\"\"\n",
    "    P = tf.placeholder(tf.float32, name=\"Preds\", shape=[n_x, m])\n",
    "    Y = tf.placeholder(tf.float32, name=\"Price\", shape=[n_x, 1])\n",
    "    return P, Y\n",
    "\n",
    "def compute_cost(P, A, Y, lmbda=0.8):\n",
    "    \"\"\"\n",
    "    Computes cost between predicted prices and actual ones.\n",
    "    \n",
    "    Arguments:\n",
    "        P -- matrix of stacked predictions\n",
    "        A -- vector of parameters\n",
    "        Y -- actual prices\n",
    "        lmbda -- regularazation parameter\n",
    "    Returns:\n",
    "        loss -- mean squared error + L1-regularazation\n",
    "    \"\"\"\n",
    "    prediction = tf.matmul(P, A) / tf.reduce_sum(A) # this is formula for WAP\n",
    "    \n",
    "    # L1-regularazation has shown better score on LB than L2\n",
    "    loss = tf.reduce_mean(tf.squared_difference(prediction, Y)) + lmbda*tf.reduce_mean(tf.abs(A))\n",
    "    return loss\n",
    "\n",
    "def initialize_parameters(m):\n",
    "    print(\"m\",m)\n",
    "    \"\"\"\n",
    "    Initializes parameters A with ones.\n",
    "    \n",
    "    Arguments:\n",
    "        m -- number of models\n",
    "    Returns:\n",
    "        A -- vector of parameters\n",
    "    \"\"\"\n",
    "    A = tf.get_variable(\"Params\", dtype=tf.float32, \n",
    "                        initializer=tf.constant(np.ones((m,1)).astype(np.float32)))\n",
    "    return A\n",
    "\n",
    "def tuning(preds, actual_price, num_iterations=100):\n",
    "    \"\"\"\n",
    "    Implements gradient descent optimizations for WAP.\n",
    "    \n",
    "    Arguments:\n",
    "        pred -- matrix of stacked predictions P\n",
    "        actual_price -- actual price Y\n",
    "        num_iterations -- number of iterations\n",
    "    Returns:\n",
    "        parameters -- vector A for WAP\n",
    "    \"\"\"\n",
    "    np.random.seed(21)\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    (n_x, m) = preds.shape\n",
    "    costs = []\n",
    "    # create placeholders for P and Y\n",
    "    P,  Y = create_placeholders(n_x, m)\n",
    "    # initialize A\n",
    "    A = initialize_parameters(m)\n",
    "    # define loss as a function of A\n",
    "    loss = compute_cost(P, A, Y)\n",
    "    # Implement Gradient Descent optimization to minimize loss\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # initialize global variables\n",
    "        sess.run(init)\n",
    "        for i in range(num_iterations):\n",
    "            _ , current_cost = sess.run([optimizer, loss], feed_dict={P: preds,\n",
    "                                                                      Y:actual_price})\n",
    "            costs.append(current_cost)\n",
    "        print(\"A\",A)\n",
    "        parameters = sess.run(A)\n",
    "        print(parameters)\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.plot(costs)\n",
    "#     plt.xlabel(\"iterations\")\n",
    "#     plt.ylabel(\"cost\")\n",
    "#     plt.grid(True)\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "_cell_guid": "c6f41a85-9ba3-4d29-9579-469e2ed28bd3",
    "_execution_state": "idle",
    "_uuid": "8f49c17cd8b0af7d00a11ee46c9bbe8614e2b20e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = (ENet, GBoost, KRR, lasso, model_lgb, model_xgb)\n",
    "p = []\n",
    "# Let's train models and stack their predictions in one matrix\n",
    "for model in models:\n",
    "    model.fit(train, y_train)\n",
    "    p.append(model.predict(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "_cell_guid": "1ab79310-9cf6-4021-b165-642e93e7c744",
    "_execution_state": "idle",
    "_uuid": "af257467251f7e776550feacf7b1f62fbef2c7d5",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1458,)\n",
      "<class 'numpy.ndarray'>\n",
      "actual_price [[ 12.24769912]\n",
      " [ 12.10901644]\n",
      " [ 12.31717117]\n",
      " ..., \n",
      " [ 12.49313327]\n",
      " [ 11.86446927]\n",
      " [ 11.90159023]]\n",
      "m 6\n",
      "A <tf.Variable 'Params:0' shape=(6, 1) dtype=float32_ref>\n",
      "[[ 0.05334369]\n",
      " [ 0.07969163]\n",
      " [ 0.0617164 ]\n",
      " [ 0.05325885]\n",
      " [ 0.06762624]\n",
      " [ 0.08161904]]\n",
      "params [[ 0.05334369]\n",
      " [ 0.07969163]\n",
      " [ 0.0617164 ]\n",
      " [ 0.05325885]\n",
      " [ 0.06762624]\n",
      " [ 0.08161904]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(type(y_train))\n",
    "p = np.array(p)\n",
    "# transpose p to get P\n",
    "preds = p.T\n",
    "# 标签值\n",
    "actual_price = y_train.reshape(y_train.shape[0], -1)\n",
    "print(\"actual_price\",actual_price)\n",
    "# And finally let's tune parameters!\n",
    "params = tuning(preds, actual_price, 700)\n",
    "print(\"params\",params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "_cell_guid": "fe358813-1c4d-4006-88d4-12ed4f176d58",
    "_execution_state": "idle",
    "_uuid": "614ef51100259b0810114231b396acddd1f1ac5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds [[ 11.68411348  11.73866734  11.67876406  11.68347394  11.6816907\n",
      "   11.7478075 ]\n",
      " [ 11.94276469  12.01039104  12.11291599  11.94195368  11.97548588\n",
      "   11.97410965]\n",
      " [ 12.12601446  12.11465803  12.1081634   12.12638098  12.1304395\n",
      "   12.09677792]\n",
      " ..., \n",
      " [ 12.06887196  11.99214493  12.07128546  12.06860291  12.00319876\n",
      "   12.03083038]\n",
      " [ 11.6738236   11.67696441  11.56008413  11.67363713  11.70578611\n",
      "   11.64300251]\n",
      " [ 12.32809364  12.26151963  12.32436998  12.32913196  12.25621448\n",
      "   12.20633507]]\n",
      "params [[ 11.68411348  11.73866734  11.67876406  11.68347394  11.6816907\n",
      "   11.7478075 ]\n",
      " [ 11.94276469  12.01039104  12.11291599  11.94195368  11.97548588\n",
      "   11.97410965]\n",
      " [ 12.12601446  12.11465803  12.1081634   12.12638098  12.1304395\n",
      "   12.09677792]\n",
      " ..., \n",
      " [ 12.06887196  11.99214493  12.07128546  12.06860291  12.00319876\n",
      "   12.03083038]\n",
      " [ 11.6738236   11.67696441  11.56008413  11.67363713  11.70578611\n",
      "   11.64300251]\n",
      " [ 12.32809364  12.26151963  12.32436998  12.32913196  12.25621448\n",
      "   12.20633507]]\n"
     ]
    }
   ],
   "source": [
    "# And now let's compute WAP on test dataset\n",
    "p = []\n",
    "for model in models:\n",
    "    p.append(model.predict(test))\n",
    "p = np.array(p)\n",
    "preds = p.T\n",
    "print(\"preds\",preds)\n",
    "print(\"params\",preds)\n",
    "WAP = np.squeeze(np.dot(preds, params) / np.sum(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "_cell_guid": "5d8eb446-027f-4740-bd7d-8f02fdfdf983",
    "_execution_state": "idle",
    "_uuid": "207c06955c97aa01e0bc571550d5fd4f4a18b6cf",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1459,)\n"
     ]
    }
   ],
   "source": [
    "WAP\n",
    "print(WAP.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "_cell_guid": "7d83fbfe-f268-4260-b5ff-ada572b46070",
    "_execution_state": "idle",
    "_uuid": "c3fa437868fbafc668c6c5fd2f3c748f8380e85e",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save results to .csv file\n",
    "submit(test_ID, np.exp(WAP))\n",
    "# This gave me 0.11417 on LB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
