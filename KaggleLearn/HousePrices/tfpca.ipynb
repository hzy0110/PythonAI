{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d680f230-0c97-3375-f47a-16cb2cb12e77"
   },
   "source": [
    "**The idea is:**\n",
    "\n",
    " - Feature reduction with PCA\n",
    " - Data transformation (log, hot encoding, nan)\n",
    " - Test different regression models\n",
    "\n",
    "**Things found:**\n",
    "\n",
    "- Applying log transformation really increases the accuracy.\n",
    "- Using PCA with 36 components makes the learning and testing much (much much) faster.\n",
    "- Removing columns with more than 1000 NaNs gives better result than applying \"mean\" to them.\n",
    "- There are outliers. Instead of removing them, using Huber seems to provide a good result. Huber is a model robust to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "c2d0b1a9-a6e3-7ace-ab73-6375fccb210a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_submission.csv\n",
      "test.csv\n",
      "train.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import seaborn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "acc6da78-9491-216e-56c1-61d2f3e6da81"
   },
   "source": [
    "## Data Load ##\n",
    "\n",
    "I mix data and test to manipulate all the data just once. SalePrice is extracted to its own variable \"labels\". Finally, SalesPrice is remove from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "6fad5e5f-097d-0916-8549-8ada7d29cdfe"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv')\n",
    "labels=train[\"SalePrice\"]\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "data = pd.concat([train,test],ignore_index=True)\n",
    "data = data.drop(\"SalePrice\", 1)\n",
    "ids = test[\"Id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "e769e045-f25a-a25d-047a-a03b931e43a4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities    ...     PoolArea PoolQC Fence MiscFeature MiscVal  \\\n",
       "0         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "1         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "2         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "3         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "4         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "\n",
       "  MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0      2   2008        WD         Normal     208500  \n",
       "1      5   2007        WD         Normal     181500  \n",
       "2      9   2008        WD         Normal     223500  \n",
       "3      2   2006        WD        Abnorml     140000  \n",
       "4     12   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "35a5b9cb-3383-ba7b-efb5-4662bdd46c47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1460"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of rows in train\n",
    "train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "041d9472-1f4a-3b15-d624-cfb9ea59710b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2919"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of rows in total\n",
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "eb27ffcc-3f7b-d0e1-3887-e955307c21b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Alley           2721\n",
       "BsmtCond          82\n",
       "BsmtExposure      82\n",
       "BsmtFinSF1         1\n",
       "BsmtFinSF2         1\n",
       "BsmtFinType1      79\n",
       "BsmtFinType2      80\n",
       "BsmtFullBath       2\n",
       "BsmtHalfBath       2\n",
       "BsmtQual          81\n",
       "BsmtUnfSF          1\n",
       "Electrical         1\n",
       "Exterior1st        1\n",
       "Exterior2nd        1\n",
       "Fence           2348\n",
       "FireplaceQu     1420\n",
       "Functional         2\n",
       "GarageArea         1\n",
       "GarageCars         1\n",
       "GarageCond       159\n",
       "GarageFinish     159\n",
       "GarageQual       159\n",
       "GarageType       157\n",
       "GarageYrBlt      159\n",
       "KitchenQual        1\n",
       "LotFrontage      486\n",
       "MSZoning           4\n",
       "MasVnrArea        23\n",
       "MasVnrType        24\n",
       "MiscFeature     2814\n",
       "PoolQC          2909\n",
       "SaleType           1\n",
       "TotalBsmtSF        1\n",
       "Utilities          2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of NaNs each column has.\n",
    "nans=pd.isnull(data).sum()\n",
    "nans[nans>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "92ba1dbf-0b3b-7d19-ba9f-702c59991612"
   },
   "outputs": [],
   "source": [
    "# Remove id and columns with more than a thousand missing values\n",
    "data=data.drop(\"Id\", 1)\n",
    "data=data.drop(\"Alley\", 1)\n",
    "data=data.drop(\"Fence\", 1)\n",
    "data=data.drop(\"MiscFeature\", 1)\n",
    "data=data.drop(\"PoolQC\", 1)\n",
    "data=data.drop(\"FireplaceQu\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "1978e020-6ca4-124e-1063-3c8262fbc3e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "object     38\n",
       "int64      25\n",
       "float64    11\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the column types\n",
    "data.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "82816c00-ff5e-247e-2a55-04f96e2ed313"
   },
   "source": [
    "## Data Manipulation ##\n",
    "\n",
    "- Apply hot encoding, convert categorical variable into dummy/indicator variables.\n",
    "- Fill NaN with median for that column.\n",
    "- Log transformation.\n",
    "- Change -inf to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "24356f20-6c36-de3e-38b1-1a3f4e01f867"
   },
   "outputs": [],
   "source": [
    "all_columns = data.columns.values\n",
    "non_categorical = [\"LotFrontage\", \"LotArea\", \"MasVnrArea\", \"BsmtFinSF1\", \n",
    "                   \"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"1stFlrSF\", \n",
    "                   \"2ndFlrSF\", \"LowQualFinSF\", \"GrLivArea\", \"GarageArea\", \n",
    "                   \"WoodDeckSF\", \"OpenPorchSF\", \"EnclosedPorch\", \"3SsnPorch\", \n",
    "                   \"ScreenPorch\",\"PoolArea\", \"MiscVal\"]\n",
    "\n",
    "categorical = [value for value in all_columns if value not in non_categorical]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "bd7155af-7e2e-12a0-5ef3-030bfc16a945"
   },
   "outputs": [],
   "source": [
    "#Â One Hot Encoding and nan transformation\n",
    "data = pd.get_dummies(data)\n",
    "\n",
    "imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n",
    "data = imp.fit_transform(data)\n",
    "\n",
    "# Log transformation\n",
    "data = np.log(data)\n",
    "labels = np.log(labels)\n",
    "\n",
    "# Change -inf to 0 again\n",
    "data[data==-np.inf]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4affb7d5-578e-6862-243f-630f21a05934"
   },
   "source": [
    "## Feature reduction ##\n",
    "\n",
    "There are many features, so I am going to use PCA to reduce them. The idea is to start with n_components = number of columns. Then select the number of components that add up to 1 variance_ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "6954036d-1415-4bcf-d67b-ac7a410f8a65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.2248857 ,  0.40281429,  0.52425789,  0.62418823,  0.69580422,\n",
       "        0.75944463,  0.8116806 ,  0.85647038,  0.89178708,  0.92273755,\n",
       "        0.94898868,  0.95842727,  0.96637545,  0.97380464,  0.97971901,\n",
       "        0.98501952,  0.98918839,  0.99199181,  0.99386559,  0.99520919,\n",
       "        0.99611479,  0.99695667,  0.99771023,  0.99842564,  0.9989402 ,\n",
       "        0.99933882,  0.99959949,  0.99978254,  0.99988174,  0.99993998,\n",
       "        0.99998599,  0.99999658,  0.99999871,  0.99999943,  0.99999999,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(whiten=True)\n",
    "pca.fit(data)\n",
    "variance = pd.DataFrame(pca.explained_variance_ratio_)\n",
    "np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "e9a015a1-642e-0a50-2219-2e2f92db582a"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=36,whiten=True)\n",
    "pca = pca.fit(data)\n",
    "dataPCA = pca.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8901b708-1465-3e16-aca4-f3ea1bac97d8"
   },
   "source": [
    "## Data Model Selection ##\n",
    "\n",
    "Simple test to run multiple models against our data. First, with raw features. No PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "a2d1aafa-ea0a-5753-8886-581e88fe1cc0"
   },
   "outputs": [],
   "source": [
    "# Split traing and test\n",
    "train = data[:1460]\n",
    "test = data[1460:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a85a2fa0-59ec-c055-f088-ab0a29ea68bd"
   },
   "outputs": [],
   "source": [
    "# R2 Score\n",
    "\n",
    "def lets_try(train,labels):\n",
    "    results={}\n",
    "    def test_model(clf):\n",
    "        \n",
    "        cv = KFold(n_splits=5,shuffle=True,random_state=45)\n",
    "        r2 = make_scorer(r2_score)\n",
    "        r2_val_score = cross_val_score(clf, train, labels, cv=cv,scoring=r2)\n",
    "        scores=[r2_val_score.mean()]\n",
    "        return scores\n",
    "\n",
    "    clf = linear_model.LinearRegression()\n",
    "    results[\"Linear\"]=test_model(clf)\n",
    "    \n",
    "    clf = linear_model.Ridge()\n",
    "    results[\"Ridge\"]=test_model(clf)\n",
    "    \n",
    "    clf = linear_model.BayesianRidge()\n",
    "    results[\"Bayesian Ridge\"]=test_model(clf)\n",
    "    \n",
    "    clf = linear_model.HuberRegressor()\n",
    "    results[\"Hubber\"]=test_model(clf)\n",
    "    \n",
    "    clf = linear_model.Lasso(alpha=1e-4)\n",
    "    results[\"Lasso\"]=test_model(clf)\n",
    "    \n",
    "    clf = BaggingRegressor()\n",
    "    results[\"Bagging\"]=test_model(clf)\n",
    "    \n",
    "    clf = RandomForestRegressor()\n",
    "    results[\"RandomForest\"]=test_model(clf)\n",
    "    \n",
    "    clf = AdaBoostRegressor()\n",
    "    results[\"AdaBoost\"]=test_model(clf)\n",
    "    \n",
    "    clf = svm.SVR()\n",
    "    results[\"SVM RBF\"]=test_model(clf)\n",
    "    \n",
    "    clf = svm.SVR(kernel=\"linear\")\n",
    "    results[\"SVM Linear\"]=test_model(clf)\n",
    "    \n",
    "    results = pd.DataFrame.from_dict(results,orient='index')\n",
    "    results.columns=[\"R Square Score\"] \n",
    "    results=results.sort(columns=[\"R Square Score\"],ascending=False)\n",
    "    results.plot(kind=\"bar\",title=\"Model Scores\")\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([0.5,1])\n",
    "    return results\n",
    "\n",
    "lets_try(train,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f8f5cc3a-09bf-629b-c8de-0c3ef68e226b"
   },
   "source": [
    "Now, let's try the same but using data with PCA applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "af6df7bc-a49f-a5d8-7dc1-391aa0b9b2e3"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lets_try' is not defined",
     "output_type": "error",
     "traceback": [
      "---------------------------------------------------------------------------",
      "NameError                                 Traceback (most recent call last)",
      "<ipython-input-14-e31870f932e1> in <module>()\n      3 test = dataPCA[1460:]\n      4 \n----> 5 lets_try(train,labels)\n",
      "NameError: name 'lets_try' is not defined"
     ]
    }
   ],
   "source": [
    "# Split traing and test\n",
    "train = dataPCA[:1460]\n",
    "test = dataPCA[1460:]\n",
    "\n",
    "lets_try(train,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "77b2423f-1b69-0d57-8cc7-ebf3b6d54b23"
   },
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5,shuffle=True,random_state=45)\n",
    "\n",
    "parameters = {'alpha': [1000,100,10],\n",
    "              'epsilon' : [1.2,1.25,1.50],\n",
    "              'tol' : [1e-10]}\n",
    "\n",
    "clf = linear_model.HuberRegressor()\n",
    "r2 = make_scorer(r2_score)\n",
    "grid_obj = GridSearchCV(clf, parameters, cv=cv,scoring=r2)\n",
    "grid_fit = grid_obj.fit(train, labels)\n",
    "best_clf = grid_fit.best_estimator_ \n",
    "\n",
    "best_clf.fit(train,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8f0e5865-ea95-a696-9d3a-31e0b67315cc"
   },
   "source": [
    "Simple Neural Network\n",
    "---------------------\n",
    "\n",
    "Now I am going to try a simple neural network, to see if i can improve the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "89da5919-66d6-22f7-65cc-f3f8bd3b5c80"
   },
   "outputs": [],
   "source": [
    "# Shape the labels\n",
    "labels_nl = labels\n",
    "labels_nl = labels_nl.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_cell_guid": "c1c49f31-68e9-5364-6c8c-eb1b94109230"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "r2 = tflearn.R2()\n",
    "net = tflearn.input_data(shape=[None, train.shape[1]])\n",
    "net = tflearn.fully_connected(net, 30, activation='linear')\n",
    "net = tflearn.fully_connected(net, 30, activation='linear')\n",
    "net = tflearn.fully_connected(net, 20, activation='linear')\n",
    "net = tflearn.fully_connected(net, 20, activation='linear')\n",
    "net = tflearn.fully_connected(net, 10, activation='linear')\n",
    "net = tflearn.fully_connected(net, 10, activation='linear')\n",
    "net = tflearn.fully_connected(net, 1, activation='linear')\n",
    "sgd = tflearn.SGD(learning_rate=0.1, lr_decay=0.01, decay_step=100)\n",
    "net = tflearn.regression(net, optimizer=sgd, loss='mean_square',metric=r2)\n",
    "model = tflearn.DNN(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_cell_guid": "be5ae562-8e8d-c204-f786-6cb0a065b1d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Run id: XYUAEX\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "INFO:tensorflow:Summary name StandardError/ (raw) is illegal; using StandardError/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 1168\n",
      "Validation samples: 292\n",
      "--\n",
      "Training Step: 1  | total loss: \u001b[1m\u001b[32m132.00549\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 001 | loss: 132.00549 - R2: 0.0000 -- iter: 0020/1168\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m142.06914\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 001 | loss: 142.06914 - R2: 0.0000 -- iter: 0040/1168\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m143.14580\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 001 | loss: 143.14580 - R2: 0.0001 -- iter: 0060/1168\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m141.59778\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 001 | loss: 141.59778 - R2: 0.0001 -- iter: 0080/1168\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m141.59778\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 001 | loss: 141.59778 - R2: 0.0001 -- iter: 0100/1168\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m141.35030\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 001 | loss: 141.35030 - R2: 0.0003 -- iter: 0120/1168\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m139.02881\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 001 | loss: 139.02881 - R2: 0.0005 -- iter: 0140/1168\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m136.77301\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 001 | loss: 136.77301 - R2: 0.0007 -- iter: 0160/1168\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m136.77301\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 001 | loss: 136.77301 - R2: 0.0007 -- iter: 0180/1168\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m136.64172\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 001 | loss: 136.64172 - R2: 0.0011 -- iter: 0200/1168\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m135.29965\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 001 | loss: 135.29965 - R2: 0.0014 -- iter: 0220/1168\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m135.58498\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 001 | loss: 135.58498 - R2: 0.0017 -- iter: 0240/1168\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m134.27527\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 001 | loss: 134.27527 - R2: 0.0020 -- iter: 0260/1168\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m131.37073\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 001 | loss: 131.37073 - R2: 0.0024 -- iter: 0280/1168\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m130.55724\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 001 | loss: 130.55724 - R2: 0.0027 -- iter: 0300/1168\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m128.73183\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 001 | loss: 128.73183 - R2: 0.0032 -- iter: 0320/1168\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m127.49880\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 001 | loss: 127.49880 - R2: 0.0036 -- iter: 0340/1168\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m125.87119\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 001 | loss: 125.87119 - R2: 0.0041 -- iter: 0360/1168\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m124.54195\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 001 | loss: 124.54195 - R2: 0.0046 -- iter: 0380/1168\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m124.54195\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 001 | loss: 124.54195 - R2: 0.0046 -- iter: 0400/1168\n",
      "Training Step: 21  | total loss: \u001b[1m\u001b[32m122.99759\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 001 | loss: 122.99759 - R2: 0.0056 -- iter: 0420/1168\n",
      "Training Step: 22  | total loss: \u001b[1m\u001b[32m123.21742\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 001 | loss: 123.21742 - R2: 0.0061 -- iter: 0440/1168\n",
      "Training Step: 23  | total loss: \u001b[1m\u001b[32m123.21742\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 001 | loss: 123.21742 - R2: 0.0061 -- iter: 0460/1168\n",
      "Training Step: 24  | total loss: \u001b[1m\u001b[32m121.35477\u001b[0m\u001b[0m | time: 0.169s\n",
      "| SGD | epoch: 001 | loss: 121.35477 - R2: 0.0073 -- iter: 0480/1168\n",
      "Training Step: 25  | total loss: \u001b[1m\u001b[32m120.11423\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 001 | loss: 120.11423 - R2: 0.0080 -- iter: 0500/1168\n",
      "Training Step: 26  | total loss: \u001b[1m\u001b[32m118.78580\u001b[0m\u001b[0m | time: 0.182s\n",
      "| SGD | epoch: 001 | loss: 118.78580 - R2: 0.0087 -- iter: 0520/1168\n",
      "Training Step: 27  | total loss: \u001b[1m\u001b[32m118.78580\u001b[0m\u001b[0m | time: 0.188s\n",
      "| SGD | epoch: 001 | loss: 118.78580 - R2: 0.0093 -- iter: 0540/1168\n",
      "Training Step: 28  | total loss: \u001b[1m\u001b[32m118.55836\u001b[0m\u001b[0m | time: 0.193s\n",
      "| SGD | epoch: 001 | loss: 118.55836 - R2: 0.0100 -- iter: 0560/1168\n",
      "Training Step: 29  | total loss: \u001b[1m\u001b[32m118.55836\u001b[0m\u001b[0m | time: 0.196s\n",
      "| SGD | epoch: 001 | loss: 118.55836 - R2: 0.0100 -- iter: 0580/1168\n",
      "Training Step: 30  | total loss: \u001b[1m\u001b[32m116.51406\u001b[0m\u001b[0m | time: 0.201s\n",
      "| SGD | epoch: 001 | loss: 116.51406 - R2: 0.0115 -- iter: 0600/1168\n",
      "Training Step: 31  | total loss: \u001b[1m\u001b[32m114.64999\u001b[0m\u001b[0m | time: 0.204s\n",
      "| SGD | epoch: 001 | loss: 114.64999 - R2: 0.0125 -- iter: 0620/1168\n",
      "Training Step: 32  | total loss: \u001b[1m\u001b[32m114.04209\u001b[0m\u001b[0m | time: 0.207s\n",
      "| SGD | epoch: 001 | loss: 114.04209 - R2: 0.0133 -- iter: 0640/1168\n",
      "Training Step: 33  | total loss: \u001b[1m\u001b[32m112.94991\u001b[0m\u001b[0m | time: 0.211s\n",
      "| SGD | epoch: 001 | loss: 112.94991 - R2: 0.0141 -- iter: 0660/1168\n",
      "Training Step: 34  | total loss: \u001b[1m\u001b[32m112.94991\u001b[0m\u001b[0m | time: 0.213s\n",
      "| SGD | epoch: 001 | loss: 112.94991 - R2: 0.0141 -- iter: 0680/1168\n",
      "Training Step: 35  | total loss: \u001b[1m\u001b[32m110.41103\u001b[0m\u001b[0m | time: 0.216s\n",
      "| SGD | epoch: 001 | loss: 110.41103 - R2: 0.0161 -- iter: 0700/1168\n",
      "Training Step: 36  | total loss: \u001b[1m\u001b[32m110.41103\u001b[0m\u001b[0m | time: 0.218s\n",
      "| SGD | epoch: 001 | loss: 110.41103 - R2: 0.0161 -- iter: 0720/1168\n",
      "Training Step: 37  | total loss: \u001b[1m\u001b[32m108.63972\u001b[0m\u001b[0m | time: 0.225s\n",
      "| SGD | epoch: 001 | loss: 108.63972 - R2: 0.0180 -- iter: 0740/1168\n",
      "Training Step: 38  | total loss: \u001b[1m\u001b[32m108.08496\u001b[0m\u001b[0m | time: 0.229s\n",
      "| SGD | epoch: 001 | loss: 108.08496 - R2: 0.0189 -- iter: 0760/1168\n",
      "Training Step: 39  | total loss: \u001b[1m\u001b[32m108.08496\u001b[0m\u001b[0m | time: 0.231s\n",
      "| SGD | epoch: 001 | loss: 108.08496 - R2: 0.0189 -- iter: 0780/1168\n",
      "Training Step: 40  | total loss: \u001b[1m\u001b[32m104.84836\u001b[0m\u001b[0m | time: 0.234s\n",
      "| SGD | epoch: 001 | loss: 104.84836 - R2: 0.0213 -- iter: 0800/1168\n",
      "Training Step: 41  | total loss: \u001b[1m\u001b[32m104.46510\u001b[0m\u001b[0m | time: 0.236s\n",
      "| SGD | epoch: 001 | loss: 104.46510 - R2: 0.0223 -- iter: 0820/1168\n",
      "Training Step: 42  | total loss: \u001b[1m\u001b[32m103.81110\u001b[0m\u001b[0m | time: 0.239s\n",
      "| SGD | epoch: 001 | loss: 103.81110 - R2: 0.0234 -- iter: 0840/1168\n",
      "Training Step: 43  | total loss: \u001b[1m\u001b[32m102.69569\u001b[0m\u001b[0m | time: 0.243s\n",
      "| SGD | epoch: 001 | loss: 102.69569 - R2: 0.0246 -- iter: 0860/1168\n",
      "Training Step: 44  | total loss: \u001b[1m\u001b[32m101.79852\u001b[0m\u001b[0m | time: 0.248s\n",
      "| SGD | epoch: 001 | loss: 101.79852 - R2: 0.0258 -- iter: 0880/1168\n",
      "Training Step: 45  | total loss: \u001b[1m\u001b[32m101.11236\u001b[0m\u001b[0m | time: 0.251s\n",
      "| SGD | epoch: 001 | loss: 101.11236 - R2: 0.0270 -- iter: 0900/1168\n",
      "Training Step: 46  | total loss: \u001b[1m\u001b[32m100.23212\u001b[0m\u001b[0m | time: 0.254s\n",
      "| SGD | epoch: 001 | loss: 100.23212 - R2: 0.0282 -- iter: 0920/1168\n",
      "Training Step: 47  | total loss: \u001b[1m\u001b[32m99.09179\u001b[0m\u001b[0m | time: 0.258s\n",
      "| SGD | epoch: 001 | loss: 99.09179 - R2: 0.0296 -- iter: 0940/1168\n",
      "Training Step: 48  | total loss: \u001b[1m\u001b[32m99.09179\u001b[0m\u001b[0m | time: 0.261s\n",
      "| SGD | epoch: 001 | loss: 99.09179 - R2: 0.0296 -- iter: 0960/1168\n",
      "Training Step: 49  | total loss: \u001b[1m\u001b[32m97.53632\u001b[0m\u001b[0m | time: 0.266s\n",
      "| SGD | epoch: 001 | loss: 97.53632 - R2: 0.0322 -- iter: 0980/1168\n",
      "Training Step: 50  | total loss: \u001b[1m\u001b[32m96.68098\u001b[0m\u001b[0m | time: 0.269s\n",
      "| SGD | epoch: 001 | loss: 96.68098 - R2: 0.0336 -- iter: 1000/1168\n",
      "Training Step: 51  | total loss: \u001b[1m\u001b[32m96.03718\u001b[0m\u001b[0m | time: 0.272s\n",
      "| SGD | epoch: 001 | loss: 96.03718 - R2: 0.0349 -- iter: 1020/1168\n",
      "Training Step: 52  | total loss: \u001b[1m\u001b[32m95.01181\u001b[0m\u001b[0m | time: 0.277s\n",
      "| SGD | epoch: 001 | loss: 95.01181 - R2: 0.0364 -- iter: 1040/1168\n",
      "Training Step: 53  | total loss: \u001b[1m\u001b[32m93.87105\u001b[0m\u001b[0m | time: 0.280s\n",
      "| SGD | epoch: 001 | loss: 93.87105 - R2: 0.0380 -- iter: 1060/1168\n",
      "Training Step: 54  | total loss: \u001b[1m\u001b[32m93.87105\u001b[0m\u001b[0m | time: 0.284s\n",
      "| SGD | epoch: 001 | loss: 93.87105 - R2: 0.0380 -- iter: 1080/1168\n",
      "Training Step: 55  | total loss: \u001b[1m\u001b[32m93.19240\u001b[0m\u001b[0m | time: 0.286s\n",
      "| SGD | epoch: 001 | loss: 93.19240 - R2: 0.0395 -- iter: 1100/1168\n",
      "Training Step: 56  | total loss: \u001b[1m\u001b[32m92.35426\u001b[0m\u001b[0m | time: 0.288s\n",
      "| SGD | epoch: 001 | loss: 92.35426 - R2: 0.0411 -- iter: 1120/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 57  | total loss: \u001b[1m\u001b[32m91.51981\u001b[0m\u001b[0m | time: 0.291s\n",
      "| SGD | epoch: 001 | loss: 91.51981 - R2: 0.0426 -- iter: 1140/1168\n",
      "Training Step: 58  | total loss: \u001b[1m\u001b[32m89.66460\u001b[0m\u001b[0m | time: 0.294s\n",
      "| SGD | epoch: 001 | loss: 89.66460 - R2: 0.0460 -- iter: 1160/1168\n",
      "Training Step: 59  | total loss: \u001b[1m\u001b[32m88.88686\u001b[0m\u001b[0m | time: 1.316s\n",
      "| SGD | epoch: 001 | loss: 88.88686 - R2: 0.0477 | val_loss: 81.62693 - val_acc: 0.0617 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 60  | total loss: \u001b[1m\u001b[32m88.08608\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 002 | loss: 88.08608 - R2: 0.0495 -- iter: 0020/1168\n",
      "Training Step: 61  | total loss: \u001b[1m\u001b[32m86.81692\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 002 | loss: 86.81692 - R2: 0.0515 -- iter: 0040/1168\n",
      "Training Step: 62  | total loss: \u001b[1m\u001b[32m85.67319\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 002 | loss: 85.67319 - R2: 0.0536 -- iter: 0060/1168\n",
      "Training Step: 63  | total loss: \u001b[1m\u001b[32m84.95834\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 002 | loss: 84.95834 - R2: 0.0554 -- iter: 0080/1168\n",
      "Training Step: 64  | total loss: \u001b[1m\u001b[32m83.99094\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 002 | loss: 83.99094 - R2: 0.0575 -- iter: 0100/1168\n",
      "Training Step: 65  | total loss: \u001b[1m\u001b[32m83.40006\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 002 | loss: 83.40006 - R2: 0.0593 -- iter: 0120/1168\n",
      "Training Step: 66  | total loss: \u001b[1m\u001b[32m83.40006\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 002 | loss: 83.40006 - R2: 0.0593 -- iter: 0140/1168\n",
      "Training Step: 67  | total loss: \u001b[1m\u001b[32m82.68572\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 002 | loss: 82.68572 - R2: 0.0613 -- iter: 0160/1168\n",
      "Training Step: 68  | total loss: \u001b[1m\u001b[32m81.66340\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 002 | loss: 81.66340 - R2: 0.0658 -- iter: 0180/1168\n",
      "Training Step: 69  | total loss: \u001b[1m\u001b[32m80.61740\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 002 | loss: 80.61740 - R2: 0.0658 -- iter: 0200/1168\n",
      "Training Step: 70  | total loss: \u001b[1m\u001b[32m78.86035\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 002 | loss: 78.86035 - R2: 0.0705 -- iter: 0220/1168\n",
      "Training Step: 71  | total loss: \u001b[1m\u001b[32m77.89348\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 002 | loss: 77.89348 - R2: 0.0705 -- iter: 0240/1168\n",
      "Training Step: 72  | total loss: \u001b[1m\u001b[32m77.89348\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 002 | loss: 77.89348 - R2: 0.0729 -- iter: 0260/1168\n",
      "Training Step: 73  | total loss: \u001b[1m\u001b[32m76.66048\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 002 | loss: 76.66048 - R2: 0.0757 -- iter: 0280/1168\n",
      "Training Step: 74  | total loss: \u001b[1m\u001b[32m75.83926\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 002 | loss: 75.83926 - R2: 0.0782 -- iter: 0300/1168\n",
      "Training Step: 75  | total loss: \u001b[1m\u001b[32m73.97814\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 002 | loss: 73.97814 - R2: 0.0835 -- iter: 0320/1168\n",
      "Training Step: 76  | total loss: \u001b[1m\u001b[32m73.97814\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 002 | loss: 73.97814 - R2: 0.0835 -- iter: 0340/1168\n",
      "Training Step: 77  | total loss: \u001b[1m\u001b[32m73.34944\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 002 | loss: 73.34944 - R2: 0.0860 -- iter: 0360/1168\n",
      "Training Step: 78  | total loss: \u001b[1m\u001b[32m71.81625\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 002 | loss: 71.81625 - R2: 0.0914 -- iter: 0380/1168\n",
      "Training Step: 79  | total loss: \u001b[1m\u001b[32m70.69418\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 002 | loss: 70.69418 - R2: 0.0947 -- iter: 0400/1168\n",
      "Training Step: 80  | total loss: \u001b[1m\u001b[32m69.51952\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 002 | loss: 69.51952 - R2: 0.0982 -- iter: 0420/1168\n",
      "Training Step: 81  | total loss: \u001b[1m\u001b[32m68.44572\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 002 | loss: 68.44572 - R2: 0.1015 -- iter: 0440/1168\n",
      "Training Step: 82  | total loss: \u001b[1m\u001b[32m67.41731\u001b[0m\u001b[0m | time: 0.165s\n",
      "| SGD | epoch: 002 | loss: 67.41731 - R2: 0.1050 -- iter: 0460/1168\n",
      "Training Step: 83  | total loss: \u001b[1m\u001b[32m66.31936\u001b[0m\u001b[0m | time: 0.173s\n",
      "| SGD | epoch: 002 | loss: 66.31936 - R2: 0.1087 -- iter: 0480/1168\n",
      "Training Step: 84  | total loss: \u001b[1m\u001b[32m65.05972\u001b[0m\u001b[0m | time: 0.190s\n",
      "| SGD | epoch: 002 | loss: 65.05972 - R2: 0.1127 -- iter: 0500/1168\n",
      "Training Step: 85  | total loss: \u001b[1m\u001b[32m63.97525\u001b[0m\u001b[0m | time: 0.200s\n",
      "| SGD | epoch: 002 | loss: 63.97525 - R2: 0.1166 -- iter: 0520/1168\n",
      "Training Step: 86  | total loss: \u001b[1m\u001b[32m62.92307\u001b[0m\u001b[0m | time: 0.205s\n",
      "| SGD | epoch: 002 | loss: 62.92307 - R2: 0.1206 -- iter: 0540/1168\n",
      "Training Step: 87  | total loss: \u001b[1m\u001b[32m61.75846\u001b[0m\u001b[0m | time: 0.208s\n",
      "| SGD | epoch: 002 | loss: 61.75846 - R2: 0.1249 -- iter: 0560/1168\n",
      "Training Step: 88  | total loss: \u001b[1m\u001b[32m61.75846\u001b[0m\u001b[0m | time: 0.211s\n",
      "| SGD | epoch: 002 | loss: 61.75846 - R2: 0.1249 -- iter: 0580/1168\n",
      "Training Step: 89  | total loss: \u001b[1m\u001b[32m60.74476\u001b[0m\u001b[0m | time: 0.213s\n",
      "| SGD | epoch: 002 | loss: 60.74476 - R2: 0.1338 -- iter: 0600/1168\n",
      "Training Step: 90  | total loss: \u001b[1m\u001b[32m59.51734\u001b[0m\u001b[0m | time: 0.216s\n",
      "| SGD | epoch: 002 | loss: 59.51734 - R2: 0.1338 -- iter: 0620/1168\n",
      "Training Step: 91  | total loss: \u001b[1m\u001b[32m58.28112\u001b[0m\u001b[0m | time: 0.220s\n",
      "| SGD | epoch: 002 | loss: 58.28112 - R2: 0.1388 -- iter: 0640/1168\n",
      "Training Step: 92  | total loss: \u001b[1m\u001b[32m55.93115\u001b[0m\u001b[0m | time: 0.223s\n",
      "| SGD | epoch: 002 | loss: 55.93115 - R2: 0.1488 -- iter: 0660/1168\n",
      "Training Step: 93  | total loss: \u001b[1m\u001b[32m54.85853\u001b[0m\u001b[0m | time: 0.226s\n",
      "| SGD | epoch: 002 | loss: 54.85853 - R2: 0.1538 -- iter: 0680/1168\n",
      "Training Step: 94  | total loss: \u001b[1m\u001b[32m53.82616\u001b[0m\u001b[0m | time: 0.232s\n",
      "| SGD | epoch: 002 | loss: 53.82616 - R2: 0.1588 -- iter: 0700/1168\n",
      "Training Step: 95  | total loss: \u001b[1m\u001b[32m53.82616\u001b[0m\u001b[0m | time: 0.236s\n",
      "| SGD | epoch: 002 | loss: 53.82616 - R2: 0.1588 -- iter: 0720/1168\n",
      "Training Step: 96  | total loss: \u001b[1m\u001b[32m51.36569\u001b[0m\u001b[0m | time: 0.239s\n",
      "| SGD | epoch: 002 | loss: 51.36569 - R2: 0.1705 -- iter: 0740/1168\n",
      "Training Step: 97  | total loss: \u001b[1m\u001b[32m50.19838\u001b[0m\u001b[0m | time: 0.265s\n",
      "| SGD | epoch: 002 | loss: 50.19838 - R2: 0.1764 -- iter: 0760/1168\n",
      "Training Step: 98  | total loss: \u001b[1m\u001b[32m48.88992\u001b[0m\u001b[0m | time: 0.277s\n",
      "| SGD | epoch: 002 | loss: 48.88992 - R2: 0.1830 -- iter: 0780/1168\n",
      "Training Step: 99  | total loss: \u001b[1m\u001b[32m47.59296\u001b[0m\u001b[0m | time: 0.281s\n",
      "| SGD | epoch: 002 | loss: 47.59296 - R2: 0.1897 -- iter: 0800/1168\n",
      "Training Step: 100  | total loss: \u001b[1m\u001b[32m47.59296\u001b[0m\u001b[0m | time: 0.287s\n",
      "| SGD | epoch: 002 | loss: 47.59296 - R2: 0.1897 -- iter: 0820/1168\n",
      "Training Step: 101  | total loss: \u001b[1m\u001b[32m46.31345\u001b[0m\u001b[0m | time: 0.289s\n",
      "| SGD | epoch: 002 | loss: 46.31345 - R2: 0.1966 -- iter: 0840/1168\n",
      "Training Step: 102  | total loss: \u001b[1m\u001b[32m45.13036\u001b[0m\u001b[0m | time: 0.298s\n",
      "| SGD | epoch: 002 | loss: 45.13036 - R2: 0.2034 -- iter: 0860/1168\n",
      "Training Step: 103  | total loss: \u001b[1m\u001b[32m43.85184\u001b[0m\u001b[0m | time: 0.300s\n",
      "| SGD | epoch: 002 | loss: 43.85184 - R2: 0.2108 -- iter: 0880/1168\n",
      "Training Step: 104  | total loss: \u001b[1m\u001b[32m42.52740\u001b[0m\u001b[0m | time: 0.303s\n",
      "| SGD | epoch: 002 | loss: 42.52740 - R2: 0.2186 -- iter: 0900/1168\n",
      "Training Step: 105  | total loss: \u001b[1m\u001b[32m41.28120\u001b[0m\u001b[0m | time: 0.305s\n",
      "| SGD | epoch: 002 | loss: 41.28120 - R2: 0.2264 -- iter: 0920/1168\n",
      "Training Step: 106  | total loss: \u001b[1m\u001b[32m39.98027\u001b[0m\u001b[0m | time: 0.307s\n",
      "| SGD | epoch: 002 | loss: 39.98027 - R2: 0.2347 -- iter: 0940/1168\n",
      "Training Step: 107  | total loss: \u001b[1m\u001b[32m38.73932\u001b[0m\u001b[0m | time: 0.309s\n",
      "| SGD | epoch: 002 | loss: 38.73932 - R2: 0.2429 -- iter: 0960/1168\n",
      "Training Step: 108  | total loss: \u001b[1m\u001b[32m36.24208\u001b[0m\u001b[0m | time: 0.312s\n",
      "| SGD | epoch: 002 | loss: 36.24208 - R2: 0.2604 -- iter: 0980/1168\n",
      "Training Step: 109  | total loss: \u001b[1m\u001b[32m36.24208\u001b[0m\u001b[0m | time: 0.321s\n",
      "| SGD | epoch: 002 | loss: 36.24208 - R2: 0.2604 -- iter: 1000/1168\n",
      "Training Step: 110  | total loss: \u001b[1m\u001b[32m35.16944\u001b[0m\u001b[0m | time: 0.330s\n",
      "| SGD | epoch: 002 | loss: 35.16944 - R2: 0.2687 -- iter: 1020/1168\n",
      "Training Step: 111  | total loss: \u001b[1m\u001b[32m33.89053\u001b[0m\u001b[0m | time: 0.337s\n",
      "| SGD | epoch: 002 | loss: 33.89053 - R2: 0.2784 -- iter: 1040/1168\n",
      "Training Step: 112  | total loss: \u001b[1m\u001b[32m31.43614\u001b[0m\u001b[0m | time: 0.344s\n",
      "| SGD | epoch: 002 | loss: 31.43614 - R2: 0.2984 -- iter: 1060/1168\n",
      "Training Step: 113  | total loss: \u001b[1m\u001b[32m30.23280\u001b[0m\u001b[0m | time: 0.352s\n",
      "| SGD | epoch: 002 | loss: 30.23280 - R2: 0.3087 -- iter: 1080/1168\n",
      "Training Step: 114  | total loss: \u001b[1m\u001b[32m29.01768\u001b[0m\u001b[0m | time: 0.363s\n",
      "| SGD | epoch: 002 | loss: 29.01768 - R2: 0.3194 -- iter: 1100/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 115  | total loss: \u001b[1m\u001b[32m27.83018\u001b[0m\u001b[0m | time: 0.367s\n",
      "| SGD | epoch: 002 | loss: 27.83018 - R2: 0.3305 -- iter: 1120/1168\n",
      "Training Step: 116  | total loss: \u001b[1m\u001b[32m26.70437\u001b[0m\u001b[0m | time: 0.371s\n",
      "| SGD | epoch: 002 | loss: 26.70437 - R2: 0.3413 -- iter: 1140/1168\n",
      "Training Step: 117  | total loss: \u001b[1m\u001b[32m26.70437\u001b[0m\u001b[0m | time: 0.375s\n",
      "| SGD | epoch: 002 | loss: 26.70437 - R2: 0.3413 -- iter: 1160/1168\n",
      "Training Step: 118  | total loss: \u001b[1m\u001b[32m25.62698\u001b[0m\u001b[0m | time: 1.380s\n",
      "| SGD | epoch: 002 | loss: 25.62698 - R2: 0.3523 | val_loss: 13.28438 - val_acc: 0.4872 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 119  | total loss: \u001b[1m\u001b[32m24.54867\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 003 | loss: 24.54867 - R2: 0.3765 -- iter: 0020/1168\n",
      "Training Step: 120  | total loss: \u001b[1m\u001b[32m22.19878\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 003 | loss: 22.19878 - R2: 0.3900 -- iter: 0040/1168\n",
      "Training Step: 121  | total loss: \u001b[1m\u001b[32m21.05578\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 003 | loss: 21.05578 - R2: 0.4038 -- iter: 0060/1168\n",
      "Training Step: 122  | total loss: \u001b[1m\u001b[32m20.05187\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 003 | loss: 20.05187 - R2: 0.4160 -- iter: 0080/1168\n",
      "Training Step: 123  | total loss: \u001b[1m\u001b[32m18.98696\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 003 | loss: 18.98696 - R2: 0.4299 -- iter: 0100/1168\n",
      "Training Step: 124  | total loss: \u001b[1m\u001b[32m18.98696\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 003 | loss: 18.98696 - R2: 0.4299 -- iter: 0120/1168\n",
      "Training Step: 125  | total loss: \u001b[1m\u001b[32m17.95284\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 003 | loss: 17.95284 - R2: 0.4576 -- iter: 0140/1168\n",
      "Training Step: 126  | total loss: \u001b[1m\u001b[32m16.12757\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 003 | loss: 16.12757 - R2: 0.4707 -- iter: 0160/1168\n",
      "Training Step: 127  | total loss: \u001b[1m\u001b[32m15.19542\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 003 | loss: 15.19542 - R2: 0.4854 -- iter: 0180/1168\n",
      "Training Step: 128  | total loss: \u001b[1m\u001b[32m14.22825\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 003 | loss: 14.22825 - R2: 0.5018 -- iter: 0200/1168\n",
      "Training Step: 129  | total loss: \u001b[1m\u001b[32m14.22825\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 003 | loss: 14.22825 - R2: 0.5018 -- iter: 0220/1168\n",
      "Training Step: 130  | total loss: \u001b[1m\u001b[32m13.33546\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 003 | loss: 13.33546 - R2: 0.5357 -- iter: 0240/1168\n",
      "Training Step: 131  | total loss: \u001b[1m\u001b[32m11.49526\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 003 | loss: 11.49526 - R2: 0.5539 -- iter: 0260/1168\n",
      "Training Step: 132  | total loss: \u001b[1m\u001b[32m10.65119\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 003 | loss: 10.65119 - R2: 0.5720 -- iter: 0280/1168\n",
      "Training Step: 133  | total loss: \u001b[1m\u001b[32m9.82859\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 003 | loss: 9.82859 - R2: 0.5909 -- iter: 0300/1168\n",
      "Training Step: 134  | total loss: \u001b[1m\u001b[32m9.82859\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 003 | loss: 9.82859 - R2: 0.5909 -- iter: 0320/1168\n",
      "Training Step: 135  | total loss: \u001b[1m\u001b[32m9.07675\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 003 | loss: 9.07675 - R2: 0.6094 -- iter: 0340/1168\n",
      "Training Step: 136  | total loss: \u001b[1m\u001b[32m7.64165\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 003 | loss: 7.64165 - R2: 0.6480 -- iter: 0360/1168\n",
      "Training Step: 137  | total loss: \u001b[1m\u001b[32m7.64165\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 003 | loss: 7.64165 - R2: 0.6480 -- iter: 0380/1168\n",
      "Training Step: 138  | total loss: \u001b[1m\u001b[32m6.97469\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 003 | loss: 6.97469 - R2: 0.6901 -- iter: 0400/1168\n",
      "Training Step: 139  | total loss: \u001b[1m\u001b[32m6.33191\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 003 | loss: 6.33191 - R2: 0.6901 -- iter: 0420/1168\n",
      "Training Step: 140  | total loss: \u001b[1m\u001b[32m5.74342\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 003 | loss: 5.74342 - R2: 0.7120 -- iter: 0440/1168\n",
      "Training Step: 141  | total loss: \u001b[1m\u001b[32m5.19168\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 003 | loss: 5.19168 - R2: 0.7341 -- iter: 0460/1168\n",
      "Training Step: 142  | total loss: \u001b[1m\u001b[32m4.69185\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 003 | loss: 4.69185 - R2: 0.7552 -- iter: 0480/1168\n",
      "Training Step: 143  | total loss: \u001b[1m\u001b[32m4.23775\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 003 | loss: 4.23775 - R2: 0.7787 -- iter: 0500/1168\n",
      "Training Step: 144  | total loss: \u001b[1m\u001b[32m3.82417\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 003 | loss: 3.82417 - R2: 0.7981 -- iter: 0520/1168\n",
      "Training Step: 145  | total loss: \u001b[1m\u001b[32m3.44649\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 003 | loss: 3.44649 - R2: 0.8180 -- iter: 0540/1168\n",
      "Training Step: 146  | total loss: \u001b[1m\u001b[32m3.11467\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 003 | loss: 3.11467 - R2: 0.8334 -- iter: 0560/1168\n",
      "Training Step: 147  | total loss: \u001b[1m\u001b[32m2.81255\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 003 | loss: 2.81255 - R2: 0.8647 -- iter: 0580/1168\n",
      "Training Step: 148  | total loss: \u001b[1m\u001b[32m2.53445\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 003 | loss: 2.53445 - R2: 0.8647 -- iter: 0600/1168\n",
      "Training Step: 149  | total loss: \u001b[1m\u001b[32m2.28369\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 003 | loss: 2.28369 - R2: 0.8894 -- iter: 0620/1168\n",
      "Training Step: 150  | total loss: \u001b[1m\u001b[32m2.06285\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 003 | loss: 2.06285 - R2: 0.8894 -- iter: 0640/1168\n",
      "Training Step: 151  | total loss: \u001b[1m\u001b[32m1.86117\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 003 | loss: 1.86117 - R2: 0.9010 -- iter: 0660/1168\n",
      "Training Step: 152  | total loss: \u001b[1m\u001b[32m1.68016\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 003 | loss: 1.68016 - R2: 0.9134 -- iter: 0680/1168\n",
      "Training Step: 153  | total loss: \u001b[1m\u001b[32m1.51932\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 003 | loss: 1.51932 - R2: 0.9224 -- iter: 0700/1168\n",
      "Training Step: 154  | total loss: \u001b[1m\u001b[32m1.37101\u001b[0m\u001b[0m | time: 0.178s\n",
      "| SGD | epoch: 003 | loss: 1.37101 - R2: 0.9302 -- iter: 0720/1168\n",
      "Training Step: 155  | total loss: \u001b[1m\u001b[32m1.23728\u001b[0m\u001b[0m | time: 0.180s\n",
      "| SGD | epoch: 003 | loss: 1.23728 - R2: 0.9381 -- iter: 0740/1168\n",
      "Training Step: 156  | total loss: \u001b[1m\u001b[32m1.11844\u001b[0m\u001b[0m | time: 0.184s\n",
      "| SGD | epoch: 003 | loss: 1.11844 - R2: 0.9430 -- iter: 0760/1168\n",
      "Training Step: 157  | total loss: \u001b[1m\u001b[32m1.01277\u001b[0m\u001b[0m | time: 0.187s\n",
      "| SGD | epoch: 003 | loss: 1.01277 - R2: 0.9491 -- iter: 0780/1168\n",
      "Training Step: 158  | total loss: \u001b[1m\u001b[32m0.91487\u001b[0m\u001b[0m | time: 0.189s\n",
      "| SGD | epoch: 003 | loss: 0.91487 - R2: 0.9525 -- iter: 0800/1168\n",
      "Training Step: 159  | total loss: \u001b[1m\u001b[32m0.82618\u001b[0m\u001b[0m | time: 0.193s\n",
      "| SGD | epoch: 003 | loss: 0.82618 - R2: 0.9569 -- iter: 0820/1168\n",
      "Training Step: 160  | total loss: \u001b[1m\u001b[32m0.74702\u001b[0m\u001b[0m | time: 0.196s\n",
      "| SGD | epoch: 003 | loss: 0.74702 - R2: 0.9600 -- iter: 0840/1168\n",
      "Training Step: 161  | total loss: \u001b[1m\u001b[32m0.68251\u001b[0m\u001b[0m | time: 0.200s\n",
      "| SGD | epoch: 003 | loss: 0.68251 - R2: 0.9657 -- iter: 0860/1168\n",
      "Training Step: 162  | total loss: \u001b[1m\u001b[32m0.61836\u001b[0m\u001b[0m | time: 0.203s\n",
      "| SGD | epoch: 003 | loss: 0.61836 - R2: 0.9715 -- iter: 0880/1168\n",
      "Training Step: 163  | total loss: \u001b[1m\u001b[32m0.50790\u001b[0m\u001b[0m | time: 0.205s\n",
      "| SGD | epoch: 003 | loss: 0.50790 - R2: 0.9740 -- iter: 0900/1168\n",
      "Training Step: 164  | total loss: \u001b[1m\u001b[32m0.50790\u001b[0m\u001b[0m | time: 0.209s\n",
      "| SGD | epoch: 003 | loss: 0.50790 - R2: 0.9740 -- iter: 0920/1168\n",
      "Training Step: 165  | total loss: \u001b[1m\u001b[32m0.41849\u001b[0m\u001b[0m | time: 0.212s\n",
      "| SGD | epoch: 003 | loss: 0.41849 - R2: 0.9804 -- iter: 0940/1168\n",
      "Training Step: 166  | total loss: \u001b[1m\u001b[32m0.37876\u001b[0m\u001b[0m | time: 0.215s\n",
      "| SGD | epoch: 003 | loss: 0.37876 - R2: 0.9818 -- iter: 0960/1168\n",
      "Training Step: 167  | total loss: \u001b[1m\u001b[32m0.34497\u001b[0m\u001b[0m | time: 0.225s\n",
      "| SGD | epoch: 003 | loss: 0.34497 - R2: 0.9833 -- iter: 0980/1168\n",
      "Training Step: 168  | total loss: \u001b[1m\u001b[32m0.34497\u001b[0m\u001b[0m | time: 0.228s\n",
      "| SGD | epoch: 003 | loss: 0.34497 - R2: 0.9856 -- iter: 1000/1168\n",
      "Training Step: 169  | total loss: \u001b[1m\u001b[32m0.28612\u001b[0m\u001b[0m | time: 0.231s\n",
      "| SGD | epoch: 003 | loss: 0.28612 - R2: 0.9869 -- iter: 1020/1168\n",
      "Training Step: 170  | total loss: \u001b[1m\u001b[32m0.25986\u001b[0m\u001b[0m | time: 0.236s\n",
      "| SGD | epoch: 003 | loss: 0.25986 - R2: 0.9879 -- iter: 1040/1168\n",
      "Training Step: 171  | total loss: \u001b[1m\u001b[32m0.23598\u001b[0m\u001b[0m | time: 0.240s\n",
      "| SGD | epoch: 003 | loss: 0.23598 - R2: 0.9889 -- iter: 1060/1168\n",
      "Training Step: 172  | total loss: \u001b[1m\u001b[32m0.23598\u001b[0m\u001b[0m | time: 0.243s\n",
      "| SGD | epoch: 003 | loss: 0.23598 - R2: 0.9889 -- iter: 1080/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 173  | total loss: \u001b[1m\u001b[32m0.21417\u001b[0m\u001b[0m | time: 0.245s\n",
      "| SGD | epoch: 003 | loss: 0.21417 - R2: 0.9901 -- iter: 1100/1168\n",
      "Training Step: 174  | total loss: \u001b[1m\u001b[32m0.17586\u001b[0m\u001b[0m | time: 0.248s\n",
      "| SGD | epoch: 003 | loss: 0.17586 - R2: 0.9920 -- iter: 1120/1168\n",
      "Training Step: 175  | total loss: \u001b[1m\u001b[32m0.17586\u001b[0m\u001b[0m | time: 0.251s\n",
      "| SGD | epoch: 003 | loss: 0.17586 - R2: 0.9920 -- iter: 1140/1168\n",
      "Training Step: 176  | total loss: \u001b[1m\u001b[32m0.14538\u001b[0m\u001b[0m | time: 0.253s\n",
      "| SGD | epoch: 003 | loss: 0.14538 - R2: 0.9931 -- iter: 1160/1168\n",
      "Training Step: 177  | total loss: \u001b[1m\u001b[32m0.13345\u001b[0m\u001b[0m | time: 1.266s\n",
      "| SGD | epoch: 003 | loss: 0.13345 - R2: 0.9943 | val_loss: 0.03061 - val_acc: 1.0001 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 178  | total loss: \u001b[1m\u001b[32m0.12247\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 004 | loss: 0.12247 - R2: 0.9941 -- iter: 0020/1168\n",
      "Training Step: 179  | total loss: \u001b[1m\u001b[32m0.12247\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 004 | loss: 0.12247 - R2: 0.9941 -- iter: 0040/1168\n",
      "Training Step: 180  | total loss: \u001b[1m\u001b[32m0.10866\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 004 | loss: 0.10866 - R2: 0.9966 -- iter: 0060/1168\n",
      "Training Step: 181  | total loss: \u001b[1m\u001b[32m0.10866\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 004 | loss: 0.10866 - R2: 0.9965 -- iter: 0080/1168\n",
      "Training Step: 182  | total loss: \u001b[1m\u001b[32m0.09530\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 004 | loss: 0.09530 - R2: 0.9971 -- iter: 0100/1168\n",
      "Training Step: 183  | total loss: \u001b[1m\u001b[32m0.09530\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 004 | loss: 0.09530 - R2: 0.9971 -- iter: 0120/1168\n",
      "Training Step: 184  | total loss: \u001b[1m\u001b[32m0.08817\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 004 | loss: 0.08817 - R2: 0.9980 -- iter: 0140/1168\n",
      "Training Step: 185  | total loss: \u001b[1m\u001b[32m0.08204\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 004 | loss: 0.08204 - R2: 0.9974 -- iter: 0160/1168\n",
      "Training Step: 186  | total loss: \u001b[1m\u001b[32m0.07484\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 004 | loss: 0.07484 - R2: 0.9977 -- iter: 0180/1168\n",
      "Training Step: 187  | total loss: \u001b[1m\u001b[32m0.06379\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 004 | loss: 0.06379 - R2: 0.9980 -- iter: 0200/1168\n",
      "Training Step: 188  | total loss: \u001b[1m\u001b[32m0.06379\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 004 | loss: 0.06379 - R2: 0.9979 -- iter: 0220/1168\n",
      "Training Step: 189  | total loss: \u001b[1m\u001b[32m0.05499\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 004 | loss: 0.05499 - R2: 0.9980 -- iter: 0240/1168\n",
      "Training Step: 190  | total loss: \u001b[1m\u001b[32m0.05241\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 004 | loss: 0.05241 - R2: 0.9968 -- iter: 0260/1168\n",
      "Training Step: 191  | total loss: \u001b[1m\u001b[32m0.04942\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 004 | loss: 0.04942 - R2: 0.9980 -- iter: 0280/1168\n",
      "Training Step: 192  | total loss: \u001b[1m\u001b[32m0.04942\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 004 | loss: 0.04942 - R2: 0.9980 -- iter: 0300/1168\n",
      "Training Step: 193  | total loss: \u001b[1m\u001b[32m0.04377\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 004 | loss: 0.04377 - R2: 0.9992 -- iter: 0320/1168\n",
      "Training Step: 194  | total loss: \u001b[1m\u001b[32m0.04377\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 004 | loss: 0.04377 - R2: 0.9992 -- iter: 0340/1168\n",
      "Training Step: 195  | total loss: \u001b[1m\u001b[32m0.03924\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 004 | loss: 0.03924 - R2: 0.9985 -- iter: 0360/1168\n",
      "Training Step: 196  | total loss: \u001b[1m\u001b[32m0.03924\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 004 | loss: 0.03924 - R2: 0.9985 -- iter: 0380/1168\n",
      "Training Step: 197  | total loss: \u001b[1m\u001b[32m0.03677\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 004 | loss: 0.03677 - R2: 0.9991 -- iter: 0400/1168\n",
      "Training Step: 198  | total loss: \u001b[1m\u001b[32m0.03548\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 004 | loss: 0.03548 - R2: 0.9991 -- iter: 0420/1168\n",
      "Training Step: 199  | total loss: \u001b[1m\u001b[32m0.03275\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 004 | loss: 0.03275 - R2: 0.9995 -- iter: 0440/1168\n",
      "Training Step: 200  | total loss: \u001b[1m\u001b[32m0.03087\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 004 | loss: 0.03087 - R2: 0.9999 -- iter: 0460/1168\n",
      "Training Step: 201  | total loss: \u001b[1m\u001b[32m0.03038\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 004 | loss: 0.03038 - R2: 1.0004 -- iter: 0480/1168\n",
      "Training Step: 202  | total loss: \u001b[1m\u001b[32m0.02902\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 004 | loss: 0.02902 - R2: 0.9995 -- iter: 0500/1168\n",
      "Training Step: 203  | total loss: \u001b[1m\u001b[32m0.02709\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 004 | loss: 0.02709 - R2: 0.9997 -- iter: 0520/1168\n",
      "Training Step: 204  | total loss: \u001b[1m\u001b[32m0.02510\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 004 | loss: 0.02510 - R2: 0.9997 -- iter: 0540/1168\n",
      "Training Step: 205  | total loss: \u001b[1m\u001b[32m0.02293\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 004 | loss: 0.02293 - R2: 1.0000 -- iter: 0560/1168\n",
      "Training Step: 206  | total loss: \u001b[1m\u001b[32m0.02293\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 004 | loss: 0.02293 - R2: 1.0000 -- iter: 0580/1168\n",
      "Training Step: 207  | total loss: \u001b[1m\u001b[32m0.02215\u001b[0m\u001b[0m | time: 0.169s\n",
      "| SGD | epoch: 004 | loss: 0.02215 - R2: 1.0005 -- iter: 0600/1168\n",
      "Training Step: 208  | total loss: \u001b[1m\u001b[32m0.03552\u001b[0m\u001b[0m | time: 0.171s\n",
      "| SGD | epoch: 004 | loss: 0.03552 - R2: 1.0009 -- iter: 0620/1168\n",
      "Training Step: 209  | total loss: \u001b[1m\u001b[32m0.03288\u001b[0m\u001b[0m | time: 0.173s\n",
      "| SGD | epoch: 004 | loss: 0.03288 - R2: 1.0009 -- iter: 0640/1168\n",
      "Training Step: 210  | total loss: \u001b[1m\u001b[32m0.03459\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 004 | loss: 0.03459 - R2: 1.0003 -- iter: 0660/1168\n",
      "Training Step: 211  | total loss: \u001b[1m\u001b[32m0.03290\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 004 | loss: 0.03290 - R2: 1.0003 -- iter: 0680/1168\n",
      "Training Step: 212  | total loss: \u001b[1m\u001b[32m0.03121\u001b[0m\u001b[0m | time: 0.179s\n",
      "| SGD | epoch: 004 | loss: 0.03121 - R2: 0.9994 -- iter: 0700/1168\n",
      "Training Step: 213  | total loss: \u001b[1m\u001b[32m0.02926\u001b[0m\u001b[0m | time: 0.182s\n",
      "| SGD | epoch: 004 | loss: 0.02926 - R2: 0.9995 -- iter: 0720/1168\n",
      "Training Step: 214  | total loss: \u001b[1m\u001b[32m0.02943\u001b[0m\u001b[0m | time: 0.187s\n",
      "| SGD | epoch: 004 | loss: 0.02943 - R2: 1.0006 -- iter: 0740/1168\n",
      "Training Step: 215  | total loss: \u001b[1m\u001b[32m0.02599\u001b[0m\u001b[0m | time: 0.189s\n",
      "| SGD | epoch: 004 | loss: 0.02599 - R2: 0.9991 -- iter: 0760/1168\n",
      "Training Step: 216  | total loss: \u001b[1m\u001b[32m0.02599\u001b[0m\u001b[0m | time: 0.191s\n",
      "| SGD | epoch: 004 | loss: 0.02599 - R2: 0.9991 -- iter: 0780/1168\n",
      "Training Step: 217  | total loss: \u001b[1m\u001b[32m0.02553\u001b[0m\u001b[0m | time: 0.194s\n",
      "| SGD | epoch: 004 | loss: 0.02553 - R2: 0.9982 -- iter: 0800/1168\n",
      "Training Step: 218  | total loss: \u001b[1m\u001b[32m0.02520\u001b[0m\u001b[0m | time: 0.198s\n",
      "| SGD | epoch: 004 | loss: 0.02520 - R2: 0.9994 -- iter: 0820/1168\n",
      "Training Step: 219  | total loss: \u001b[1m\u001b[32m0.02369\u001b[0m\u001b[0m | time: 0.200s\n",
      "| SGD | epoch: 004 | loss: 0.02369 - R2: 0.9989 -- iter: 0840/1168\n",
      "Training Step: 220  | total loss: \u001b[1m\u001b[32m0.02451\u001b[0m\u001b[0m | time: 0.202s\n",
      "| SGD | epoch: 004 | loss: 0.02451 - R2: 1.0001 -- iter: 0860/1168\n",
      "Training Step: 221  | total loss: \u001b[1m\u001b[32m0.02337\u001b[0m\u001b[0m | time: 0.210s\n",
      "| SGD | epoch: 004 | loss: 0.02337 - R2: 0.9996 -- iter: 0880/1168\n",
      "Training Step: 222  | total loss: \u001b[1m\u001b[32m0.02265\u001b[0m\u001b[0m | time: 0.219s\n",
      "| SGD | epoch: 004 | loss: 0.02265 - R2: 1.0003 -- iter: 0900/1168\n",
      "Training Step: 223  | total loss: \u001b[1m\u001b[32m0.02104\u001b[0m\u001b[0m | time: 0.224s\n",
      "| SGD | epoch: 004 | loss: 0.02104 - R2: 0.9999 -- iter: 0920/1168\n",
      "Training Step: 224  | total loss: \u001b[1m\u001b[32m0.01941\u001b[0m\u001b[0m | time: 0.229s\n",
      "| SGD | epoch: 004 | loss: 0.01941 - R2: 1.0007 -- iter: 0940/1168\n",
      "Training Step: 225  | total loss: \u001b[1m\u001b[32m0.01878\u001b[0m\u001b[0m | time: 0.244s\n",
      "| SGD | epoch: 004 | loss: 0.01878 - R2: 1.0003 -- iter: 0960/1168\n",
      "Training Step: 226  | total loss: \u001b[1m\u001b[32m0.01832\u001b[0m\u001b[0m | time: 0.250s\n",
      "| SGD | epoch: 004 | loss: 0.01832 - R2: 0.9999 -- iter: 0980/1168\n",
      "Training Step: 227  | total loss: \u001b[1m\u001b[32m0.02058\u001b[0m\u001b[0m | time: 0.255s\n",
      "| SGD | epoch: 004 | loss: 0.02058 - R2: 0.9996 -- iter: 1000/1168\n",
      "Training Step: 228  | total loss: \u001b[1m\u001b[32m0.02058\u001b[0m\u001b[0m | time: 0.261s\n",
      "| SGD | epoch: 004 | loss: 0.02058 - R2: 0.9997 -- iter: 1020/1168\n",
      "Training Step: 229  | total loss: \u001b[1m\u001b[32m0.02824\u001b[0m\u001b[0m | time: 0.268s\n",
      "| SGD | epoch: 004 | loss: 0.02824 - R2: 1.0010 -- iter: 1040/1168\n",
      "Training Step: 230  | total loss: \u001b[1m\u001b[32m0.02824\u001b[0m\u001b[0m | time: 0.271s\n",
      "| SGD | epoch: 004 | loss: 0.02824 - R2: 1.0010 -- iter: 1060/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 231  | total loss: \u001b[1m\u001b[32m0.02748\u001b[0m\u001b[0m | time: 0.274s\n",
      "| SGD | epoch: 004 | loss: 0.02748 - R2: 0.9996 -- iter: 1080/1168\n",
      "Training Step: 232  | total loss: \u001b[1m\u001b[32m0.02516\u001b[0m\u001b[0m | time: 0.277s\n",
      "| SGD | epoch: 004 | loss: 0.02516 - R2: 0.9998 -- iter: 1100/1168\n",
      "Training Step: 233  | total loss: \u001b[1m\u001b[32m0.02336\u001b[0m\u001b[0m | time: 0.281s\n",
      "| SGD | epoch: 004 | loss: 0.02336 - R2: 1.0000 -- iter: 1120/1168\n",
      "Training Step: 234  | total loss: \u001b[1m\u001b[32m0.02336\u001b[0m\u001b[0m | time: 0.300s\n",
      "| SGD | epoch: 004 | loss: 0.02336 - R2: 1.0000 -- iter: 1140/1168\n",
      "Training Step: 235  | total loss: \u001b[1m\u001b[32m0.02129\u001b[0m\u001b[0m | time: 0.305s\n",
      "| SGD | epoch: 004 | loss: 0.02129 - R2: 0.9997 -- iter: 1160/1168\n",
      "Training Step: 236  | total loss: \u001b[1m\u001b[32m0.02129\u001b[0m\u001b[0m | time: 1.312s\n",
      "| SGD | epoch: 004 | loss: 0.02129 - R2: 0.9997 | val_loss: 0.03318 - val_acc: 0.9993 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 237  | total loss: \u001b[1m\u001b[32m0.02162\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 005 | loss: 0.02162 - R2: 1.0000 -- iter: 0020/1168\n",
      "Training Step: 238  | total loss: \u001b[1m\u001b[32m0.02073\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 005 | loss: 0.02073 - R2: 0.9997 -- iter: 0040/1168\n",
      "Training Step: 239  | total loss: \u001b[1m\u001b[32m0.02036\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 005 | loss: 0.02036 - R2: 0.9993 -- iter: 0060/1168\n",
      "Training Step: 240  | total loss: \u001b[1m\u001b[32m0.02026\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 005 | loss: 0.02026 - R2: 1.0000 -- iter: 0080/1168\n",
      "Training Step: 241  | total loss: \u001b[1m\u001b[32m0.01957\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 005 | loss: 0.01957 - R2: 1.0003 -- iter: 0100/1168\n",
      "Training Step: 242  | total loss: \u001b[1m\u001b[32m0.01937\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 005 | loss: 0.01937 - R2: 1.0008 -- iter: 0120/1168\n",
      "Training Step: 243  | total loss: \u001b[1m\u001b[32m0.01848\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 005 | loss: 0.01848 - R2: 1.0010 -- iter: 0140/1168\n",
      "Training Step: 244  | total loss: \u001b[1m\u001b[32m0.01944\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 005 | loss: 0.01944 - R2: 1.0000 -- iter: 0160/1168\n",
      "Training Step: 245  | total loss: \u001b[1m\u001b[32m0.01944\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 005 | loss: 0.01944 - R2: 1.0000 -- iter: 0180/1168\n",
      "Training Step: 246  | total loss: \u001b[1m\u001b[32m0.01896\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 005 | loss: 0.01896 - R2: 0.9992 -- iter: 0200/1168\n",
      "Training Step: 247  | total loss: \u001b[1m\u001b[32m0.01970\u001b[0m\u001b[0m | time: 0.178s\n",
      "| SGD | epoch: 005 | loss: 0.01970 - R2: 0.9993 -- iter: 0220/1168\n",
      "Training Step: 248  | total loss: \u001b[1m\u001b[32m0.01993\u001b[0m\u001b[0m | time: 0.181s\n",
      "| SGD | epoch: 005 | loss: 0.01993 - R2: 1.0004 -- iter: 0240/1168\n",
      "Training Step: 249  | total loss: \u001b[1m\u001b[32m0.01993\u001b[0m\u001b[0m | time: 0.184s\n",
      "| SGD | epoch: 005 | loss: 0.01993 - R2: 1.0004 -- iter: 0260/1168\n",
      "Training Step: 250  | total loss: \u001b[1m\u001b[32m0.01966\u001b[0m\u001b[0m | time: 0.209s\n",
      "| SGD | epoch: 005 | loss: 0.01966 - R2: 0.9995 -- iter: 0280/1168\n",
      "Training Step: 251  | total loss: \u001b[1m\u001b[32m0.01857\u001b[0m\u001b[0m | time: 0.212s\n",
      "| SGD | epoch: 005 | loss: 0.01857 - R2: 1.0000 -- iter: 0300/1168\n",
      "Training Step: 252  | total loss: \u001b[1m\u001b[32m0.01788\u001b[0m\u001b[0m | time: 0.221s\n",
      "| SGD | epoch: 005 | loss: 0.01788 - R2: 0.9995 -- iter: 0320/1168\n",
      "Training Step: 253  | total loss: \u001b[1m\u001b[32m0.01708\u001b[0m\u001b[0m | time: 0.223s\n",
      "| SGD | epoch: 005 | loss: 0.01708 - R2: 0.9995 -- iter: 0340/1168\n",
      "Training Step: 254  | total loss: \u001b[1m\u001b[32m0.01636\u001b[0m\u001b[0m | time: 0.227s\n",
      "| SGD | epoch: 005 | loss: 0.01636 - R2: 0.9996 -- iter: 0360/1168\n",
      "Training Step: 255  | total loss: \u001b[1m\u001b[32m0.01623\u001b[0m\u001b[0m | time: 0.249s\n",
      "| SGD | epoch: 005 | loss: 0.01623 - R2: 0.9994 -- iter: 0380/1168\n",
      "Training Step: 256  | total loss: \u001b[1m\u001b[32m0.01673\u001b[0m\u001b[0m | time: 0.266s\n",
      "| SGD | epoch: 005 | loss: 0.01673 - R2: 0.9991 -- iter: 0400/1168\n",
      "Training Step: 257  | total loss: \u001b[1m\u001b[32m0.01538\u001b[0m\u001b[0m | time: 0.272s\n",
      "| SGD | epoch: 005 | loss: 0.01538 - R2: 0.9996 -- iter: 0420/1168\n",
      "Training Step: 258  | total loss: \u001b[1m\u001b[32m0.01741\u001b[0m\u001b[0m | time: 0.276s\n",
      "| SGD | epoch: 005 | loss: 0.01741 - R2: 0.9991 -- iter: 0440/1168\n",
      "Training Step: 259  | total loss: \u001b[1m\u001b[32m0.01741\u001b[0m\u001b[0m | time: 0.288s\n",
      "| SGD | epoch: 005 | loss: 0.01741 - R2: 0.9991 -- iter: 0460/1168\n",
      "Training Step: 260  | total loss: \u001b[1m\u001b[32m0.01744\u001b[0m\u001b[0m | time: 0.298s\n",
      "| SGD | epoch: 005 | loss: 0.01744 - R2: 0.9996 -- iter: 0480/1168\n",
      "Training Step: 261  | total loss: \u001b[1m\u001b[32m0.01856\u001b[0m\u001b[0m | time: 0.302s\n",
      "| SGD | epoch: 005 | loss: 0.01856 - R2: 0.9986 -- iter: 0500/1168\n",
      "Training Step: 262  | total loss: \u001b[1m\u001b[32m0.01778\u001b[0m\u001b[0m | time: 0.306s\n",
      "| SGD | epoch: 005 | loss: 0.01778 - R2: 0.9993 -- iter: 0520/1168\n",
      "Training Step: 263  | total loss: \u001b[1m\u001b[32m0.01747\u001b[0m\u001b[0m | time: 0.309s\n",
      "| SGD | epoch: 005 | loss: 0.01747 - R2: 0.9995 -- iter: 0540/1168\n",
      "Training Step: 264  | total loss: \u001b[1m\u001b[32m0.01731\u001b[0m\u001b[0m | time: 0.322s\n",
      "| SGD | epoch: 005 | loss: 0.01731 - R2: 0.9999 -- iter: 0560/1168\n",
      "Training Step: 265  | total loss: \u001b[1m\u001b[32m0.01747\u001b[0m\u001b[0m | time: 0.325s\n",
      "| SGD | epoch: 005 | loss: 0.01747 - R2: 0.9999 -- iter: 0580/1168\n",
      "Training Step: 266  | total loss: \u001b[1m\u001b[32m0.01763\u001b[0m\u001b[0m | time: 0.329s\n",
      "| SGD | epoch: 005 | loss: 0.01763 - R2: 0.9996 -- iter: 0600/1168\n",
      "Training Step: 267  | total loss: \u001b[1m\u001b[32m0.02302\u001b[0m\u001b[0m | time: 0.341s\n",
      "| SGD | epoch: 005 | loss: 0.02302 - R2: 1.0001 -- iter: 0620/1168\n",
      "Training Step: 268  | total loss: \u001b[1m\u001b[32m0.02310\u001b[0m\u001b[0m | time: 0.345s\n",
      "| SGD | epoch: 005 | loss: 0.02310 - R2: 1.0004 -- iter: 0640/1168\n",
      "Training Step: 269  | total loss: \u001b[1m\u001b[32m0.02324\u001b[0m\u001b[0m | time: 0.352s\n",
      "| SGD | epoch: 005 | loss: 0.02324 - R2: 0.9999 -- iter: 0660/1168\n",
      "Training Step: 270  | total loss: \u001b[1m\u001b[32m0.02360\u001b[0m\u001b[0m | time: 0.364s\n",
      "| SGD | epoch: 005 | loss: 0.02360 - R2: 0.9991 -- iter: 0680/1168\n",
      "Training Step: 271  | total loss: \u001b[1m\u001b[32m0.02201\u001b[0m\u001b[0m | time: 0.368s\n",
      "| SGD | epoch: 005 | loss: 0.02201 - R2: 0.9993 -- iter: 0700/1168\n",
      "Training Step: 272  | total loss: \u001b[1m\u001b[32m0.02201\u001b[0m\u001b[0m | time: 0.372s\n",
      "| SGD | epoch: 005 | loss: 0.02201 - R2: 1.0000 -- iter: 0720/1168\n",
      "Training Step: 273  | total loss: \u001b[1m\u001b[32m0.02174\u001b[0m\u001b[0m | time: 0.375s\n",
      "| SGD | epoch: 005 | loss: 0.02174 - R2: 1.0001 -- iter: 0740/1168\n",
      "Training Step: 274  | total loss: \u001b[1m\u001b[32m0.02536\u001b[0m\u001b[0m | time: 0.385s\n",
      "| SGD | epoch: 005 | loss: 0.02536 - R2: 1.0011 -- iter: 0760/1168\n",
      "Training Step: 275  | total loss: \u001b[1m\u001b[32m0.02646\u001b[0m\u001b[0m | time: 0.394s\n",
      "| SGD | epoch: 005 | loss: 0.02646 - R2: 1.0015 -- iter: 0780/1168\n",
      "Training Step: 276  | total loss: \u001b[1m\u001b[32m0.02494\u001b[0m\u001b[0m | time: 0.404s\n",
      "| SGD | epoch: 005 | loss: 0.02494 - R2: 1.0008 -- iter: 0800/1168\n",
      "Training Step: 277  | total loss: \u001b[1m\u001b[32m0.02376\u001b[0m\u001b[0m | time: 0.417s\n",
      "| SGD | epoch: 005 | loss: 0.02376 - R2: 1.0005 -- iter: 0820/1168\n",
      "Training Step: 278  | total loss: \u001b[1m\u001b[32m0.02288\u001b[0m\u001b[0m | time: 0.425s\n",
      "| SGD | epoch: 005 | loss: 0.02288 - R2: 1.0001 -- iter: 0840/1168\n",
      "Training Step: 279  | total loss: \u001b[1m\u001b[32m0.02258\u001b[0m\u001b[0m | time: 0.430s\n",
      "| SGD | epoch: 005 | loss: 0.02258 - R2: 1.0004 -- iter: 0860/1168\n",
      "Training Step: 280  | total loss: \u001b[1m\u001b[32m0.02139\u001b[0m\u001b[0m | time: 0.434s\n",
      "| SGD | epoch: 005 | loss: 0.02139 - R2: 0.9996 -- iter: 0880/1168\n",
      "Training Step: 281  | total loss: \u001b[1m\u001b[32m0.02139\u001b[0m\u001b[0m | time: 0.442s\n",
      "| SGD | epoch: 005 | loss: 0.02139 - R2: 0.9996 -- iter: 0900/1168\n",
      "Training Step: 282  | total loss: \u001b[1m\u001b[32m0.01855\u001b[0m\u001b[0m | time: 0.446s\n",
      "| SGD | epoch: 005 | loss: 0.01855 - R2: 0.9999 -- iter: 0920/1168\n",
      "Training Step: 283  | total loss: \u001b[1m\u001b[32m0.01855\u001b[0m\u001b[0m | time: 0.451s\n",
      "| SGD | epoch: 005 | loss: 0.01855 - R2: 0.9999 -- iter: 0940/1168\n",
      "Training Step: 284  | total loss: \u001b[1m\u001b[32m0.01749\u001b[0m\u001b[0m | time: 0.456s\n",
      "| SGD | epoch: 005 | loss: 0.01749 - R2: 0.9996 -- iter: 0960/1168\n",
      "Training Step: 285  | total loss: \u001b[1m\u001b[32m0.01585\u001b[0m\u001b[0m | time: 0.462s\n",
      "| SGD | epoch: 005 | loss: 0.01585 - R2: 1.0002 -- iter: 0980/1168\n",
      "Training Step: 286  | total loss: \u001b[1m\u001b[32m0.01750\u001b[0m\u001b[0m | time: 0.465s\n",
      "| SGD | epoch: 005 | loss: 0.01750 - R2: 1.0007 -- iter: 1000/1168\n",
      "Training Step: 287  | total loss: \u001b[1m\u001b[32m0.01750\u001b[0m\u001b[0m | time: 0.468s\n",
      "| SGD | epoch: 005 | loss: 0.01750 - R2: 1.0007 -- iter: 1020/1168\n",
      "Training Step: 288  | total loss: \u001b[1m\u001b[32m0.01639\u001b[0m\u001b[0m | time: 0.470s\n",
      "| SGD | epoch: 005 | loss: 0.01639 - R2: 0.9999 -- iter: 1040/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 289  | total loss: \u001b[1m\u001b[32m0.01639\u001b[0m\u001b[0m | time: 0.472s\n",
      "| SGD | epoch: 005 | loss: 0.01639 - R2: 0.9999 -- iter: 1060/1168\n",
      "Training Step: 290  | total loss: \u001b[1m\u001b[32m0.02368\u001b[0m\u001b[0m | time: 0.476s\n",
      "| SGD | epoch: 005 | loss: 0.02368 - R2: 1.0004 -- iter: 1080/1168\n",
      "Training Step: 291  | total loss: \u001b[1m\u001b[32m0.02368\u001b[0m\u001b[0m | time: 0.479s\n",
      "| SGD | epoch: 005 | loss: 0.02368 - R2: 1.0004 -- iter: 1100/1168\n",
      "Training Step: 292  | total loss: \u001b[1m\u001b[32m0.02321\u001b[0m\u001b[0m | time: 0.481s\n",
      "| SGD | epoch: 005 | loss: 0.02321 - R2: 0.9994 -- iter: 1120/1168\n",
      "Training Step: 293  | total loss: \u001b[1m\u001b[32m0.02247\u001b[0m\u001b[0m | time: 0.484s\n",
      "| SGD | epoch: 005 | loss: 0.02247 - R2: 0.9989 -- iter: 1140/1168\n",
      "Training Step: 294  | total loss: \u001b[1m\u001b[32m0.02025\u001b[0m\u001b[0m | time: 0.489s\n",
      "| SGD | epoch: 005 | loss: 0.02025 - R2: 0.9988 -- iter: 1160/1168\n",
      "Training Step: 295  | total loss: \u001b[1m\u001b[32m0.02135\u001b[0m\u001b[0m | time: 1.509s\n",
      "| SGD | epoch: 005 | loss: 0.02135 - R2: 0.9988 | val_loss: 0.03197 - val_acc: 1.0041 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 296  | total loss: \u001b[1m\u001b[32m0.02135\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 006 | loss: 0.02135 - R2: 0.9992 -- iter: 0020/1168\n",
      "Training Step: 297  | total loss: \u001b[1m\u001b[32m0.02097\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 006 | loss: 0.02097 - R2: 0.9988 -- iter: 0040/1168\n",
      "Training Step: 298  | total loss: \u001b[1m\u001b[32m0.02014\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 006 | loss: 0.02014 - R2: 0.9988 -- iter: 0060/1168\n",
      "Training Step: 299  | total loss: \u001b[1m\u001b[32m0.01918\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 006 | loss: 0.01918 - R2: 0.9986 -- iter: 0080/1168\n",
      "Training Step: 300  | total loss: \u001b[1m\u001b[32m0.01869\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 006 | loss: 0.01869 - R2: 0.9999 -- iter: 0100/1168\n",
      "Training Step: 301  | total loss: \u001b[1m\u001b[32m0.01758\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 006 | loss: 0.01758 - R2: 1.0005 -- iter: 0120/1168\n",
      "Training Step: 302  | total loss: \u001b[1m\u001b[32m0.01906\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 006 | loss: 0.01906 - R2: 0.9997 -- iter: 0140/1168\n",
      "Training Step: 303  | total loss: \u001b[1m\u001b[32m0.02043\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 006 | loss: 0.02043 - R2: 0.9999 -- iter: 0160/1168\n",
      "Training Step: 304  | total loss: \u001b[1m\u001b[32m0.01972\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 006 | loss: 0.01972 - R2: 0.9998 -- iter: 0180/1168\n",
      "Training Step: 305  | total loss: \u001b[1m\u001b[32m0.01916\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 006 | loss: 0.01916 - R2: 0.9996 -- iter: 0200/1168\n",
      "Training Step: 306  | total loss: \u001b[1m\u001b[32m0.01878\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 006 | loss: 0.01878 - R2: 0.9998 -- iter: 0220/1168\n",
      "Training Step: 307  | total loss: \u001b[1m\u001b[32m0.02142\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 006 | loss: 0.02142 - R2: 1.0009 -- iter: 0240/1168\n",
      "Training Step: 308  | total loss: \u001b[1m\u001b[32m0.02063\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 006 | loss: 0.02063 - R2: 1.0007 -- iter: 0260/1168\n",
      "Training Step: 309  | total loss: \u001b[1m\u001b[32m0.02060\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 006 | loss: 0.02060 - R2: 1.0011 -- iter: 0280/1168\n",
      "Training Step: 310  | total loss: \u001b[1m\u001b[32m0.01870\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 006 | loss: 0.01870 - R2: 1.0010 -- iter: 0300/1168\n",
      "Training Step: 311  | total loss: \u001b[1m\u001b[32m0.01870\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 006 | loss: 0.01870 - R2: 1.0010 -- iter: 0320/1168\n",
      "Training Step: 312  | total loss: \u001b[1m\u001b[32m0.01878\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 006 | loss: 0.01878 - R2: 1.0002 -- iter: 0340/1168\n",
      "Training Step: 313  | total loss: \u001b[1m\u001b[32m0.02046\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 006 | loss: 0.02046 - R2: 0.9997 -- iter: 0360/1168\n",
      "Training Step: 314  | total loss: \u001b[1m\u001b[32m0.02027\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 006 | loss: 0.02027 - R2: 0.9996 -- iter: 0380/1168\n",
      "Training Step: 315  | total loss: \u001b[1m\u001b[32m0.01958\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 006 | loss: 0.01958 - R2: 0.9996 -- iter: 0400/1168\n",
      "Training Step: 316  | total loss: \u001b[1m\u001b[32m0.02093\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 006 | loss: 0.02093 - R2: 1.0010 -- iter: 0420/1168\n",
      "Training Step: 317  | total loss: \u001b[1m\u001b[32m0.02030\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 006 | loss: 0.02030 - R2: 1.0001 -- iter: 0440/1168\n",
      "Training Step: 318  | total loss: \u001b[1m\u001b[32m0.01933\u001b[0m\u001b[0m | time: 0.174s\n",
      "| SGD | epoch: 006 | loss: 0.01933 - R2: 1.0007 -- iter: 0460/1168\n",
      "Training Step: 319  | total loss: \u001b[1m\u001b[32m0.01867\u001b[0m\u001b[0m | time: 0.176s\n",
      "| SGD | epoch: 006 | loss: 0.01867 - R2: 1.0003 -- iter: 0480/1168\n",
      "Training Step: 320  | total loss: \u001b[1m\u001b[32m0.01934\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 006 | loss: 0.01934 - R2: 0.9993 -- iter: 0500/1168\n",
      "Training Step: 321  | total loss: \u001b[1m\u001b[32m0.01896\u001b[0m\u001b[0m | time: 0.180s\n",
      "| SGD | epoch: 006 | loss: 0.01896 - R2: 0.9999 -- iter: 0520/1168\n",
      "Training Step: 322  | total loss: \u001b[1m\u001b[32m0.01952\u001b[0m\u001b[0m | time: 0.187s\n",
      "| SGD | epoch: 006 | loss: 0.01952 - R2: 0.9992 -- iter: 0540/1168\n",
      "Training Step: 323  | total loss: \u001b[1m\u001b[32m0.01821\u001b[0m\u001b[0m | time: 0.191s\n",
      "| SGD | epoch: 006 | loss: 0.01821 - R2: 0.9990 -- iter: 0560/1168\n",
      "Training Step: 324  | total loss: \u001b[1m\u001b[32m0.01972\u001b[0m\u001b[0m | time: 0.194s\n",
      "| SGD | epoch: 006 | loss: 0.01972 - R2: 0.9997 -- iter: 0580/1168\n",
      "Training Step: 325  | total loss: \u001b[1m\u001b[32m0.01977\u001b[0m\u001b[0m | time: 0.197s\n",
      "| SGD | epoch: 006 | loss: 0.01977 - R2: 1.0003 -- iter: 0600/1168\n",
      "Training Step: 326  | total loss: \u001b[1m\u001b[32m0.01977\u001b[0m\u001b[0m | time: 0.198s\n",
      "| SGD | epoch: 006 | loss: 0.01977 - R2: 1.0003 -- iter: 0620/1168\n",
      "Training Step: 327  | total loss: \u001b[1m\u001b[32m0.01796\u001b[0m\u001b[0m | time: 0.202s\n",
      "| SGD | epoch: 006 | loss: 0.01796 - R2: 1.0001 -- iter: 0640/1168\n",
      "Training Step: 328  | total loss: \u001b[1m\u001b[32m0.01796\u001b[0m\u001b[0m | time: 0.217s\n",
      "| SGD | epoch: 006 | loss: 0.01796 - R2: 1.0001 -- iter: 0660/1168\n",
      "Training Step: 329  | total loss: \u001b[1m\u001b[32m0.01795\u001b[0m\u001b[0m | time: 0.219s\n",
      "| SGD | epoch: 006 | loss: 0.01795 - R2: 0.9997 -- iter: 0680/1168\n",
      "Training Step: 330  | total loss: \u001b[1m\u001b[32m0.01814\u001b[0m\u001b[0m | time: 0.223s\n",
      "| SGD | epoch: 006 | loss: 0.01814 - R2: 0.9996 -- iter: 0700/1168\n",
      "Training Step: 331  | total loss: \u001b[1m\u001b[32m0.01814\u001b[0m\u001b[0m | time: 0.225s\n",
      "| SGD | epoch: 006 | loss: 0.01814 - R2: 0.9994 -- iter: 0720/1168\n",
      "Training Step: 332  | total loss: \u001b[1m\u001b[32m0.01699\u001b[0m\u001b[0m | time: 0.229s\n",
      "| SGD | epoch: 006 | loss: 0.01699 - R2: 0.9990 -- iter: 0740/1168\n",
      "Training Step: 333  | total loss: \u001b[1m\u001b[32m0.01699\u001b[0m\u001b[0m | time: 0.232s\n",
      "| SGD | epoch: 006 | loss: 0.01699 - R2: 0.9990 -- iter: 0760/1168\n",
      "Training Step: 334  | total loss: \u001b[1m\u001b[32m0.01789\u001b[0m\u001b[0m | time: 0.234s\n",
      "| SGD | epoch: 006 | loss: 0.01789 - R2: 0.9989 -- iter: 0780/1168\n",
      "Training Step: 335  | total loss: \u001b[1m\u001b[32m0.01713\u001b[0m\u001b[0m | time: 0.240s\n",
      "| SGD | epoch: 006 | loss: 0.01713 - R2: 0.9994 -- iter: 0800/1168\n",
      "Training Step: 336  | total loss: \u001b[1m\u001b[32m0.01625\u001b[0m\u001b[0m | time: 0.242s\n",
      "| SGD | epoch: 006 | loss: 0.01625 - R2: 0.9994 -- iter: 0820/1168\n",
      "Training Step: 337  | total loss: \u001b[1m\u001b[32m0.01567\u001b[0m\u001b[0m | time: 0.245s\n",
      "| SGD | epoch: 006 | loss: 0.01567 - R2: 1.0001 -- iter: 0840/1168\n",
      "Training Step: 338  | total loss: \u001b[1m\u001b[32m0.01501\u001b[0m\u001b[0m | time: 0.247s\n",
      "| SGD | epoch: 006 | loss: 0.01501 - R2: 0.9996 -- iter: 0860/1168\n",
      "Training Step: 339  | total loss: \u001b[1m\u001b[32m0.01524\u001b[0m\u001b[0m | time: 0.248s\n",
      "| SGD | epoch: 006 | loss: 0.01524 - R2: 1.0005 -- iter: 0880/1168\n",
      "Training Step: 340  | total loss: \u001b[1m\u001b[32m0.01428\u001b[0m\u001b[0m | time: 0.251s\n",
      "| SGD | epoch: 006 | loss: 0.01428 - R2: 1.0005 -- iter: 0900/1168\n",
      "Training Step: 341  | total loss: \u001b[1m\u001b[32m0.01428\u001b[0m\u001b[0m | time: 0.253s\n",
      "| SGD | epoch: 006 | loss: 0.01428 - R2: 1.0003 -- iter: 0920/1168\n",
      "Training Step: 342  | total loss: \u001b[1m\u001b[32m0.01692\u001b[0m\u001b[0m | time: 0.255s\n",
      "| SGD | epoch: 006 | loss: 0.01692 - R2: 0.9999 -- iter: 0940/1168\n",
      "Training Step: 343  | total loss: \u001b[1m\u001b[32m0.01823\u001b[0m\u001b[0m | time: 0.257s\n",
      "| SGD | epoch: 006 | loss: 0.01823 - R2: 1.0005 -- iter: 0960/1168\n",
      "Training Step: 344  | total loss: \u001b[1m\u001b[32m0.02087\u001b[0m\u001b[0m | time: 0.259s\n",
      "| SGD | epoch: 006 | loss: 0.02087 - R2: 1.0005 -- iter: 0980/1168\n",
      "Training Step: 345  | total loss: \u001b[1m\u001b[32m0.02039\u001b[0m\u001b[0m | time: 0.261s\n",
      "| SGD | epoch: 006 | loss: 0.02039 - R2: 1.0001 -- iter: 1000/1168\n",
      "Training Step: 346  | total loss: \u001b[1m\u001b[32m0.01933\u001b[0m\u001b[0m | time: 0.263s\n",
      "| SGD | epoch: 006 | loss: 0.01933 - R2: 0.9998 -- iter: 1020/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 347  | total loss: \u001b[1m\u001b[32m0.02273\u001b[0m\u001b[0m | time: 0.265s\n",
      "| SGD | epoch: 006 | loss: 0.02273 - R2: 1.0000 -- iter: 1040/1168\n",
      "Training Step: 348  | total loss: \u001b[1m\u001b[32m0.02226\u001b[0m\u001b[0m | time: 0.268s\n",
      "| SGD | epoch: 006 | loss: 0.02226 - R2: 0.9995 -- iter: 1060/1168\n",
      "Training Step: 349  | total loss: \u001b[1m\u001b[32m0.02290\u001b[0m\u001b[0m | time: 0.271s\n",
      "| SGD | epoch: 006 | loss: 0.02290 - R2: 0.9998 -- iter: 1080/1168\n",
      "Training Step: 350  | total loss: \u001b[1m\u001b[32m0.02158\u001b[0m\u001b[0m | time: 0.275s\n",
      "| SGD | epoch: 006 | loss: 0.02158 - R2: 0.9994 -- iter: 1100/1168\n",
      "Training Step: 351  | total loss: \u001b[1m\u001b[32m0.02193\u001b[0m\u001b[0m | time: 0.279s\n",
      "| SGD | epoch: 006 | loss: 0.02193 - R2: 0.9992 -- iter: 1120/1168\n",
      "Training Step: 352  | total loss: \u001b[1m\u001b[32m0.02193\u001b[0m\u001b[0m | time: 0.281s\n",
      "| SGD | epoch: 006 | loss: 0.02193 - R2: 0.9990 -- iter: 1140/1168\n",
      "Training Step: 353  | total loss: \u001b[1m\u001b[32m0.02056\u001b[0m\u001b[0m | time: 0.283s\n",
      "| SGD | epoch: 006 | loss: 0.02056 - R2: 0.9989 -- iter: 1160/1168\n",
      "Training Step: 354  | total loss: \u001b[1m\u001b[32m0.01993\u001b[0m\u001b[0m | time: 1.288s\n",
      "| SGD | epoch: 006 | loss: 0.01993 - R2: 0.9989 | val_loss: 0.03134 - val_acc: 1.0018 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 355  | total loss: \u001b[1m\u001b[32m0.01968\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 007 | loss: 0.01968 - R2: 0.9998 -- iter: 0020/1168\n",
      "Training Step: 356  | total loss: \u001b[1m\u001b[32m0.02023\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 007 | loss: 0.02023 - R2: 0.9998 -- iter: 0040/1168\n",
      "Training Step: 357  | total loss: \u001b[1m\u001b[32m0.01980\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 007 | loss: 0.01980 - R2: 0.9998 -- iter: 0060/1168\n",
      "Training Step: 358  | total loss: \u001b[1m\u001b[32m0.01979\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 007 | loss: 0.01979 - R2: 0.9997 -- iter: 0080/1168\n",
      "Training Step: 359  | total loss: \u001b[1m\u001b[32m0.01898\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 007 | loss: 0.01898 - R2: 1.0001 -- iter: 0100/1168\n",
      "Training Step: 360  | total loss: \u001b[1m\u001b[32m0.01898\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 007 | loss: 0.01898 - R2: 1.0001 -- iter: 0120/1168\n",
      "Training Step: 361  | total loss: \u001b[1m\u001b[32m0.01846\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 007 | loss: 0.01846 - R2: 1.0000 -- iter: 0140/1168\n",
      "Training Step: 362  | total loss: \u001b[1m\u001b[32m0.01846\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 007 | loss: 0.01846 - R2: 1.0000 -- iter: 0160/1168\n",
      "Training Step: 363  | total loss: \u001b[1m\u001b[32m0.01775\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 007 | loss: 0.01775 - R2: 1.0005 -- iter: 0180/1168\n",
      "Training Step: 364  | total loss: \u001b[1m\u001b[32m0.01742\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 007 | loss: 0.01742 - R2: 1.0006 -- iter: 0200/1168\n",
      "Training Step: 365  | total loss: \u001b[1m\u001b[32m0.02426\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 007 | loss: 0.02426 - R2: 1.0009 -- iter: 0220/1168\n",
      "Training Step: 366  | total loss: \u001b[1m\u001b[32m0.02202\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 007 | loss: 0.02202 - R2: 0.9999 -- iter: 0240/1168\n",
      "Training Step: 367  | total loss: \u001b[1m\u001b[32m0.02202\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 007 | loss: 0.02202 - R2: 0.9999 -- iter: 0260/1168\n",
      "Training Step: 368  | total loss: \u001b[1m\u001b[32m0.02195\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 007 | loss: 0.02195 - R2: 1.0010 -- iter: 0280/1168\n",
      "Training Step: 369  | total loss: \u001b[1m\u001b[32m0.02139\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 007 | loss: 0.02139 - R2: 0.9999 -- iter: 0300/1168\n",
      "Training Step: 370  | total loss: \u001b[1m\u001b[32m0.02025\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 007 | loss: 0.02025 - R2: 0.9996 -- iter: 0320/1168\n",
      "Training Step: 371  | total loss: \u001b[1m\u001b[32m0.01886\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 007 | loss: 0.01886 - R2: 1.0002 -- iter: 0340/1168\n",
      "Training Step: 372  | total loss: \u001b[1m\u001b[32m0.02080\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 007 | loss: 0.02080 - R2: 1.0002 -- iter: 0360/1168\n",
      "Training Step: 373  | total loss: \u001b[1m\u001b[32m0.02159\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 007 | loss: 0.02159 - R2: 1.0008 -- iter: 0380/1168\n",
      "Training Step: 374  | total loss: \u001b[1m\u001b[32m0.02404\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 007 | loss: 0.02404 - R2: 1.0003 -- iter: 0400/1168\n",
      "Training Step: 375  | total loss: \u001b[1m\u001b[32m0.02323\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 007 | loss: 0.02323 - R2: 1.0003 -- iter: 0420/1168\n",
      "Training Step: 376  | total loss: \u001b[1m\u001b[32m0.02232\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 007 | loss: 0.02232 - R2: 1.0007 -- iter: 0440/1168\n",
      "Training Step: 377  | total loss: \u001b[1m\u001b[32m0.02077\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 007 | loss: 0.02077 - R2: 1.0004 -- iter: 0460/1168\n",
      "Training Step: 378  | total loss: \u001b[1m\u001b[32m0.02021\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 007 | loss: 0.02021 - R2: 1.0004 -- iter: 0480/1168\n",
      "Training Step: 379  | total loss: \u001b[1m\u001b[32m0.02033\u001b[0m\u001b[0m | time: 0.178s\n",
      "| SGD | epoch: 007 | loss: 0.02033 - R2: 0.9997 -- iter: 0500/1168\n",
      "Training Step: 380  | total loss: \u001b[1m\u001b[32m0.01960\u001b[0m\u001b[0m | time: 0.180s\n",
      "| SGD | epoch: 007 | loss: 0.01960 - R2: 0.9996 -- iter: 0520/1168\n",
      "Training Step: 381  | total loss: \u001b[1m\u001b[32m0.01867\u001b[0m\u001b[0m | time: 0.187s\n",
      "| SGD | epoch: 007 | loss: 0.01867 - R2: 0.9991 -- iter: 0540/1168\n",
      "Training Step: 382  | total loss: \u001b[1m\u001b[32m0.01731\u001b[0m\u001b[0m | time: 0.189s\n",
      "| SGD | epoch: 007 | loss: 0.01731 - R2: 0.9993 -- iter: 0560/1168\n",
      "Training Step: 383  | total loss: \u001b[1m\u001b[32m0.01731\u001b[0m\u001b[0m | time: 0.192s\n",
      "| SGD | epoch: 007 | loss: 0.01731 - R2: 0.9992 -- iter: 0580/1168\n",
      "Training Step: 384  | total loss: \u001b[1m\u001b[32m0.01715\u001b[0m\u001b[0m | time: 0.194s\n",
      "| SGD | epoch: 007 | loss: 0.01715 - R2: 0.9995 -- iter: 0600/1168\n",
      "Training Step: 385  | total loss: \u001b[1m\u001b[32m0.01680\u001b[0m\u001b[0m | time: 0.197s\n",
      "| SGD | epoch: 007 | loss: 0.01680 - R2: 0.9990 -- iter: 0620/1168\n",
      "Training Step: 386  | total loss: \u001b[1m\u001b[32m0.01680\u001b[0m\u001b[0m | time: 0.199s\n",
      "| SGD | epoch: 007 | loss: 0.01680 - R2: 0.9990 -- iter: 0640/1168\n",
      "Training Step: 387  | total loss: \u001b[1m\u001b[32m0.01680\u001b[0m\u001b[0m | time: 0.202s\n",
      "| SGD | epoch: 007 | loss: 0.01680 - R2: 0.9986 -- iter: 0660/1168\n",
      "Training Step: 388  | total loss: \u001b[1m\u001b[32m0.01879\u001b[0m\u001b[0m | time: 0.203s\n",
      "| SGD | epoch: 007 | loss: 0.01879 - R2: 0.9993 -- iter: 0680/1168\n",
      "Training Step: 389  | total loss: \u001b[1m\u001b[32m0.01801\u001b[0m\u001b[0m | time: 0.205s\n",
      "| SGD | epoch: 007 | loss: 0.01801 - R2: 0.9990 -- iter: 0700/1168\n",
      "Training Step: 390  | total loss: \u001b[1m\u001b[32m0.01704\u001b[0m\u001b[0m | time: 0.207s\n",
      "| SGD | epoch: 007 | loss: 0.01704 - R2: 0.9995 -- iter: 0720/1168\n",
      "Training Step: 391  | total loss: \u001b[1m\u001b[32m0.01866\u001b[0m\u001b[0m | time: 0.209s\n",
      "| SGD | epoch: 007 | loss: 0.01866 - R2: 1.0002 -- iter: 0740/1168\n",
      "Training Step: 392  | total loss: \u001b[1m\u001b[32m0.01757\u001b[0m\u001b[0m | time: 0.212s\n",
      "| SGD | epoch: 007 | loss: 0.01757 - R2: 0.9996 -- iter: 0760/1168\n",
      "Training Step: 393  | total loss: \u001b[1m\u001b[32m0.01738\u001b[0m\u001b[0m | time: 0.213s\n",
      "| SGD | epoch: 007 | loss: 0.01738 - R2: 1.0003 -- iter: 0780/1168\n",
      "Training Step: 394  | total loss: \u001b[1m\u001b[32m0.01714\u001b[0m\u001b[0m | time: 0.216s\n",
      "| SGD | epoch: 007 | loss: 0.01714 - R2: 1.0006 -- iter: 0800/1168\n",
      "Training Step: 395  | total loss: \u001b[1m\u001b[32m0.01719\u001b[0m\u001b[0m | time: 0.219s\n",
      "| SGD | epoch: 007 | loss: 0.01719 - R2: 0.9992 -- iter: 0820/1168\n",
      "Training Step: 396  | total loss: \u001b[1m\u001b[32m0.01721\u001b[0m\u001b[0m | time: 0.225s\n",
      "| SGD | epoch: 007 | loss: 0.01721 - R2: 0.9987 -- iter: 0840/1168\n",
      "Training Step: 397  | total loss: \u001b[1m\u001b[32m0.01721\u001b[0m\u001b[0m | time: 0.227s\n",
      "| SGD | epoch: 007 | loss: 0.01721 - R2: 0.9987 -- iter: 0860/1168\n",
      "Training Step: 398  | total loss: \u001b[1m\u001b[32m0.01708\u001b[0m\u001b[0m | time: 0.229s\n",
      "| SGD | epoch: 007 | loss: 0.01708 - R2: 0.9984 -- iter: 0880/1168\n",
      "Training Step: 399  | total loss: \u001b[1m\u001b[32m0.01757\u001b[0m\u001b[0m | time: 0.232s\n",
      "| SGD | epoch: 007 | loss: 0.01757 - R2: 0.9995 -- iter: 0900/1168\n",
      "Training Step: 400  | total loss: \u001b[1m\u001b[32m0.01796\u001b[0m\u001b[0m | time: 0.234s\n",
      "| SGD | epoch: 007 | loss: 0.01796 - R2: 0.9991 -- iter: 0920/1168\n",
      "Training Step: 401  | total loss: \u001b[1m\u001b[32m0.01657\u001b[0m\u001b[0m | time: 0.237s\n",
      "| SGD | epoch: 007 | loss: 0.01657 - R2: 0.9985 -- iter: 0940/1168\n",
      "Training Step: 402  | total loss: \u001b[1m\u001b[32m0.01669\u001b[0m\u001b[0m | time: 0.246s\n",
      "| SGD | epoch: 007 | loss: 0.01669 - R2: 0.9995 -- iter: 0960/1168\n",
      "Training Step: 403  | total loss: \u001b[1m\u001b[32m0.01618\u001b[0m\u001b[0m | time: 0.250s\n",
      "| SGD | epoch: 007 | loss: 0.01618 - R2: 0.9995 -- iter: 0980/1168\n",
      "Training Step: 404  | total loss: \u001b[1m\u001b[32m0.01602\u001b[0m\u001b[0m | time: 0.254s\n",
      "| SGD | epoch: 007 | loss: 0.01602 - R2: 1.0001 -- iter: 1000/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 405  | total loss: \u001b[1m\u001b[32m0.01602\u001b[0m\u001b[0m | time: 0.256s\n",
      "| SGD | epoch: 007 | loss: 0.01602 - R2: 1.0001 -- iter: 1020/1168\n",
      "Training Step: 406  | total loss: \u001b[1m\u001b[32m0.01563\u001b[0m\u001b[0m | time: 0.259s\n",
      "| SGD | epoch: 007 | loss: 0.01563 - R2: 0.9997 -- iter: 1040/1168\n",
      "Training Step: 407  | total loss: \u001b[1m\u001b[32m0.01556\u001b[0m\u001b[0m | time: 0.265s\n",
      "| SGD | epoch: 007 | loss: 0.01556 - R2: 0.9992 -- iter: 1060/1168\n",
      "Training Step: 408  | total loss: \u001b[1m\u001b[32m0.01603\u001b[0m\u001b[0m | time: 0.268s\n",
      "| SGD | epoch: 007 | loss: 0.01603 - R2: 1.0009 -- iter: 1080/1168\n",
      "Training Step: 409  | total loss: \u001b[1m\u001b[32m0.01574\u001b[0m\u001b[0m | time: 0.274s\n",
      "| SGD | epoch: 007 | loss: 0.01574 - R2: 1.0009 -- iter: 1100/1168\n",
      "Training Step: 410  | total loss: \u001b[1m\u001b[32m0.01574\u001b[0m\u001b[0m | time: 0.276s\n",
      "| SGD | epoch: 007 | loss: 0.01574 - R2: 1.0009 -- iter: 1120/1168\n",
      "Training Step: 411  | total loss: \u001b[1m\u001b[32m0.01569\u001b[0m\u001b[0m | time: 0.278s\n",
      "| SGD | epoch: 007 | loss: 0.01569 - R2: 1.0013 -- iter: 1140/1168\n",
      "Training Step: 412  | total loss: \u001b[1m\u001b[32m0.01669\u001b[0m\u001b[0m | time: 0.280s\n",
      "| SGD | epoch: 007 | loss: 0.01669 - R2: 1.0004 -- iter: 1160/1168\n",
      "Training Step: 413  | total loss: \u001b[1m\u001b[32m0.01636\u001b[0m\u001b[0m | time: 1.286s\n",
      "| SGD | epoch: 007 | loss: 0.01636 - R2: 1.0007 | val_loss: 0.03173 - val_acc: 1.0005 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 414  | total loss: \u001b[1m\u001b[32m0.01636\u001b[0m\u001b[0m | time: 0.012s\n",
      "| SGD | epoch: 008 | loss: 0.01636 - R2: 1.0007 -- iter: 0020/1168\n",
      "Training Step: 415  | total loss: \u001b[1m\u001b[32m0.01681\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 008 | loss: 0.01681 - R2: 1.0003 -- iter: 0040/1168\n",
      "Training Step: 416  | total loss: \u001b[1m\u001b[32m0.01705\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 008 | loss: 0.01705 - R2: 0.9997 -- iter: 0060/1168\n",
      "Training Step: 417  | total loss: \u001b[1m\u001b[32m0.01705\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 008 | loss: 0.01705 - R2: 0.9997 -- iter: 0080/1168\n",
      "Training Step: 418  | total loss: \u001b[1m\u001b[32m0.01717\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 008 | loss: 0.01717 - R2: 1.0000 -- iter: 0100/1168\n",
      "Training Step: 419  | total loss: \u001b[1m\u001b[32m0.01768\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 008 | loss: 0.01768 - R2: 1.0005 -- iter: 0120/1168\n",
      "Training Step: 420  | total loss: \u001b[1m\u001b[32m0.01805\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 008 | loss: 0.01805 - R2: 1.0008 -- iter: 0140/1168\n",
      "Training Step: 421  | total loss: \u001b[1m\u001b[32m0.01786\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 008 | loss: 0.01786 - R2: 1.0008 -- iter: 0160/1168\n",
      "Training Step: 422  | total loss: \u001b[1m\u001b[32m0.01836\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 008 | loss: 0.01836 - R2: 1.0010 -- iter: 0180/1168\n",
      "Training Step: 423  | total loss: \u001b[1m\u001b[32m0.01620\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 008 | loss: 0.01620 - R2: 1.0009 -- iter: 0200/1168\n",
      "Training Step: 424  | total loss: \u001b[1m\u001b[32m0.01620\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 008 | loss: 0.01620 - R2: 1.0009 -- iter: 0220/1168\n",
      "Training Step: 425  | total loss: \u001b[1m\u001b[32m0.01595\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 008 | loss: 0.01595 - R2: 1.0013 -- iter: 0240/1168\n",
      "Training Step: 426  | total loss: \u001b[1m\u001b[32m0.02012\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 008 | loss: 0.02012 - R2: 1.0008 -- iter: 0260/1168\n",
      "Training Step: 427  | total loss: \u001b[1m\u001b[32m0.02012\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 008 | loss: 0.02012 - R2: 1.0008 -- iter: 0280/1168\n",
      "Training Step: 428  | total loss: \u001b[1m\u001b[32m0.01948\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 008 | loss: 0.01948 - R2: 1.0011 -- iter: 0300/1168\n",
      "Training Step: 429  | total loss: \u001b[1m\u001b[32m0.01871\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 008 | loss: 0.01871 - R2: 1.0006 -- iter: 0320/1168\n",
      "Training Step: 430  | total loss: \u001b[1m\u001b[32m0.01871\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 008 | loss: 0.01871 - R2: 1.0000 -- iter: 0340/1168\n",
      "Training Step: 431  | total loss: \u001b[1m\u001b[32m0.01837\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 008 | loss: 0.01837 - R2: 1.0004 -- iter: 0360/1168\n",
      "Training Step: 432  | total loss: \u001b[1m\u001b[32m0.01801\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 008 | loss: 0.01801 - R2: 1.0002 -- iter: 0380/1168\n",
      "Training Step: 433  | total loss: \u001b[1m\u001b[32m0.02369\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 008 | loss: 0.02369 - R2: 1.0002 -- iter: 0400/1168\n",
      "Training Step: 434  | total loss: \u001b[1m\u001b[32m0.02265\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 008 | loss: 0.02265 - R2: 0.9996 -- iter: 0420/1168\n",
      "Training Step: 435  | total loss: \u001b[1m\u001b[32m0.02145\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 008 | loss: 0.02145 - R2: 0.9991 -- iter: 0440/1168\n",
      "Training Step: 436  | total loss: \u001b[1m\u001b[32m0.02264\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 008 | loss: 0.02264 - R2: 0.9990 -- iter: 0460/1168\n",
      "Training Step: 437  | total loss: \u001b[1m\u001b[32m0.02329\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 008 | loss: 0.02329 - R2: 0.9986 -- iter: 0480/1168\n",
      "Training Step: 438  | total loss: \u001b[1m\u001b[32m0.02299\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 008 | loss: 0.02299 - R2: 0.9998 -- iter: 0500/1168\n",
      "Training Step: 439  | total loss: \u001b[1m\u001b[32m0.02174\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 008 | loss: 0.02174 - R2: 0.9996 -- iter: 0520/1168\n",
      "Training Step: 440  | total loss: \u001b[1m\u001b[32m0.02352\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 008 | loss: 0.02352 - R2: 1.0000 -- iter: 0540/1168\n",
      "Training Step: 441  | total loss: \u001b[1m\u001b[32m0.02326\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 008 | loss: 0.02326 - R2: 1.0003 -- iter: 0560/1168\n",
      "Training Step: 442  | total loss: \u001b[1m\u001b[32m0.02164\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 008 | loss: 0.02164 - R2: 1.0003 -- iter: 0580/1168\n",
      "Training Step: 443  | total loss: \u001b[1m\u001b[32m0.02340\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 008 | loss: 0.02340 - R2: 1.0008 -- iter: 0600/1168\n",
      "Training Step: 444  | total loss: \u001b[1m\u001b[32m0.02340\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 008 | loss: 0.02340 - R2: 1.0008 -- iter: 0620/1168\n",
      "Training Step: 445  | total loss: \u001b[1m\u001b[32m0.02195\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 008 | loss: 0.02195 - R2: 0.9997 -- iter: 0640/1168\n",
      "Training Step: 446  | total loss: \u001b[1m\u001b[32m0.02203\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 008 | loss: 0.02203 - R2: 1.0004 -- iter: 0660/1168\n",
      "Training Step: 447  | total loss: \u001b[1m\u001b[32m0.02218\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 008 | loss: 0.02218 - R2: 0.9995 -- iter: 0680/1168\n",
      "Training Step: 448  | total loss: \u001b[1m\u001b[32m0.02182\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 008 | loss: 0.02182 - R2: 0.9992 -- iter: 0700/1168\n",
      "Training Step: 449  | total loss: \u001b[1m\u001b[32m0.02472\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 008 | loss: 0.02472 - R2: 0.9998 -- iter: 0720/1168\n",
      "Training Step: 450  | total loss: \u001b[1m\u001b[32m0.02472\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 008 | loss: 0.02472 - R2: 0.9998 -- iter: 0740/1168\n",
      "Training Step: 451  | total loss: \u001b[1m\u001b[32m0.02368\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 008 | loss: 0.02368 - R2: 0.9987 -- iter: 0760/1168\n",
      "Training Step: 452  | total loss: \u001b[1m\u001b[32m0.02368\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 008 | loss: 0.02368 - R2: 0.9994 -- iter: 0780/1168\n",
      "Training Step: 453  | total loss: \u001b[1m\u001b[32m0.02156\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 008 | loss: 0.02156 - R2: 0.9994 -- iter: 0800/1168\n",
      "Training Step: 454  | total loss: \u001b[1m\u001b[32m0.02044\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 008 | loss: 0.02044 - R2: 0.9994 -- iter: 0820/1168\n",
      "Training Step: 455  | total loss: \u001b[1m\u001b[32m0.02044\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 008 | loss: 0.02044 - R2: 0.9994 -- iter: 0840/1168\n",
      "Training Step: 456  | total loss: \u001b[1m\u001b[32m0.02015\u001b[0m\u001b[0m | time: 0.173s\n",
      "| SGD | epoch: 008 | loss: 0.02015 - R2: 0.9984 -- iter: 0860/1168\n",
      "Training Step: 457  | total loss: \u001b[1m\u001b[32m0.02280\u001b[0m\u001b[0m | time: 0.188s\n",
      "| SGD | epoch: 008 | loss: 0.02280 - R2: 0.9992 -- iter: 0880/1168\n",
      "Training Step: 458  | total loss: \u001b[1m\u001b[32m0.02257\u001b[0m\u001b[0m | time: 0.192s\n",
      "| SGD | epoch: 008 | loss: 0.02257 - R2: 1.0000 -- iter: 0900/1168\n",
      "Training Step: 459  | total loss: \u001b[1m\u001b[32m0.02303\u001b[0m\u001b[0m | time: 0.196s\n",
      "| SGD | epoch: 008 | loss: 0.02303 - R2: 1.0005 -- iter: 0920/1168\n",
      "Training Step: 460  | total loss: \u001b[1m\u001b[32m0.02255\u001b[0m\u001b[0m | time: 0.199s\n",
      "| SGD | epoch: 008 | loss: 0.02255 - R2: 1.0004 -- iter: 0940/1168\n",
      "Training Step: 461  | total loss: \u001b[1m\u001b[32m0.02255\u001b[0m\u001b[0m | time: 0.202s\n",
      "| SGD | epoch: 008 | loss: 0.02255 - R2: 1.0004 -- iter: 0960/1168\n",
      "Training Step: 462  | total loss: \u001b[1m\u001b[32m0.02181\u001b[0m\u001b[0m | time: 0.206s\n",
      "| SGD | epoch: 008 | loss: 0.02181 - R2: 1.0006 -- iter: 0980/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 463  | total loss: \u001b[1m\u001b[32m0.02075\u001b[0m\u001b[0m | time: 0.210s\n",
      "| SGD | epoch: 008 | loss: 0.02075 - R2: 1.0004 -- iter: 1000/1168\n",
      "Training Step: 464  | total loss: \u001b[1m\u001b[32m0.02075\u001b[0m\u001b[0m | time: 0.212s\n",
      "| SGD | epoch: 008 | loss: 0.02075 - R2: 1.0004 -- iter: 1020/1168\n",
      "Training Step: 465  | total loss: \u001b[1m\u001b[32m0.02223\u001b[0m\u001b[0m | time: 0.215s\n",
      "| SGD | epoch: 008 | loss: 0.02223 - R2: 0.9994 -- iter: 1040/1168\n",
      "Training Step: 466  | total loss: \u001b[1m\u001b[32m0.02094\u001b[0m\u001b[0m | time: 0.218s\n",
      "| SGD | epoch: 008 | loss: 0.02094 - R2: 0.9999 -- iter: 1060/1168\n",
      "Training Step: 467  | total loss: \u001b[1m\u001b[32m0.02046\u001b[0m\u001b[0m | time: 0.220s\n",
      "| SGD | epoch: 008 | loss: 0.02046 - R2: 1.0000 -- iter: 1080/1168\n",
      "Training Step: 468  | total loss: \u001b[1m\u001b[32m0.02046\u001b[0m\u001b[0m | time: 0.223s\n",
      "| SGD | epoch: 008 | loss: 0.02046 - R2: 1.0000 -- iter: 1100/1168\n",
      "Training Step: 469  | total loss: \u001b[1m\u001b[32m0.02157\u001b[0m\u001b[0m | time: 0.227s\n",
      "| SGD | epoch: 008 | loss: 0.02157 - R2: 1.0001 -- iter: 1120/1168\n",
      "Training Step: 470  | total loss: \u001b[1m\u001b[32m0.02068\u001b[0m\u001b[0m | time: 0.229s\n",
      "| SGD | epoch: 008 | loss: 0.02068 - R2: 0.9993 -- iter: 1140/1168\n",
      "Training Step: 471  | total loss: \u001b[1m\u001b[32m0.02002\u001b[0m\u001b[0m | time: 0.232s\n",
      "| SGD | epoch: 008 | loss: 0.02002 - R2: 0.9993 -- iter: 1160/1168\n",
      "Training Step: 472  | total loss: \u001b[1m\u001b[32m0.02094\u001b[0m\u001b[0m | time: 1.238s\n",
      "| SGD | epoch: 008 | loss: 0.02094 - R2: 0.9989 | val_loss: 0.03531 - val_acc: 1.0032 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 473  | total loss: \u001b[1m\u001b[32m0.02094\u001b[0m\u001b[0m | time: 0.014s\n",
      "| SGD | epoch: 009 | loss: 0.02094 - R2: 0.9989 -- iter: 0020/1168\n",
      "Training Step: 474  | total loss: \u001b[1m\u001b[32m0.02037\u001b[0m\u001b[0m | time: 0.017s\n",
      "| SGD | epoch: 009 | loss: 0.02037 - R2: 0.9993 -- iter: 0040/1168\n",
      "Training Step: 475  | total loss: \u001b[1m\u001b[32m0.02517\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 009 | loss: 0.02517 - R2: 1.0005 -- iter: 0060/1168\n",
      "Training Step: 476  | total loss: \u001b[1m\u001b[32m0.02517\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 009 | loss: 0.02517 - R2: 1.0005 -- iter: 0080/1168\n",
      "Training Step: 477  | total loss: \u001b[1m\u001b[32m0.02534\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 009 | loss: 0.02534 - R2: 0.9997 -- iter: 0100/1168\n",
      "Training Step: 478  | total loss: \u001b[1m\u001b[32m0.02444\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 009 | loss: 0.02444 - R2: 0.9993 -- iter: 0120/1168\n",
      "Training Step: 479  | total loss: \u001b[1m\u001b[32m0.02393\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 009 | loss: 0.02393 - R2: 1.0004 -- iter: 0140/1168\n",
      "Training Step: 480  | total loss: \u001b[1m\u001b[32m0.02362\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 009 | loss: 0.02362 - R2: 1.0004 -- iter: 0160/1168\n",
      "Training Step: 481  | total loss: \u001b[1m\u001b[32m0.02275\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 009 | loss: 0.02275 - R2: 1.0009 -- iter: 0180/1168\n",
      "Training Step: 482  | total loss: \u001b[1m\u001b[32m0.02126\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 009 | loss: 0.02126 - R2: 0.9998 -- iter: 0200/1168\n",
      "Training Step: 483  | total loss: \u001b[1m\u001b[32m0.02064\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 009 | loss: 0.02064 - R2: 0.9997 -- iter: 0220/1168\n",
      "Training Step: 484  | total loss: \u001b[1m\u001b[32m0.01966\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 009 | loss: 0.01966 - R2: 1.0006 -- iter: 0240/1168\n",
      "Training Step: 485  | total loss: \u001b[1m\u001b[32m0.01966\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 009 | loss: 0.01966 - R2: 1.0006 -- iter: 0260/1168\n",
      "Training Step: 486  | total loss: \u001b[1m\u001b[32m0.01879\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 009 | loss: 0.01879 - R2: 1.0005 -- iter: 0280/1168\n",
      "Training Step: 487  | total loss: \u001b[1m\u001b[32m0.01933\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 009 | loss: 0.01933 - R2: 1.0003 -- iter: 0300/1168\n",
      "Training Step: 488  | total loss: \u001b[1m\u001b[32m0.01865\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 009 | loss: 0.01865 - R2: 1.0009 -- iter: 0320/1168\n",
      "Training Step: 489  | total loss: \u001b[1m\u001b[32m0.01818\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 009 | loss: 0.01818 - R2: 1.0007 -- iter: 0340/1168\n",
      "Training Step: 490  | total loss: \u001b[1m\u001b[32m0.01705\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 009 | loss: 0.01705 - R2: 1.0005 -- iter: 0360/1168\n",
      "Training Step: 491  | total loss: \u001b[1m\u001b[32m0.01824\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 009 | loss: 0.01824 - R2: 1.0012 -- iter: 0380/1168\n",
      "Training Step: 492  | total loss: \u001b[1m\u001b[32m0.01924\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 009 | loss: 0.01924 - R2: 1.0013 -- iter: 0400/1168\n",
      "Training Step: 493  | total loss: \u001b[1m\u001b[32m0.01866\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 009 | loss: 0.01866 - R2: 1.0012 -- iter: 0420/1168\n",
      "Training Step: 494  | total loss: \u001b[1m\u001b[32m0.01756\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 009 | loss: 0.01756 - R2: 0.9998 -- iter: 0440/1168\n",
      "Training Step: 495  | total loss: \u001b[1m\u001b[32m0.01756\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 009 | loss: 0.01756 - R2: 0.9998 -- iter: 0460/1168\n",
      "Training Step: 496  | total loss: \u001b[1m\u001b[32m0.01788\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 009 | loss: 0.01788 - R2: 0.9992 -- iter: 0480/1168\n",
      "Training Step: 497  | total loss: \u001b[1m\u001b[32m0.01852\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 009 | loss: 0.01852 - R2: 0.9988 -- iter: 0500/1168\n",
      "Training Step: 498  | total loss: \u001b[1m\u001b[32m0.01765\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 009 | loss: 0.01765 - R2: 0.9988 -- iter: 0520/1168\n",
      "Training Step: 499  | total loss: \u001b[1m\u001b[32m0.01776\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 009 | loss: 0.01776 - R2: 0.9993 -- iter: 0540/1168\n",
      "Training Step: 500  | total loss: \u001b[1m\u001b[32m0.01695\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 009 | loss: 0.01695 - R2: 0.9993 -- iter: 0560/1168\n",
      "Training Step: 501  | total loss: \u001b[1m\u001b[32m0.01645\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 009 | loss: 0.01645 - R2: 0.9994 -- iter: 0580/1168\n",
      "Training Step: 502  | total loss: \u001b[1m\u001b[32m0.01546\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 009 | loss: 0.01546 - R2: 0.9992 -- iter: 0600/1168\n",
      "Training Step: 503  | total loss: \u001b[1m\u001b[32m0.01490\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 009 | loss: 0.01490 - R2: 0.9996 -- iter: 0620/1168\n",
      "Training Step: 504  | total loss: \u001b[1m\u001b[32m0.01452\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 009 | loss: 0.01452 - R2: 0.9996 -- iter: 0640/1168\n",
      "Training Step: 505  | total loss: \u001b[1m\u001b[32m0.01454\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 009 | loss: 0.01454 - R2: 0.9988 -- iter: 0660/1168\n",
      "Training Step: 506  | total loss: \u001b[1m\u001b[32m0.01603\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 009 | loss: 0.01603 - R2: 0.9983 -- iter: 0680/1168\n",
      "Training Step: 507  | total loss: \u001b[1m\u001b[32m0.01815\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 009 | loss: 0.01815 - R2: 0.9993 -- iter: 0700/1168\n",
      "Training Step: 508  | total loss: \u001b[1m\u001b[32m0.01815\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 009 | loss: 0.01815 - R2: 0.9993 -- iter: 0720/1168\n",
      "Training Step: 509  | total loss: \u001b[1m\u001b[32m0.02624\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 009 | loss: 0.02624 - R2: 1.0007 -- iter: 0740/1168\n",
      "Training Step: 510  | total loss: \u001b[1m\u001b[32m0.02535\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 009 | loss: 0.02535 - R2: 1.0004 -- iter: 0760/1168\n",
      "Training Step: 511  | total loss: \u001b[1m\u001b[32m0.02397\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 009 | loss: 0.02397 - R2: 0.9998 -- iter: 0780/1168\n",
      "Training Step: 512  | total loss: \u001b[1m\u001b[32m0.02246\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 009 | loss: 0.02246 - R2: 0.9992 -- iter: 0800/1168\n",
      "Training Step: 513  | total loss: \u001b[1m\u001b[32m0.02255\u001b[0m\u001b[0m | time: 0.176s\n",
      "| SGD | epoch: 009 | loss: 0.02255 - R2: 0.9990 -- iter: 0820/1168\n",
      "Training Step: 514  | total loss: \u001b[1m\u001b[32m0.02157\u001b[0m\u001b[0m | time: 0.182s\n",
      "| SGD | epoch: 009 | loss: 0.02157 - R2: 0.9999 -- iter: 0840/1168\n",
      "Training Step: 515  | total loss: \u001b[1m\u001b[32m0.02136\u001b[0m\u001b[0m | time: 0.185s\n",
      "| SGD | epoch: 009 | loss: 0.02136 - R2: 0.9994 -- iter: 0860/1168\n",
      "Training Step: 516  | total loss: \u001b[1m\u001b[32m0.02068\u001b[0m\u001b[0m | time: 0.188s\n",
      "| SGD | epoch: 009 | loss: 0.02068 - R2: 0.9995 -- iter: 0880/1168\n",
      "Training Step: 517  | total loss: \u001b[1m\u001b[32m0.02072\u001b[0m\u001b[0m | time: 0.191s\n",
      "| SGD | epoch: 009 | loss: 0.02072 - R2: 0.9999 -- iter: 0900/1168\n",
      "Training Step: 518  | total loss: \u001b[1m\u001b[32m0.02321\u001b[0m\u001b[0m | time: 0.193s\n",
      "| SGD | epoch: 009 | loss: 0.02321 - R2: 0.9997 -- iter: 0920/1168\n",
      "Training Step: 519  | total loss: \u001b[1m\u001b[32m0.02705\u001b[0m\u001b[0m | time: 0.196s\n",
      "| SGD | epoch: 009 | loss: 0.02705 - R2: 1.0008 -- iter: 0940/1168\n",
      "Training Step: 520  | total loss: \u001b[1m\u001b[32m0.02608\u001b[0m\u001b[0m | time: 0.200s\n",
      "| SGD | epoch: 009 | loss: 0.02608 - R2: 0.9998 -- iter: 0960/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 521  | total loss: \u001b[1m\u001b[32m0.02457\u001b[0m\u001b[0m | time: 0.207s\n",
      "| SGD | epoch: 009 | loss: 0.02457 - R2: 0.9997 -- iter: 0980/1168\n",
      "Training Step: 522  | total loss: \u001b[1m\u001b[32m0.02457\u001b[0m\u001b[0m | time: 0.211s\n",
      "| SGD | epoch: 009 | loss: 0.02457 - R2: 0.9997 -- iter: 1000/1168\n",
      "Training Step: 523  | total loss: \u001b[1m\u001b[32m0.02344\u001b[0m\u001b[0m | time: 0.214s\n",
      "| SGD | epoch: 009 | loss: 0.02344 - R2: 0.9999 -- iter: 1020/1168\n",
      "Training Step: 524  | total loss: \u001b[1m\u001b[32m0.02344\u001b[0m\u001b[0m | time: 0.217s\n",
      "| SGD | epoch: 009 | loss: 0.02344 - R2: 0.9997 -- iter: 1040/1168\n",
      "Training Step: 525  | total loss: \u001b[1m\u001b[32m0.02184\u001b[0m\u001b[0m | time: 0.221s\n",
      "| SGD | epoch: 009 | loss: 0.02184 - R2: 1.0006 -- iter: 1060/1168\n",
      "Training Step: 526  | total loss: \u001b[1m\u001b[32m0.02117\u001b[0m\u001b[0m | time: 0.225s\n",
      "| SGD | epoch: 009 | loss: 0.02117 - R2: 1.0010 -- iter: 1080/1168\n",
      "Training Step: 527  | total loss: \u001b[1m\u001b[32m0.02117\u001b[0m\u001b[0m | time: 0.232s\n",
      "| SGD | epoch: 009 | loss: 0.02117 - R2: 1.0010 -- iter: 1100/1168\n",
      "Training Step: 528  | total loss: \u001b[1m\u001b[32m0.01970\u001b[0m\u001b[0m | time: 0.237s\n",
      "| SGD | epoch: 009 | loss: 0.01970 - R2: 1.0010 -- iter: 1120/1168\n",
      "Training Step: 529  | total loss: \u001b[1m\u001b[32m0.02325\u001b[0m\u001b[0m | time: 0.241s\n",
      "| SGD | epoch: 009 | loss: 0.02325 - R2: 1.0011 -- iter: 1140/1168\n",
      "Training Step: 530  | total loss: \u001b[1m\u001b[32m0.02325\u001b[0m\u001b[0m | time: 0.243s\n",
      "| SGD | epoch: 009 | loss: 0.02325 - R2: 1.0011 -- iter: 1160/1168\n",
      "Training Step: 531  | total loss: \u001b[1m\u001b[32m0.02205\u001b[0m\u001b[0m | time: 1.252s\n",
      "| SGD | epoch: 009 | loss: 0.02205 - R2: 1.0003 | val_loss: 0.03234 - val_acc: 1.0007 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 532  | total loss: \u001b[1m\u001b[32m0.02091\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 010 | loss: 0.02091 - R2: 0.9997 -- iter: 0020/1168\n",
      "Training Step: 533  | total loss: \u001b[1m\u001b[32m0.01990\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 010 | loss: 0.01990 - R2: 0.9997 -- iter: 0040/1168\n",
      "Training Step: 534  | total loss: \u001b[1m\u001b[32m0.01923\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 010 | loss: 0.01923 - R2: 0.9999 -- iter: 0060/1168\n",
      "Training Step: 535  | total loss: \u001b[1m\u001b[32m0.01849\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 010 | loss: 0.01849 - R2: 0.9996 -- iter: 0080/1168\n",
      "Training Step: 536  | total loss: \u001b[1m\u001b[32m0.01635\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 010 | loss: 0.01635 - R2: 1.0001 -- iter: 0100/1168\n",
      "Training Step: 537  | total loss: \u001b[1m\u001b[32m0.01635\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 010 | loss: 0.01635 - R2: 1.0001 -- iter: 0120/1168\n",
      "Training Step: 538  | total loss: \u001b[1m\u001b[32m0.01626\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 010 | loss: 0.01626 - R2: 0.9999 -- iter: 0140/1168\n",
      "Training Step: 539  | total loss: \u001b[1m\u001b[32m0.01535\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 010 | loss: 0.01535 - R2: 0.9997 -- iter: 0160/1168\n",
      "Training Step: 540  | total loss: \u001b[1m\u001b[32m0.01535\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 010 | loss: 0.01535 - R2: 0.9997 -- iter: 0180/1168\n",
      "Training Step: 541  | total loss: \u001b[1m\u001b[32m0.01443\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 010 | loss: 0.01443 - R2: 0.9996 -- iter: 0200/1168\n",
      "Training Step: 542  | total loss: \u001b[1m\u001b[32m0.01377\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 010 | loss: 0.01377 - R2: 0.9997 -- iter: 0220/1168\n",
      "Training Step: 543  | total loss: \u001b[1m\u001b[32m0.01405\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 010 | loss: 0.01405 - R2: 0.9997 -- iter: 0240/1168\n",
      "Training Step: 544  | total loss: \u001b[1m\u001b[32m0.01468\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 010 | loss: 0.01468 - R2: 0.9998 -- iter: 0260/1168\n",
      "Training Step: 545  | total loss: \u001b[1m\u001b[32m0.01543\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 010 | loss: 0.01543 - R2: 1.0005 -- iter: 0280/1168\n",
      "Training Step: 546  | total loss: \u001b[1m\u001b[32m0.01521\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 010 | loss: 0.01521 - R2: 0.9998 -- iter: 0300/1168\n",
      "Training Step: 547  | total loss: \u001b[1m\u001b[32m0.01535\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 010 | loss: 0.01535 - R2: 0.9989 -- iter: 0320/1168\n",
      "Training Step: 548  | total loss: \u001b[1m\u001b[32m0.01509\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 010 | loss: 0.01509 - R2: 0.9989 -- iter: 0340/1168\n",
      "Training Step: 549  | total loss: \u001b[1m\u001b[32m0.01451\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 010 | loss: 0.01451 - R2: 0.9994 -- iter: 0360/1168\n",
      "Training Step: 550  | total loss: \u001b[1m\u001b[32m0.01751\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 010 | loss: 0.01751 - R2: 0.9996 -- iter: 0380/1168\n",
      "Training Step: 551  | total loss: \u001b[1m\u001b[32m0.01751\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 010 | loss: 0.01751 - R2: 0.9996 -- iter: 0400/1168\n",
      "Training Step: 552  | total loss: \u001b[1m\u001b[32m0.01706\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 010 | loss: 0.01706 - R2: 1.0006 -- iter: 0420/1168\n",
      "Training Step: 553  | total loss: \u001b[1m\u001b[32m0.01694\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 010 | loss: 0.01694 - R2: 1.0002 -- iter: 0440/1168\n",
      "Training Step: 554  | total loss: \u001b[1m\u001b[32m0.01827\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 010 | loss: 0.01827 - R2: 1.0005 -- iter: 0460/1168\n",
      "Training Step: 555  | total loss: \u001b[1m\u001b[32m0.01853\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 010 | loss: 0.01853 - R2: 1.0000 -- iter: 0480/1168\n",
      "Training Step: 556  | total loss: \u001b[1m\u001b[32m0.01853\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 010 | loss: 0.01853 - R2: 1.0000 -- iter: 0500/1168\n",
      "Training Step: 557  | total loss: \u001b[1m\u001b[32m0.01748\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 010 | loss: 0.01748 - R2: 0.9998 -- iter: 0520/1168\n",
      "Training Step: 558  | total loss: \u001b[1m\u001b[32m0.01716\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 010 | loss: 0.01716 - R2: 1.0002 -- iter: 0540/1168\n",
      "Training Step: 559  | total loss: \u001b[1m\u001b[32m0.01716\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 010 | loss: 0.01716 - R2: 1.0002 -- iter: 0560/1168\n",
      "Training Step: 560  | total loss: \u001b[1m\u001b[32m0.01665\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 010 | loss: 0.01665 - R2: 0.9999 -- iter: 0580/1168\n",
      "Training Step: 561  | total loss: \u001b[1m\u001b[32m0.01760\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 010 | loss: 0.01760 - R2: 0.9995 -- iter: 0600/1168\n",
      "Training Step: 562  | total loss: \u001b[1m\u001b[32m0.01791\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 010 | loss: 0.01791 - R2: 0.9992 -- iter: 0620/1168\n",
      "Training Step: 563  | total loss: \u001b[1m\u001b[32m0.01791\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 010 | loss: 0.01791 - R2: 0.9992 -- iter: 0640/1168\n",
      "Training Step: 564  | total loss: \u001b[1m\u001b[32m0.01884\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 010 | loss: 0.01884 - R2: 0.9982 -- iter: 0660/1168\n",
      "Training Step: 565  | total loss: \u001b[1m\u001b[32m0.01868\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 010 | loss: 0.01868 - R2: 0.9981 -- iter: 0680/1168\n",
      "Training Step: 566  | total loss: \u001b[1m\u001b[32m0.01914\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 010 | loss: 0.01914 - R2: 0.9990 -- iter: 0700/1168\n",
      "Training Step: 567  | total loss: \u001b[1m\u001b[32m0.01799\u001b[0m\u001b[0m | time: 0.172s\n",
      "| SGD | epoch: 010 | loss: 0.01799 - R2: 0.9990 -- iter: 0720/1168\n",
      "Training Step: 568  | total loss: \u001b[1m\u001b[32m0.01623\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 010 | loss: 0.01623 - R2: 0.9990 -- iter: 0740/1168\n",
      "Training Step: 569  | total loss: \u001b[1m\u001b[32m0.02059\u001b[0m\u001b[0m | time: 0.181s\n",
      "| SGD | epoch: 010 | loss: 0.02059 - R2: 0.9998 -- iter: 0760/1168\n",
      "Training Step: 570  | total loss: \u001b[1m\u001b[32m0.01950\u001b[0m\u001b[0m | time: 0.186s\n",
      "| SGD | epoch: 010 | loss: 0.01950 - R2: 1.0002 -- iter: 0780/1168\n",
      "Training Step: 571  | total loss: \u001b[1m\u001b[32m0.01935\u001b[0m\u001b[0m | time: 0.190s\n",
      "| SGD | epoch: 010 | loss: 0.01935 - R2: 1.0007 -- iter: 0800/1168\n",
      "Training Step: 572  | total loss: \u001b[1m\u001b[32m0.01857\u001b[0m\u001b[0m | time: 0.196s\n",
      "| SGD | epoch: 010 | loss: 0.01857 - R2: 1.0000 -- iter: 0820/1168\n",
      "Training Step: 573  | total loss: \u001b[1m\u001b[32m0.01831\u001b[0m\u001b[0m | time: 0.200s\n",
      "| SGD | epoch: 010 | loss: 0.01831 - R2: 1.0003 -- iter: 0840/1168\n",
      "Training Step: 574  | total loss: \u001b[1m\u001b[32m0.01761\u001b[0m\u001b[0m | time: 0.203s\n",
      "| SGD | epoch: 010 | loss: 0.01761 - R2: 0.9999 -- iter: 0860/1168\n",
      "Training Step: 575  | total loss: \u001b[1m\u001b[32m0.01704\u001b[0m\u001b[0m | time: 0.211s\n",
      "| SGD | epoch: 010 | loss: 0.01704 - R2: 0.9997 -- iter: 0880/1168\n",
      "Training Step: 576  | total loss: \u001b[1m\u001b[32m0.01704\u001b[0m\u001b[0m | time: 0.213s\n",
      "| SGD | epoch: 010 | loss: 0.01704 - R2: 0.9997 -- iter: 0900/1168\n",
      "Training Step: 577  | total loss: \u001b[1m\u001b[32m0.02478\u001b[0m\u001b[0m | time: 0.215s\n",
      "| SGD | epoch: 010 | loss: 0.02478 - R2: 0.9997 -- iter: 0920/1168\n",
      "Training Step: 578  | total loss: \u001b[1m\u001b[32m0.02412\u001b[0m\u001b[0m | time: 0.217s\n",
      "| SGD | epoch: 010 | loss: 0.02412 - R2: 0.9997 -- iter: 0940/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 579  | total loss: \u001b[1m\u001b[32m0.02221\u001b[0m\u001b[0m | time: 0.219s\n",
      "| SGD | epoch: 010 | loss: 0.02221 - R2: 0.9997 -- iter: 0960/1168\n",
      "Training Step: 580  | total loss: \u001b[1m\u001b[32m0.02099\u001b[0m\u001b[0m | time: 0.222s\n",
      "| SGD | epoch: 010 | loss: 0.02099 - R2: 1.0000 -- iter: 0980/1168\n",
      "Training Step: 581  | total loss: \u001b[1m\u001b[32m0.01950\u001b[0m\u001b[0m | time: 0.225s\n",
      "| SGD | epoch: 010 | loss: 0.01950 - R2: 1.0000 -- iter: 1000/1168\n",
      "Training Step: 582  | total loss: \u001b[1m\u001b[32m0.01994\u001b[0m\u001b[0m | time: 0.229s\n",
      "| SGD | epoch: 010 | loss: 0.01994 - R2: 0.9995 -- iter: 1020/1168\n",
      "Training Step: 583  | total loss: \u001b[1m\u001b[32m0.01994\u001b[0m\u001b[0m | time: 0.232s\n",
      "| SGD | epoch: 010 | loss: 0.01994 - R2: 0.9997 -- iter: 1040/1168\n",
      "Training Step: 584  | total loss: \u001b[1m\u001b[32m0.01858\u001b[0m\u001b[0m | time: 0.235s\n",
      "| SGD | epoch: 010 | loss: 0.01858 - R2: 0.9997 -- iter: 1060/1168\n",
      "Training Step: 585  | total loss: \u001b[1m\u001b[32m0.01774\u001b[0m\u001b[0m | time: 0.237s\n",
      "| SGD | epoch: 010 | loss: 0.01774 - R2: 0.9994 -- iter: 1080/1168\n",
      "Training Step: 586  | total loss: \u001b[1m\u001b[32m0.01743\u001b[0m\u001b[0m | time: 0.240s\n",
      "| SGD | epoch: 010 | loss: 0.01743 - R2: 0.9997 -- iter: 1100/1168\n",
      "Training Step: 587  | total loss: \u001b[1m\u001b[32m0.01804\u001b[0m\u001b[0m | time: 0.245s\n",
      "| SGD | epoch: 010 | loss: 0.01804 - R2: 0.9995 -- iter: 1120/1168\n",
      "Training Step: 588  | total loss: \u001b[1m\u001b[32m0.01756\u001b[0m\u001b[0m | time: 0.248s\n",
      "| SGD | epoch: 010 | loss: 0.01756 - R2: 0.9999 -- iter: 1140/1168\n",
      "Training Step: 589  | total loss: \u001b[1m\u001b[32m0.01775\u001b[0m\u001b[0m | time: 0.252s\n",
      "| SGD | epoch: 010 | loss: 0.01775 - R2: 1.0001 -- iter: 1160/1168\n",
      "Training Step: 590  | total loss: \u001b[1m\u001b[32m0.01688\u001b[0m\u001b[0m | time: 1.256s\n",
      "| SGD | epoch: 010 | loss: 0.01688 - R2: 1.0001 | val_loss: 0.03245 - val_acc: 1.0024 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 591  | total loss: \u001b[1m\u001b[32m0.01811\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 011 | loss: 0.01811 - R2: 0.9998 -- iter: 0020/1168\n",
      "Training Step: 592  | total loss: \u001b[1m\u001b[32m0.02055\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 011 | loss: 0.02055 - R2: 1.0006 -- iter: 0040/1168\n",
      "Training Step: 593  | total loss: \u001b[1m\u001b[32m0.02012\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 011 | loss: 0.02012 - R2: 0.9998 -- iter: 0060/1168\n",
      "Training Step: 594  | total loss: \u001b[1m\u001b[32m0.01937\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 011 | loss: 0.01937 - R2: 1.0002 -- iter: 0080/1168\n",
      "Training Step: 595  | total loss: \u001b[1m\u001b[32m0.01865\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 011 | loss: 0.01865 - R2: 1.0004 -- iter: 0100/1168\n",
      "Training Step: 596  | total loss: \u001b[1m\u001b[32m0.01836\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 011 | loss: 0.01836 - R2: 0.9999 -- iter: 0120/1168\n",
      "Training Step: 597  | total loss: \u001b[1m\u001b[32m0.01898\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 011 | loss: 0.01898 - R2: 0.9999 -- iter: 0140/1168\n",
      "Training Step: 598  | total loss: \u001b[1m\u001b[32m0.01839\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 011 | loss: 0.01839 - R2: 1.0007 -- iter: 0160/1168\n",
      "Training Step: 599  | total loss: \u001b[1m\u001b[32m0.01922\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 011 | loss: 0.01922 - R2: 1.0020 -- iter: 0180/1168\n",
      "Training Step: 600  | total loss: \u001b[1m\u001b[32m0.01889\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 011 | loss: 0.01889 - R2: 1.0028 -- iter: 0200/1168\n",
      "Training Step: 601  | total loss: \u001b[1m\u001b[32m0.01778\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 011 | loss: 0.01778 - R2: 1.0029 -- iter: 0220/1168\n",
      "Training Step: 602  | total loss: \u001b[1m\u001b[32m0.01677\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 011 | loss: 0.01677 - R2: 1.0019 -- iter: 0240/1168\n",
      "Training Step: 603  | total loss: \u001b[1m\u001b[32m0.01498\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 011 | loss: 0.01498 - R2: 1.0012 -- iter: 0260/1168\n",
      "Training Step: 604  | total loss: \u001b[1m\u001b[32m0.01498\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 011 | loss: 0.01498 - R2: 1.0012 -- iter: 0280/1168\n",
      "Training Step: 605  | total loss: \u001b[1m\u001b[32m0.01513\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 011 | loss: 0.01513 - R2: 0.9998 -- iter: 0300/1168\n",
      "Training Step: 606  | total loss: \u001b[1m\u001b[32m0.01478\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 011 | loss: 0.01478 - R2: 1.0003 -- iter: 0320/1168\n",
      "Training Step: 607  | total loss: \u001b[1m\u001b[32m0.01412\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 011 | loss: 0.01412 - R2: 1.0003 -- iter: 0340/1168\n",
      "Training Step: 608  | total loss: \u001b[1m\u001b[32m0.01411\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 011 | loss: 0.01411 - R2: 1.0004 -- iter: 0360/1168\n",
      "Training Step: 609  | total loss: \u001b[1m\u001b[32m0.01526\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 011 | loss: 0.01526 - R2: 0.9990 -- iter: 0380/1168\n",
      "Training Step: 610  | total loss: \u001b[1m\u001b[32m0.01606\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 011 | loss: 0.01606 - R2: 0.9984 -- iter: 0400/1168\n",
      "Training Step: 611  | total loss: \u001b[1m\u001b[32m0.01547\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 011 | loss: 0.01547 - R2: 0.9983 -- iter: 0420/1168\n",
      "Training Step: 612  | total loss: \u001b[1m\u001b[32m0.01590\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 011 | loss: 0.01590 - R2: 0.9994 -- iter: 0440/1168\n",
      "Training Step: 613  | total loss: \u001b[1m\u001b[32m0.01927\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 011 | loss: 0.01927 - R2: 1.0007 -- iter: 0460/1168\n",
      "Training Step: 614  | total loss: \u001b[1m\u001b[32m0.01811\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 011 | loss: 0.01811 - R2: 1.0007 -- iter: 0480/1168\n",
      "Training Step: 615  | total loss: \u001b[1m\u001b[32m0.01906\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 011 | loss: 0.01906 - R2: 0.9997 -- iter: 0500/1168\n",
      "Training Step: 616  | total loss: \u001b[1m\u001b[32m0.01861\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 011 | loss: 0.01861 - R2: 0.9997 -- iter: 0520/1168\n",
      "Training Step: 617  | total loss: \u001b[1m\u001b[32m0.01839\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 011 | loss: 0.01839 - R2: 0.9994 -- iter: 0540/1168\n",
      "Training Step: 618  | total loss: \u001b[1m\u001b[32m0.01754\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 011 | loss: 0.01754 - R2: 0.9987 -- iter: 0560/1168\n",
      "Training Step: 619  | total loss: \u001b[1m\u001b[32m0.01809\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 011 | loss: 0.01809 - R2: 0.9998 -- iter: 0580/1168\n",
      "Training Step: 620  | total loss: \u001b[1m\u001b[32m0.01731\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 011 | loss: 0.01731 - R2: 0.9992 -- iter: 0600/1168\n",
      "Training Step: 621  | total loss: \u001b[1m\u001b[32m0.01731\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 011 | loss: 0.01731 - R2: 0.9992 -- iter: 0620/1168\n",
      "Training Step: 622  | total loss: \u001b[1m\u001b[32m0.01800\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 011 | loss: 0.01800 - R2: 0.9995 -- iter: 0640/1168\n",
      "Training Step: 623  | total loss: \u001b[1m\u001b[32m0.02244\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 011 | loss: 0.02244 - R2: 1.0007 -- iter: 0660/1168\n",
      "Training Step: 624  | total loss: \u001b[1m\u001b[32m0.02148\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 011 | loss: 0.02148 - R2: 1.0001 -- iter: 0680/1168\n",
      "Training Step: 625  | total loss: \u001b[1m\u001b[32m0.02148\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 011 | loss: 0.02148 - R2: 1.0003 -- iter: 0700/1168\n",
      "Training Step: 626  | total loss: \u001b[1m\u001b[32m0.02125\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 011 | loss: 0.02125 - R2: 0.9989 -- iter: 0720/1168\n",
      "Training Step: 627  | total loss: \u001b[1m\u001b[32m0.02294\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 011 | loss: 0.02294 - R2: 0.9989 -- iter: 0740/1168\n",
      "Training Step: 628  | total loss: \u001b[1m\u001b[32m0.04677\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 011 | loss: 0.04677 - R2: 0.9957 -- iter: 0760/1168\n",
      "Training Step: 629  | total loss: \u001b[1m\u001b[32m0.04288\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 011 | loss: 0.04288 - R2: 0.9967 -- iter: 0780/1168\n",
      "Training Step: 630  | total loss: \u001b[1m\u001b[32m0.04104\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 011 | loss: 0.04104 - R2: 0.9977 -- iter: 0800/1168\n",
      "Training Step: 631  | total loss: \u001b[1m\u001b[32m0.03854\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 011 | loss: 0.03854 - R2: 0.9979 -- iter: 0820/1168\n",
      "Training Step: 632  | total loss: \u001b[1m\u001b[32m0.03515\u001b[0m\u001b[0m | time: 0.171s\n",
      "| SGD | epoch: 011 | loss: 0.03515 - R2: 0.9981 -- iter: 0840/1168\n",
      "Training Step: 633  | total loss: \u001b[1m\u001b[32m0.03366\u001b[0m\u001b[0m | time: 0.180s\n",
      "| SGD | epoch: 011 | loss: 0.03366 - R2: 0.9984 -- iter: 0860/1168\n",
      "Training Step: 634  | total loss: \u001b[1m\u001b[32m0.03047\u001b[0m\u001b[0m | time: 0.182s\n",
      "| SGD | epoch: 011 | loss: 0.03047 - R2: 0.9991 -- iter: 0880/1168\n",
      "Training Step: 635  | total loss: \u001b[1m\u001b[32m0.03047\u001b[0m\u001b[0m | time: 0.184s\n",
      "| SGD | epoch: 011 | loss: 0.03047 - R2: 0.9990 -- iter: 0900/1168\n",
      "Training Step: 636  | total loss: \u001b[1m\u001b[32m0.03114\u001b[0m\u001b[0m | time: 0.193s\n",
      "| SGD | epoch: 011 | loss: 0.03114 - R2: 1.0003 -- iter: 0920/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 637  | total loss: \u001b[1m\u001b[32m0.03030\u001b[0m\u001b[0m | time: 0.198s\n",
      "| SGD | epoch: 011 | loss: 0.03030 - R2: 0.9997 -- iter: 0940/1168\n",
      "Training Step: 638  | total loss: \u001b[1m\u001b[32m0.03082\u001b[0m\u001b[0m | time: 0.204s\n",
      "| SGD | epoch: 011 | loss: 0.03082 - R2: 1.0011 -- iter: 0960/1168\n",
      "Training Step: 639  | total loss: \u001b[1m\u001b[32m0.02935\u001b[0m\u001b[0m | time: 0.210s\n",
      "| SGD | epoch: 011 | loss: 0.02935 - R2: 1.0014 -- iter: 0980/1168\n",
      "Training Step: 640  | total loss: \u001b[1m\u001b[32m0.02742\u001b[0m\u001b[0m | time: 0.213s\n",
      "| SGD | epoch: 011 | loss: 0.02742 - R2: 1.0016 -- iter: 1000/1168\n",
      "Training Step: 641  | total loss: \u001b[1m\u001b[32m0.02704\u001b[0m\u001b[0m | time: 0.216s\n",
      "| SGD | epoch: 011 | loss: 0.02704 - R2: 0.9999 -- iter: 1020/1168\n",
      "Training Step: 642  | total loss: \u001b[1m\u001b[32m0.02543\u001b[0m\u001b[0m | time: 0.227s\n",
      "| SGD | epoch: 011 | loss: 0.02543 - R2: 1.0007 -- iter: 1040/1168\n",
      "Training Step: 643  | total loss: \u001b[1m\u001b[32m0.02343\u001b[0m\u001b[0m | time: 0.237s\n",
      "| SGD | epoch: 011 | loss: 0.02343 - R2: 1.0005 -- iter: 1060/1168\n",
      "Training Step: 644  | total loss: \u001b[1m\u001b[32m0.02225\u001b[0m\u001b[0m | time: 0.241s\n",
      "| SGD | epoch: 011 | loss: 0.02225 - R2: 1.0005 -- iter: 1080/1168\n",
      "Training Step: 645  | total loss: \u001b[1m\u001b[32m0.02223\u001b[0m\u001b[0m | time: 0.247s\n",
      "| SGD | epoch: 011 | loss: 0.02223 - R2: 0.9999 -- iter: 1100/1168\n",
      "Training Step: 646  | total loss: \u001b[1m\u001b[32m0.02223\u001b[0m\u001b[0m | time: 0.249s\n",
      "| SGD | epoch: 011 | loss: 0.02223 - R2: 0.9999 -- iter: 1120/1168\n",
      "Training Step: 647  | total loss: \u001b[1m\u001b[32m0.02143\u001b[0m\u001b[0m | time: 0.252s\n",
      "| SGD | epoch: 011 | loss: 0.02143 - R2: 1.0008 -- iter: 1140/1168\n",
      "Training Step: 648  | total loss: \u001b[1m\u001b[32m0.02034\u001b[0m\u001b[0m | time: 0.254s\n",
      "| SGD | epoch: 011 | loss: 0.02034 - R2: 1.0002 -- iter: 1160/1168\n",
      "Training Step: 649  | total loss: \u001b[1m\u001b[32m0.02198\u001b[0m\u001b[0m | time: 1.268s\n",
      "| SGD | epoch: 011 | loss: 0.02198 - R2: 1.0005 | val_loss: 0.03068 - val_acc: 1.0015 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 650  | total loss: \u001b[1m\u001b[32m0.02013\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 012 | loss: 0.02013 - R2: 0.9999 -- iter: 0020/1168\n",
      "Training Step: 651  | total loss: \u001b[1m\u001b[32m0.01929\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 012 | loss: 0.01929 - R2: 1.0005 -- iter: 0040/1168\n",
      "Training Step: 652  | total loss: \u001b[1m\u001b[32m0.01834\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 012 | loss: 0.01834 - R2: 1.0002 -- iter: 0060/1168\n",
      "Training Step: 653  | total loss: \u001b[1m\u001b[32m0.01866\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 012 | loss: 0.01866 - R2: 0.9999 -- iter: 0080/1168\n",
      "Training Step: 654  | total loss: \u001b[1m\u001b[32m0.01788\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 012 | loss: 0.01788 - R2: 0.9997 -- iter: 0100/1168\n",
      "Training Step: 655  | total loss: \u001b[1m\u001b[32m0.01788\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 012 | loss: 0.01788 - R2: 0.9997 -- iter: 0120/1168\n",
      "Training Step: 656  | total loss: \u001b[1m\u001b[32m0.01928\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 012 | loss: 0.01928 - R2: 1.0002 -- iter: 0140/1168\n",
      "Training Step: 657  | total loss: \u001b[1m\u001b[32m0.01928\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 012 | loss: 0.01928 - R2: 1.0003 -- iter: 0160/1168\n",
      "Training Step: 658  | total loss: \u001b[1m\u001b[32m0.01851\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 012 | loss: 0.01851 - R2: 1.0003 -- iter: 0180/1168\n",
      "Training Step: 659  | total loss: \u001b[1m\u001b[32m0.01757\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 012 | loss: 0.01757 - R2: 0.9999 -- iter: 0200/1168\n",
      "Training Step: 660  | total loss: \u001b[1m\u001b[32m0.01530\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 012 | loss: 0.01530 - R2: 0.9993 -- iter: 0220/1168\n",
      "Training Step: 661  | total loss: \u001b[1m\u001b[32m0.01591\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 012 | loss: 0.01591 - R2: 1.0000 -- iter: 0240/1168\n",
      "Training Step: 662  | total loss: \u001b[1m\u001b[32m0.01513\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 012 | loss: 0.01513 - R2: 0.9999 -- iter: 0260/1168\n",
      "Training Step: 663  | total loss: \u001b[1m\u001b[32m0.01513\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 012 | loss: 0.01513 - R2: 0.9999 -- iter: 0280/1168\n",
      "Training Step: 664  | total loss: \u001b[1m\u001b[32m0.01446\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 012 | loss: 0.01446 - R2: 1.0001 -- iter: 0300/1168\n",
      "Training Step: 665  | total loss: \u001b[1m\u001b[32m0.01400\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 012 | loss: 0.01400 - R2: 0.9998 -- iter: 0320/1168\n",
      "Training Step: 666  | total loss: \u001b[1m\u001b[32m0.01393\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 012 | loss: 0.01393 - R2: 0.9998 -- iter: 0340/1168\n",
      "Training Step: 667  | total loss: \u001b[1m\u001b[32m0.01513\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 012 | loss: 0.01513 - R2: 0.9999 -- iter: 0360/1168\n",
      "Training Step: 668  | total loss: \u001b[1m\u001b[32m0.01742\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 012 | loss: 0.01742 - R2: 1.0009 -- iter: 0380/1168\n",
      "Training Step: 669  | total loss: \u001b[1m\u001b[32m0.01806\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 012 | loss: 0.01806 - R2: 1.0012 -- iter: 0400/1168\n",
      "Training Step: 670  | total loss: \u001b[1m\u001b[32m0.01806\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 012 | loss: 0.01806 - R2: 1.0012 -- iter: 0420/1168\n",
      "Training Step: 671  | total loss: \u001b[1m\u001b[32m0.01838\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 012 | loss: 0.01838 - R2: 1.0010 -- iter: 0440/1168\n",
      "Training Step: 672  | total loss: \u001b[1m\u001b[32m0.02586\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 012 | loss: 0.02586 - R2: 1.0005 -- iter: 0460/1168\n",
      "Training Step: 673  | total loss: \u001b[1m\u001b[32m0.02396\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 012 | loss: 0.02396 - R2: 1.0002 -- iter: 0480/1168\n",
      "Training Step: 674  | total loss: \u001b[1m\u001b[32m0.02508\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 012 | loss: 0.02508 - R2: 1.0001 -- iter: 0500/1168\n",
      "Training Step: 675  | total loss: \u001b[1m\u001b[32m0.02487\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 012 | loss: 0.02487 - R2: 0.9994 -- iter: 0520/1168\n",
      "Training Step: 676  | total loss: \u001b[1m\u001b[32m0.02520\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 012 | loss: 0.02520 - R2: 0.9996 -- iter: 0540/1168\n",
      "Training Step: 677  | total loss: \u001b[1m\u001b[32m0.02520\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 012 | loss: 0.02520 - R2: 0.9996 -- iter: 0560/1168\n",
      "Training Step: 678  | total loss: \u001b[1m\u001b[32m0.02314\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 012 | loss: 0.02314 - R2: 0.9998 -- iter: 0580/1168\n",
      "Training Step: 679  | total loss: \u001b[1m\u001b[32m0.02222\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 012 | loss: 0.02222 - R2: 1.0005 -- iter: 0600/1168\n",
      "Training Step: 680  | total loss: \u001b[1m\u001b[32m0.02222\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 012 | loss: 0.02222 - R2: 1.0005 -- iter: 0620/1168\n",
      "Training Step: 681  | total loss: \u001b[1m\u001b[32m0.01962\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 012 | loss: 0.01962 - R2: 0.9997 -- iter: 0640/1168\n",
      "Training Step: 682  | total loss: \u001b[1m\u001b[32m0.01962\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 012 | loss: 0.01962 - R2: 0.9997 -- iter: 0660/1168\n",
      "Training Step: 683  | total loss: \u001b[1m\u001b[32m0.02043\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 012 | loss: 0.02043 - R2: 0.9994 -- iter: 0680/1168\n",
      "Training Step: 684  | total loss: \u001b[1m\u001b[32m0.01958\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 012 | loss: 0.01958 - R2: 0.9992 -- iter: 0700/1168\n",
      "Training Step: 685  | total loss: \u001b[1m\u001b[32m0.01958\u001b[0m\u001b[0m | time: 0.165s\n",
      "| SGD | epoch: 012 | loss: 0.01958 - R2: 0.9992 -- iter: 0720/1168\n",
      "Training Step: 686  | total loss: \u001b[1m\u001b[32m0.02337\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 012 | loss: 0.02337 - R2: 0.9999 -- iter: 0740/1168\n",
      "Training Step: 687  | total loss: \u001b[1m\u001b[32m0.02264\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 012 | loss: 0.02264 - R2: 0.9999 -- iter: 0760/1168\n",
      "Training Step: 688  | total loss: \u001b[1m\u001b[32m0.02127\u001b[0m\u001b[0m | time: 0.174s\n",
      "| SGD | epoch: 012 | loss: 0.02127 - R2: 0.9998 -- iter: 0780/1168\n",
      "Training Step: 689  | total loss: \u001b[1m\u001b[32m0.02692\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 012 | loss: 0.02692 - R2: 1.0006 -- iter: 0800/1168\n",
      "Training Step: 690  | total loss: \u001b[1m\u001b[32m0.02538\u001b[0m\u001b[0m | time: 0.179s\n",
      "| SGD | epoch: 012 | loss: 0.02538 - R2: 0.9999 -- iter: 0820/1168\n",
      "Training Step: 691  | total loss: \u001b[1m\u001b[32m0.02437\u001b[0m\u001b[0m | time: 0.183s\n",
      "| SGD | epoch: 012 | loss: 0.02437 - R2: 1.0003 -- iter: 0840/1168\n",
      "Training Step: 692  | total loss: \u001b[1m\u001b[32m0.02437\u001b[0m\u001b[0m | time: 0.187s\n",
      "| SGD | epoch: 012 | loss: 0.02437 - R2: 1.0003 -- iter: 0860/1168\n",
      "Training Step: 693  | total loss: \u001b[1m\u001b[32m0.02350\u001b[0m\u001b[0m | time: 0.196s\n",
      "| SGD | epoch: 012 | loss: 0.02350 - R2: 0.9999 -- iter: 0880/1168\n",
      "Training Step: 694  | total loss: \u001b[1m\u001b[32m0.02303\u001b[0m\u001b[0m | time: 0.200s\n",
      "| SGD | epoch: 012 | loss: 0.02303 - R2: 1.0005 -- iter: 0900/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 695  | total loss: \u001b[1m\u001b[32m0.02057\u001b[0m\u001b[0m | time: 0.206s\n",
      "| SGD | epoch: 012 | loss: 0.02057 - R2: 1.0005 -- iter: 0920/1168\n",
      "Training Step: 696  | total loss: \u001b[1m\u001b[32m0.02057\u001b[0m\u001b[0m | time: 0.209s\n",
      "| SGD | epoch: 012 | loss: 0.02057 - R2: 1.0005 -- iter: 0940/1168\n",
      "Training Step: 697  | total loss: \u001b[1m\u001b[32m0.02204\u001b[0m\u001b[0m | time: 0.214s\n",
      "| SGD | epoch: 012 | loss: 0.02204 - R2: 1.0013 -- iter: 0960/1168\n",
      "Training Step: 698  | total loss: \u001b[1m\u001b[32m0.02204\u001b[0m\u001b[0m | time: 0.224s\n",
      "| SGD | epoch: 012 | loss: 0.02204 - R2: 1.0013 -- iter: 0980/1168\n",
      "Training Step: 699  | total loss: \u001b[1m\u001b[32m0.01988\u001b[0m\u001b[0m | time: 0.228s\n",
      "| SGD | epoch: 012 | loss: 0.01988 - R2: 1.0008 -- iter: 1000/1168\n",
      "Training Step: 700  | total loss: \u001b[1m\u001b[32m0.01988\u001b[0m\u001b[0m | time: 0.231s\n",
      "| SGD | epoch: 012 | loss: 0.01988 - R2: 1.0008 -- iter: 1020/1168\n",
      "Training Step: 701  | total loss: \u001b[1m\u001b[32m0.01970\u001b[0m\u001b[0m | time: 0.233s\n",
      "| SGD | epoch: 012 | loss: 0.01970 - R2: 0.9998 -- iter: 1040/1168\n",
      "Training Step: 702  | total loss: \u001b[1m\u001b[32m0.01929\u001b[0m\u001b[0m | time: 0.234s\n",
      "| SGD | epoch: 012 | loss: 0.01929 - R2: 0.9999 -- iter: 1060/1168\n",
      "Training Step: 703  | total loss: \u001b[1m\u001b[32m0.01876\u001b[0m\u001b[0m | time: 0.237s\n",
      "| SGD | epoch: 012 | loss: 0.01876 - R2: 1.0001 -- iter: 1080/1168\n",
      "Training Step: 704  | total loss: \u001b[1m\u001b[32m0.01931\u001b[0m\u001b[0m | time: 0.243s\n",
      "| SGD | epoch: 012 | loss: 0.01931 - R2: 1.0005 -- iter: 1100/1168\n",
      "Training Step: 705  | total loss: \u001b[1m\u001b[32m0.01931\u001b[0m\u001b[0m | time: 0.247s\n",
      "| SGD | epoch: 012 | loss: 0.01931 - R2: 1.0009 -- iter: 1120/1168\n",
      "Training Step: 706  | total loss: \u001b[1m\u001b[32m0.01943\u001b[0m\u001b[0m | time: 0.254s\n",
      "| SGD | epoch: 012 | loss: 0.01943 - R2: 1.0009 -- iter: 1140/1168\n",
      "Training Step: 707  | total loss: \u001b[1m\u001b[32m0.02046\u001b[0m\u001b[0m | time: 0.257s\n",
      "| SGD | epoch: 012 | loss: 0.02046 - R2: 0.9997 -- iter: 1160/1168\n",
      "Training Step: 708  | total loss: \u001b[1m\u001b[32m0.01982\u001b[0m\u001b[0m | time: 1.261s\n",
      "| SGD | epoch: 012 | loss: 0.01982 - R2: 1.0003 | val_loss: 0.03196 - val_acc: 0.9984 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 709  | total loss: \u001b[1m\u001b[32m0.01881\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 013 | loss: 0.01881 - R2: 1.0003 -- iter: 0020/1168\n",
      "Training Step: 710  | total loss: \u001b[1m\u001b[32m0.01934\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 013 | loss: 0.01934 - R2: 0.9994 -- iter: 0040/1168\n",
      "Training Step: 711  | total loss: \u001b[1m\u001b[32m0.02100\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 013 | loss: 0.02100 - R2: 0.9995 -- iter: 0060/1168\n",
      "Training Step: 712  | total loss: \u001b[1m\u001b[32m0.02100\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 013 | loss: 0.02100 - R2: 0.9995 -- iter: 0080/1168\n",
      "Training Step: 713  | total loss: \u001b[1m\u001b[32m0.02122\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 013 | loss: 0.02122 - R2: 0.9979 -- iter: 0100/1168\n",
      "Training Step: 714  | total loss: \u001b[1m\u001b[32m0.02122\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 013 | loss: 0.02122 - R2: 0.9979 -- iter: 0120/1168\n",
      "Training Step: 715  | total loss: \u001b[1m\u001b[32m0.02033\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 013 | loss: 0.02033 - R2: 0.9980 -- iter: 0140/1168\n",
      "Training Step: 716  | total loss: \u001b[1m\u001b[32m0.01881\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 013 | loss: 0.01881 - R2: 0.9978 -- iter: 0160/1168\n",
      "Training Step: 717  | total loss: \u001b[1m\u001b[32m0.01758\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 013 | loss: 0.01758 - R2: 0.9982 -- iter: 0180/1168\n",
      "Training Step: 718  | total loss: \u001b[1m\u001b[32m0.01767\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 013 | loss: 0.01767 - R2: 0.9992 -- iter: 0200/1168\n",
      "Training Step: 719  | total loss: \u001b[1m\u001b[32m0.01765\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 013 | loss: 0.01765 - R2: 1.0000 -- iter: 0220/1168\n",
      "Training Step: 720  | total loss: \u001b[1m\u001b[32m0.01821\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 013 | loss: 0.01821 - R2: 1.0005 -- iter: 0240/1168\n",
      "Training Step: 721  | total loss: \u001b[1m\u001b[32m0.01834\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 013 | loss: 0.01834 - R2: 1.0008 -- iter: 0260/1168\n",
      "Training Step: 722  | total loss: \u001b[1m\u001b[32m0.01743\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 013 | loss: 0.01743 - R2: 1.0004 -- iter: 0280/1168\n",
      "Training Step: 723  | total loss: \u001b[1m\u001b[32m0.01705\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 013 | loss: 0.01705 - R2: 1.0004 -- iter: 0300/1168\n",
      "Training Step: 724  | total loss: \u001b[1m\u001b[32m0.01644\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 013 | loss: 0.01644 - R2: 1.0008 -- iter: 0320/1168\n",
      "Training Step: 725  | total loss: \u001b[1m\u001b[32m0.01579\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 013 | loss: 0.01579 - R2: 1.0002 -- iter: 0340/1168\n",
      "Training Step: 726  | total loss: \u001b[1m\u001b[32m0.01670\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 013 | loss: 0.01670 - R2: 0.9989 -- iter: 0360/1168\n",
      "Training Step: 727  | total loss: \u001b[1m\u001b[32m0.01670\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 013 | loss: 0.01670 - R2: 0.9989 -- iter: 0380/1168\n",
      "Training Step: 728  | total loss: \u001b[1m\u001b[32m0.01751\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 013 | loss: 0.01751 - R2: 0.9993 -- iter: 0400/1168\n",
      "Training Step: 729  | total loss: \u001b[1m\u001b[32m0.01813\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 013 | loss: 0.01813 - R2: 0.9993 -- iter: 0420/1168\n",
      "Training Step: 730  | total loss: \u001b[1m\u001b[32m0.01708\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 013 | loss: 0.01708 - R2: 0.9989 -- iter: 0440/1168\n",
      "Training Step: 731  | total loss: \u001b[1m\u001b[32m0.01708\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 013 | loss: 0.01708 - R2: 0.9989 -- iter: 0460/1168\n",
      "Training Step: 732  | total loss: \u001b[1m\u001b[32m0.01736\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 013 | loss: 0.01736 - R2: 0.9997 -- iter: 0480/1168\n",
      "Training Step: 733  | total loss: \u001b[1m\u001b[32m0.01641\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 013 | loss: 0.01641 - R2: 0.9994 -- iter: 0500/1168\n",
      "Training Step: 734  | total loss: \u001b[1m\u001b[32m0.01943\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 013 | loss: 0.01943 - R2: 0.9998 -- iter: 0520/1168\n",
      "Training Step: 735  | total loss: \u001b[1m\u001b[32m0.02127\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 013 | loss: 0.02127 - R2: 1.0001 -- iter: 0540/1168\n",
      "Training Step: 736  | total loss: \u001b[1m\u001b[32m0.02164\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 013 | loss: 0.02164 - R2: 0.9998 -- iter: 0560/1168\n",
      "Training Step: 737  | total loss: \u001b[1m\u001b[32m0.02092\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 013 | loss: 0.02092 - R2: 0.9995 -- iter: 0580/1168\n",
      "Training Step: 738  | total loss: \u001b[1m\u001b[32m0.02092\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 013 | loss: 0.02092 - R2: 0.9995 -- iter: 0600/1168\n",
      "Training Step: 739  | total loss: \u001b[1m\u001b[32m0.02178\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 013 | loss: 0.02178 - R2: 1.0001 -- iter: 0620/1168\n",
      "Training Step: 740  | total loss: \u001b[1m\u001b[32m0.02039\u001b[0m\u001b[0m | time: 0.178s\n",
      "| SGD | epoch: 013 | loss: 0.02039 - R2: 1.0004 -- iter: 0640/1168\n",
      "Training Step: 741  | total loss: \u001b[1m\u001b[32m0.02015\u001b[0m\u001b[0m | time: 0.182s\n",
      "| SGD | epoch: 013 | loss: 0.02015 - R2: 0.9998 -- iter: 0660/1168\n",
      "Training Step: 742  | total loss: \u001b[1m\u001b[32m0.02340\u001b[0m\u001b[0m | time: 0.184s\n",
      "| SGD | epoch: 013 | loss: 0.02340 - R2: 1.0006 -- iter: 0680/1168\n",
      "Training Step: 743  | total loss: \u001b[1m\u001b[32m0.02197\u001b[0m\u001b[0m | time: 0.186s\n",
      "| SGD | epoch: 013 | loss: 0.02197 - R2: 1.0006 -- iter: 0700/1168\n",
      "Training Step: 744  | total loss: \u001b[1m\u001b[32m0.02171\u001b[0m\u001b[0m | time: 0.189s\n",
      "| SGD | epoch: 013 | loss: 0.02171 - R2: 1.0000 -- iter: 0720/1168\n",
      "Training Step: 745  | total loss: \u001b[1m\u001b[32m0.02053\u001b[0m\u001b[0m | time: 0.193s\n",
      "| SGD | epoch: 013 | loss: 0.02053 - R2: 0.9998 -- iter: 0740/1168\n",
      "Training Step: 746  | total loss: \u001b[1m\u001b[32m0.01977\u001b[0m\u001b[0m | time: 0.196s\n",
      "| SGD | epoch: 013 | loss: 0.01977 - R2: 1.0000 -- iter: 0760/1168\n",
      "Training Step: 747  | total loss: \u001b[1m\u001b[32m0.01837\u001b[0m\u001b[0m | time: 0.198s\n",
      "| SGD | epoch: 013 | loss: 0.01837 - R2: 0.9994 -- iter: 0780/1168\n",
      "Training Step: 748  | total loss: \u001b[1m\u001b[32m0.03292\u001b[0m\u001b[0m | time: 0.205s\n",
      "| SGD | epoch: 013 | loss: 0.03292 - R2: 1.0010 -- iter: 0800/1168\n",
      "Training Step: 749  | total loss: \u001b[1m\u001b[32m0.03207\u001b[0m\u001b[0m | time: 0.208s\n",
      "| SGD | epoch: 013 | loss: 0.03207 - R2: 1.0011 -- iter: 0820/1168\n",
      "Training Step: 750  | total loss: \u001b[1m\u001b[32m0.03207\u001b[0m\u001b[0m | time: 0.210s\n",
      "| SGD | epoch: 013 | loss: 0.03207 - R2: 1.0011 -- iter: 0840/1168\n",
      "Training Step: 751  | total loss: \u001b[1m\u001b[32m0.03017\u001b[0m\u001b[0m | time: 0.213s\n",
      "| SGD | epoch: 013 | loss: 0.03017 - R2: 1.0010 -- iter: 0860/1168\n",
      "Training Step: 752  | total loss: \u001b[1m\u001b[32m0.02863\u001b[0m\u001b[0m | time: 0.230s\n",
      "| SGD | epoch: 013 | loss: 0.02863 - R2: 1.0008 -- iter: 0880/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 753  | total loss: \u001b[1m\u001b[32m0.02699\u001b[0m\u001b[0m | time: 0.233s\n",
      "| SGD | epoch: 013 | loss: 0.02699 - R2: 1.0002 -- iter: 0900/1168\n",
      "Training Step: 754  | total loss: \u001b[1m\u001b[32m0.02567\u001b[0m\u001b[0m | time: 0.235s\n",
      "| SGD | epoch: 013 | loss: 0.02567 - R2: 0.9990 -- iter: 0920/1168\n",
      "Training Step: 755  | total loss: \u001b[1m\u001b[32m0.02571\u001b[0m\u001b[0m | time: 0.237s\n",
      "| SGD | epoch: 013 | loss: 0.02571 - R2: 1.0002 -- iter: 0940/1168\n",
      "Training Step: 756  | total loss: \u001b[1m\u001b[32m0.02527\u001b[0m\u001b[0m | time: 0.239s\n",
      "| SGD | epoch: 013 | loss: 0.02527 - R2: 0.9998 -- iter: 0960/1168\n",
      "Training Step: 757  | total loss: \u001b[1m\u001b[32m0.02441\u001b[0m\u001b[0m | time: 0.242s\n",
      "| SGD | epoch: 013 | loss: 0.02441 - R2: 1.0001 -- iter: 0980/1168\n",
      "Training Step: 758  | total loss: \u001b[1m\u001b[32m0.02493\u001b[0m\u001b[0m | time: 0.244s\n",
      "| SGD | epoch: 013 | loss: 0.02493 - R2: 1.0003 -- iter: 1000/1168\n",
      "Training Step: 759  | total loss: \u001b[1m\u001b[32m0.02490\u001b[0m\u001b[0m | time: 0.248s\n",
      "| SGD | epoch: 013 | loss: 0.02490 - R2: 0.9989 -- iter: 1020/1168\n",
      "Training Step: 760  | total loss: \u001b[1m\u001b[32m0.02425\u001b[0m\u001b[0m | time: 0.251s\n",
      "| SGD | epoch: 013 | loss: 0.02425 - R2: 0.9990 -- iter: 1040/1168\n",
      "Training Step: 761  | total loss: \u001b[1m\u001b[32m0.02252\u001b[0m\u001b[0m | time: 0.255s\n",
      "| SGD | epoch: 013 | loss: 0.02252 - R2: 0.9988 -- iter: 1060/1168\n",
      "Training Step: 762  | total loss: \u001b[1m\u001b[32m0.02252\u001b[0m\u001b[0m | time: 0.257s\n",
      "| SGD | epoch: 013 | loss: 0.02252 - R2: 0.9988 -- iter: 1080/1168\n",
      "Training Step: 763  | total loss: \u001b[1m\u001b[32m0.02187\u001b[0m\u001b[0m | time: 0.259s\n",
      "| SGD | epoch: 013 | loss: 0.02187 - R2: 0.9988 -- iter: 1100/1168\n",
      "Training Step: 764  | total loss: \u001b[1m\u001b[32m0.02738\u001b[0m\u001b[0m | time: 0.268s\n",
      "| SGD | epoch: 013 | loss: 0.02738 - R2: 1.0000 -- iter: 1120/1168\n",
      "Training Step: 765  | total loss: \u001b[1m\u001b[32m0.02605\u001b[0m\u001b[0m | time: 0.272s\n",
      "| SGD | epoch: 013 | loss: 0.02605 - R2: 0.9995 -- iter: 1140/1168\n",
      "Training Step: 766  | total loss: \u001b[1m\u001b[32m0.02605\u001b[0m\u001b[0m | time: 0.275s\n",
      "| SGD | epoch: 013 | loss: 0.02605 - R2: 0.9995 -- iter: 1160/1168\n",
      "Training Step: 767  | total loss: \u001b[1m\u001b[32m0.02495\u001b[0m\u001b[0m | time: 1.281s\n",
      "| SGD | epoch: 013 | loss: 0.02495 - R2: 0.9995 | val_loss: 0.03265 - val_acc: 1.0009 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 768  | total loss: \u001b[1m\u001b[32m0.02453\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 014 | loss: 0.02453 - R2: 0.9998 -- iter: 0020/1168\n",
      "Training Step: 769  | total loss: \u001b[1m\u001b[32m0.02394\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 014 | loss: 0.02394 - R2: 0.9998 -- iter: 0040/1168\n",
      "Training Step: 770  | total loss: \u001b[1m\u001b[32m0.02247\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 014 | loss: 0.02247 - R2: 0.9995 -- iter: 0060/1168\n",
      "Training Step: 771  | total loss: \u001b[1m\u001b[32m0.02207\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 014 | loss: 0.02207 - R2: 0.9995 -- iter: 0080/1168\n",
      "Training Step: 772  | total loss: \u001b[1m\u001b[32m0.02045\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 014 | loss: 0.02045 - R2: 0.9994 -- iter: 0100/1168\n",
      "Training Step: 773  | total loss: \u001b[1m\u001b[32m0.01911\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 014 | loss: 0.01911 - R2: 0.9997 -- iter: 0120/1168\n",
      "Training Step: 774  | total loss: \u001b[1m\u001b[32m0.01911\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 014 | loss: 0.01911 - R2: 0.9997 -- iter: 0140/1168\n",
      "Training Step: 775  | total loss: \u001b[1m\u001b[32m0.01932\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 014 | loss: 0.01932 - R2: 0.9995 -- iter: 0160/1168\n",
      "Training Step: 776  | total loss: \u001b[1m\u001b[32m0.02193\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 014 | loss: 0.02193 - R2: 0.9993 -- iter: 0180/1168\n",
      "Training Step: 777  | total loss: \u001b[1m\u001b[32m0.02023\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 014 | loss: 0.02023 - R2: 0.9988 -- iter: 0200/1168\n",
      "Training Step: 778  | total loss: \u001b[1m\u001b[32m0.02023\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 014 | loss: 0.02023 - R2: 0.9988 -- iter: 0220/1168\n",
      "Training Step: 779  | total loss: \u001b[1m\u001b[32m0.02034\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 014 | loss: 0.02034 - R2: 0.9993 -- iter: 0240/1168\n",
      "Training Step: 780  | total loss: \u001b[1m\u001b[32m0.02034\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 014 | loss: 0.02034 - R2: 0.9993 -- iter: 0260/1168\n",
      "Training Step: 781  | total loss: \u001b[1m\u001b[32m0.01960\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 014 | loss: 0.01960 - R2: 0.9997 -- iter: 0280/1168\n",
      "Training Step: 782  | total loss: \u001b[1m\u001b[32m0.02046\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 014 | loss: 0.02046 - R2: 0.9998 -- iter: 0300/1168\n",
      "Training Step: 783  | total loss: \u001b[1m\u001b[32m0.02419\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 014 | loss: 0.02419 - R2: 1.0006 -- iter: 0320/1168\n",
      "Training Step: 784  | total loss: \u001b[1m\u001b[32m0.02419\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 014 | loss: 0.02419 - R2: 1.0006 -- iter: 0340/1168\n",
      "Training Step: 785  | total loss: \u001b[1m\u001b[32m0.02470\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 014 | loss: 0.02470 - R2: 1.0001 -- iter: 0360/1168\n",
      "Training Step: 786  | total loss: \u001b[1m\u001b[32m0.02320\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 014 | loss: 0.02320 - R2: 1.0001 -- iter: 0380/1168\n",
      "Training Step: 787  | total loss: \u001b[1m\u001b[32m0.02245\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 014 | loss: 0.02245 - R2: 1.0005 -- iter: 0400/1168\n",
      "Training Step: 788  | total loss: \u001b[1m\u001b[32m0.02207\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 014 | loss: 0.02207 - R2: 1.0001 -- iter: 0420/1168\n",
      "Training Step: 789  | total loss: \u001b[1m\u001b[32m0.02048\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 014 | loss: 0.02048 - R2: 1.0003 -- iter: 0440/1168\n",
      "Training Step: 790  | total loss: \u001b[1m\u001b[32m0.02086\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 014 | loss: 0.02086 - R2: 0.9991 -- iter: 0460/1168\n",
      "Training Step: 791  | total loss: \u001b[1m\u001b[32m0.02086\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 014 | loss: 0.02086 - R2: 0.9991 -- iter: 0480/1168\n",
      "Training Step: 792  | total loss: \u001b[1m\u001b[32m0.02017\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 014 | loss: 0.02017 - R2: 0.9997 -- iter: 0500/1168\n",
      "Training Step: 793  | total loss: \u001b[1m\u001b[32m0.01928\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 014 | loss: 0.01928 - R2: 1.0004 -- iter: 0520/1168\n",
      "Training Step: 794  | total loss: \u001b[1m\u001b[32m0.01995\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 014 | loss: 0.01995 - R2: 1.0008 -- iter: 0540/1168\n",
      "Training Step: 795  | total loss: \u001b[1m\u001b[32m0.01903\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 014 | loss: 0.01903 - R2: 0.9999 -- iter: 0560/1168\n",
      "Training Step: 796  | total loss: \u001b[1m\u001b[32m0.01903\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 014 | loss: 0.01903 - R2: 0.9999 -- iter: 0580/1168\n",
      "Training Step: 797  | total loss: \u001b[1m\u001b[32m0.01787\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 014 | loss: 0.01787 - R2: 1.0005 -- iter: 0600/1168\n",
      "Training Step: 798  | total loss: \u001b[1m\u001b[32m0.01759\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 014 | loss: 0.01759 - R2: 1.0013 -- iter: 0620/1168\n",
      "Training Step: 799  | total loss: \u001b[1m\u001b[32m0.01759\u001b[0m\u001b[0m | time: 0.173s\n",
      "| SGD | epoch: 014 | loss: 0.01759 - R2: 1.0013 -- iter: 0640/1168\n",
      "Training Step: 800  | total loss: \u001b[1m\u001b[32m0.01749\u001b[0m\u001b[0m | time: 0.181s\n",
      "| SGD | epoch: 014 | loss: 0.01749 - R2: 1.0003 -- iter: 0660/1168\n",
      "Training Step: 801  | total loss: \u001b[1m\u001b[32m0.01637\u001b[0m\u001b[0m | time: 0.185s\n",
      "| SGD | epoch: 014 | loss: 0.01637 - R2: 1.0002 -- iter: 0680/1168\n",
      "Training Step: 802  | total loss: \u001b[1m\u001b[32m0.01562\u001b[0m\u001b[0m | time: 0.191s\n",
      "| SGD | epoch: 014 | loss: 0.01562 - R2: 1.0003 -- iter: 0700/1168\n",
      "Training Step: 803  | total loss: \u001b[1m\u001b[32m0.01465\u001b[0m\u001b[0m | time: 0.195s\n",
      "| SGD | epoch: 014 | loss: 0.01465 - R2: 1.0000 -- iter: 0720/1168\n",
      "Training Step: 804  | total loss: \u001b[1m\u001b[32m0.01387\u001b[0m\u001b[0m | time: 0.204s\n",
      "| SGD | epoch: 014 | loss: 0.01387 - R2: 1.0006 -- iter: 0740/1168\n",
      "Training Step: 805  | total loss: \u001b[1m\u001b[32m0.01387\u001b[0m\u001b[0m | time: 0.215s\n",
      "| SGD | epoch: 014 | loss: 0.01387 - R2: 1.0011 -- iter: 0760/1168\n",
      "Training Step: 806  | total loss: \u001b[1m\u001b[32m0.01406\u001b[0m\u001b[0m | time: 0.219s\n",
      "| SGD | epoch: 014 | loss: 0.01406 - R2: 1.0003 -- iter: 0780/1168\n",
      "Training Step: 807  | total loss: \u001b[1m\u001b[32m0.01406\u001b[0m\u001b[0m | time: 0.232s\n",
      "| SGD | epoch: 014 | loss: 0.01406 - R2: 1.0006 -- iter: 0800/1168\n",
      "Training Step: 808  | total loss: \u001b[1m\u001b[32m0.01508\u001b[0m\u001b[0m | time: 0.241s\n",
      "| SGD | epoch: 014 | loss: 0.01508 - R2: 1.0006 -- iter: 0820/1168\n",
      "Training Step: 809  | total loss: \u001b[1m\u001b[32m0.01496\u001b[0m\u001b[0m | time: 0.243s\n",
      "| SGD | epoch: 014 | loss: 0.01496 - R2: 1.0007 -- iter: 0840/1168\n",
      "Training Step: 810  | total loss: \u001b[1m\u001b[32m0.01472\u001b[0m\u001b[0m | time: 0.268s\n",
      "| SGD | epoch: 014 | loss: 0.01472 - R2: 1.0003 -- iter: 0860/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 811  | total loss: \u001b[1m\u001b[32m0.01501\u001b[0m\u001b[0m | time: 0.291s\n",
      "| SGD | epoch: 014 | loss: 0.01501 - R2: 0.9992 -- iter: 0880/1168\n",
      "Training Step: 812  | total loss: \u001b[1m\u001b[32m0.01578\u001b[0m\u001b[0m | time: 0.300s\n",
      "| SGD | epoch: 014 | loss: 0.01578 - R2: 1.0005 -- iter: 0900/1168\n",
      "Training Step: 813  | total loss: \u001b[1m\u001b[32m0.01636\u001b[0m\u001b[0m | time: 0.302s\n",
      "| SGD | epoch: 014 | loss: 0.01636 - R2: 1.0002 -- iter: 0920/1168\n",
      "Training Step: 814  | total loss: \u001b[1m\u001b[32m0.01580\u001b[0m\u001b[0m | time: 0.305s\n",
      "| SGD | epoch: 014 | loss: 0.01580 - R2: 0.9996 -- iter: 0940/1168\n",
      "Training Step: 815  | total loss: \u001b[1m\u001b[32m0.01548\u001b[0m\u001b[0m | time: 0.307s\n",
      "| SGD | epoch: 014 | loss: 0.01548 - R2: 0.9997 -- iter: 0960/1168\n",
      "Training Step: 816  | total loss: \u001b[1m\u001b[32m0.01515\u001b[0m\u001b[0m | time: 0.308s\n",
      "| SGD | epoch: 014 | loss: 0.01515 - R2: 0.9990 -- iter: 0980/1168\n",
      "Training Step: 817  | total loss: \u001b[1m\u001b[32m0.01427\u001b[0m\u001b[0m | time: 0.312s\n",
      "| SGD | epoch: 014 | loss: 0.01427 - R2: 0.9990 -- iter: 1000/1168\n",
      "Training Step: 818  | total loss: \u001b[1m\u001b[32m0.01392\u001b[0m\u001b[0m | time: 0.314s\n",
      "| SGD | epoch: 014 | loss: 0.01392 - R2: 0.9992 -- iter: 1020/1168\n",
      "Training Step: 819  | total loss: \u001b[1m\u001b[32m0.01425\u001b[0m\u001b[0m | time: 0.317s\n",
      "| SGD | epoch: 014 | loss: 0.01425 - R2: 0.9989 -- iter: 1040/1168\n",
      "Training Step: 820  | total loss: \u001b[1m\u001b[32m0.01404\u001b[0m\u001b[0m | time: 0.346s\n",
      "| SGD | epoch: 014 | loss: 0.01404 - R2: 0.9989 -- iter: 1060/1168\n",
      "Training Step: 821  | total loss: \u001b[1m\u001b[32m0.01404\u001b[0m\u001b[0m | time: 0.354s\n",
      "| SGD | epoch: 014 | loss: 0.01404 - R2: 0.9989 -- iter: 1080/1168\n",
      "Training Step: 822  | total loss: \u001b[1m\u001b[32m0.01429\u001b[0m\u001b[0m | time: 0.356s\n",
      "| SGD | epoch: 014 | loss: 0.01429 - R2: 0.9989 -- iter: 1100/1168\n",
      "Training Step: 823  | total loss: \u001b[1m\u001b[32m0.01424\u001b[0m\u001b[0m | time: 0.364s\n",
      "| SGD | epoch: 014 | loss: 0.01424 - R2: 1.0001 -- iter: 1120/1168\n",
      "Training Step: 824  | total loss: \u001b[1m\u001b[32m0.01358\u001b[0m\u001b[0m | time: 0.372s\n",
      "| SGD | epoch: 014 | loss: 0.01358 - R2: 1.0004 -- iter: 1140/1168\n",
      "Training Step: 825  | total loss: \u001b[1m\u001b[32m0.01585\u001b[0m\u001b[0m | time: 0.376s\n",
      "| SGD | epoch: 014 | loss: 0.01585 - R2: 0.9996 -- iter: 1160/1168\n",
      "Training Step: 826  | total loss: \u001b[1m\u001b[32m0.01585\u001b[0m\u001b[0m | time: 1.386s\n",
      "| SGD | epoch: 014 | loss: 0.01585 - R2: 0.9986 | val_loss: 0.03439 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 827  | total loss: \u001b[1m\u001b[32m0.01575\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 015 | loss: 0.01575 - R2: 0.9986 -- iter: 0020/1168\n",
      "Training Step: 828  | total loss: \u001b[1m\u001b[32m0.02031\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 015 | loss: 0.02031 - R2: 0.9999 -- iter: 0040/1168\n",
      "Training Step: 829  | total loss: \u001b[1m\u001b[32m0.01843\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 015 | loss: 0.01843 - R2: 1.0008 -- iter: 0060/1168\n",
      "Training Step: 830  | total loss: \u001b[1m\u001b[32m0.01750\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 015 | loss: 0.01750 - R2: 1.0002 -- iter: 0080/1168\n",
      "Training Step: 831  | total loss: \u001b[1m\u001b[32m0.02177\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 015 | loss: 0.02177 - R2: 1.0004 -- iter: 0100/1168\n",
      "Training Step: 832  | total loss: \u001b[1m\u001b[32m0.02135\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 015 | loss: 0.02135 - R2: 1.0004 -- iter: 0120/1168\n",
      "Training Step: 833  | total loss: \u001b[1m\u001b[32m0.02135\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 015 | loss: 0.02135 - R2: 1.0004 -- iter: 0140/1168\n",
      "Training Step: 834  | total loss: \u001b[1m\u001b[32m0.01890\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 015 | loss: 0.01890 - R2: 1.0002 -- iter: 0160/1168\n",
      "Training Step: 835  | total loss: \u001b[1m\u001b[32m0.01890\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 015 | loss: 0.01890 - R2: 1.0002 -- iter: 0180/1168\n",
      "Training Step: 836  | total loss: \u001b[1m\u001b[32m0.01691\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 015 | loss: 0.01691 - R2: 1.0000 -- iter: 0200/1168\n",
      "Training Step: 837  | total loss: \u001b[1m\u001b[32m0.01691\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 015 | loss: 0.01691 - R2: 1.0000 -- iter: 0220/1168\n",
      "Training Step: 838  | total loss: \u001b[1m\u001b[32m0.01648\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 015 | loss: 0.01648 - R2: 1.0004 -- iter: 0240/1168\n",
      "Training Step: 839  | total loss: \u001b[1m\u001b[32m0.03215\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 015 | loss: 0.03215 - R2: 1.0032 -- iter: 0260/1168\n",
      "Training Step: 840  | total loss: \u001b[1m\u001b[32m0.04197\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 015 | loss: 0.04197 - R2: 1.0049 -- iter: 0280/1168\n",
      "Training Step: 841  | total loss: \u001b[1m\u001b[32m0.04197\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 015 | loss: 0.04197 - R2: 1.0049 -- iter: 0300/1168\n",
      "Training Step: 842  | total loss: \u001b[1m\u001b[32m0.04116\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 015 | loss: 0.04116 - R2: 1.0032 -- iter: 0320/1168\n",
      "Training Step: 843  | total loss: \u001b[1m\u001b[32m0.03952\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 015 | loss: 0.03952 - R2: 1.0001 -- iter: 0340/1168\n",
      "Training Step: 844  | total loss: \u001b[1m\u001b[32m0.03952\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 015 | loss: 0.03952 - R2: 0.9995 -- iter: 0360/1168\n",
      "Training Step: 845  | total loss: \u001b[1m\u001b[32m0.03708\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 015 | loss: 0.03708 - R2: 0.9995 -- iter: 0380/1168\n",
      "Training Step: 846  | total loss: \u001b[1m\u001b[32m0.03249\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 015 | loss: 0.03249 - R2: 0.9993 -- iter: 0400/1168\n",
      "Training Step: 847  | total loss: \u001b[1m\u001b[32m0.03249\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 015 | loss: 0.03249 - R2: 0.9983 -- iter: 0420/1168\n",
      "Training Step: 848  | total loss: \u001b[1m\u001b[32m0.02991\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 015 | loss: 0.02991 - R2: 0.9984 -- iter: 0440/1168\n",
      "Training Step: 849  | total loss: \u001b[1m\u001b[32m0.02991\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 015 | loss: 0.02991 - R2: 0.9984 -- iter: 0460/1168\n",
      "Training Step: 850  | total loss: \u001b[1m\u001b[32m0.02943\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 015 | loss: 0.02943 - R2: 0.9978 -- iter: 0480/1168\n",
      "Training Step: 851  | total loss: \u001b[1m\u001b[32m0.03031\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 015 | loss: 0.03031 - R2: 0.9995 -- iter: 0500/1168\n",
      "Training Step: 852  | total loss: \u001b[1m\u001b[32m0.02793\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 015 | loss: 0.02793 - R2: 0.9992 -- iter: 0520/1168\n",
      "Training Step: 853  | total loss: \u001b[1m\u001b[32m0.02656\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 015 | loss: 0.02656 - R2: 0.9987 -- iter: 0540/1168\n",
      "Training Step: 854  | total loss: \u001b[1m\u001b[32m0.02656\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 015 | loss: 0.02656 - R2: 0.9987 -- iter: 0560/1168\n",
      "Training Step: 855  | total loss: \u001b[1m\u001b[32m0.02539\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 015 | loss: 0.02539 - R2: 0.9994 -- iter: 0580/1168\n",
      "Training Step: 856  | total loss: \u001b[1m\u001b[32m0.02545\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 015 | loss: 0.02545 - R2: 1.0000 -- iter: 0600/1168\n",
      "Training Step: 857  | total loss: \u001b[1m\u001b[32m0.02441\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 015 | loss: 0.02441 - R2: 0.9995 -- iter: 0620/1168\n",
      "Training Step: 858  | total loss: \u001b[1m\u001b[32m0.02396\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 015 | loss: 0.02396 - R2: 1.0004 -- iter: 0640/1168\n",
      "Training Step: 859  | total loss: \u001b[1m\u001b[32m0.02300\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 015 | loss: 0.02300 - R2: 0.9996 -- iter: 0660/1168\n",
      "Training Step: 860  | total loss: \u001b[1m\u001b[32m0.02160\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 015 | loss: 0.02160 - R2: 0.9993 -- iter: 0680/1168\n",
      "Training Step: 861  | total loss: \u001b[1m\u001b[32m0.02172\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 015 | loss: 0.02172 - R2: 0.9986 -- iter: 0700/1168\n",
      "Training Step: 862  | total loss: \u001b[1m\u001b[32m0.02177\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 015 | loss: 0.02177 - R2: 0.9992 -- iter: 0720/1168\n",
      "Training Step: 863  | total loss: \u001b[1m\u001b[32m0.02177\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 015 | loss: 0.02177 - R2: 0.9992 -- iter: 0740/1168\n",
      "Training Step: 864  | total loss: \u001b[1m\u001b[32m0.02196\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 015 | loss: 0.02196 - R2: 0.9990 -- iter: 0760/1168\n",
      "Training Step: 865  | total loss: \u001b[1m\u001b[32m0.02121\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 015 | loss: 0.02121 - R2: 0.9994 -- iter: 0780/1168\n",
      "Training Step: 866  | total loss: \u001b[1m\u001b[32m0.02007\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 015 | loss: 0.02007 - R2: 0.9990 -- iter: 0800/1168\n",
      "Training Step: 867  | total loss: \u001b[1m\u001b[32m0.02007\u001b[0m\u001b[0m | time: 0.174s\n",
      "| SGD | epoch: 015 | loss: 0.02007 - R2: 0.9990 -- iter: 0820/1168\n",
      "Training Step: 868  | total loss: \u001b[1m\u001b[32m0.02291\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 015 | loss: 0.02291 - R2: 0.9990 -- iter: 0840/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 869  | total loss: \u001b[1m\u001b[32m0.02066\u001b[0m\u001b[0m | time: 0.180s\n",
      "| SGD | epoch: 015 | loss: 0.02066 - R2: 0.9984 -- iter: 0860/1168\n",
      "Training Step: 870  | total loss: \u001b[1m\u001b[32m0.02066\u001b[0m\u001b[0m | time: 0.185s\n",
      "| SGD | epoch: 015 | loss: 0.02066 - R2: 0.9984 -- iter: 0880/1168\n",
      "Training Step: 871  | total loss: \u001b[1m\u001b[32m0.01942\u001b[0m\u001b[0m | time: 0.188s\n",
      "| SGD | epoch: 015 | loss: 0.01942 - R2: 0.9982 -- iter: 0900/1168\n",
      "Training Step: 872  | total loss: \u001b[1m\u001b[32m0.01873\u001b[0m\u001b[0m | time: 0.193s\n",
      "| SGD | epoch: 015 | loss: 0.01873 - R2: 0.9986 -- iter: 0920/1168\n",
      "Training Step: 873  | total loss: \u001b[1m\u001b[32m0.01935\u001b[0m\u001b[0m | time: 0.197s\n",
      "| SGD | epoch: 015 | loss: 0.01935 - R2: 0.9981 -- iter: 0940/1168\n",
      "Training Step: 874  | total loss: \u001b[1m\u001b[32m0.01882\u001b[0m\u001b[0m | time: 0.201s\n",
      "| SGD | epoch: 015 | loss: 0.01882 - R2: 0.9986 -- iter: 0960/1168\n",
      "Training Step: 875  | total loss: \u001b[1m\u001b[32m0.01954\u001b[0m\u001b[0m | time: 0.207s\n",
      "| SGD | epoch: 015 | loss: 0.01954 - R2: 0.9989 -- iter: 0980/1168\n",
      "Training Step: 876  | total loss: \u001b[1m\u001b[32m0.01883\u001b[0m\u001b[0m | time: 0.210s\n",
      "| SGD | epoch: 015 | loss: 0.01883 - R2: 0.9998 -- iter: 1000/1168\n",
      "Training Step: 877  | total loss: \u001b[1m\u001b[32m0.01883\u001b[0m\u001b[0m | time: 0.212s\n",
      "| SGD | epoch: 015 | loss: 0.01883 - R2: 0.9998 -- iter: 1020/1168\n",
      "Training Step: 878  | total loss: \u001b[1m\u001b[32m0.01881\u001b[0m\u001b[0m | time: 0.218s\n",
      "| SGD | epoch: 015 | loss: 0.01881 - R2: 0.9998 -- iter: 1040/1168\n",
      "Training Step: 879  | total loss: \u001b[1m\u001b[32m0.01911\u001b[0m\u001b[0m | time: 0.225s\n",
      "| SGD | epoch: 015 | loss: 0.01911 - R2: 1.0000 -- iter: 1060/1168\n",
      "Training Step: 880  | total loss: \u001b[1m\u001b[32m0.01850\u001b[0m\u001b[0m | time: 0.227s\n",
      "| SGD | epoch: 015 | loss: 0.01850 - R2: 0.9998 -- iter: 1080/1168\n",
      "Training Step: 881  | total loss: \u001b[1m\u001b[32m0.01816\u001b[0m\u001b[0m | time: 0.230s\n",
      "| SGD | epoch: 015 | loss: 0.01816 - R2: 0.9997 -- iter: 1100/1168\n",
      "Training Step: 882  | total loss: \u001b[1m\u001b[32m0.01816\u001b[0m\u001b[0m | time: 0.232s\n",
      "| SGD | epoch: 015 | loss: 0.01816 - R2: 0.9997 -- iter: 1120/1168\n",
      "Training Step: 883  | total loss: \u001b[1m\u001b[32m0.02478\u001b[0m\u001b[0m | time: 0.235s\n",
      "| SGD | epoch: 015 | loss: 0.02478 - R2: 1.0010 -- iter: 1140/1168\n",
      "Training Step: 884  | total loss: \u001b[1m\u001b[32m0.02390\u001b[0m\u001b[0m | time: 0.245s\n",
      "| SGD | epoch: 015 | loss: 0.02390 - R2: 1.0010 -- iter: 1160/1168\n",
      "Training Step: 885  | total loss: \u001b[1m\u001b[32m0.02244\u001b[0m\u001b[0m | time: 1.251s\n",
      "| SGD | epoch: 015 | loss: 0.02244 - R2: 1.0013 | val_loss: 0.03276 - val_acc: 0.9998 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 886  | total loss: \u001b[1m\u001b[32m0.02244\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 016 | loss: 0.02244 - R2: 1.0013 -- iter: 0020/1168\n",
      "Training Step: 887  | total loss: \u001b[1m\u001b[32m0.02258\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 016 | loss: 0.02258 - R2: 1.0015 -- iter: 0040/1168\n",
      "Training Step: 888  | total loss: \u001b[1m\u001b[32m0.02216\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 016 | loss: 0.02216 - R2: 1.0016 -- iter: 0060/1168\n",
      "Training Step: 889  | total loss: \u001b[1m\u001b[32m0.02127\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 016 | loss: 0.02127 - R2: 1.0004 -- iter: 0080/1168\n",
      "Training Step: 890  | total loss: \u001b[1m\u001b[32m0.02097\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 016 | loss: 0.02097 - R2: 1.0001 -- iter: 0100/1168\n",
      "Training Step: 891  | total loss: \u001b[1m\u001b[32m0.02236\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 016 | loss: 0.02236 - R2: 1.0006 -- iter: 0120/1168\n",
      "Training Step: 892  | total loss: \u001b[1m\u001b[32m0.02206\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 016 | loss: 0.02206 - R2: 1.0012 -- iter: 0140/1168\n",
      "Training Step: 893  | total loss: \u001b[1m\u001b[32m0.02212\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 016 | loss: 0.02212 - R2: 1.0010 -- iter: 0160/1168\n",
      "Training Step: 894  | total loss: \u001b[1m\u001b[32m0.02103\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 016 | loss: 0.02103 - R2: 1.0008 -- iter: 0180/1168\n",
      "Training Step: 895  | total loss: \u001b[1m\u001b[32m0.01976\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 016 | loss: 0.01976 - R2: 1.0002 -- iter: 0200/1168\n",
      "Training Step: 896  | total loss: \u001b[1m\u001b[32m0.01971\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 016 | loss: 0.01971 - R2: 1.0011 -- iter: 0220/1168\n",
      "Training Step: 897  | total loss: \u001b[1m\u001b[32m0.02620\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 016 | loss: 0.02620 - R2: 1.0011 -- iter: 0240/1168\n",
      "Training Step: 898  | total loss: \u001b[1m\u001b[32m0.02474\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 016 | loss: 0.02474 - R2: 1.0003 -- iter: 0260/1168\n",
      "Training Step: 899  | total loss: \u001b[1m\u001b[32m0.02332\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 016 | loss: 0.02332 - R2: 1.0009 -- iter: 0280/1168\n",
      "Training Step: 900  | total loss: \u001b[1m\u001b[32m0.02350\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 016 | loss: 0.02350 - R2: 0.9993 -- iter: 0300/1168\n",
      "Training Step: 901  | total loss: \u001b[1m\u001b[32m0.02303\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 016 | loss: 0.02303 - R2: 0.9982 -- iter: 0320/1168\n",
      "Training Step: 902  | total loss: \u001b[1m\u001b[32m0.02113\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 016 | loss: 0.02113 - R2: 0.9978 -- iter: 0340/1168\n",
      "Training Step: 903  | total loss: \u001b[1m\u001b[32m0.02113\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 016 | loss: 0.02113 - R2: 0.9978 -- iter: 0360/1168\n",
      "Training Step: 904  | total loss: \u001b[1m\u001b[32m0.01998\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 016 | loss: 0.01998 - R2: 0.9980 -- iter: 0380/1168\n",
      "Training Step: 905  | total loss: \u001b[1m\u001b[32m0.02046\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 016 | loss: 0.02046 - R2: 0.9996 -- iter: 0400/1168\n",
      "Training Step: 906  | total loss: \u001b[1m\u001b[32m0.02046\u001b[0m\u001b[0m | time: 0.172s\n",
      "| SGD | epoch: 016 | loss: 0.02046 - R2: 0.9996 -- iter: 0420/1168\n",
      "Training Step: 907  | total loss: \u001b[1m\u001b[32m0.02115\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 016 | loss: 0.02115 - R2: 0.9997 -- iter: 0440/1168\n",
      "Training Step: 908  | total loss: \u001b[1m\u001b[32m0.02115\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 016 | loss: 0.02115 - R2: 0.9997 -- iter: 0460/1168\n",
      "Training Step: 909  | total loss: \u001b[1m\u001b[32m0.01881\u001b[0m\u001b[0m | time: 0.179s\n",
      "| SGD | epoch: 016 | loss: 0.01881 - R2: 0.9993 -- iter: 0480/1168\n",
      "Training Step: 910  | total loss: \u001b[1m\u001b[32m0.01881\u001b[0m\u001b[0m | time: 0.181s\n",
      "| SGD | epoch: 016 | loss: 0.01881 - R2: 0.9995 -- iter: 0500/1168\n",
      "Training Step: 911  | total loss: \u001b[1m\u001b[32m0.01924\u001b[0m\u001b[0m | time: 0.183s\n",
      "| SGD | epoch: 016 | loss: 0.01924 - R2: 0.9986 -- iter: 0520/1168\n",
      "Training Step: 912  | total loss: \u001b[1m\u001b[32m0.01924\u001b[0m\u001b[0m | time: 0.185s\n",
      "| SGD | epoch: 016 | loss: 0.01924 - R2: 0.9986 -- iter: 0540/1168\n",
      "Training Step: 913  | total loss: \u001b[1m\u001b[32m0.01766\u001b[0m\u001b[0m | time: 0.187s\n",
      "| SGD | epoch: 016 | loss: 0.01766 - R2: 1.0003 -- iter: 0560/1168\n",
      "Training Step: 914  | total loss: \u001b[1m\u001b[32m0.01766\u001b[0m\u001b[0m | time: 0.190s\n",
      "| SGD | epoch: 016 | loss: 0.01766 - R2: 1.0003 -- iter: 0580/1168\n",
      "Training Step: 915  | total loss: \u001b[1m\u001b[32m0.01643\u001b[0m\u001b[0m | time: 0.193s\n",
      "| SGD | epoch: 016 | loss: 0.01643 - R2: 0.9998 -- iter: 0600/1168\n",
      "Training Step: 916  | total loss: \u001b[1m\u001b[32m0.01643\u001b[0m\u001b[0m | time: 0.195s\n",
      "| SGD | epoch: 016 | loss: 0.01643 - R2: 1.0004 -- iter: 0620/1168\n",
      "Training Step: 917  | total loss: \u001b[1m\u001b[32m0.01642\u001b[0m\u001b[0m | time: 0.199s\n",
      "| SGD | epoch: 016 | loss: 0.01642 - R2: 1.0007 -- iter: 0640/1168\n",
      "Training Step: 918  | total loss: \u001b[1m\u001b[32m0.01642\u001b[0m\u001b[0m | time: 0.202s\n",
      "| SGD | epoch: 016 | loss: 0.01642 - R2: 1.0007 -- iter: 0660/1168\n",
      "Training Step: 919  | total loss: \u001b[1m\u001b[32m0.01478\u001b[0m\u001b[0m | time: 0.205s\n",
      "| SGD | epoch: 016 | loss: 0.01478 - R2: 1.0012 -- iter: 0680/1168\n",
      "Training Step: 920  | total loss: \u001b[1m\u001b[32m0.01569\u001b[0m\u001b[0m | time: 0.208s\n",
      "| SGD | epoch: 016 | loss: 0.01569 - R2: 1.0015 -- iter: 0700/1168\n",
      "Training Step: 921  | total loss: \u001b[1m\u001b[32m0.01758\u001b[0m\u001b[0m | time: 0.213s\n",
      "| SGD | epoch: 016 | loss: 0.01758 - R2: 1.0012 -- iter: 0720/1168\n",
      "Training Step: 922  | total loss: \u001b[1m\u001b[32m0.01758\u001b[0m\u001b[0m | time: 0.215s\n",
      "| SGD | epoch: 016 | loss: 0.01758 - R2: 1.0012 -- iter: 0740/1168\n",
      "Training Step: 923  | total loss: \u001b[1m\u001b[32m0.02052\u001b[0m\u001b[0m | time: 0.217s\n",
      "| SGD | epoch: 016 | loss: 0.02052 - R2: 1.0013 -- iter: 0760/1168\n",
      "Training Step: 924  | total loss: \u001b[1m\u001b[32m0.02048\u001b[0m\u001b[0m | time: 0.219s\n",
      "| SGD | epoch: 016 | loss: 0.02048 - R2: 1.0004 -- iter: 0780/1168\n",
      "Training Step: 925  | total loss: \u001b[1m\u001b[32m0.02003\u001b[0m\u001b[0m | time: 0.227s\n",
      "| SGD | epoch: 016 | loss: 0.02003 - R2: 0.9999 -- iter: 0800/1168\n",
      "Training Step: 926  | total loss: \u001b[1m\u001b[32m0.01910\u001b[0m\u001b[0m | time: 0.229s\n",
      "| SGD | epoch: 016 | loss: 0.01910 - R2: 1.0001 -- iter: 0820/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 927  | total loss: \u001b[1m\u001b[32m0.01809\u001b[0m\u001b[0m | time: 0.231s\n",
      "| SGD | epoch: 016 | loss: 0.01809 - R2: 1.0001 -- iter: 0840/1168\n",
      "Training Step: 928  | total loss: \u001b[1m\u001b[32m0.01653\u001b[0m\u001b[0m | time: 0.233s\n",
      "| SGD | epoch: 016 | loss: 0.01653 - R2: 0.9997 -- iter: 0860/1168\n",
      "Training Step: 929  | total loss: \u001b[1m\u001b[32m0.01653\u001b[0m\u001b[0m | time: 0.236s\n",
      "| SGD | epoch: 016 | loss: 0.01653 - R2: 0.9997 -- iter: 0880/1168\n",
      "Training Step: 930  | total loss: \u001b[1m\u001b[32m0.01519\u001b[0m\u001b[0m | time: 0.239s\n",
      "| SGD | epoch: 016 | loss: 0.01519 - R2: 1.0001 -- iter: 0900/1168\n",
      "Training Step: 931  | total loss: \u001b[1m\u001b[32m0.01534\u001b[0m\u001b[0m | time: 0.243s\n",
      "| SGD | epoch: 016 | loss: 0.01534 - R2: 0.9996 -- iter: 0920/1168\n",
      "Training Step: 932  | total loss: \u001b[1m\u001b[32m0.01534\u001b[0m\u001b[0m | time: 0.249s\n",
      "| SGD | epoch: 016 | loss: 0.01534 - R2: 0.9996 -- iter: 0940/1168\n",
      "Training Step: 933  | total loss: \u001b[1m\u001b[32m0.01481\u001b[0m\u001b[0m | time: 0.251s\n",
      "| SGD | epoch: 016 | loss: 0.01481 - R2: 0.9999 -- iter: 0960/1168\n",
      "Training Step: 934  | total loss: \u001b[1m\u001b[32m0.01413\u001b[0m\u001b[0m | time: 0.253s\n",
      "| SGD | epoch: 016 | loss: 0.01413 - R2: 0.9994 -- iter: 0980/1168\n",
      "Training Step: 935  | total loss: \u001b[1m\u001b[32m0.01765\u001b[0m\u001b[0m | time: 0.257s\n",
      "| SGD | epoch: 016 | loss: 0.01765 - R2: 1.0008 -- iter: 1000/1168\n",
      "Training Step: 936  | total loss: \u001b[1m\u001b[32m0.01765\u001b[0m\u001b[0m | time: 0.259s\n",
      "| SGD | epoch: 016 | loss: 0.01765 - R2: 1.0008 -- iter: 1020/1168\n",
      "Training Step: 937  | total loss: \u001b[1m\u001b[32m0.01733\u001b[0m\u001b[0m | time: 0.262s\n",
      "| SGD | epoch: 016 | loss: 0.01733 - R2: 1.0010 -- iter: 1040/1168\n",
      "Training Step: 938  | total loss: \u001b[1m\u001b[32m0.01808\u001b[0m\u001b[0m | time: 0.265s\n",
      "| SGD | epoch: 016 | loss: 0.01808 - R2: 0.9990 -- iter: 1060/1168\n",
      "Training Step: 939  | total loss: \u001b[1m\u001b[32m0.01698\u001b[0m\u001b[0m | time: 0.269s\n",
      "| SGD | epoch: 016 | loss: 0.01698 - R2: 0.9993 -- iter: 1080/1168\n",
      "Training Step: 940  | total loss: \u001b[1m\u001b[32m0.01603\u001b[0m\u001b[0m | time: 0.271s\n",
      "| SGD | epoch: 016 | loss: 0.01603 - R2: 0.9987 -- iter: 1100/1168\n",
      "Training Step: 941  | total loss: \u001b[1m\u001b[32m0.01603\u001b[0m\u001b[0m | time: 0.274s\n",
      "| SGD | epoch: 016 | loss: 0.01603 - R2: 0.9987 -- iter: 1120/1168\n",
      "Training Step: 942  | total loss: \u001b[1m\u001b[32m0.01736\u001b[0m\u001b[0m | time: 0.276s\n",
      "| SGD | epoch: 016 | loss: 0.01736 - R2: 0.9990 -- iter: 1140/1168\n",
      "Training Step: 943  | total loss: \u001b[1m\u001b[32m0.01713\u001b[0m\u001b[0m | time: 0.279s\n",
      "| SGD | epoch: 016 | loss: 0.01713 - R2: 0.9991 -- iter: 1160/1168\n",
      "Training Step: 944  | total loss: \u001b[1m\u001b[32m0.01713\u001b[0m\u001b[0m | time: 1.283s\n",
      "| SGD | epoch: 016 | loss: 0.01713 - R2: 0.9991 | val_loss: 0.03175 - val_acc: 1.0009 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 945  | total loss: \u001b[1m\u001b[32m0.01777\u001b[0m\u001b[0m | time: 0.019s\n",
      "| SGD | epoch: 017 | loss: 0.01777 - R2: 1.0003 -- iter: 0020/1168\n",
      "Training Step: 946  | total loss: \u001b[1m\u001b[32m0.01777\u001b[0m\u001b[0m | time: 0.023s\n",
      "| SGD | epoch: 017 | loss: 0.01777 - R2: 1.0003 -- iter: 0040/1168\n",
      "Training Step: 947  | total loss: \u001b[1m\u001b[32m0.01704\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 017 | loss: 0.01704 - R2: 1.0000 -- iter: 0060/1168\n",
      "Training Step: 948  | total loss: \u001b[1m\u001b[32m0.01907\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 017 | loss: 0.01907 - R2: 1.0012 -- iter: 0080/1168\n",
      "Training Step: 949  | total loss: \u001b[1m\u001b[32m0.02288\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 017 | loss: 0.02288 - R2: 1.0019 -- iter: 0100/1168\n",
      "Training Step: 950  | total loss: \u001b[1m\u001b[32m0.02218\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 017 | loss: 0.02218 - R2: 1.0021 -- iter: 0120/1168\n",
      "Training Step: 951  | total loss: \u001b[1m\u001b[32m0.02274\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 017 | loss: 0.02274 - R2: 1.0007 -- iter: 0140/1168\n",
      "Training Step: 952  | total loss: \u001b[1m\u001b[32m0.02344\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 017 | loss: 0.02344 - R2: 0.9998 -- iter: 0160/1168\n",
      "Training Step: 953  | total loss: \u001b[1m\u001b[32m0.02253\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 017 | loss: 0.02253 - R2: 1.0003 -- iter: 0180/1168\n",
      "Training Step: 954  | total loss: \u001b[1m\u001b[32m0.02169\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 017 | loss: 0.02169 - R2: 1.0006 -- iter: 0200/1168\n",
      "Training Step: 955  | total loss: \u001b[1m\u001b[32m0.01927\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 017 | loss: 0.01927 - R2: 1.0003 -- iter: 0220/1168\n",
      "Training Step: 956  | total loss: \u001b[1m\u001b[32m0.01996\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 017 | loss: 0.01996 - R2: 1.0000 -- iter: 0240/1168\n",
      "Training Step: 957  | total loss: \u001b[1m\u001b[32m0.01996\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 017 | loss: 0.01996 - R2: 1.0000 -- iter: 0260/1168\n",
      "Training Step: 958  | total loss: \u001b[1m\u001b[32m0.01867\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 017 | loss: 0.01867 - R2: 0.9994 -- iter: 0280/1168\n",
      "Training Step: 959  | total loss: \u001b[1m\u001b[32m0.01867\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 017 | loss: 0.01867 - R2: 0.9994 -- iter: 0300/1168\n",
      "Training Step: 960  | total loss: \u001b[1m\u001b[32m0.01741\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 017 | loss: 0.01741 - R2: 0.9989 -- iter: 0320/1168\n",
      "Training Step: 961  | total loss: \u001b[1m\u001b[32m0.01864\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 017 | loss: 0.01864 - R2: 0.9997 -- iter: 0340/1168\n",
      "Training Step: 962  | total loss: \u001b[1m\u001b[32m0.01864\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 017 | loss: 0.01864 - R2: 0.9997 -- iter: 0360/1168\n",
      "Training Step: 963  | total loss: \u001b[1m\u001b[32m0.01815\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 017 | loss: 0.01815 - R2: 0.9993 -- iter: 0380/1168\n",
      "Training Step: 964  | total loss: \u001b[1m\u001b[32m0.01815\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 017 | loss: 0.01815 - R2: 0.9993 -- iter: 0400/1168\n",
      "Training Step: 965  | total loss: \u001b[1m\u001b[32m0.01835\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 017 | loss: 0.01835 - R2: 0.9994 -- iter: 0420/1168\n",
      "Training Step: 966  | total loss: \u001b[1m\u001b[32m0.01751\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 017 | loss: 0.01751 - R2: 1.0005 -- iter: 0440/1168\n",
      "Training Step: 967  | total loss: \u001b[1m\u001b[32m0.01751\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 017 | loss: 0.01751 - R2: 1.0005 -- iter: 0460/1168\n",
      "Training Step: 968  | total loss: \u001b[1m\u001b[32m0.01716\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 017 | loss: 0.01716 - R2: 0.9997 -- iter: 0480/1168\n",
      "Training Step: 969  | total loss: \u001b[1m\u001b[32m0.01564\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 017 | loss: 0.01564 - R2: 0.9999 -- iter: 0500/1168\n",
      "Training Step: 970  | total loss: \u001b[1m\u001b[32m0.01682\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 017 | loss: 0.01682 - R2: 1.0002 -- iter: 0520/1168\n",
      "Training Step: 971  | total loss: \u001b[1m\u001b[32m0.01682\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 017 | loss: 0.01682 - R2: 1.0002 -- iter: 0540/1168\n",
      "Training Step: 972  | total loss: \u001b[1m\u001b[32m0.01658\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 017 | loss: 0.01658 - R2: 1.0002 -- iter: 0560/1168\n",
      "Training Step: 973  | total loss: \u001b[1m\u001b[32m0.01612\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 017 | loss: 0.01612 - R2: 1.0000 -- iter: 0580/1168\n",
      "Training Step: 974  | total loss: \u001b[1m\u001b[32m0.01600\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 017 | loss: 0.01600 - R2: 0.9999 -- iter: 0600/1168\n",
      "Training Step: 975  | total loss: \u001b[1m\u001b[32m0.01639\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 017 | loss: 0.01639 - R2: 0.9997 -- iter: 0620/1168\n",
      "Training Step: 976  | total loss: \u001b[1m\u001b[32m0.01809\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 017 | loss: 0.01809 - R2: 1.0003 -- iter: 0640/1168\n",
      "Training Step: 977  | total loss: \u001b[1m\u001b[32m0.01809\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 017 | loss: 0.01809 - R2: 1.0003 -- iter: 0660/1168\n",
      "Training Step: 978  | total loss: \u001b[1m\u001b[32m0.02409\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 017 | loss: 0.02409 - R2: 1.0011 -- iter: 0680/1168\n",
      "Training Step: 979  | total loss: \u001b[1m\u001b[32m0.02303\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 017 | loss: 0.02303 - R2: 1.0006 -- iter: 0700/1168\n",
      "Training Step: 980  | total loss: \u001b[1m\u001b[32m0.02225\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 017 | loss: 0.02225 - R2: 0.9989 -- iter: 0720/1168\n",
      "Training Step: 981  | total loss: \u001b[1m\u001b[32m0.02160\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 017 | loss: 0.02160 - R2: 0.9987 -- iter: 0740/1168\n",
      "Training Step: 982  | total loss: \u001b[1m\u001b[32m0.02091\u001b[0m\u001b[0m | time: 0.169s\n",
      "| SGD | epoch: 017 | loss: 0.02091 - R2: 0.9978 -- iter: 0760/1168\n",
      "Training Step: 983  | total loss: \u001b[1m\u001b[32m0.02065\u001b[0m\u001b[0m | time: 0.174s\n",
      "| SGD | epoch: 017 | loss: 0.02065 - R2: 0.9971 -- iter: 0780/1168\n",
      "Training Step: 984  | total loss: \u001b[1m\u001b[32m0.02065\u001b[0m\u001b[0m | time: 0.179s\n",
      "| SGD | epoch: 017 | loss: 0.02065 - R2: 0.9972 -- iter: 0800/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 985  | total loss: \u001b[1m\u001b[32m0.02159\u001b[0m\u001b[0m | time: 0.183s\n",
      "| SGD | epoch: 017 | loss: 0.02159 - R2: 0.9976 -- iter: 0820/1168\n",
      "Training Step: 986  | total loss: \u001b[1m\u001b[32m0.02036\u001b[0m\u001b[0m | time: 0.190s\n",
      "| SGD | epoch: 017 | loss: 0.02036 - R2: 0.9983 -- iter: 0840/1168\n",
      "Training Step: 987  | total loss: \u001b[1m\u001b[32m0.01988\u001b[0m\u001b[0m | time: 0.193s\n",
      "| SGD | epoch: 017 | loss: 0.01988 - R2: 0.9991 -- iter: 0860/1168\n",
      "Training Step: 988  | total loss: \u001b[1m\u001b[32m0.01944\u001b[0m\u001b[0m | time: 0.196s\n",
      "| SGD | epoch: 017 | loss: 0.01944 - R2: 0.9981 -- iter: 0880/1168\n",
      "Training Step: 989  | total loss: \u001b[1m\u001b[32m0.01865\u001b[0m\u001b[0m | time: 0.200s\n",
      "| SGD | epoch: 017 | loss: 0.01865 - R2: 0.9982 -- iter: 0900/1168\n",
      "Training Step: 990  | total loss: \u001b[1m\u001b[32m0.01799\u001b[0m\u001b[0m | time: 0.206s\n",
      "| SGD | epoch: 017 | loss: 0.01799 - R2: 0.9986 -- iter: 0920/1168\n",
      "Training Step: 991  | total loss: \u001b[1m\u001b[32m0.01799\u001b[0m\u001b[0m | time: 0.216s\n",
      "| SGD | epoch: 017 | loss: 0.01799 - R2: 0.9986 -- iter: 0940/1168\n",
      "Training Step: 992  | total loss: \u001b[1m\u001b[32m0.01811\u001b[0m\u001b[0m | time: 0.218s\n",
      "| SGD | epoch: 017 | loss: 0.01811 - R2: 0.9985 -- iter: 0960/1168\n",
      "Training Step: 993  | total loss: \u001b[1m\u001b[32m0.01772\u001b[0m\u001b[0m | time: 0.221s\n",
      "| SGD | epoch: 017 | loss: 0.01772 - R2: 0.9987 -- iter: 0980/1168\n",
      "Training Step: 994  | total loss: \u001b[1m\u001b[32m0.01879\u001b[0m\u001b[0m | time: 0.223s\n",
      "| SGD | epoch: 017 | loss: 0.01879 - R2: 0.9993 -- iter: 1000/1168\n",
      "Training Step: 995  | total loss: \u001b[1m\u001b[32m0.01802\u001b[0m\u001b[0m | time: 0.229s\n",
      "| SGD | epoch: 017 | loss: 0.01802 - R2: 1.0004 -- iter: 1020/1168\n",
      "Training Step: 996  | total loss: \u001b[1m\u001b[32m0.01792\u001b[0m\u001b[0m | time: 0.231s\n",
      "| SGD | epoch: 017 | loss: 0.01792 - R2: 1.0004 -- iter: 1040/1168\n",
      "Training Step: 997  | total loss: \u001b[1m\u001b[32m0.02089\u001b[0m\u001b[0m | time: 0.234s\n",
      "| SGD | epoch: 017 | loss: 0.02089 - R2: 1.0001 -- iter: 1060/1168\n",
      "Training Step: 998  | total loss: \u001b[1m\u001b[32m0.01901\u001b[0m\u001b[0m | time: 0.238s\n",
      "| SGD | epoch: 017 | loss: 0.01901 - R2: 1.0002 -- iter: 1080/1168\n",
      "Training Step: 999  | total loss: \u001b[1m\u001b[32m0.01928\u001b[0m\u001b[0m | time: 0.242s\n",
      "| SGD | epoch: 017 | loss: 0.01928 - R2: 1.0001 -- iter: 1100/1168\n",
      "Training Step: 1000  | total loss: \u001b[1m\u001b[32m0.01928\u001b[0m\u001b[0m | time: 0.245s\n",
      "| SGD | epoch: 017 | loss: 0.01928 - R2: 1.0001 -- iter: 1120/1168\n",
      "Training Step: 1001  | total loss: \u001b[1m\u001b[32m0.01922\u001b[0m\u001b[0m | time: 0.250s\n",
      "| SGD | epoch: 017 | loss: 0.01922 - R2: 1.0005 -- iter: 1140/1168\n",
      "Training Step: 1002  | total loss: \u001b[1m\u001b[32m0.01797\u001b[0m\u001b[0m | time: 0.254s\n",
      "| SGD | epoch: 017 | loss: 0.01797 - R2: 1.0001 -- iter: 1160/1168\n",
      "Training Step: 1003  | total loss: \u001b[1m\u001b[32m0.01766\u001b[0m\u001b[0m | time: 1.263s\n",
      "| SGD | epoch: 017 | loss: 0.01766 - R2: 1.0008 | val_loss: 0.03250 - val_acc: 1.0017 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 1004  | total loss: \u001b[1m\u001b[32m0.01717\u001b[0m\u001b[0m | time: 0.008s\n",
      "| SGD | epoch: 018 | loss: 0.01717 - R2: 1.0009 -- iter: 0020/1168\n",
      "Training Step: 1005  | total loss: \u001b[1m\u001b[32m0.01637\u001b[0m\u001b[0m | time: 0.018s\n",
      "| SGD | epoch: 018 | loss: 0.01637 - R2: 1.0013 -- iter: 0040/1168\n",
      "Training Step: 1006  | total loss: \u001b[1m\u001b[32m0.01637\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 018 | loss: 0.01637 - R2: 1.0013 -- iter: 0060/1168\n",
      "Training Step: 1007  | total loss: \u001b[1m\u001b[32m0.02119\u001b[0m\u001b[0m | time: 0.026s\n",
      "| SGD | epoch: 018 | loss: 0.02119 - R2: 1.0024 -- iter: 0080/1168\n",
      "Training Step: 1008  | total loss: \u001b[1m\u001b[32m0.02216\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 018 | loss: 0.02216 - R2: 1.0018 -- iter: 0100/1168\n",
      "Training Step: 1009  | total loss: \u001b[1m\u001b[32m0.02109\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 018 | loss: 0.02109 - R2: 1.0008 -- iter: 0120/1168\n",
      "Training Step: 1010  | total loss: \u001b[1m\u001b[32m0.02099\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 018 | loss: 0.02099 - R2: 1.0007 -- iter: 0140/1168\n",
      "Training Step: 1011  | total loss: \u001b[1m\u001b[32m0.02099\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 018 | loss: 0.02099 - R2: 1.0007 -- iter: 0160/1168\n",
      "Training Step: 1012  | total loss: \u001b[1m\u001b[32m0.01974\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 018 | loss: 0.01974 - R2: 1.0005 -- iter: 0180/1168\n",
      "Training Step: 1013  | total loss: \u001b[1m\u001b[32m0.02318\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 018 | loss: 0.02318 - R2: 1.0013 -- iter: 0200/1168\n",
      "Training Step: 1014  | total loss: \u001b[1m\u001b[32m0.02308\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 018 | loss: 0.02308 - R2: 1.0001 -- iter: 0220/1168\n",
      "Training Step: 1015  | total loss: \u001b[1m\u001b[32m0.02339\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 018 | loss: 0.02339 - R2: 0.9998 -- iter: 0240/1168\n",
      "Training Step: 1016  | total loss: \u001b[1m\u001b[32m0.02308\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 018 | loss: 0.02308 - R2: 0.9997 -- iter: 0260/1168\n",
      "Training Step: 1017  | total loss: \u001b[1m\u001b[32m0.02239\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 018 | loss: 0.02239 - R2: 0.9988 -- iter: 0280/1168\n",
      "Training Step: 1018  | total loss: \u001b[1m\u001b[32m0.02166\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 018 | loss: 0.02166 - R2: 0.9992 -- iter: 0300/1168\n",
      "Training Step: 1019  | total loss: \u001b[1m\u001b[32m0.02025\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 018 | loss: 0.02025 - R2: 0.9994 -- iter: 0320/1168\n",
      "Training Step: 1020  | total loss: \u001b[1m\u001b[32m0.02006\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 018 | loss: 0.02006 - R2: 0.9988 -- iter: 0340/1168\n",
      "Training Step: 1021  | total loss: \u001b[1m\u001b[32m0.01963\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 018 | loss: 0.01963 - R2: 0.9989 -- iter: 0360/1168\n",
      "Training Step: 1022  | total loss: \u001b[1m\u001b[32m0.01883\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 018 | loss: 0.01883 - R2: 0.9989 -- iter: 0380/1168\n",
      "Training Step: 1023  | total loss: \u001b[1m\u001b[32m0.01802\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 018 | loss: 0.01802 - R2: 0.9995 -- iter: 0400/1168\n",
      "Training Step: 1024  | total loss: \u001b[1m\u001b[32m0.01786\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 018 | loss: 0.01786 - R2: 0.9998 -- iter: 0420/1168\n",
      "Training Step: 1025  | total loss: \u001b[1m\u001b[32m0.01786\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 018 | loss: 0.01786 - R2: 0.9998 -- iter: 0440/1168\n",
      "Training Step: 1026  | total loss: \u001b[1m\u001b[32m0.01777\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 018 | loss: 0.01777 - R2: 1.0002 -- iter: 0460/1168\n",
      "Training Step: 1027  | total loss: \u001b[1m\u001b[32m0.01871\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 018 | loss: 0.01871 - R2: 1.0006 -- iter: 0480/1168\n",
      "Training Step: 1028  | total loss: \u001b[1m\u001b[32m0.01727\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 018 | loss: 0.01727 - R2: 0.9998 -- iter: 0500/1168\n",
      "Training Step: 1029  | total loss: \u001b[1m\u001b[32m0.01777\u001b[0m\u001b[0m | time: 0.165s\n",
      "| SGD | epoch: 018 | loss: 0.01777 - R2: 1.0002 -- iter: 0520/1168\n",
      "Training Step: 1030  | total loss: \u001b[1m\u001b[32m0.01842\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 018 | loss: 0.01842 - R2: 1.0008 -- iter: 0540/1168\n",
      "Training Step: 1031  | total loss: \u001b[1m\u001b[32m0.01842\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 018 | loss: 0.01842 - R2: 1.0008 -- iter: 0560/1168\n",
      "Training Step: 1032  | total loss: \u001b[1m\u001b[32m0.01823\u001b[0m\u001b[0m | time: 0.173s\n",
      "| SGD | epoch: 018 | loss: 0.01823 - R2: 1.0005 -- iter: 0580/1168\n",
      "Training Step: 1033  | total loss: \u001b[1m\u001b[32m0.01823\u001b[0m\u001b[0m | time: 0.176s\n",
      "| SGD | epoch: 018 | loss: 0.01823 - R2: 1.0005 -- iter: 0600/1168\n",
      "Training Step: 1034  | total loss: \u001b[1m\u001b[32m0.01913\u001b[0m\u001b[0m | time: 0.180s\n",
      "| SGD | epoch: 018 | loss: 0.01913 - R2: 1.0007 -- iter: 0620/1168\n",
      "Training Step: 1035  | total loss: \u001b[1m\u001b[32m0.01777\u001b[0m\u001b[0m | time: 0.181s\n",
      "| SGD | epoch: 018 | loss: 0.01777 - R2: 1.0005 -- iter: 0640/1168\n",
      "Training Step: 1036  | total loss: \u001b[1m\u001b[32m0.01777\u001b[0m\u001b[0m | time: 0.183s\n",
      "| SGD | epoch: 018 | loss: 0.01777 - R2: 1.0005 -- iter: 0660/1168\n",
      "Training Step: 1037  | total loss: \u001b[1m\u001b[32m0.01713\u001b[0m\u001b[0m | time: 0.198s\n",
      "| SGD | epoch: 018 | loss: 0.01713 - R2: 1.0009 -- iter: 0680/1168\n",
      "Training Step: 1038  | total loss: \u001b[1m\u001b[32m0.01904\u001b[0m\u001b[0m | time: 0.205s\n",
      "| SGD | epoch: 018 | loss: 0.01904 - R2: 1.0002 -- iter: 0700/1168\n",
      "Training Step: 1039  | total loss: \u001b[1m\u001b[32m0.01816\u001b[0m\u001b[0m | time: 0.215s\n",
      "| SGD | epoch: 018 | loss: 0.01816 - R2: 1.0007 -- iter: 0720/1168\n",
      "Training Step: 1040  | total loss: \u001b[1m\u001b[32m0.01736\u001b[0m\u001b[0m | time: 0.218s\n",
      "| SGD | epoch: 018 | loss: 0.01736 - R2: 1.0005 -- iter: 0740/1168\n",
      "Training Step: 1041  | total loss: \u001b[1m\u001b[32m0.01742\u001b[0m\u001b[0m | time: 0.221s\n",
      "| SGD | epoch: 018 | loss: 0.01742 - R2: 1.0005 -- iter: 0760/1168\n",
      "Training Step: 1042  | total loss: \u001b[1m\u001b[32m0.01779\u001b[0m\u001b[0m | time: 0.227s\n",
      "| SGD | epoch: 018 | loss: 0.01779 - R2: 0.9997 -- iter: 0780/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1043  | total loss: \u001b[1m\u001b[32m0.01703\u001b[0m\u001b[0m | time: 0.230s\n",
      "| SGD | epoch: 018 | loss: 0.01703 - R2: 0.9997 -- iter: 0800/1168\n",
      "Training Step: 1044  | total loss: \u001b[1m\u001b[32m0.01605\u001b[0m\u001b[0m | time: 0.239s\n",
      "| SGD | epoch: 018 | loss: 0.01605 - R2: 0.9995 -- iter: 0820/1168\n",
      "Training Step: 1045  | total loss: \u001b[1m\u001b[32m0.01537\u001b[0m\u001b[0m | time: 0.245s\n",
      "| SGD | epoch: 018 | loss: 0.01537 - R2: 0.9994 -- iter: 0840/1168\n",
      "Training Step: 1046  | total loss: \u001b[1m\u001b[32m0.01979\u001b[0m\u001b[0m | time: 0.249s\n",
      "| SGD | epoch: 018 | loss: 0.01979 - R2: 1.0001 -- iter: 0860/1168\n",
      "Training Step: 1047  | total loss: \u001b[1m\u001b[32m0.01882\u001b[0m\u001b[0m | time: 0.253s\n",
      "| SGD | epoch: 018 | loss: 0.01882 - R2: 1.0002 -- iter: 0880/1168\n",
      "Training Step: 1048  | total loss: \u001b[1m\u001b[32m0.01840\u001b[0m\u001b[0m | time: 0.264s\n",
      "| SGD | epoch: 018 | loss: 0.01840 - R2: 1.0000 -- iter: 0900/1168\n",
      "Training Step: 1049  | total loss: \u001b[1m\u001b[32m0.01781\u001b[0m\u001b[0m | time: 0.270s\n",
      "| SGD | epoch: 018 | loss: 0.01781 - R2: 0.9995 -- iter: 0920/1168\n",
      "Training Step: 1050  | total loss: \u001b[1m\u001b[32m0.01781\u001b[0m\u001b[0m | time: 0.272s\n",
      "| SGD | epoch: 018 | loss: 0.01781 - R2: 0.9995 -- iter: 0940/1168\n",
      "Training Step: 1051  | total loss: \u001b[1m\u001b[32m0.01678\u001b[0m\u001b[0m | time: 0.277s\n",
      "| SGD | epoch: 018 | loss: 0.01678 - R2: 1.0003 -- iter: 0960/1168\n",
      "Training Step: 1052  | total loss: \u001b[1m\u001b[32m0.01678\u001b[0m\u001b[0m | time: 0.283s\n",
      "| SGD | epoch: 018 | loss: 0.01678 - R2: 1.0003 -- iter: 0980/1168\n",
      "Training Step: 1053  | total loss: \u001b[1m\u001b[32m0.01804\u001b[0m\u001b[0m | time: 0.285s\n",
      "| SGD | epoch: 018 | loss: 0.01804 - R2: 1.0008 -- iter: 1000/1168\n",
      "Training Step: 1054  | total loss: \u001b[1m\u001b[32m0.01758\u001b[0m\u001b[0m | time: 0.289s\n",
      "| SGD | epoch: 018 | loss: 0.01758 - R2: 1.0006 -- iter: 1020/1168\n",
      "Training Step: 1055  | total loss: \u001b[1m\u001b[32m0.01661\u001b[0m\u001b[0m | time: 0.293s\n",
      "| SGD | epoch: 018 | loss: 0.01661 - R2: 1.0003 -- iter: 1040/1168\n",
      "Training Step: 1056  | total loss: \u001b[1m\u001b[32m0.01661\u001b[0m\u001b[0m | time: 0.299s\n",
      "| SGD | epoch: 018 | loss: 0.01661 - R2: 1.0003 -- iter: 1060/1168\n",
      "Training Step: 1057  | total loss: \u001b[1m\u001b[32m0.01800\u001b[0m\u001b[0m | time: 0.304s\n",
      "| SGD | epoch: 018 | loss: 0.01800 - R2: 0.9996 -- iter: 1080/1168\n",
      "Training Step: 1058  | total loss: \u001b[1m\u001b[32m0.01715\u001b[0m\u001b[0m | time: 0.306s\n",
      "| SGD | epoch: 018 | loss: 0.01715 - R2: 1.0000 -- iter: 1100/1168\n",
      "Training Step: 1059  | total loss: \u001b[1m\u001b[32m0.01867\u001b[0m\u001b[0m | time: 0.308s\n",
      "| SGD | epoch: 018 | loss: 0.01867 - R2: 1.0001 -- iter: 1120/1168\n",
      "Training Step: 1060  | total loss: \u001b[1m\u001b[32m0.01925\u001b[0m\u001b[0m | time: 0.310s\n",
      "| SGD | epoch: 018 | loss: 0.01925 - R2: 0.9998 -- iter: 1140/1168\n",
      "Training Step: 1061  | total loss: \u001b[1m\u001b[32m0.01842\u001b[0m\u001b[0m | time: 0.312s\n",
      "| SGD | epoch: 018 | loss: 0.01842 - R2: 0.9994 -- iter: 1160/1168\n",
      "Training Step: 1062  | total loss: \u001b[1m\u001b[32m0.01722\u001b[0m\u001b[0m | time: 1.316s\n",
      "| SGD | epoch: 018 | loss: 0.01722 - R2: 0.9989 | val_loss: 0.03192 - val_acc: 1.0023 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 1063  | total loss: \u001b[1m\u001b[32m0.01784\u001b[0m\u001b[0m | time: 0.008s\n",
      "| SGD | epoch: 019 | loss: 0.01784 - R2: 0.9980 -- iter: 0020/1168\n",
      "Training Step: 1064  | total loss: \u001b[1m\u001b[32m0.01784\u001b[0m\u001b[0m | time: 0.010s\n",
      "| SGD | epoch: 019 | loss: 0.01784 - R2: 0.9980 -- iter: 0040/1168\n",
      "Training Step: 1065  | total loss: \u001b[1m\u001b[32m0.01877\u001b[0m\u001b[0m | time: 0.014s\n",
      "| SGD | epoch: 019 | loss: 0.01877 - R2: 0.9990 -- iter: 0060/1168\n",
      "Training Step: 1066  | total loss: \u001b[1m\u001b[32m0.01805\u001b[0m\u001b[0m | time: 0.020s\n",
      "| SGD | epoch: 019 | loss: 0.01805 - R2: 0.9986 -- iter: 0080/1168\n",
      "Training Step: 1067  | total loss: \u001b[1m\u001b[32m0.01764\u001b[0m\u001b[0m | time: 0.023s\n",
      "| SGD | epoch: 019 | loss: 0.01764 - R2: 0.9984 -- iter: 0100/1168\n",
      "Training Step: 1068  | total loss: \u001b[1m\u001b[32m0.01833\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 019 | loss: 0.01833 - R2: 1.0004 -- iter: 0120/1168\n",
      "Training Step: 1069  | total loss: \u001b[1m\u001b[32m0.01825\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 019 | loss: 0.01825 - R2: 1.0000 -- iter: 0140/1168\n",
      "Training Step: 1070  | total loss: \u001b[1m\u001b[32m0.01878\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 019 | loss: 0.01878 - R2: 0.9991 -- iter: 0160/1168\n",
      "Training Step: 1071  | total loss: \u001b[1m\u001b[32m0.01878\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 019 | loss: 0.01878 - R2: 0.9991 -- iter: 0180/1168\n",
      "Training Step: 1072  | total loss: \u001b[1m\u001b[32m0.01828\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 019 | loss: 0.01828 - R2: 0.9996 -- iter: 0200/1168\n",
      "Training Step: 1073  | total loss: \u001b[1m\u001b[32m0.02163\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 019 | loss: 0.02163 - R2: 1.0001 -- iter: 0220/1168\n",
      "Training Step: 1074  | total loss: \u001b[1m\u001b[32m0.01988\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 019 | loss: 0.01988 - R2: 0.9999 -- iter: 0240/1168\n",
      "Training Step: 1075  | total loss: \u001b[1m\u001b[32m0.01845\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 019 | loss: 0.01845 - R2: 1.0003 -- iter: 0260/1168\n",
      "Training Step: 1076  | total loss: \u001b[1m\u001b[32m0.01789\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 019 | loss: 0.01789 - R2: 0.9999 -- iter: 0280/1168\n",
      "Training Step: 1077  | total loss: \u001b[1m\u001b[32m0.01839\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 019 | loss: 0.01839 - R2: 0.9997 -- iter: 0300/1168\n",
      "Training Step: 1078  | total loss: \u001b[1m\u001b[32m0.01839\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 019 | loss: 0.01839 - R2: 0.9997 -- iter: 0320/1168\n",
      "Training Step: 1079  | total loss: \u001b[1m\u001b[32m0.01996\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 019 | loss: 0.01996 - R2: 0.9987 -- iter: 0340/1168\n",
      "Training Step: 1080  | total loss: \u001b[1m\u001b[32m0.02087\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 019 | loss: 0.02087 - R2: 0.9983 -- iter: 0360/1168\n",
      "Training Step: 1081  | total loss: \u001b[1m\u001b[32m0.01991\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 019 | loss: 0.01991 - R2: 0.9982 -- iter: 0380/1168\n",
      "Training Step: 1082  | total loss: \u001b[1m\u001b[32m0.01937\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 019 | loss: 0.01937 - R2: 0.9983 -- iter: 0400/1168\n",
      "Training Step: 1083  | total loss: \u001b[1m\u001b[32m0.01932\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 019 | loss: 0.01932 - R2: 0.9989 -- iter: 0420/1168\n",
      "Training Step: 1084  | total loss: \u001b[1m\u001b[32m0.01932\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 019 | loss: 0.01932 - R2: 0.9989 -- iter: 0440/1168\n",
      "Training Step: 1085  | total loss: \u001b[1m\u001b[32m0.01830\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 019 | loss: 0.01830 - R2: 0.9991 -- iter: 0460/1168\n",
      "Training Step: 1086  | total loss: \u001b[1m\u001b[32m0.01961\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 019 | loss: 0.01961 - R2: 1.0004 -- iter: 0480/1168\n",
      "Training Step: 1087  | total loss: \u001b[1m\u001b[32m0.01849\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 019 | loss: 0.01849 - R2: 1.0006 -- iter: 0500/1168\n",
      "Training Step: 1088  | total loss: \u001b[1m\u001b[32m0.01743\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 019 | loss: 0.01743 - R2: 1.0006 -- iter: 0520/1168\n",
      "Training Step: 1089  | total loss: \u001b[1m\u001b[32m0.01743\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 019 | loss: 0.01743 - R2: 1.0005 -- iter: 0540/1168\n",
      "Training Step: 1090  | total loss: \u001b[1m\u001b[32m0.01676\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 019 | loss: 0.01676 - R2: 1.0004 -- iter: 0560/1168\n",
      "Training Step: 1091  | total loss: \u001b[1m\u001b[32m0.01714\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 019 | loss: 0.01714 - R2: 1.0002 -- iter: 0580/1168\n",
      "Training Step: 1092  | total loss: \u001b[1m\u001b[32m0.01739\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 019 | loss: 0.01739 - R2: 1.0012 -- iter: 0600/1168\n",
      "Training Step: 1093  | total loss: \u001b[1m\u001b[32m0.01668\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 019 | loss: 0.01668 - R2: 1.0015 -- iter: 0620/1168\n",
      "Training Step: 1094  | total loss: \u001b[1m\u001b[32m0.01668\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 019 | loss: 0.01668 - R2: 1.0015 -- iter: 0640/1168\n",
      "Training Step: 1095  | total loss: \u001b[1m\u001b[32m0.01733\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 019 | loss: 0.01733 - R2: 0.9998 -- iter: 0660/1168\n",
      "Training Step: 1096  | total loss: \u001b[1m\u001b[32m0.01766\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 019 | loss: 0.01766 - R2: 0.9988 -- iter: 0680/1168\n",
      "Training Step: 1097  | total loss: \u001b[1m\u001b[32m0.01766\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 019 | loss: 0.01766 - R2: 0.9988 -- iter: 0700/1168\n",
      "Training Step: 1098  | total loss: \u001b[1m\u001b[32m0.01706\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 019 | loss: 0.01706 - R2: 0.9994 -- iter: 0720/1168\n",
      "Training Step: 1099  | total loss: \u001b[1m\u001b[32m0.01562\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 019 | loss: 0.01562 - R2: 0.9994 -- iter: 0740/1168\n",
      "Training Step: 1100  | total loss: \u001b[1m\u001b[32m0.01562\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 019 | loss: 0.01562 - R2: 0.9994 -- iter: 0760/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1101  | total loss: \u001b[1m\u001b[32m0.02312\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 019 | loss: 0.02312 - R2: 1.0010 -- iter: 0780/1168\n",
      "Training Step: 1102  | total loss: \u001b[1m\u001b[32m0.02817\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 019 | loss: 0.02817 - R2: 1.0017 -- iter: 0800/1168\n",
      "Training Step: 1103  | total loss: \u001b[1m\u001b[32m0.02817\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 019 | loss: 0.02817 - R2: 1.0017 -- iter: 0820/1168\n",
      "Training Step: 1104  | total loss: \u001b[1m\u001b[32m0.02605\u001b[0m\u001b[0m | time: 0.169s\n",
      "| SGD | epoch: 019 | loss: 0.02605 - R2: 1.0015 -- iter: 0840/1168\n",
      "Training Step: 1105  | total loss: \u001b[1m\u001b[32m0.02605\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 019 | loss: 0.02605 - R2: 1.0015 -- iter: 0860/1168\n",
      "Training Step: 1106  | total loss: \u001b[1m\u001b[32m0.02390\u001b[0m\u001b[0m | time: 0.178s\n",
      "| SGD | epoch: 019 | loss: 0.02390 - R2: 1.0015 -- iter: 0880/1168\n",
      "Training Step: 1107  | total loss: \u001b[1m\u001b[32m0.02245\u001b[0m\u001b[0m | time: 0.181s\n",
      "| SGD | epoch: 019 | loss: 0.02245 - R2: 1.0013 -- iter: 0900/1168\n",
      "Training Step: 1108  | total loss: \u001b[1m\u001b[32m0.02691\u001b[0m\u001b[0m | time: 0.184s\n",
      "| SGD | epoch: 019 | loss: 0.02691 - R2: 1.0013 -- iter: 0920/1168\n",
      "Training Step: 1109  | total loss: \u001b[1m\u001b[32m0.02552\u001b[0m\u001b[0m | time: 0.188s\n",
      "| SGD | epoch: 019 | loss: 0.02552 - R2: 1.0006 -- iter: 0940/1168\n",
      "Training Step: 1110  | total loss: \u001b[1m\u001b[32m0.02493\u001b[0m\u001b[0m | time: 0.192s\n",
      "| SGD | epoch: 019 | loss: 0.02493 - R2: 1.0004 -- iter: 0960/1168\n",
      "Training Step: 1111  | total loss: \u001b[1m\u001b[32m0.02397\u001b[0m\u001b[0m | time: 0.196s\n",
      "| SGD | epoch: 019 | loss: 0.02397 - R2: 1.0001 -- iter: 0980/1168\n",
      "Training Step: 1112  | total loss: \u001b[1m\u001b[32m0.02320\u001b[0m\u001b[0m | time: 0.200s\n",
      "| SGD | epoch: 019 | loss: 0.02320 - R2: 0.9994 -- iter: 1000/1168\n",
      "Training Step: 1113  | total loss: \u001b[1m\u001b[32m0.02394\u001b[0m\u001b[0m | time: 0.204s\n",
      "| SGD | epoch: 019 | loss: 0.02394 - R2: 0.9997 -- iter: 1020/1168\n",
      "Training Step: 1114  | total loss: \u001b[1m\u001b[32m0.02394\u001b[0m\u001b[0m | time: 0.213s\n",
      "| SGD | epoch: 019 | loss: 0.02394 - R2: 0.9997 -- iter: 1040/1168\n",
      "Training Step: 1115  | total loss: \u001b[1m\u001b[32m0.02323\u001b[0m\u001b[0m | time: 0.218s\n",
      "| SGD | epoch: 019 | loss: 0.02323 - R2: 1.0000 -- iter: 1060/1168\n",
      "Training Step: 1116  | total loss: \u001b[1m\u001b[32m0.02309\u001b[0m\u001b[0m | time: 0.223s\n",
      "| SGD | epoch: 019 | loss: 0.02309 - R2: 0.9995 -- iter: 1080/1168\n",
      "Training Step: 1117  | total loss: \u001b[1m\u001b[32m0.02309\u001b[0m\u001b[0m | time: 0.225s\n",
      "| SGD | epoch: 019 | loss: 0.02309 - R2: 0.9995 -- iter: 1100/1168\n",
      "Training Step: 1118  | total loss: \u001b[1m\u001b[32m0.02246\u001b[0m\u001b[0m | time: 0.238s\n",
      "| SGD | epoch: 019 | loss: 0.02246 - R2: 1.0000 -- iter: 1120/1168\n",
      "Training Step: 1119  | total loss: \u001b[1m\u001b[32m0.02207\u001b[0m\u001b[0m | time: 0.240s\n",
      "| SGD | epoch: 019 | loss: 0.02207 - R2: 1.0007 -- iter: 1140/1168\n",
      "Training Step: 1120  | total loss: \u001b[1m\u001b[32m0.02051\u001b[0m\u001b[0m | time: 0.242s\n",
      "| SGD | epoch: 019 | loss: 0.02051 - R2: 1.0002 -- iter: 1160/1168\n",
      "Training Step: 1121  | total loss: \u001b[1m\u001b[32m0.01907\u001b[0m\u001b[0m | time: 1.252s\n",
      "| SGD | epoch: 019 | loss: 0.01907 - R2: 0.9991 | val_loss: 0.03212 - val_acc: 1.0012 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 1122  | total loss: \u001b[1m\u001b[32m0.01907\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 020 | loss: 0.01907 - R2: 0.9991 -- iter: 0020/1168\n",
      "Training Step: 1123  | total loss: \u001b[1m\u001b[32m0.01800\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 020 | loss: 0.01800 - R2: 0.9991 -- iter: 0040/1168\n",
      "Training Step: 1124  | total loss: \u001b[1m\u001b[32m0.01709\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 020 | loss: 0.01709 - R2: 0.9997 -- iter: 0060/1168\n",
      "Training Step: 1125  | total loss: \u001b[1m\u001b[32m0.01664\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 020 | loss: 0.01664 - R2: 0.9992 -- iter: 0080/1168\n",
      "Training Step: 1126  | total loss: \u001b[1m\u001b[32m0.01575\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 020 | loss: 0.01575 - R2: 0.9994 -- iter: 0100/1168\n",
      "Training Step: 1127  | total loss: \u001b[1m\u001b[32m0.01525\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 020 | loss: 0.01525 - R2: 0.9995 -- iter: 0120/1168\n",
      "Training Step: 1128  | total loss: \u001b[1m\u001b[32m0.01540\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 020 | loss: 0.01540 - R2: 0.9990 -- iter: 0140/1168\n",
      "Training Step: 1129  | total loss: \u001b[1m\u001b[32m0.01673\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 020 | loss: 0.01673 - R2: 0.9982 -- iter: 0160/1168\n",
      "Training Step: 1130  | total loss: \u001b[1m\u001b[32m0.01606\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 020 | loss: 0.01606 - R2: 0.9980 -- iter: 0180/1168\n",
      "Training Step: 1131  | total loss: \u001b[1m\u001b[32m0.01533\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 020 | loss: 0.01533 - R2: 0.9982 -- iter: 0200/1168\n",
      "Training Step: 1132  | total loss: \u001b[1m\u001b[32m0.01585\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 020 | loss: 0.01585 - R2: 0.9993 -- iter: 0220/1168\n",
      "Training Step: 1133  | total loss: \u001b[1m\u001b[32m0.01637\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 020 | loss: 0.01637 - R2: 1.0007 -- iter: 0240/1168\n",
      "Training Step: 1134  | total loss: \u001b[1m\u001b[32m0.01637\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 020 | loss: 0.01637 - R2: 1.0007 -- iter: 0260/1168\n",
      "Training Step: 1135  | total loss: \u001b[1m\u001b[32m0.01586\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 020 | loss: 0.01586 - R2: 1.0006 -- iter: 0280/1168\n",
      "Training Step: 1136  | total loss: \u001b[1m\u001b[32m0.01657\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 020 | loss: 0.01657 - R2: 0.9998 -- iter: 0300/1168\n",
      "Training Step: 1137  | total loss: \u001b[1m\u001b[32m0.02039\u001b[0m\u001b[0m | time: 0.173s\n",
      "| SGD | epoch: 020 | loss: 0.02039 - R2: 1.0009 -- iter: 0320/1168\n",
      "Training Step: 1138  | total loss: \u001b[1m\u001b[32m0.02002\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 020 | loss: 0.02002 - R2: 1.0005 -- iter: 0340/1168\n",
      "Training Step: 1139  | total loss: \u001b[1m\u001b[32m0.02002\u001b[0m\u001b[0m | time: 0.179s\n",
      "| SGD | epoch: 020 | loss: 0.02002 - R2: 1.0005 -- iter: 0360/1168\n",
      "Training Step: 1140  | total loss: \u001b[1m\u001b[32m0.01728\u001b[0m\u001b[0m | time: 0.184s\n",
      "| SGD | epoch: 020 | loss: 0.01728 - R2: 1.0004 -- iter: 0380/1168\n",
      "Training Step: 1141  | total loss: \u001b[1m\u001b[32m0.01771\u001b[0m\u001b[0m | time: 0.189s\n",
      "| SGD | epoch: 020 | loss: 0.01771 - R2: 1.0003 -- iter: 0400/1168\n",
      "Training Step: 1142  | total loss: \u001b[1m\u001b[32m0.01771\u001b[0m\u001b[0m | time: 0.192s\n",
      "| SGD | epoch: 020 | loss: 0.01771 - R2: 1.0003 -- iter: 0420/1168\n",
      "Training Step: 1143  | total loss: \u001b[1m\u001b[32m0.01755\u001b[0m\u001b[0m | time: 0.194s\n",
      "| SGD | epoch: 020 | loss: 0.01755 - R2: 0.9994 -- iter: 0440/1168\n",
      "Training Step: 1144  | total loss: \u001b[1m\u001b[32m0.01755\u001b[0m\u001b[0m | time: 0.199s\n",
      "| SGD | epoch: 020 | loss: 0.01755 - R2: 0.9997 -- iter: 0460/1168\n",
      "Training Step: 1145  | total loss: \u001b[1m\u001b[32m0.01697\u001b[0m\u001b[0m | time: 0.213s\n",
      "| SGD | epoch: 020 | loss: 0.01697 - R2: 0.9990 -- iter: 0480/1168\n",
      "Training Step: 1146  | total loss: \u001b[1m\u001b[32m0.01758\u001b[0m\u001b[0m | time: 0.225s\n",
      "| SGD | epoch: 020 | loss: 0.01758 - R2: 0.9990 -- iter: 0500/1168\n",
      "Training Step: 1147  | total loss: \u001b[1m\u001b[32m0.01663\u001b[0m\u001b[0m | time: 0.234s\n",
      "| SGD | epoch: 020 | loss: 0.01663 - R2: 0.9991 -- iter: 0520/1168\n",
      "Training Step: 1148  | total loss: \u001b[1m\u001b[32m0.01840\u001b[0m\u001b[0m | time: 0.238s\n",
      "| SGD | epoch: 020 | loss: 0.01840 - R2: 0.9991 -- iter: 0540/1168\n",
      "Training Step: 1149  | total loss: \u001b[1m\u001b[32m0.01759\u001b[0m\u001b[0m | time: 0.242s\n",
      "| SGD | epoch: 020 | loss: 0.01759 - R2: 0.9997 -- iter: 0560/1168\n",
      "Training Step: 1150  | total loss: \u001b[1m\u001b[32m0.01666\u001b[0m\u001b[0m | time: 0.244s\n",
      "| SGD | epoch: 020 | loss: 0.01666 - R2: 1.0001 -- iter: 0580/1168\n",
      "Training Step: 1151  | total loss: \u001b[1m\u001b[32m0.01614\u001b[0m\u001b[0m | time: 0.246s\n",
      "| SGD | epoch: 020 | loss: 0.01614 - R2: 1.0006 -- iter: 0600/1168\n",
      "Training Step: 1152  | total loss: \u001b[1m\u001b[32m0.01579\u001b[0m\u001b[0m | time: 0.248s\n",
      "| SGD | epoch: 020 | loss: 0.01579 - R2: 1.0006 -- iter: 0620/1168\n",
      "Training Step: 1153  | total loss: \u001b[1m\u001b[32m0.01800\u001b[0m\u001b[0m | time: 0.252s\n",
      "| SGD | epoch: 020 | loss: 0.01800 - R2: 1.0014 -- iter: 0640/1168\n",
      "Training Step: 1154  | total loss: \u001b[1m\u001b[32m0.02061\u001b[0m\u001b[0m | time: 0.265s\n",
      "| SGD | epoch: 020 | loss: 0.02061 - R2: 1.0023 -- iter: 0660/1168\n",
      "Training Step: 1155  | total loss: \u001b[1m\u001b[32m0.02032\u001b[0m\u001b[0m | time: 0.269s\n",
      "| SGD | epoch: 020 | loss: 0.02032 - R2: 1.0025 -- iter: 0680/1168\n",
      "Training Step: 1156  | total loss: \u001b[1m\u001b[32m0.02102\u001b[0m\u001b[0m | time: 0.273s\n",
      "| SGD | epoch: 020 | loss: 0.02102 - R2: 1.0022 -- iter: 0700/1168\n",
      "Training Step: 1157  | total loss: \u001b[1m\u001b[32m0.01997\u001b[0m\u001b[0m | time: 0.277s\n",
      "| SGD | epoch: 020 | loss: 0.01997 - R2: 1.0020 -- iter: 0720/1168\n",
      "Training Step: 1158  | total loss: \u001b[1m\u001b[32m0.01992\u001b[0m\u001b[0m | time: 0.288s\n",
      "| SGD | epoch: 020 | loss: 0.01992 - R2: 1.0011 -- iter: 0740/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1159  | total loss: \u001b[1m\u001b[32m0.01992\u001b[0m\u001b[0m | time: 0.292s\n",
      "| SGD | epoch: 020 | loss: 0.01992 - R2: 1.0011 -- iter: 0760/1168\n",
      "Training Step: 1160  | total loss: \u001b[1m\u001b[32m0.02029\u001b[0m\u001b[0m | time: 0.296s\n",
      "| SGD | epoch: 020 | loss: 0.02029 - R2: 0.9999 -- iter: 0780/1168\n",
      "Training Step: 1161  | total loss: \u001b[1m\u001b[32m0.01935\u001b[0m\u001b[0m | time: 0.298s\n",
      "| SGD | epoch: 020 | loss: 0.01935 - R2: 1.0000 -- iter: 0800/1168\n",
      "Training Step: 1162  | total loss: \u001b[1m\u001b[32m0.01852\u001b[0m\u001b[0m | time: 0.303s\n",
      "| SGD | epoch: 020 | loss: 0.01852 - R2: 0.9996 -- iter: 0820/1168\n",
      "Training Step: 1163  | total loss: \u001b[1m\u001b[32m0.01817\u001b[0m\u001b[0m | time: 0.305s\n",
      "| SGD | epoch: 020 | loss: 0.01817 - R2: 1.0004 -- iter: 0840/1168\n",
      "Training Step: 1164  | total loss: \u001b[1m\u001b[32m0.01735\u001b[0m\u001b[0m | time: 0.308s\n",
      "| SGD | epoch: 020 | loss: 0.01735 - R2: 1.0002 -- iter: 0860/1168\n",
      "Training Step: 1165  | total loss: \u001b[1m\u001b[32m0.01720\u001b[0m\u001b[0m | time: 0.310s\n",
      "| SGD | epoch: 020 | loss: 0.01720 - R2: 1.0002 -- iter: 0880/1168\n",
      "Training Step: 1166  | total loss: \u001b[1m\u001b[32m0.01788\u001b[0m\u001b[0m | time: 0.316s\n",
      "| SGD | epoch: 020 | loss: 0.01788 - R2: 0.9998 -- iter: 0900/1168\n",
      "Training Step: 1167  | total loss: \u001b[1m\u001b[32m0.01788\u001b[0m\u001b[0m | time: 0.324s\n",
      "| SGD | epoch: 020 | loss: 0.01788 - R2: 0.9998 -- iter: 0920/1168\n",
      "Training Step: 1168  | total loss: \u001b[1m\u001b[32m0.01861\u001b[0m\u001b[0m | time: 0.326s\n",
      "| SGD | epoch: 020 | loss: 0.01861 - R2: 0.9987 -- iter: 0940/1168\n",
      "Training Step: 1169  | total loss: \u001b[1m\u001b[32m0.01765\u001b[0m\u001b[0m | time: 0.330s\n",
      "| SGD | epoch: 020 | loss: 0.01765 - R2: 0.9993 -- iter: 0960/1168\n",
      "Training Step: 1170  | total loss: \u001b[1m\u001b[32m0.01838\u001b[0m\u001b[0m | time: 0.335s\n",
      "| SGD | epoch: 020 | loss: 0.01838 - R2: 0.9999 -- iter: 0980/1168\n",
      "Training Step: 1171  | total loss: \u001b[1m\u001b[32m0.01825\u001b[0m\u001b[0m | time: 0.338s\n",
      "| SGD | epoch: 020 | loss: 0.01825 - R2: 1.0004 -- iter: 1000/1168\n",
      "Training Step: 1172  | total loss: \u001b[1m\u001b[32m0.01782\u001b[0m\u001b[0m | time: 0.340s\n",
      "| SGD | epoch: 020 | loss: 0.01782 - R2: 1.0002 -- iter: 1020/1168\n",
      "Training Step: 1173  | total loss: \u001b[1m\u001b[32m0.01833\u001b[0m\u001b[0m | time: 0.342s\n",
      "| SGD | epoch: 020 | loss: 0.01833 - R2: 1.0005 -- iter: 1040/1168\n",
      "Training Step: 1174  | total loss: \u001b[1m\u001b[32m0.01765\u001b[0m\u001b[0m | time: 0.344s\n",
      "| SGD | epoch: 020 | loss: 0.01765 - R2: 1.0005 -- iter: 1060/1168\n",
      "Training Step: 1175  | total loss: \u001b[1m\u001b[32m0.01743\u001b[0m\u001b[0m | time: 0.346s\n",
      "| SGD | epoch: 020 | loss: 0.01743 - R2: 1.0001 -- iter: 1080/1168\n",
      "Training Step: 1176  | total loss: \u001b[1m\u001b[32m0.01638\u001b[0m\u001b[0m | time: 0.349s\n",
      "| SGD | epoch: 020 | loss: 0.01638 - R2: 1.0005 -- iter: 1100/1168\n",
      "Training Step: 1177  | total loss: \u001b[1m\u001b[32m0.01584\u001b[0m\u001b[0m | time: 0.352s\n",
      "| SGD | epoch: 020 | loss: 0.01584 - R2: 0.9995 -- iter: 1120/1168\n",
      "Training Step: 1178  | total loss: \u001b[1m\u001b[32m0.01553\u001b[0m\u001b[0m | time: 0.355s\n",
      "| SGD | epoch: 020 | loss: 0.01553 - R2: 0.9998 -- iter: 1140/1168\n",
      "Training Step: 1179  | total loss: \u001b[1m\u001b[32m0.01580\u001b[0m\u001b[0m | time: 0.357s\n",
      "| SGD | epoch: 020 | loss: 0.01580 - R2: 0.9996 -- iter: 1160/1168\n",
      "Training Step: 1180  | total loss: \u001b[1m\u001b[32m0.01579\u001b[0m\u001b[0m | time: 1.362s\n",
      "| SGD | epoch: 020 | loss: 0.01579 - R2: 0.9984 | val_loss: 0.03167 - val_acc: 1.0020 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 1181  | total loss: \u001b[1m\u001b[32m0.01565\u001b[0m\u001b[0m | time: 0.020s\n",
      "| SGD | epoch: 021 | loss: 0.01565 - R2: 0.9989 -- iter: 0020/1168\n",
      "Training Step: 1182  | total loss: \u001b[1m\u001b[32m0.01802\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 021 | loss: 0.01802 - R2: 0.9991 -- iter: 0040/1168\n",
      "Training Step: 1183  | total loss: \u001b[1m\u001b[32m0.01802\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 021 | loss: 0.01802 - R2: 0.9991 -- iter: 0060/1168\n",
      "Training Step: 1184  | total loss: \u001b[1m\u001b[32m0.01741\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 021 | loss: 0.01741 - R2: 0.9991 -- iter: 0080/1168\n",
      "Training Step: 1185  | total loss: \u001b[1m\u001b[32m0.01622\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 021 | loss: 0.01622 - R2: 0.9996 -- iter: 0100/1168\n",
      "Training Step: 1186  | total loss: \u001b[1m\u001b[32m0.01562\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 021 | loss: 0.01562 - R2: 0.9999 -- iter: 0120/1168\n",
      "Training Step: 1187  | total loss: \u001b[1m\u001b[32m0.01562\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 021 | loss: 0.01562 - R2: 0.9999 -- iter: 0140/1168\n",
      "Training Step: 1188  | total loss: \u001b[1m\u001b[32m0.01557\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 021 | loss: 0.01557 - R2: 0.9996 -- iter: 0160/1168\n",
      "Training Step: 1189  | total loss: \u001b[1m\u001b[32m0.01450\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 021 | loss: 0.01450 - R2: 0.9997 -- iter: 0180/1168\n",
      "Training Step: 1190  | total loss: \u001b[1m\u001b[32m0.01670\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 021 | loss: 0.01670 - R2: 0.9997 -- iter: 0200/1168\n",
      "Training Step: 1191  | total loss: \u001b[1m\u001b[32m0.02295\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 021 | loss: 0.02295 - R2: 1.0019 -- iter: 0220/1168\n",
      "Training Step: 1192  | total loss: \u001b[1m\u001b[32m0.02295\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 021 | loss: 0.02295 - R2: 1.0009 -- iter: 0240/1168\n",
      "Training Step: 1193  | total loss: \u001b[1m\u001b[32m0.02217\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 021 | loss: 0.02217 - R2: 1.0010 -- iter: 0260/1168\n",
      "Training Step: 1194  | total loss: \u001b[1m\u001b[32m0.02172\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 021 | loss: 0.02172 - R2: 1.0010 -- iter: 0280/1168\n",
      "Training Step: 1195  | total loss: \u001b[1m\u001b[32m0.02048\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 021 | loss: 0.02048 - R2: 1.0007 -- iter: 0300/1168\n",
      "Training Step: 1196  | total loss: \u001b[1m\u001b[32m0.01890\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 021 | loss: 0.01890 - R2: 1.0004 -- iter: 0320/1168\n",
      "Training Step: 1197  | total loss: \u001b[1m\u001b[32m0.01796\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 021 | loss: 0.01796 - R2: 0.9998 -- iter: 0340/1168\n",
      "Training Step: 1198  | total loss: \u001b[1m\u001b[32m0.01796\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 021 | loss: 0.01796 - R2: 0.9998 -- iter: 0360/1168\n",
      "Training Step: 1199  | total loss: \u001b[1m\u001b[32m0.01742\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 021 | loss: 0.01742 - R2: 1.0005 -- iter: 0380/1168\n",
      "Training Step: 1200  | total loss: \u001b[1m\u001b[32m0.01641\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 021 | loss: 0.01641 - R2: 1.0001 -- iter: 0400/1168\n",
      "Training Step: 1201  | total loss: \u001b[1m\u001b[32m0.01537\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 021 | loss: 0.01537 - R2: 0.9996 -- iter: 0420/1168\n",
      "Training Step: 1202  | total loss: \u001b[1m\u001b[32m0.01522\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 021 | loss: 0.01522 - R2: 0.9995 -- iter: 0440/1168\n",
      "Training Step: 1203  | total loss: \u001b[1m\u001b[32m0.01698\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 021 | loss: 0.01698 - R2: 1.0001 -- iter: 0460/1168\n",
      "Training Step: 1204  | total loss: \u001b[1m\u001b[32m0.01698\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 021 | loss: 0.01698 - R2: 1.0001 -- iter: 0480/1168\n",
      "Training Step: 1205  | total loss: \u001b[1m\u001b[32m0.01682\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 021 | loss: 0.01682 - R2: 0.9991 -- iter: 0500/1168\n",
      "Training Step: 1206  | total loss: \u001b[1m\u001b[32m0.01653\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 021 | loss: 0.01653 - R2: 0.9989 -- iter: 0520/1168\n",
      "Training Step: 1207  | total loss: \u001b[1m\u001b[32m0.01881\u001b[0m\u001b[0m | time: 0.165s\n",
      "| SGD | epoch: 021 | loss: 0.01881 - R2: 0.9987 -- iter: 0540/1168\n",
      "Training Step: 1208  | total loss: \u001b[1m\u001b[32m0.01791\u001b[0m\u001b[0m | time: 0.169s\n",
      "| SGD | epoch: 021 | loss: 0.01791 - R2: 0.9996 -- iter: 0560/1168\n",
      "Training Step: 1209  | total loss: \u001b[1m\u001b[32m0.01791\u001b[0m\u001b[0m | time: 0.171s\n",
      "| SGD | epoch: 021 | loss: 0.01791 - R2: 0.9996 -- iter: 0580/1168\n",
      "Training Step: 1210  | total loss: \u001b[1m\u001b[32m0.01793\u001b[0m\u001b[0m | time: 0.173s\n",
      "| SGD | epoch: 021 | loss: 0.01793 - R2: 0.9989 -- iter: 0600/1168\n",
      "Training Step: 1211  | total loss: \u001b[1m\u001b[32m0.01757\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 021 | loss: 0.01757 - R2: 0.9992 -- iter: 0620/1168\n",
      "Training Step: 1212  | total loss: \u001b[1m\u001b[32m0.01993\u001b[0m\u001b[0m | time: 0.178s\n",
      "| SGD | epoch: 021 | loss: 0.01993 - R2: 0.9989 -- iter: 0640/1168\n",
      "Training Step: 1213  | total loss: \u001b[1m\u001b[32m0.01929\u001b[0m\u001b[0m | time: 0.180s\n",
      "| SGD | epoch: 021 | loss: 0.01929 - R2: 0.9989 -- iter: 0660/1168\n",
      "Training Step: 1214  | total loss: \u001b[1m\u001b[32m0.01880\u001b[0m\u001b[0m | time: 0.184s\n",
      "| SGD | epoch: 021 | loss: 0.01880 - R2: 0.9985 -- iter: 0680/1168\n",
      "Training Step: 1215  | total loss: \u001b[1m\u001b[32m0.01823\u001b[0m\u001b[0m | time: 0.188s\n",
      "| SGD | epoch: 021 | loss: 0.01823 - R2: 0.9991 -- iter: 0700/1168\n",
      "Training Step: 1216  | total loss: \u001b[1m\u001b[32m0.01780\u001b[0m\u001b[0m | time: 0.193s\n",
      "| SGD | epoch: 021 | loss: 0.01780 - R2: 0.9988 -- iter: 0720/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1217  | total loss: \u001b[1m\u001b[32m0.01780\u001b[0m\u001b[0m | time: 0.196s\n",
      "| SGD | epoch: 021 | loss: 0.01780 - R2: 0.9988 -- iter: 0740/1168\n",
      "Training Step: 1218  | total loss: \u001b[1m\u001b[32m0.01797\u001b[0m\u001b[0m | time: 0.198s\n",
      "| SGD | epoch: 021 | loss: 0.01797 - R2: 0.9994 -- iter: 0760/1168\n",
      "Training Step: 1219  | total loss: \u001b[1m\u001b[32m0.01758\u001b[0m\u001b[0m | time: 0.201s\n",
      "| SGD | epoch: 021 | loss: 0.01758 - R2: 0.9987 -- iter: 0780/1168\n",
      "Training Step: 1220  | total loss: \u001b[1m\u001b[32m0.01719\u001b[0m\u001b[0m | time: 0.203s\n",
      "| SGD | epoch: 021 | loss: 0.01719 - R2: 0.9997 -- iter: 0800/1168\n",
      "Training Step: 1221  | total loss: \u001b[1m\u001b[32m0.01719\u001b[0m\u001b[0m | time: 0.205s\n",
      "| SGD | epoch: 021 | loss: 0.01719 - R2: 0.9997 -- iter: 0820/1168\n",
      "Training Step: 1222  | total loss: \u001b[1m\u001b[32m0.01733\u001b[0m\u001b[0m | time: 0.208s\n",
      "| SGD | epoch: 021 | loss: 0.01733 - R2: 0.9996 -- iter: 0840/1168\n",
      "Training Step: 1223  | total loss: \u001b[1m\u001b[32m0.02274\u001b[0m\u001b[0m | time: 0.210s\n",
      "| SGD | epoch: 021 | loss: 0.02274 - R2: 0.9995 -- iter: 0860/1168\n",
      "Training Step: 1224  | total loss: \u001b[1m\u001b[32m0.02199\u001b[0m\u001b[0m | time: 0.212s\n",
      "| SGD | epoch: 021 | loss: 0.02199 - R2: 0.9994 -- iter: 0880/1168\n",
      "Training Step: 1225  | total loss: \u001b[1m\u001b[32m0.02317\u001b[0m\u001b[0m | time: 0.214s\n",
      "| SGD | epoch: 021 | loss: 0.02317 - R2: 0.9998 -- iter: 0900/1168\n",
      "Training Step: 1226  | total loss: \u001b[1m\u001b[32m0.02212\u001b[0m\u001b[0m | time: 0.219s\n",
      "| SGD | epoch: 021 | loss: 0.02212 - R2: 0.9997 -- iter: 0920/1168\n",
      "Training Step: 1227  | total loss: \u001b[1m\u001b[32m0.02176\u001b[0m\u001b[0m | time: 0.225s\n",
      "| SGD | epoch: 021 | loss: 0.02176 - R2: 0.9999 -- iter: 0940/1168\n",
      "Training Step: 1228  | total loss: \u001b[1m\u001b[32m0.02142\u001b[0m\u001b[0m | time: 0.230s\n",
      "| SGD | epoch: 021 | loss: 0.02142 - R2: 1.0001 -- iter: 0960/1168\n",
      "Training Step: 1229  | total loss: \u001b[1m\u001b[32m0.02101\u001b[0m\u001b[0m | time: 0.233s\n",
      "| SGD | epoch: 021 | loss: 0.02101 - R2: 1.0001 -- iter: 0980/1168\n",
      "Training Step: 1230  | total loss: \u001b[1m\u001b[32m0.01977\u001b[0m\u001b[0m | time: 0.239s\n",
      "| SGD | epoch: 021 | loss: 0.01977 - R2: 0.9998 -- iter: 1000/1168\n",
      "Training Step: 1231  | total loss: \u001b[1m\u001b[32m0.01888\u001b[0m\u001b[0m | time: 0.243s\n",
      "| SGD | epoch: 021 | loss: 0.01888 - R2: 1.0003 -- iter: 1020/1168\n",
      "Training Step: 1232  | total loss: \u001b[1m\u001b[32m0.01853\u001b[0m\u001b[0m | time: 0.246s\n",
      "| SGD | epoch: 021 | loss: 0.01853 - R2: 1.0000 -- iter: 1040/1168\n",
      "Training Step: 1233  | total loss: \u001b[1m\u001b[32m0.01853\u001b[0m\u001b[0m | time: 0.250s\n",
      "| SGD | epoch: 021 | loss: 0.01853 - R2: 1.0000 -- iter: 1060/1168\n",
      "Training Step: 1234  | total loss: \u001b[1m\u001b[32m0.01840\u001b[0m\u001b[0m | time: 0.257s\n",
      "| SGD | epoch: 021 | loss: 0.01840 - R2: 0.9994 -- iter: 1080/1168\n",
      "Training Step: 1235  | total loss: \u001b[1m\u001b[32m0.01748\u001b[0m\u001b[0m | time: 0.262s\n",
      "| SGD | epoch: 021 | loss: 0.01748 - R2: 0.9994 -- iter: 1100/1168\n",
      "Training Step: 1236  | total loss: \u001b[1m\u001b[32m0.01721\u001b[0m\u001b[0m | time: 0.266s\n",
      "| SGD | epoch: 021 | loss: 0.01721 - R2: 0.9998 -- iter: 1120/1168\n",
      "Training Step: 1237  | total loss: \u001b[1m\u001b[32m0.01745\u001b[0m\u001b[0m | time: 0.269s\n",
      "| SGD | epoch: 021 | loss: 0.01745 - R2: 1.0006 -- iter: 1140/1168\n",
      "Training Step: 1238  | total loss: \u001b[1m\u001b[32m0.01745\u001b[0m\u001b[0m | time: 0.272s\n",
      "| SGD | epoch: 021 | loss: 0.01745 - R2: 1.0006 -- iter: 1160/1168\n",
      "Training Step: 1239  | total loss: \u001b[1m\u001b[32m0.01656\u001b[0m\u001b[0m | time: 1.279s\n",
      "| SGD | epoch: 021 | loss: 0.01656 - R2: 1.0000 | val_loss: 0.03226 - val_acc: 1.0015 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 1240  | total loss: \u001b[1m\u001b[32m0.01656\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 022 | loss: 0.01656 - R2: 1.0000 -- iter: 0020/1168\n",
      "Training Step: 1241  | total loss: \u001b[1m\u001b[32m0.01536\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 022 | loss: 0.01536 - R2: 1.0001 -- iter: 0040/1168\n",
      "Training Step: 1242  | total loss: \u001b[1m\u001b[32m0.01454\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 022 | loss: 0.01454 - R2: 1.0000 -- iter: 0060/1168\n",
      "Training Step: 1243  | total loss: \u001b[1m\u001b[32m0.01484\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 022 | loss: 0.01484 - R2: 0.9993 -- iter: 0080/1168\n",
      "Training Step: 1244  | total loss: \u001b[1m\u001b[32m0.01474\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 022 | loss: 0.01474 - R2: 0.9990 -- iter: 0100/1168\n",
      "Training Step: 1245  | total loss: \u001b[1m\u001b[32m0.01432\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 022 | loss: 0.01432 - R2: 0.9990 -- iter: 0120/1168\n",
      "Training Step: 1246  | total loss: \u001b[1m\u001b[32m0.01468\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 022 | loss: 0.01468 - R2: 0.9995 -- iter: 0140/1168\n",
      "Training Step: 1247  | total loss: \u001b[1m\u001b[32m0.01486\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 022 | loss: 0.01486 - R2: 0.9988 -- iter: 0160/1168\n",
      "Training Step: 1248  | total loss: \u001b[1m\u001b[32m0.01514\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 022 | loss: 0.01514 - R2: 0.9988 -- iter: 0180/1168\n",
      "Training Step: 1249  | total loss: \u001b[1m\u001b[32m0.01514\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 022 | loss: 0.01514 - R2: 0.9988 -- iter: 0200/1168\n",
      "Training Step: 1250  | total loss: \u001b[1m\u001b[32m0.01523\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 022 | loss: 0.01523 - R2: 0.9993 -- iter: 0220/1168\n",
      "Training Step: 1251  | total loss: \u001b[1m\u001b[32m0.01462\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 022 | loss: 0.01462 - R2: 0.9998 -- iter: 0240/1168\n",
      "Training Step: 1252  | total loss: \u001b[1m\u001b[32m0.01399\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 022 | loss: 0.01399 - R2: 1.0000 -- iter: 0260/1168\n",
      "Training Step: 1253  | total loss: \u001b[1m\u001b[32m0.01399\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 022 | loss: 0.01399 - R2: 0.9988 -- iter: 0280/1168\n",
      "Training Step: 1254  | total loss: \u001b[1m\u001b[32m0.01512\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 022 | loss: 0.01512 - R2: 0.9988 -- iter: 0300/1168\n",
      "Training Step: 1255  | total loss: \u001b[1m\u001b[32m0.01566\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 022 | loss: 0.01566 - R2: 0.9982 -- iter: 0320/1168\n",
      "Training Step: 1256  | total loss: \u001b[1m\u001b[32m0.01506\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 022 | loss: 0.01506 - R2: 0.9990 -- iter: 0340/1168\n",
      "Training Step: 1257  | total loss: \u001b[1m\u001b[32m0.01504\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 022 | loss: 0.01504 - R2: 0.9990 -- iter: 0360/1168\n",
      "Training Step: 1258  | total loss: \u001b[1m\u001b[32m0.01544\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 022 | loss: 0.01544 - R2: 0.9996 -- iter: 0380/1168\n",
      "Training Step: 1259  | total loss: \u001b[1m\u001b[32m0.01544\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 022 | loss: 0.01544 - R2: 0.9996 -- iter: 0400/1168\n",
      "Training Step: 1260  | total loss: \u001b[1m\u001b[32m0.01641\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 022 | loss: 0.01641 - R2: 1.0010 -- iter: 0420/1168\n",
      "Training Step: 1261  | total loss: \u001b[1m\u001b[32m0.01661\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 022 | loss: 0.01661 - R2: 1.0020 -- iter: 0440/1168\n",
      "Training Step: 1262  | total loss: \u001b[1m\u001b[32m0.01701\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 022 | loss: 0.01701 - R2: 1.0024 -- iter: 0460/1168\n",
      "Training Step: 1263  | total loss: \u001b[1m\u001b[32m0.01701\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 022 | loss: 0.01701 - R2: 1.0024 -- iter: 0480/1168\n",
      "Training Step: 1264  | total loss: \u001b[1m\u001b[32m0.01769\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 022 | loss: 0.01769 - R2: 1.0021 -- iter: 0500/1168\n",
      "Training Step: 1265  | total loss: \u001b[1m\u001b[32m0.01630\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 022 | loss: 0.01630 - R2: 1.0019 -- iter: 0520/1168\n",
      "Training Step: 1266  | total loss: \u001b[1m\u001b[32m0.01712\u001b[0m\u001b[0m | time: 0.178s\n",
      "| SGD | epoch: 022 | loss: 0.01712 - R2: 1.0010 -- iter: 0540/1168\n",
      "Training Step: 1267  | total loss: \u001b[1m\u001b[32m0.01808\u001b[0m\u001b[0m | time: 0.183s\n",
      "| SGD | epoch: 022 | loss: 0.01808 - R2: 1.0007 -- iter: 0560/1168\n",
      "Training Step: 1268  | total loss: \u001b[1m\u001b[32m0.01808\u001b[0m\u001b[0m | time: 0.205s\n",
      "| SGD | epoch: 022 | loss: 0.01808 - R2: 1.0007 -- iter: 0580/1168\n",
      "Training Step: 1269  | total loss: \u001b[1m\u001b[32m0.01738\u001b[0m\u001b[0m | time: 0.212s\n",
      "| SGD | epoch: 022 | loss: 0.01738 - R2: 1.0005 -- iter: 0600/1168\n",
      "Training Step: 1270  | total loss: \u001b[1m\u001b[32m0.01626\u001b[0m\u001b[0m | time: 0.215s\n",
      "| SGD | epoch: 022 | loss: 0.01626 - R2: 1.0004 -- iter: 0620/1168\n",
      "Training Step: 1271  | total loss: \u001b[1m\u001b[32m0.01815\u001b[0m\u001b[0m | time: 0.218s\n",
      "| SGD | epoch: 022 | loss: 0.01815 - R2: 1.0013 -- iter: 0640/1168\n",
      "Training Step: 1272  | total loss: \u001b[1m\u001b[32m0.01815\u001b[0m\u001b[0m | time: 0.222s\n",
      "| SGD | epoch: 022 | loss: 0.01815 - R2: 1.0013 -- iter: 0660/1168\n",
      "Training Step: 1273  | total loss: \u001b[1m\u001b[32m0.01769\u001b[0m\u001b[0m | time: 0.224s\n",
      "| SGD | epoch: 022 | loss: 0.01769 - R2: 1.0015 -- iter: 0680/1168\n",
      "Training Step: 1274  | total loss: \u001b[1m\u001b[32m0.01905\u001b[0m\u001b[0m | time: 0.227s\n",
      "| SGD | epoch: 022 | loss: 0.01905 - R2: 1.0015 -- iter: 0700/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1275  | total loss: \u001b[1m\u001b[32m0.01833\u001b[0m\u001b[0m | time: 0.230s\n",
      "| SGD | epoch: 022 | loss: 0.01833 - R2: 1.0017 -- iter: 0720/1168\n",
      "Training Step: 1276  | total loss: \u001b[1m\u001b[32m0.01833\u001b[0m\u001b[0m | time: 0.232s\n",
      "| SGD | epoch: 022 | loss: 0.01833 - R2: 1.0017 -- iter: 0740/1168\n",
      "Training Step: 1277  | total loss: \u001b[1m\u001b[32m0.01823\u001b[0m\u001b[0m | time: 0.233s\n",
      "| SGD | epoch: 022 | loss: 0.01823 - R2: 1.0010 -- iter: 0760/1168\n",
      "Training Step: 1278  | total loss: \u001b[1m\u001b[32m0.01903\u001b[0m\u001b[0m | time: 0.236s\n",
      "| SGD | epoch: 022 | loss: 0.01903 - R2: 1.0011 -- iter: 0780/1168\n",
      "Training Step: 1279  | total loss: \u001b[1m\u001b[32m0.01870\u001b[0m\u001b[0m | time: 0.238s\n",
      "| SGD | epoch: 022 | loss: 0.01870 - R2: 0.9999 -- iter: 0800/1168\n",
      "Training Step: 1280  | total loss: \u001b[1m\u001b[32m0.01822\u001b[0m\u001b[0m | time: 0.254s\n",
      "| SGD | epoch: 022 | loss: 0.01822 - R2: 0.9999 -- iter: 0820/1168\n",
      "Training Step: 1281  | total loss: \u001b[1m\u001b[32m0.01812\u001b[0m\u001b[0m | time: 0.256s\n",
      "| SGD | epoch: 022 | loss: 0.01812 - R2: 0.9997 -- iter: 0840/1168\n",
      "Training Step: 1282  | total loss: \u001b[1m\u001b[32m0.02776\u001b[0m\u001b[0m | time: 0.260s\n",
      "| SGD | epoch: 022 | loss: 0.02776 - R2: 1.0007 -- iter: 0860/1168\n",
      "Training Step: 1283  | total loss: \u001b[1m\u001b[32m0.02837\u001b[0m\u001b[0m | time: 0.264s\n",
      "| SGD | epoch: 022 | loss: 0.02837 - R2: 1.0009 -- iter: 0880/1168\n",
      "Training Step: 1284  | total loss: \u001b[1m\u001b[32m0.02837\u001b[0m\u001b[0m | time: 0.267s\n",
      "| SGD | epoch: 022 | loss: 0.02837 - R2: 1.0015 -- iter: 0900/1168\n",
      "Training Step: 1285  | total loss: \u001b[1m\u001b[32m0.02825\u001b[0m\u001b[0m | time: 0.269s\n",
      "| SGD | epoch: 022 | loss: 0.02825 - R2: 1.0013 -- iter: 0920/1168\n",
      "Training Step: 1286  | total loss: \u001b[1m\u001b[32m0.02907\u001b[0m\u001b[0m | time: 0.272s\n",
      "| SGD | epoch: 022 | loss: 0.02907 - R2: 1.0013 -- iter: 0940/1168\n",
      "Training Step: 1287  | total loss: \u001b[1m\u001b[32m0.04287\u001b[0m\u001b[0m | time: 0.276s\n",
      "| SGD | epoch: 022 | loss: 0.04287 - R2: 0.9978 -- iter: 0960/1168\n",
      "Training Step: 1288  | total loss: \u001b[1m\u001b[32m0.04287\u001b[0m\u001b[0m | time: 0.278s\n",
      "| SGD | epoch: 022 | loss: 0.04287 - R2: 0.9978 -- iter: 0980/1168\n",
      "Training Step: 1289  | total loss: \u001b[1m\u001b[32m0.04043\u001b[0m\u001b[0m | time: 0.280s\n",
      "| SGD | epoch: 022 | loss: 0.04043 - R2: 0.9975 -- iter: 1000/1168\n",
      "Training Step: 1290  | total loss: \u001b[1m\u001b[32m0.03759\u001b[0m\u001b[0m | time: 0.284s\n",
      "| SGD | epoch: 022 | loss: 0.03759 - R2: 0.9975 -- iter: 1020/1168\n",
      "Training Step: 1291  | total loss: \u001b[1m\u001b[32m0.03296\u001b[0m\u001b[0m | time: 0.287s\n",
      "| SGD | epoch: 022 | loss: 0.03296 - R2: 0.9977 -- iter: 1040/1168\n",
      "Training Step: 1292  | total loss: \u001b[1m\u001b[32m0.03148\u001b[0m\u001b[0m | time: 0.291s\n",
      "| SGD | epoch: 022 | loss: 0.03148 - R2: 0.9972 -- iter: 1060/1168\n",
      "Training Step: 1293  | total loss: \u001b[1m\u001b[32m0.03045\u001b[0m\u001b[0m | time: 0.295s\n",
      "| SGD | epoch: 022 | loss: 0.03045 - R2: 0.9983 -- iter: 1080/1168\n",
      "Training Step: 1294  | total loss: \u001b[1m\u001b[32m0.03209\u001b[0m\u001b[0m | time: 0.299s\n",
      "| SGD | epoch: 022 | loss: 0.03209 - R2: 0.9990 -- iter: 1100/1168\n",
      "Training Step: 1295  | total loss: \u001b[1m\u001b[32m0.03209\u001b[0m\u001b[0m | time: 0.301s\n",
      "| SGD | epoch: 022 | loss: 0.03209 - R2: 0.9990 -- iter: 1120/1168\n",
      "Training Step: 1296  | total loss: \u001b[1m\u001b[32m0.03117\u001b[0m\u001b[0m | time: 0.305s\n",
      "| SGD | epoch: 022 | loss: 0.03117 - R2: 0.9991 -- iter: 1140/1168\n",
      "Training Step: 1297  | total loss: \u001b[1m\u001b[32m0.02924\u001b[0m\u001b[0m | time: 0.307s\n",
      "| SGD | epoch: 022 | loss: 0.02924 - R2: 0.9996 -- iter: 1160/1168\n",
      "Training Step: 1298  | total loss: \u001b[1m\u001b[32m0.02924\u001b[0m\u001b[0m | time: 1.319s\n",
      "| SGD | epoch: 022 | loss: 0.02924 - R2: 0.9995 | val_loss: 0.03227 - val_acc: 1.0013 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 1299  | total loss: \u001b[1m\u001b[32m0.02660\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 023 | loss: 0.02660 - R2: 0.9989 -- iter: 0020/1168\n",
      "Training Step: 1300  | total loss: \u001b[1m\u001b[32m0.02504\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 023 | loss: 0.02504 - R2: 0.9995 -- iter: 0040/1168\n",
      "Training Step: 1301  | total loss: \u001b[1m\u001b[32m0.02325\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 023 | loss: 0.02325 - R2: 0.9999 -- iter: 0060/1168\n",
      "Training Step: 1302  | total loss: \u001b[1m\u001b[32m0.02318\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 023 | loss: 0.02318 - R2: 1.0005 -- iter: 0080/1168\n",
      "Training Step: 1303  | total loss: \u001b[1m\u001b[32m0.02318\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 023 | loss: 0.02318 - R2: 1.0005 -- iter: 0100/1168\n",
      "Training Step: 1304  | total loss: \u001b[1m\u001b[32m0.02260\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 023 | loss: 0.02260 - R2: 1.0003 -- iter: 0120/1168\n",
      "Training Step: 1305  | total loss: \u001b[1m\u001b[32m0.02132\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 023 | loss: 0.02132 - R2: 1.0007 -- iter: 0140/1168\n",
      "Training Step: 1306  | total loss: \u001b[1m\u001b[32m0.02132\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 023 | loss: 0.02132 - R2: 1.0007 -- iter: 0160/1168\n",
      "Training Step: 1307  | total loss: \u001b[1m\u001b[32m0.02518\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 023 | loss: 0.02518 - R2: 1.0010 -- iter: 0180/1168\n",
      "Training Step: 1308  | total loss: \u001b[1m\u001b[32m0.02373\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 023 | loss: 0.02373 - R2: 1.0007 -- iter: 0200/1168\n",
      "Training Step: 1309  | total loss: \u001b[1m\u001b[32m0.02218\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 023 | loss: 0.02218 - R2: 1.0004 -- iter: 0220/1168\n",
      "Training Step: 1310  | total loss: \u001b[1m\u001b[32m0.02110\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 023 | loss: 0.02110 - R2: 0.9995 -- iter: 0240/1168\n",
      "Training Step: 1311  | total loss: \u001b[1m\u001b[32m0.02018\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 023 | loss: 0.02018 - R2: 0.9996 -- iter: 0260/1168\n",
      "Training Step: 1312  | total loss: \u001b[1m\u001b[32m0.02023\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 023 | loss: 0.02023 - R2: 0.9996 -- iter: 0280/1168\n",
      "Training Step: 1313  | total loss: \u001b[1m\u001b[32m0.01991\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 023 | loss: 0.01991 - R2: 1.0008 -- iter: 0300/1168\n",
      "Training Step: 1314  | total loss: \u001b[1m\u001b[32m0.02717\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 023 | loss: 0.02717 - R2: 1.0008 -- iter: 0320/1168\n",
      "Training Step: 1315  | total loss: \u001b[1m\u001b[32m0.02418\u001b[0m\u001b[0m | time: 0.169s\n",
      "| SGD | epoch: 023 | loss: 0.02418 - R2: 1.0000 -- iter: 0340/1168\n",
      "Training Step: 1316  | total loss: \u001b[1m\u001b[32m0.02324\u001b[0m\u001b[0m | time: 0.174s\n",
      "| SGD | epoch: 023 | loss: 0.02324 - R2: 0.9994 -- iter: 0360/1168\n",
      "Training Step: 1317  | total loss: \u001b[1m\u001b[32m0.02441\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 023 | loss: 0.02441 - R2: 1.0001 -- iter: 0380/1168\n",
      "Training Step: 1318  | total loss: \u001b[1m\u001b[32m0.02441\u001b[0m\u001b[0m | time: 0.180s\n",
      "| SGD | epoch: 023 | loss: 0.02441 - R2: 1.0001 -- iter: 0400/1168\n",
      "Training Step: 1319  | total loss: \u001b[1m\u001b[32m0.02291\u001b[0m\u001b[0m | time: 0.184s\n",
      "| SGD | epoch: 023 | loss: 0.02291 - R2: 0.9995 -- iter: 0420/1168\n",
      "Training Step: 1320  | total loss: \u001b[1m\u001b[32m0.02117\u001b[0m\u001b[0m | time: 0.187s\n",
      "| SGD | epoch: 023 | loss: 0.02117 - R2: 0.9987 -- iter: 0440/1168\n",
      "Training Step: 1321  | total loss: \u001b[1m\u001b[32m0.02117\u001b[0m\u001b[0m | time: 0.189s\n",
      "| SGD | epoch: 023 | loss: 0.02117 - R2: 0.9987 -- iter: 0460/1168\n",
      "Training Step: 1322  | total loss: \u001b[1m\u001b[32m0.02055\u001b[0m\u001b[0m | time: 0.192s\n",
      "| SGD | epoch: 023 | loss: 0.02055 - R2: 0.9983 -- iter: 0480/1168\n",
      "Training Step: 1323  | total loss: \u001b[1m\u001b[32m0.01908\u001b[0m\u001b[0m | time: 0.194s\n",
      "| SGD | epoch: 023 | loss: 0.01908 - R2: 0.9985 -- iter: 0500/1168\n",
      "Training Step: 1324  | total loss: \u001b[1m\u001b[32m0.02218\u001b[0m\u001b[0m | time: 0.197s\n",
      "| SGD | epoch: 023 | loss: 0.02218 - R2: 1.0007 -- iter: 0520/1168\n",
      "Training Step: 1325  | total loss: \u001b[1m\u001b[32m0.02218\u001b[0m\u001b[0m | time: 0.201s\n",
      "| SGD | epoch: 023 | loss: 0.02218 - R2: 1.0007 -- iter: 0540/1168\n",
      "Training Step: 1326  | total loss: \u001b[1m\u001b[32m0.02192\u001b[0m\u001b[0m | time: 0.205s\n",
      "| SGD | epoch: 023 | loss: 0.02192 - R2: 1.0013 -- iter: 0560/1168\n",
      "Training Step: 1327  | total loss: \u001b[1m\u001b[32m0.02192\u001b[0m\u001b[0m | time: 0.207s\n",
      "| SGD | epoch: 023 | loss: 0.02192 - R2: 1.0013 -- iter: 0580/1168\n",
      "Training Step: 1328  | total loss: \u001b[1m\u001b[32m0.02252\u001b[0m\u001b[0m | time: 0.208s\n",
      "| SGD | epoch: 023 | loss: 0.02252 - R2: 1.0010 -- iter: 0600/1168\n",
      "Training Step: 1329  | total loss: \u001b[1m\u001b[32m0.02126\u001b[0m\u001b[0m | time: 0.211s\n",
      "| SGD | epoch: 023 | loss: 0.02126 - R2: 1.0005 -- iter: 0620/1168\n",
      "Training Step: 1330  | total loss: \u001b[1m\u001b[32m0.02110\u001b[0m\u001b[0m | time: 0.214s\n",
      "| SGD | epoch: 023 | loss: 0.02110 - R2: 1.0003 -- iter: 0640/1168\n",
      "Training Step: 1331  | total loss: \u001b[1m\u001b[32m0.02001\u001b[0m\u001b[0m | time: 0.217s\n",
      "| SGD | epoch: 023 | loss: 0.02001 - R2: 1.0009 -- iter: 0660/1168\n",
      "Training Step: 1332  | total loss: \u001b[1m\u001b[32m0.01980\u001b[0m\u001b[0m | time: 0.220s\n",
      "| SGD | epoch: 023 | loss: 0.01980 - R2: 0.9998 -- iter: 0680/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1333  | total loss: \u001b[1m\u001b[32m0.01980\u001b[0m\u001b[0m | time: 0.223s\n",
      "| SGD | epoch: 023 | loss: 0.01980 - R2: 0.9998 -- iter: 0700/1168\n",
      "Training Step: 1334  | total loss: \u001b[1m\u001b[32m0.01971\u001b[0m\u001b[0m | time: 0.227s\n",
      "| SGD | epoch: 023 | loss: 0.01971 - R2: 1.0001 -- iter: 0720/1168\n",
      "Training Step: 1335  | total loss: \u001b[1m\u001b[32m0.02119\u001b[0m\u001b[0m | time: 0.229s\n",
      "| SGD | epoch: 023 | loss: 0.02119 - R2: 0.9991 -- iter: 0740/1168\n",
      "Training Step: 1336  | total loss: \u001b[1m\u001b[32m0.02158\u001b[0m\u001b[0m | time: 0.232s\n",
      "| SGD | epoch: 023 | loss: 0.02158 - R2: 0.9997 -- iter: 0760/1168\n",
      "Training Step: 1337  | total loss: \u001b[1m\u001b[32m0.02065\u001b[0m\u001b[0m | time: 0.236s\n",
      "| SGD | epoch: 023 | loss: 0.02065 - R2: 0.9999 -- iter: 0780/1168\n",
      "Training Step: 1338  | total loss: \u001b[1m\u001b[32m0.01975\u001b[0m\u001b[0m | time: 0.238s\n",
      "| SGD | epoch: 023 | loss: 0.01975 - R2: 0.9998 -- iter: 0800/1168\n",
      "Training Step: 1339  | total loss: \u001b[1m\u001b[32m0.01765\u001b[0m\u001b[0m | time: 0.241s\n",
      "| SGD | epoch: 023 | loss: 0.01765 - R2: 0.9991 -- iter: 0820/1168\n",
      "Training Step: 1340  | total loss: \u001b[1m\u001b[32m0.01765\u001b[0m\u001b[0m | time: 0.243s\n",
      "| SGD | epoch: 023 | loss: 0.01765 - R2: 0.9991 -- iter: 0840/1168\n",
      "Training Step: 1341  | total loss: \u001b[1m\u001b[32m0.01681\u001b[0m\u001b[0m | time: 0.247s\n",
      "| SGD | epoch: 023 | loss: 0.01681 - R2: 0.9989 -- iter: 0860/1168\n",
      "Training Step: 1342  | total loss: \u001b[1m\u001b[32m0.01681\u001b[0m\u001b[0m | time: 0.250s\n",
      "| SGD | epoch: 023 | loss: 0.01681 - R2: 0.9989 -- iter: 0880/1168\n",
      "Training Step: 1343  | total loss: \u001b[1m\u001b[32m0.01616\u001b[0m\u001b[0m | time: 0.253s\n",
      "| SGD | epoch: 023 | loss: 0.01616 - R2: 0.9993 -- iter: 0900/1168\n",
      "Training Step: 1344  | total loss: \u001b[1m\u001b[32m0.01616\u001b[0m\u001b[0m | time: 0.255s\n",
      "| SGD | epoch: 023 | loss: 0.01616 - R2: 0.9998 -- iter: 0920/1168\n",
      "Training Step: 1345  | total loss: \u001b[1m\u001b[32m0.01486\u001b[0m\u001b[0m | time: 0.267s\n",
      "| SGD | epoch: 023 | loss: 0.01486 - R2: 0.9997 -- iter: 0940/1168\n",
      "Training Step: 1346  | total loss: \u001b[1m\u001b[32m0.01486\u001b[0m\u001b[0m | time: 0.270s\n",
      "| SGD | epoch: 023 | loss: 0.01486 - R2: 0.9999 -- iter: 0960/1168\n",
      "Training Step: 1347  | total loss: \u001b[1m\u001b[32m0.03128\u001b[0m\u001b[0m | time: 0.282s\n",
      "| SGD | epoch: 023 | loss: 0.03128 - R2: 0.9990 -- iter: 0980/1168\n",
      "Training Step: 1348  | total loss: \u001b[1m\u001b[32m0.03128\u001b[0m\u001b[0m | time: 0.284s\n",
      "| SGD | epoch: 023 | loss: 0.03128 - R2: 0.9990 -- iter: 1000/1168\n",
      "Training Step: 1349  | total loss: \u001b[1m\u001b[32m0.03049\u001b[0m\u001b[0m | time: 0.287s\n",
      "| SGD | epoch: 023 | loss: 0.03049 - R2: 0.9986 -- iter: 1020/1168\n",
      "Training Step: 1350  | total loss: \u001b[1m\u001b[32m0.02811\u001b[0m\u001b[0m | time: 0.300s\n",
      "| SGD | epoch: 023 | loss: 0.02811 - R2: 0.9989 -- iter: 1040/1168\n",
      "Training Step: 1351  | total loss: \u001b[1m\u001b[32m0.02580\u001b[0m\u001b[0m | time: 0.305s\n",
      "| SGD | epoch: 023 | loss: 0.02580 - R2: 0.9988 -- iter: 1060/1168\n",
      "Training Step: 1352  | total loss: \u001b[1m\u001b[32m0.02591\u001b[0m\u001b[0m | time: 0.309s\n",
      "| SGD | epoch: 023 | loss: 0.02591 - R2: 0.9993 -- iter: 1080/1168\n",
      "Training Step: 1353  | total loss: \u001b[1m\u001b[32m0.02970\u001b[0m\u001b[0m | time: 0.314s\n",
      "| SGD | epoch: 023 | loss: 0.02970 - R2: 1.0007 -- iter: 1100/1168\n",
      "Training Step: 1354  | total loss: \u001b[1m\u001b[32m0.02787\u001b[0m\u001b[0m | time: 0.316s\n",
      "| SGD | epoch: 023 | loss: 0.02787 - R2: 1.0005 -- iter: 1120/1168\n",
      "Training Step: 1355  | total loss: \u001b[1m\u001b[32m0.02787\u001b[0m\u001b[0m | time: 0.318s\n",
      "| SGD | epoch: 023 | loss: 0.02787 - R2: 1.0005 -- iter: 1140/1168\n",
      "Training Step: 1356  | total loss: \u001b[1m\u001b[32m0.02643\u001b[0m\u001b[0m | time: 0.319s\n",
      "| SGD | epoch: 023 | loss: 0.02643 - R2: 1.0014 -- iter: 1160/1168\n",
      "Training Step: 1357  | total loss: \u001b[1m\u001b[32m0.02613\u001b[0m\u001b[0m | time: 1.325s\n",
      "| SGD | epoch: 023 | loss: 0.02613 - R2: 1.0014 | val_loss: 0.03265 - val_acc: 1.0010 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 1358  | total loss: \u001b[1m\u001b[32m0.02451\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 024 | loss: 0.02451 - R2: 1.0010 -- iter: 0020/1168\n",
      "Training Step: 1359  | total loss: \u001b[1m\u001b[32m0.02798\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 024 | loss: 0.02798 - R2: 1.0002 -- iter: 0040/1168\n",
      "Training Step: 1360  | total loss: \u001b[1m\u001b[32m0.02798\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 024 | loss: 0.02798 - R2: 1.0001 -- iter: 0060/1168\n",
      "Training Step: 1361  | total loss: \u001b[1m\u001b[32m0.02466\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 024 | loss: 0.02466 - R2: 1.0001 -- iter: 0080/1168\n",
      "Training Step: 1362  | total loss: \u001b[1m\u001b[32m0.02466\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 024 | loss: 0.02466 - R2: 1.0003 -- iter: 0100/1168\n",
      "Training Step: 1363  | total loss: \u001b[1m\u001b[32m0.02383\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 024 | loss: 0.02383 - R2: 1.0004 -- iter: 0120/1168\n",
      "Training Step: 1364  | total loss: \u001b[1m\u001b[32m0.02383\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 024 | loss: 0.02383 - R2: 0.9997 -- iter: 0140/1168\n",
      "Training Step: 1365  | total loss: \u001b[1m\u001b[32m0.02242\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 024 | loss: 0.02242 - R2: 1.0003 -- iter: 0160/1168\n",
      "Training Step: 1366  | total loss: \u001b[1m\u001b[32m0.02082\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 024 | loss: 0.02082 - R2: 1.0006 -- iter: 0180/1168\n",
      "Training Step: 1367  | total loss: \u001b[1m\u001b[32m0.02202\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 024 | loss: 0.02202 - R2: 1.0001 -- iter: 0200/1168\n",
      "Training Step: 1368  | total loss: \u001b[1m\u001b[32m0.02097\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 024 | loss: 0.02097 - R2: 0.9994 -- iter: 0220/1168\n",
      "Training Step: 1369  | total loss: \u001b[1m\u001b[32m0.02041\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 024 | loss: 0.02041 - R2: 1.0010 -- iter: 0240/1168\n",
      "Training Step: 1370  | total loss: \u001b[1m\u001b[32m0.02297\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 024 | loss: 0.02297 - R2: 1.0001 -- iter: 0260/1168\n",
      "Training Step: 1371  | total loss: \u001b[1m\u001b[32m0.02234\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 024 | loss: 0.02234 - R2: 1.0001 -- iter: 0280/1168\n",
      "Training Step: 1372  | total loss: \u001b[1m\u001b[32m0.02157\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 024 | loss: 0.02157 - R2: 0.9993 -- iter: 0300/1168\n",
      "Training Step: 1373  | total loss: \u001b[1m\u001b[32m0.01951\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 024 | loss: 0.01951 - R2: 0.9991 -- iter: 0320/1168\n",
      "Training Step: 1374  | total loss: \u001b[1m\u001b[32m0.01951\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 024 | loss: 0.01951 - R2: 0.9991 -- iter: 0340/1168\n",
      "Training Step: 1375  | total loss: \u001b[1m\u001b[32m0.01835\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 024 | loss: 0.01835 - R2: 0.9991 -- iter: 0360/1168\n",
      "Training Step: 1376  | total loss: \u001b[1m\u001b[32m0.01846\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 024 | loss: 0.01846 - R2: 0.9988 -- iter: 0380/1168\n",
      "Training Step: 1377  | total loss: \u001b[1m\u001b[32m0.01768\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 024 | loss: 0.01768 - R2: 0.9986 -- iter: 0400/1168\n",
      "Training Step: 1378  | total loss: \u001b[1m\u001b[32m0.01749\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 024 | loss: 0.01749 - R2: 0.9982 -- iter: 0420/1168\n",
      "Training Step: 1379  | total loss: \u001b[1m\u001b[32m0.01749\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 024 | loss: 0.01749 - R2: 0.9982 -- iter: 0440/1168\n",
      "Training Step: 1380  | total loss: \u001b[1m\u001b[32m0.01652\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 024 | loss: 0.01652 - R2: 0.9996 -- iter: 0460/1168\n",
      "Training Step: 1381  | total loss: \u001b[1m\u001b[32m0.01543\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 024 | loss: 0.01543 - R2: 1.0004 -- iter: 0480/1168\n",
      "Training Step: 1382  | total loss: \u001b[1m\u001b[32m0.01543\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 024 | loss: 0.01543 - R2: 1.0004 -- iter: 0500/1168\n",
      "Training Step: 1383  | total loss: \u001b[1m\u001b[32m0.01536\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 024 | loss: 0.01536 - R2: 1.0007 -- iter: 0520/1168\n",
      "Training Step: 1384  | total loss: \u001b[1m\u001b[32m0.01504\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 024 | loss: 0.01504 - R2: 1.0009 -- iter: 0540/1168\n",
      "Training Step: 1385  | total loss: \u001b[1m\u001b[32m0.01481\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 024 | loss: 0.01481 - R2: 1.0014 -- iter: 0560/1168\n",
      "Training Step: 1386  | total loss: \u001b[1m\u001b[32m0.01412\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 024 | loss: 0.01412 - R2: 1.0012 -- iter: 0580/1168\n",
      "Training Step: 1387  | total loss: \u001b[1m\u001b[32m0.01370\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 024 | loss: 0.01370 - R2: 1.0009 -- iter: 0600/1168\n",
      "Training Step: 1388  | total loss: \u001b[1m\u001b[32m0.01461\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 024 | loss: 0.01461 - R2: 1.0006 -- iter: 0620/1168\n",
      "Training Step: 1389  | total loss: \u001b[1m\u001b[32m0.01727\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 024 | loss: 0.01727 - R2: 1.0006 -- iter: 0640/1168\n",
      "Training Step: 1390  | total loss: \u001b[1m\u001b[32m0.01727\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 024 | loss: 0.01727 - R2: 1.0013 -- iter: 0660/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1391  | total loss: \u001b[1m\u001b[32m0.01714\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 024 | loss: 0.01714 - R2: 1.0013 -- iter: 0680/1168\n",
      "Training Step: 1392  | total loss: \u001b[1m\u001b[32m0.01703\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 024 | loss: 0.01703 - R2: 1.0008 -- iter: 0700/1168\n",
      "Training Step: 1393  | total loss: \u001b[1m\u001b[32m0.01674\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 024 | loss: 0.01674 - R2: 1.0003 -- iter: 0720/1168\n",
      "Training Step: 1394  | total loss: \u001b[1m\u001b[32m0.02284\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 024 | loss: 0.02284 - R2: 1.0013 -- iter: 0740/1168\n",
      "Training Step: 1395  | total loss: \u001b[1m\u001b[32m0.02190\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 024 | loss: 0.02190 - R2: 1.0013 -- iter: 0760/1168\n",
      "Training Step: 1396  | total loss: \u001b[1m\u001b[32m0.02054\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 024 | loss: 0.02054 - R2: 1.0013 -- iter: 0780/1168\n",
      "Training Step: 1397  | total loss: \u001b[1m\u001b[32m0.02069\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 024 | loss: 0.02069 - R2: 1.0013 -- iter: 0800/1168\n",
      "Training Step: 1398  | total loss: \u001b[1m\u001b[32m0.02074\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 024 | loss: 0.02074 - R2: 1.0006 -- iter: 0820/1168\n",
      "Training Step: 1399  | total loss: \u001b[1m\u001b[32m0.02287\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 024 | loss: 0.02287 - R2: 1.0000 -- iter: 0840/1168\n",
      "Training Step: 1400  | total loss: \u001b[1m\u001b[32m0.02287\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 024 | loss: 0.02287 - R2: 1.0000 -- iter: 0860/1168\n",
      "Training Step: 1401  | total loss: \u001b[1m\u001b[32m0.02155\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 024 | loss: 0.02155 - R2: 0.9998 -- iter: 0880/1168\n",
      "Training Step: 1402  | total loss: \u001b[1m\u001b[32m0.02136\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 024 | loss: 0.02136 - R2: 0.9994 -- iter: 0900/1168\n",
      "Training Step: 1403  | total loss: \u001b[1m\u001b[32m0.02058\u001b[0m\u001b[0m | time: 0.169s\n",
      "| SGD | epoch: 024 | loss: 0.02058 - R2: 0.9993 -- iter: 0920/1168\n",
      "Training Step: 1404  | total loss: \u001b[1m\u001b[32m0.01947\u001b[0m\u001b[0m | time: 0.172s\n",
      "| SGD | epoch: 024 | loss: 0.01947 - R2: 0.9993 -- iter: 0940/1168\n",
      "Training Step: 1405  | total loss: \u001b[1m\u001b[32m0.01697\u001b[0m\u001b[0m | time: 0.178s\n",
      "| SGD | epoch: 024 | loss: 0.01697 - R2: 0.9995 -- iter: 0960/1168\n",
      "Training Step: 1406  | total loss: \u001b[1m\u001b[32m0.01697\u001b[0m\u001b[0m | time: 0.182s\n",
      "| SGD | epoch: 024 | loss: 0.01697 - R2: 0.9995 -- iter: 0980/1168\n",
      "Training Step: 1407  | total loss: \u001b[1m\u001b[32m0.01683\u001b[0m\u001b[0m | time: 0.189s\n",
      "| SGD | epoch: 024 | loss: 0.01683 - R2: 0.9990 -- iter: 1000/1168\n",
      "Training Step: 1408  | total loss: \u001b[1m\u001b[32m0.01699\u001b[0m\u001b[0m | time: 0.191s\n",
      "| SGD | epoch: 024 | loss: 0.01699 - R2: 0.9984 -- iter: 1020/1168\n",
      "Training Step: 1409  | total loss: \u001b[1m\u001b[32m0.01836\u001b[0m\u001b[0m | time: 0.193s\n",
      "| SGD | epoch: 024 | loss: 0.01836 - R2: 0.9986 -- iter: 1040/1168\n",
      "Training Step: 1410  | total loss: \u001b[1m\u001b[32m0.01736\u001b[0m\u001b[0m | time: 0.195s\n",
      "| SGD | epoch: 024 | loss: 0.01736 - R2: 0.9986 -- iter: 1060/1168\n",
      "Training Step: 1411  | total loss: \u001b[1m\u001b[32m0.01843\u001b[0m\u001b[0m | time: 0.199s\n",
      "| SGD | epoch: 024 | loss: 0.01843 - R2: 1.0003 -- iter: 1080/1168\n",
      "Training Step: 1412  | total loss: \u001b[1m\u001b[32m0.02094\u001b[0m\u001b[0m | time: 0.203s\n",
      "| SGD | epoch: 024 | loss: 0.02094 - R2: 1.0003 -- iter: 1100/1168\n",
      "Training Step: 1413  | total loss: \u001b[1m\u001b[32m0.02150\u001b[0m\u001b[0m | time: 0.206s\n",
      "| SGD | epoch: 024 | loss: 0.02150 - R2: 1.0012 -- iter: 1120/1168\n",
      "Training Step: 1414  | total loss: \u001b[1m\u001b[32m0.01930\u001b[0m\u001b[0m | time: 0.209s\n",
      "| SGD | epoch: 024 | loss: 0.01930 - R2: 1.0007 -- iter: 1140/1168\n",
      "Training Step: 1415  | total loss: \u001b[1m\u001b[32m0.01930\u001b[0m\u001b[0m | time: 0.212s\n",
      "| SGD | epoch: 024 | loss: 0.01930 - R2: 1.0007 -- iter: 1160/1168\n",
      "Training Step: 1416  | total loss: \u001b[1m\u001b[32m0.01829\u001b[0m\u001b[0m | time: 1.222s\n",
      "| SGD | epoch: 024 | loss: 0.01829 - R2: 1.0005 | val_loss: 0.03206 - val_acc: 0.9999 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 1417  | total loss: \u001b[1m\u001b[32m0.01853\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 025 | loss: 0.01853 - R2: 1.0006 -- iter: 0020/1168\n",
      "Training Step: 1418  | total loss: \u001b[1m\u001b[32m0.01853\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 025 | loss: 0.01853 - R2: 1.0006 -- iter: 0040/1168\n",
      "Training Step: 1419  | total loss: \u001b[1m\u001b[32m0.01782\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 025 | loss: 0.01782 - R2: 1.0005 -- iter: 0060/1168\n",
      "Training Step: 1420  | total loss: \u001b[1m\u001b[32m0.01749\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 025 | loss: 0.01749 - R2: 1.0004 -- iter: 0080/1168\n",
      "Training Step: 1421  | total loss: \u001b[1m\u001b[32m0.01890\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 025 | loss: 0.01890 - R2: 0.9999 -- iter: 0100/1168\n",
      "Training Step: 1422  | total loss: \u001b[1m\u001b[32m0.01890\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 025 | loss: 0.01890 - R2: 0.9999 -- iter: 0120/1168\n",
      "Training Step: 1423  | total loss: \u001b[1m\u001b[32m0.01798\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 025 | loss: 0.01798 - R2: 0.9998 -- iter: 0140/1168\n",
      "Training Step: 1424  | total loss: \u001b[1m\u001b[32m0.01767\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 025 | loss: 0.01767 - R2: 0.9992 -- iter: 0160/1168\n",
      "Training Step: 1425  | total loss: \u001b[1m\u001b[32m0.02334\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 025 | loss: 0.02334 - R2: 0.9991 -- iter: 0180/1168\n",
      "Training Step: 1426  | total loss: \u001b[1m\u001b[32m0.02276\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 025 | loss: 0.02276 - R2: 0.9991 -- iter: 0200/1168\n",
      "Training Step: 1427  | total loss: \u001b[1m\u001b[32m0.02366\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 025 | loss: 0.02366 - R2: 0.9981 -- iter: 0220/1168\n",
      "Training Step: 1428  | total loss: \u001b[1m\u001b[32m0.02269\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 025 | loss: 0.02269 - R2: 0.9983 -- iter: 0240/1168\n",
      "Training Step: 1429  | total loss: \u001b[1m\u001b[32m0.02284\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 025 | loss: 0.02284 - R2: 0.9985 -- iter: 0260/1168\n",
      "Training Step: 1430  | total loss: \u001b[1m\u001b[32m0.02143\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 025 | loss: 0.02143 - R2: 0.9986 -- iter: 0280/1168\n",
      "Training Step: 1431  | total loss: \u001b[1m\u001b[32m0.02122\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 025 | loss: 0.02122 - R2: 0.9991 -- iter: 0300/1168\n",
      "Training Step: 1432  | total loss: \u001b[1m\u001b[32m0.02122\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 025 | loss: 0.02122 - R2: 0.9991 -- iter: 0320/1168\n",
      "Training Step: 1433  | total loss: \u001b[1m\u001b[32m0.02193\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 025 | loss: 0.02193 - R2: 0.9997 -- iter: 0340/1168\n",
      "Training Step: 1434  | total loss: \u001b[1m\u001b[32m0.02119\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 025 | loss: 0.02119 - R2: 1.0003 -- iter: 0360/1168\n",
      "Training Step: 1435  | total loss: \u001b[1m\u001b[32m0.01980\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 025 | loss: 0.01980 - R2: 1.0001 -- iter: 0380/1168\n",
      "Training Step: 1436  | total loss: \u001b[1m\u001b[32m0.01818\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 025 | loss: 0.01818 - R2: 0.9997 -- iter: 0400/1168\n",
      "Training Step: 1437  | total loss: \u001b[1m\u001b[32m0.01889\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 025 | loss: 0.01889 - R2: 0.9989 -- iter: 0420/1168\n",
      "Training Step: 1438  | total loss: \u001b[1m\u001b[32m0.01779\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 025 | loss: 0.01779 - R2: 0.9985 -- iter: 0440/1168\n",
      "Training Step: 1439  | total loss: \u001b[1m\u001b[32m0.01727\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 025 | loss: 0.01727 - R2: 0.9988 -- iter: 0460/1168\n",
      "Training Step: 1440  | total loss: \u001b[1m\u001b[32m0.01666\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 025 | loss: 0.01666 - R2: 0.9990 -- iter: 0480/1168\n",
      "Training Step: 1441  | total loss: \u001b[1m\u001b[32m0.01666\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 025 | loss: 0.01666 - R2: 0.9990 -- iter: 0500/1168\n",
      "Training Step: 1442  | total loss: \u001b[1m\u001b[32m0.01619\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 025 | loss: 0.01619 - R2: 0.9996 -- iter: 0520/1168\n",
      "Training Step: 1443  | total loss: \u001b[1m\u001b[32m0.01597\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 025 | loss: 0.01597 - R2: 0.9996 -- iter: 0540/1168\n",
      "Training Step: 1444  | total loss: \u001b[1m\u001b[32m0.01615\u001b[0m\u001b[0m | time: 0.169s\n",
      "| SGD | epoch: 025 | loss: 0.01615 - R2: 0.9985 -- iter: 0560/1168\n",
      "Training Step: 1445  | total loss: \u001b[1m\u001b[32m0.01641\u001b[0m\u001b[0m | time: 0.172s\n",
      "| SGD | epoch: 025 | loss: 0.01641 - R2: 0.9987 -- iter: 0580/1168\n",
      "Training Step: 1446  | total loss: \u001b[1m\u001b[32m0.01641\u001b[0m\u001b[0m | time: 0.179s\n",
      "| SGD | epoch: 025 | loss: 0.01641 - R2: 0.9987 -- iter: 0600/1168\n",
      "Training Step: 1447  | total loss: \u001b[1m\u001b[32m0.01941\u001b[0m\u001b[0m | time: 0.181s\n",
      "| SGD | epoch: 025 | loss: 0.01941 - R2: 1.0000 -- iter: 0620/1168\n",
      "Training Step: 1448  | total loss: \u001b[1m\u001b[32m0.01941\u001b[0m\u001b[0m | time: 0.183s\n",
      "| SGD | epoch: 025 | loss: 0.01941 - R2: 1.0000 -- iter: 0640/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1449  | total loss: \u001b[1m\u001b[32m0.01878\u001b[0m\u001b[0m | time: 0.187s\n",
      "| SGD | epoch: 025 | loss: 0.01878 - R2: 1.0004 -- iter: 0660/1168\n",
      "Training Step: 1450  | total loss: \u001b[1m\u001b[32m0.01861\u001b[0m\u001b[0m | time: 0.188s\n",
      "| SGD | epoch: 025 | loss: 0.01861 - R2: 0.9994 -- iter: 0680/1168\n",
      "Training Step: 1451  | total loss: \u001b[1m\u001b[32m0.01820\u001b[0m\u001b[0m | time: 0.199s\n",
      "| SGD | epoch: 025 | loss: 0.01820 - R2: 0.9996 -- iter: 0700/1168\n",
      "Training Step: 1452  | total loss: \u001b[1m\u001b[32m0.01704\u001b[0m\u001b[0m | time: 0.202s\n",
      "| SGD | epoch: 025 | loss: 0.01704 - R2: 1.0001 -- iter: 0720/1168\n",
      "Training Step: 1453  | total loss: \u001b[1m\u001b[32m0.01704\u001b[0m\u001b[0m | time: 0.205s\n",
      "| SGD | epoch: 025 | loss: 0.01704 - R2: 1.0001 -- iter: 0740/1168\n",
      "Training Step: 1454  | total loss: \u001b[1m\u001b[32m0.01672\u001b[0m\u001b[0m | time: 0.208s\n",
      "| SGD | epoch: 025 | loss: 0.01672 - R2: 1.0007 -- iter: 0760/1168\n",
      "Training Step: 1455  | total loss: \u001b[1m\u001b[32m0.01566\u001b[0m\u001b[0m | time: 0.211s\n",
      "| SGD | epoch: 025 | loss: 0.01566 - R2: 1.0003 -- iter: 0780/1168\n",
      "Training Step: 1456  | total loss: \u001b[1m\u001b[32m0.01499\u001b[0m\u001b[0m | time: 0.213s\n",
      "| SGD | epoch: 025 | loss: 0.01499 - R2: 1.0003 -- iter: 0800/1168\n",
      "Training Step: 1457  | total loss: \u001b[1m\u001b[32m0.01415\u001b[0m\u001b[0m | time: 0.215s\n",
      "| SGD | epoch: 025 | loss: 0.01415 - R2: 1.0003 -- iter: 0820/1168\n",
      "Training Step: 1458  | total loss: \u001b[1m\u001b[32m0.01359\u001b[0m\u001b[0m | time: 0.218s\n",
      "| SGD | epoch: 025 | loss: 0.01359 - R2: 1.0003 -- iter: 0840/1168\n",
      "Training Step: 1459  | total loss: \u001b[1m\u001b[32m0.01384\u001b[0m\u001b[0m | time: 0.220s\n",
      "| SGD | epoch: 025 | loss: 0.01384 - R2: 1.0008 -- iter: 0860/1168\n",
      "Training Step: 1460  | total loss: \u001b[1m\u001b[32m0.01527\u001b[0m\u001b[0m | time: 0.223s\n",
      "| SGD | epoch: 025 | loss: 0.01527 - R2: 1.0017 -- iter: 0880/1168\n",
      "Training Step: 1461  | total loss: \u001b[1m\u001b[32m0.01892\u001b[0m\u001b[0m | time: 0.225s\n",
      "| SGD | epoch: 025 | loss: 0.01892 - R2: 1.0029 -- iter: 0900/1168\n",
      "Training Step: 1462  | total loss: \u001b[1m\u001b[32m0.01892\u001b[0m\u001b[0m | time: 0.228s\n",
      "| SGD | epoch: 025 | loss: 0.01892 - R2: 1.0029 -- iter: 0920/1168\n",
      "Training Step: 1463  | total loss: \u001b[1m\u001b[32m0.01892\u001b[0m\u001b[0m | time: 0.230s\n",
      "| SGD | epoch: 025 | loss: 0.01892 - R2: 1.0020 -- iter: 0940/1168\n",
      "Training Step: 1464  | total loss: \u001b[1m\u001b[32m0.01885\u001b[0m\u001b[0m | time: 0.233s\n",
      "| SGD | epoch: 025 | loss: 0.01885 - R2: 1.0012 -- iter: 0960/1168\n",
      "Training Step: 1465  | total loss: \u001b[1m\u001b[32m0.01885\u001b[0m\u001b[0m | time: 0.235s\n",
      "| SGD | epoch: 025 | loss: 0.01885 - R2: 1.0012 -- iter: 0980/1168\n",
      "Training Step: 1466  | total loss: \u001b[1m\u001b[32m0.01795\u001b[0m\u001b[0m | time: 0.237s\n",
      "| SGD | epoch: 025 | loss: 0.01795 - R2: 1.0005 -- iter: 1000/1168\n",
      "Training Step: 1467  | total loss: \u001b[1m\u001b[32m0.01792\u001b[0m\u001b[0m | time: 0.240s\n",
      "| SGD | epoch: 025 | loss: 0.01792 - R2: 1.0004 -- iter: 1020/1168\n",
      "Training Step: 1468  | total loss: \u001b[1m\u001b[32m0.01806\u001b[0m\u001b[0m | time: 0.242s\n",
      "| SGD | epoch: 025 | loss: 0.01806 - R2: 0.9995 -- iter: 1040/1168\n",
      "Training Step: 1469  | total loss: \u001b[1m\u001b[32m0.01813\u001b[0m\u001b[0m | time: 0.246s\n",
      "| SGD | epoch: 025 | loss: 0.01813 - R2: 1.0001 -- iter: 1060/1168\n",
      "Training Step: 1470  | total loss: \u001b[1m\u001b[32m0.02324\u001b[0m\u001b[0m | time: 0.251s\n",
      "| SGD | epoch: 025 | loss: 0.02324 - R2: 1.0015 -- iter: 1080/1168\n",
      "Training Step: 1471  | total loss: \u001b[1m\u001b[32m0.02324\u001b[0m\u001b[0m | time: 0.253s\n",
      "| SGD | epoch: 025 | loss: 0.02324 - R2: 1.0015 -- iter: 1100/1168\n",
      "Training Step: 1472  | total loss: \u001b[1m\u001b[32m0.02238\u001b[0m\u001b[0m | time: 0.254s\n",
      "| SGD | epoch: 025 | loss: 0.02238 - R2: 1.0006 -- iter: 1120/1168\n",
      "Training Step: 1473  | total loss: \u001b[1m\u001b[32m0.02242\u001b[0m\u001b[0m | time: 0.260s\n",
      "| SGD | epoch: 025 | loss: 0.02242 - R2: 1.0000 -- iter: 1140/1168\n",
      "Training Step: 1474  | total loss: \u001b[1m\u001b[32m0.02493\u001b[0m\u001b[0m | time: 0.263s\n",
      "| SGD | epoch: 025 | loss: 0.02493 - R2: 1.0000 -- iter: 1160/1168\n",
      "Training Step: 1475  | total loss: \u001b[1m\u001b[32m0.02324\u001b[0m\u001b[0m | time: 1.268s\n",
      "| SGD | epoch: 025 | loss: 0.02324 - R2: 0.9998 | val_loss: 0.03212 - val_acc: 1.0011 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 1476  | total loss: \u001b[1m\u001b[32m0.02411\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 026 | loss: 0.02411 - R2: 0.9992 -- iter: 0020/1168\n",
      "Training Step: 1477  | total loss: \u001b[1m\u001b[32m0.02291\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 026 | loss: 0.02291 - R2: 0.9994 -- iter: 0040/1168\n",
      "Training Step: 1478  | total loss: \u001b[1m\u001b[32m0.02233\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 026 | loss: 0.02233 - R2: 1.0002 -- iter: 0060/1168\n",
      "Training Step: 1479  | total loss: \u001b[1m\u001b[32m0.02141\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 026 | loss: 0.02141 - R2: 1.0001 -- iter: 0080/1168\n",
      "Training Step: 1480  | total loss: \u001b[1m\u001b[32m0.02056\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 026 | loss: 0.02056 - R2: 1.0010 -- iter: 0100/1168\n",
      "Training Step: 1481  | total loss: \u001b[1m\u001b[32m0.02056\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 026 | loss: 0.02056 - R2: 1.0010 -- iter: 0120/1168\n",
      "Training Step: 1482  | total loss: \u001b[1m\u001b[32m0.02031\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 026 | loss: 0.02031 - R2: 0.9996 -- iter: 0140/1168\n",
      "Training Step: 1483  | total loss: \u001b[1m\u001b[32m0.02040\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 026 | loss: 0.02040 - R2: 1.0000 -- iter: 0160/1168\n",
      "Training Step: 1484  | total loss: \u001b[1m\u001b[32m0.01929\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 026 | loss: 0.01929 - R2: 0.9997 -- iter: 0180/1168\n",
      "Training Step: 1485  | total loss: \u001b[1m\u001b[32m0.02018\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 026 | loss: 0.02018 - R2: 0.9989 -- iter: 0200/1168\n",
      "Training Step: 1486  | total loss: \u001b[1m\u001b[32m0.02018\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 026 | loss: 0.02018 - R2: 0.9989 -- iter: 0220/1168\n",
      "Training Step: 1487  | total loss: \u001b[1m\u001b[32m0.01934\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 026 | loss: 0.01934 - R2: 0.9994 -- iter: 0240/1168\n",
      "Training Step: 1488  | total loss: \u001b[1m\u001b[32m0.01901\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 026 | loss: 0.01901 - R2: 0.9997 -- iter: 0260/1168\n",
      "Training Step: 1489  | total loss: \u001b[1m\u001b[32m0.02004\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 026 | loss: 0.02004 - R2: 1.0003 -- iter: 0280/1168\n",
      "Training Step: 1490  | total loss: \u001b[1m\u001b[32m0.02184\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 026 | loss: 0.02184 - R2: 0.9998 -- iter: 0300/1168\n",
      "Training Step: 1491  | total loss: \u001b[1m\u001b[32m0.02184\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 026 | loss: 0.02184 - R2: 1.0009 -- iter: 0320/1168\n",
      "Training Step: 1492  | total loss: \u001b[1m\u001b[32m0.02150\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 026 | loss: 0.02150 - R2: 1.0009 -- iter: 0340/1168\n",
      "Training Step: 1493  | total loss: \u001b[1m\u001b[32m0.02150\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 026 | loss: 0.02150 - R2: 1.0004 -- iter: 0360/1168\n",
      "Training Step: 1494  | total loss: \u001b[1m\u001b[32m0.02008\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 026 | loss: 0.02008 - R2: 1.0004 -- iter: 0380/1168\n",
      "Training Step: 1495  | total loss: \u001b[1m\u001b[32m0.01881\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 026 | loss: 0.01881 - R2: 0.9996 -- iter: 0400/1168\n",
      "Training Step: 1496  | total loss: \u001b[1m\u001b[32m0.01966\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 026 | loss: 0.01966 - R2: 1.0002 -- iter: 0420/1168\n",
      "Training Step: 1497  | total loss: \u001b[1m\u001b[32m0.01966\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 026 | loss: 0.01966 - R2: 1.0002 -- iter: 0440/1168\n",
      "Training Step: 1498  | total loss: \u001b[1m\u001b[32m0.01954\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 026 | loss: 0.01954 - R2: 0.9999 -- iter: 0460/1168\n",
      "Training Step: 1499  | total loss: \u001b[1m\u001b[32m0.02143\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 026 | loss: 0.02143 - R2: 0.9973 -- iter: 0480/1168\n",
      "Training Step: 1500  | total loss: \u001b[1m\u001b[32m0.02244\u001b[0m\u001b[0m | time: 0.169s\n",
      "| SGD | epoch: 026 | loss: 0.02244 - R2: 0.9958 -- iter: 0500/1168\n",
      "Training Step: 1501  | total loss: \u001b[1m\u001b[32m0.02244\u001b[0m\u001b[0m | time: 0.171s\n",
      "| SGD | epoch: 026 | loss: 0.02244 - R2: 0.9958 -- iter: 0520/1168\n",
      "Training Step: 1502  | total loss: \u001b[1m\u001b[32m0.02083\u001b[0m\u001b[0m | time: 0.174s\n",
      "| SGD | epoch: 026 | loss: 0.02083 - R2: 0.9964 -- iter: 0540/1168\n",
      "Training Step: 1503  | total loss: \u001b[1m\u001b[32m0.01953\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 026 | loss: 0.01953 - R2: 0.9962 -- iter: 0560/1168\n",
      "Training Step: 1504  | total loss: \u001b[1m\u001b[32m0.01953\u001b[0m\u001b[0m | time: 0.179s\n",
      "| SGD | epoch: 026 | loss: 0.01953 - R2: 0.9962 -- iter: 0580/1168\n",
      "Training Step: 1505  | total loss: \u001b[1m\u001b[32m0.01831\u001b[0m\u001b[0m | time: 0.186s\n",
      "| SGD | epoch: 026 | loss: 0.01831 - R2: 0.9968 -- iter: 0600/1168\n",
      "Training Step: 1506  | total loss: \u001b[1m\u001b[32m0.01699\u001b[0m\u001b[0m | time: 0.189s\n",
      "| SGD | epoch: 026 | loss: 0.01699 - R2: 0.9978 -- iter: 0620/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1507  | total loss: \u001b[1m\u001b[32m0.01687\u001b[0m\u001b[0m | time: 0.192s\n",
      "| SGD | epoch: 026 | loss: 0.01687 - R2: 0.9988 -- iter: 0640/1168\n",
      "Training Step: 1508  | total loss: \u001b[1m\u001b[32m0.01687\u001b[0m\u001b[0m | time: 0.195s\n",
      "| SGD | epoch: 026 | loss: 0.01687 - R2: 0.9994 -- iter: 0660/1168\n",
      "Training Step: 1509  | total loss: \u001b[1m\u001b[32m0.01728\u001b[0m\u001b[0m | time: 0.197s\n",
      "| SGD | epoch: 026 | loss: 0.01728 - R2: 1.0003 -- iter: 0680/1168\n",
      "Training Step: 1510  | total loss: \u001b[1m\u001b[32m0.01692\u001b[0m\u001b[0m | time: 0.199s\n",
      "| SGD | epoch: 026 | loss: 0.01692 - R2: 1.0002 -- iter: 0700/1168\n",
      "Training Step: 1511  | total loss: \u001b[1m\u001b[32m0.01743\u001b[0m\u001b[0m | time: 0.202s\n",
      "| SGD | epoch: 026 | loss: 0.01743 - R2: 1.0000 -- iter: 0720/1168\n",
      "Training Step: 1512  | total loss: \u001b[1m\u001b[32m0.01743\u001b[0m\u001b[0m | time: 0.203s\n",
      "| SGD | epoch: 026 | loss: 0.01743 - R2: 1.0000 -- iter: 0740/1168\n",
      "Training Step: 1513  | total loss: \u001b[1m\u001b[32m0.01680\u001b[0m\u001b[0m | time: 0.205s\n",
      "| SGD | epoch: 026 | loss: 0.01680 - R2: 1.0007 -- iter: 0760/1168\n",
      "Training Step: 1514  | total loss: \u001b[1m\u001b[32m0.01569\u001b[0m\u001b[0m | time: 0.212s\n",
      "| SGD | epoch: 026 | loss: 0.01569 - R2: 1.0008 -- iter: 0780/1168\n",
      "Training Step: 1515  | total loss: \u001b[1m\u001b[32m0.01569\u001b[0m\u001b[0m | time: 0.215s\n",
      "| SGD | epoch: 026 | loss: 0.01569 - R2: 1.0009 -- iter: 0800/1168\n",
      "Training Step: 1516  | total loss: \u001b[1m\u001b[32m0.01526\u001b[0m\u001b[0m | time: 0.218s\n",
      "| SGD | epoch: 026 | loss: 0.01526 - R2: 1.0009 -- iter: 0820/1168\n",
      "Training Step: 1517  | total loss: \u001b[1m\u001b[32m0.01542\u001b[0m\u001b[0m | time: 0.221s\n",
      "| SGD | epoch: 026 | loss: 0.01542 - R2: 0.9996 -- iter: 0840/1168\n",
      "Training Step: 1518  | total loss: \u001b[1m\u001b[32m0.01542\u001b[0m\u001b[0m | time: 0.223s\n",
      "| SGD | epoch: 026 | loss: 0.01542 - R2: 0.9996 -- iter: 0860/1168\n",
      "Training Step: 1519  | total loss: \u001b[1m\u001b[32m0.01503\u001b[0m\u001b[0m | time: 0.226s\n",
      "| SGD | epoch: 026 | loss: 0.01503 - R2: 0.9994 -- iter: 0880/1168\n",
      "Training Step: 1520  | total loss: \u001b[1m\u001b[32m0.01440\u001b[0m\u001b[0m | time: 0.228s\n",
      "| SGD | epoch: 026 | loss: 0.01440 - R2: 0.9995 -- iter: 0900/1168\n",
      "Training Step: 1521  | total loss: \u001b[1m\u001b[32m0.01422\u001b[0m\u001b[0m | time: 0.231s\n",
      "| SGD | epoch: 026 | loss: 0.01422 - R2: 1.0011 -- iter: 0920/1168\n",
      "Training Step: 1522  | total loss: \u001b[1m\u001b[32m0.01471\u001b[0m\u001b[0m | time: 0.233s\n",
      "| SGD | epoch: 026 | loss: 0.01471 - R2: 1.0015 -- iter: 0940/1168\n",
      "Training Step: 1523  | total loss: \u001b[1m\u001b[32m0.01559\u001b[0m\u001b[0m | time: 0.236s\n",
      "| SGD | epoch: 026 | loss: 0.01559 - R2: 1.0015 -- iter: 0960/1168\n",
      "Training Step: 1524  | total loss: \u001b[1m\u001b[32m0.01638\u001b[0m\u001b[0m | time: 0.238s\n",
      "| SGD | epoch: 026 | loss: 0.01638 - R2: 1.0024 -- iter: 0980/1168\n",
      "Training Step: 1525  | total loss: \u001b[1m\u001b[32m0.01949\u001b[0m\u001b[0m | time: 0.241s\n",
      "| SGD | epoch: 026 | loss: 0.01949 - R2: 1.0021 -- iter: 1000/1168\n",
      "Training Step: 1526  | total loss: \u001b[1m\u001b[32m0.01880\u001b[0m\u001b[0m | time: 0.244s\n",
      "| SGD | epoch: 026 | loss: 0.01880 - R2: 1.0021 -- iter: 1020/1168\n",
      "Training Step: 1527  | total loss: \u001b[1m\u001b[32m0.01806\u001b[0m\u001b[0m | time: 0.246s\n",
      "| SGD | epoch: 026 | loss: 0.01806 - R2: 1.0007 -- iter: 1040/1168\n",
      "Training Step: 1528  | total loss: \u001b[1m\u001b[32m0.01890\u001b[0m\u001b[0m | time: 0.248s\n",
      "| SGD | epoch: 026 | loss: 0.01890 - R2: 1.0007 -- iter: 1060/1168\n",
      "Training Step: 1529  | total loss: \u001b[1m\u001b[32m0.01797\u001b[0m\u001b[0m | time: 0.250s\n",
      "| SGD | epoch: 026 | loss: 0.01797 - R2: 1.0008 -- iter: 1080/1168\n",
      "Training Step: 1530  | total loss: \u001b[1m\u001b[32m0.01705\u001b[0m\u001b[0m | time: 0.253s\n",
      "| SGD | epoch: 026 | loss: 0.01705 - R2: 1.0009 -- iter: 1100/1168\n",
      "Training Step: 1531  | total loss: \u001b[1m\u001b[32m0.01821\u001b[0m\u001b[0m | time: 0.255s\n",
      "| SGD | epoch: 026 | loss: 0.01821 - R2: 1.0002 -- iter: 1120/1168\n",
      "Training Step: 1532  | total loss: \u001b[1m\u001b[32m0.01756\u001b[0m\u001b[0m | time: 0.257s\n",
      "| SGD | epoch: 026 | loss: 0.01756 - R2: 1.0001 -- iter: 1140/1168\n",
      "Training Step: 1533  | total loss: \u001b[1m\u001b[32m0.01859\u001b[0m\u001b[0m | time: 0.261s\n",
      "| SGD | epoch: 026 | loss: 0.01859 - R2: 1.0001 -- iter: 1160/1168\n",
      "Training Step: 1534  | total loss: \u001b[1m\u001b[32m0.01813\u001b[0m\u001b[0m | time: 1.269s\n",
      "| SGD | epoch: 026 | loss: 0.01813 - R2: 0.9994 | val_loss: 0.03201 - val_acc: 1.0017 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 1535  | total loss: \u001b[1m\u001b[32m0.01750\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 027 | loss: 0.01750 - R2: 0.9999 -- iter: 0020/1168\n",
      "Training Step: 1536  | total loss: \u001b[1m\u001b[32m0.01673\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 027 | loss: 0.01673 - R2: 1.0003 -- iter: 0040/1168\n",
      "Training Step: 1537  | total loss: \u001b[1m\u001b[32m0.01673\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 027 | loss: 0.01673 - R2: 1.0003 -- iter: 0060/1168\n",
      "Training Step: 1538  | total loss: \u001b[1m\u001b[32m0.01570\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 027 | loss: 0.01570 - R2: 0.9999 -- iter: 0080/1168\n",
      "Training Step: 1539  | total loss: \u001b[1m\u001b[32m0.01533\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 027 | loss: 0.01533 - R2: 0.9997 -- iter: 0100/1168\n",
      "Training Step: 1540  | total loss: \u001b[1m\u001b[32m0.01591\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 027 | loss: 0.01591 - R2: 1.0003 -- iter: 0120/1168\n",
      "Training Step: 1541  | total loss: \u001b[1m\u001b[32m0.02243\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 027 | loss: 0.02243 - R2: 1.0017 -- iter: 0140/1168\n",
      "Training Step: 1542  | total loss: \u001b[1m\u001b[32m0.02149\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 027 | loss: 0.02149 - R2: 1.0013 -- iter: 0160/1168\n",
      "Training Step: 1543  | total loss: \u001b[1m\u001b[32m0.02149\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 027 | loss: 0.02149 - R2: 1.0013 -- iter: 0180/1168\n",
      "Training Step: 1544  | total loss: \u001b[1m\u001b[32m0.02007\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 027 | loss: 0.02007 - R2: 1.0004 -- iter: 0200/1168\n",
      "Training Step: 1545  | total loss: \u001b[1m\u001b[32m0.01909\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 027 | loss: 0.01909 - R2: 1.0004 -- iter: 0220/1168\n",
      "Training Step: 1546  | total loss: \u001b[1m\u001b[32m0.01854\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 027 | loss: 0.01854 - R2: 0.9995 -- iter: 0240/1168\n",
      "Training Step: 1547  | total loss: \u001b[1m\u001b[32m0.02164\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 027 | loss: 0.02164 - R2: 1.0008 -- iter: 0260/1168\n",
      "Training Step: 1548  | total loss: \u001b[1m\u001b[32m0.02066\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 027 | loss: 0.02066 - R2: 1.0000 -- iter: 0280/1168\n",
      "Training Step: 1549  | total loss: \u001b[1m\u001b[32m0.02059\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 027 | loss: 0.02059 - R2: 1.0000 -- iter: 0300/1168\n",
      "Training Step: 1550  | total loss: \u001b[1m\u001b[32m0.02224\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 027 | loss: 0.02224 - R2: 0.9995 -- iter: 0320/1168\n",
      "Training Step: 1551  | total loss: \u001b[1m\u001b[32m0.02127\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 027 | loss: 0.02127 - R2: 1.0000 -- iter: 0340/1168\n",
      "Training Step: 1552  | total loss: \u001b[1m\u001b[32m0.02004\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 027 | loss: 0.02004 - R2: 1.0001 -- iter: 0360/1168\n",
      "Training Step: 1553  | total loss: \u001b[1m\u001b[32m0.02004\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 027 | loss: 0.02004 - R2: 1.0001 -- iter: 0380/1168\n",
      "Training Step: 1554  | total loss: \u001b[1m\u001b[32m0.01849\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 027 | loss: 0.01849 - R2: 1.0008 -- iter: 0400/1168\n",
      "Training Step: 1555  | total loss: \u001b[1m\u001b[32m0.01849\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 027 | loss: 0.01849 - R2: 1.0008 -- iter: 0420/1168\n",
      "Training Step: 1556  | total loss: \u001b[1m\u001b[32m0.01977\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 027 | loss: 0.01977 - R2: 1.0011 -- iter: 0440/1168\n",
      "Training Step: 1557  | total loss: \u001b[1m\u001b[32m0.02382\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 027 | loss: 0.02382 - R2: 1.0003 -- iter: 0460/1168\n",
      "Training Step: 1558  | total loss: \u001b[1m\u001b[32m0.02382\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 027 | loss: 0.02382 - R2: 1.0003 -- iter: 0480/1168\n",
      "Training Step: 1559  | total loss: \u001b[1m\u001b[32m0.02192\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 027 | loss: 0.02192 - R2: 1.0006 -- iter: 0500/1168\n",
      "Training Step: 1560  | total loss: \u001b[1m\u001b[32m0.02192\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 027 | loss: 0.02192 - R2: 1.0006 -- iter: 0520/1168\n",
      "Training Step: 1561  | total loss: \u001b[1m\u001b[32m0.02066\u001b[0m\u001b[0m | time: 0.172s\n",
      "| SGD | epoch: 027 | loss: 0.02066 - R2: 1.0000 -- iter: 0540/1168\n",
      "Training Step: 1562  | total loss: \u001b[1m\u001b[32m0.02066\u001b[0m\u001b[0m | time: 0.174s\n",
      "| SGD | epoch: 027 | loss: 0.02066 - R2: 1.0000 -- iter: 0560/1168\n",
      "Training Step: 1563  | total loss: \u001b[1m\u001b[32m0.01964\u001b[0m\u001b[0m | time: 0.176s\n",
      "| SGD | epoch: 027 | loss: 0.01964 - R2: 0.9986 -- iter: 0580/1168\n",
      "Training Step: 1564  | total loss: \u001b[1m\u001b[32m0.01964\u001b[0m\u001b[0m | time: 0.178s\n",
      "| SGD | epoch: 027 | loss: 0.01964 - R2: 0.9982 -- iter: 0600/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1565  | total loss: \u001b[1m\u001b[32m0.01793\u001b[0m\u001b[0m | time: 0.181s\n",
      "| SGD | epoch: 027 | loss: 0.01793 - R2: 0.9990 -- iter: 0620/1168\n",
      "Training Step: 1566  | total loss: \u001b[1m\u001b[32m0.01793\u001b[0m\u001b[0m | time: 0.183s\n",
      "| SGD | epoch: 027 | loss: 0.01793 - R2: 0.9987 -- iter: 0640/1168\n",
      "Training Step: 1567  | total loss: \u001b[1m\u001b[32m0.01637\u001b[0m\u001b[0m | time: 0.200s\n",
      "| SGD | epoch: 027 | loss: 0.01637 - R2: 0.9986 -- iter: 0660/1168\n",
      "Training Step: 1568  | total loss: \u001b[1m\u001b[32m0.01637\u001b[0m\u001b[0m | time: 0.202s\n",
      "| SGD | epoch: 027 | loss: 0.01637 - R2: 0.9986 -- iter: 0680/1168\n",
      "Training Step: 1569  | total loss: \u001b[1m\u001b[32m0.01624\u001b[0m\u001b[0m | time: 0.204s\n",
      "| SGD | epoch: 027 | loss: 0.01624 - R2: 0.9986 -- iter: 0700/1168\n",
      "Training Step: 1570  | total loss: \u001b[1m\u001b[32m0.01679\u001b[0m\u001b[0m | time: 0.206s\n",
      "| SGD | epoch: 027 | loss: 0.01679 - R2: 0.9986 -- iter: 0720/1168\n",
      "Training Step: 1571  | total loss: \u001b[1m\u001b[32m0.01923\u001b[0m\u001b[0m | time: 0.210s\n",
      "| SGD | epoch: 027 | loss: 0.01923 - R2: 0.9988 -- iter: 0740/1168\n",
      "Training Step: 1572  | total loss: \u001b[1m\u001b[32m0.01915\u001b[0m\u001b[0m | time: 0.213s\n",
      "| SGD | epoch: 027 | loss: 0.01915 - R2: 0.9989 -- iter: 0760/1168\n",
      "Training Step: 1573  | total loss: \u001b[1m\u001b[32m0.01878\u001b[0m\u001b[0m | time: 0.217s\n",
      "| SGD | epoch: 027 | loss: 0.01878 - R2: 0.9993 -- iter: 0780/1168\n",
      "Training Step: 1574  | total loss: \u001b[1m\u001b[32m0.01878\u001b[0m\u001b[0m | time: 0.220s\n",
      "| SGD | epoch: 027 | loss: 0.01878 - R2: 0.9996 -- iter: 0800/1168\n",
      "Training Step: 1575  | total loss: \u001b[1m\u001b[32m0.01804\u001b[0m\u001b[0m | time: 0.222s\n",
      "| SGD | epoch: 027 | loss: 0.01804 - R2: 0.9996 -- iter: 0820/1168\n",
      "Training Step: 1576  | total loss: \u001b[1m\u001b[32m0.01955\u001b[0m\u001b[0m | time: 0.225s\n",
      "| SGD | epoch: 027 | loss: 0.01955 - R2: 0.9994 -- iter: 0840/1168\n",
      "Training Step: 1577  | total loss: \u001b[1m\u001b[32m0.01823\u001b[0m\u001b[0m | time: 0.229s\n",
      "| SGD | epoch: 027 | loss: 0.01823 - R2: 1.0003 -- iter: 0860/1168\n",
      "Training Step: 1578  | total loss: \u001b[1m\u001b[32m0.01813\u001b[0m\u001b[0m | time: 0.233s\n",
      "| SGD | epoch: 027 | loss: 0.01813 - R2: 1.0004 -- iter: 0880/1168\n",
      "Training Step: 1579  | total loss: \u001b[1m\u001b[32m0.01813\u001b[0m\u001b[0m | time: 0.237s\n",
      "| SGD | epoch: 027 | loss: 0.01813 - R2: 1.0004 -- iter: 0900/1168\n",
      "Training Step: 1580  | total loss: \u001b[1m\u001b[32m0.01761\u001b[0m\u001b[0m | time: 0.239s\n",
      "| SGD | epoch: 027 | loss: 0.01761 - R2: 1.0005 -- iter: 0920/1168\n",
      "Training Step: 1581  | total loss: \u001b[1m\u001b[32m0.01760\u001b[0m\u001b[0m | time: 0.240s\n",
      "| SGD | epoch: 027 | loss: 0.01760 - R2: 0.9998 -- iter: 0940/1168\n",
      "Training Step: 1582  | total loss: \u001b[1m\u001b[32m0.01744\u001b[0m\u001b[0m | time: 0.243s\n",
      "| SGD | epoch: 027 | loss: 0.01744 - R2: 0.9996 -- iter: 0960/1168\n",
      "Training Step: 1583  | total loss: \u001b[1m\u001b[32m0.01669\u001b[0m\u001b[0m | time: 0.245s\n",
      "| SGD | epoch: 027 | loss: 0.01669 - R2: 0.9998 -- iter: 0980/1168\n",
      "Training Step: 1584  | total loss: \u001b[1m\u001b[32m0.01639\u001b[0m\u001b[0m | time: 0.247s\n",
      "| SGD | epoch: 027 | loss: 0.01639 - R2: 0.9998 -- iter: 1000/1168\n",
      "Training Step: 1585  | total loss: \u001b[1m\u001b[32m0.01694\u001b[0m\u001b[0m | time: 0.250s\n",
      "| SGD | epoch: 027 | loss: 0.01694 - R2: 0.9999 -- iter: 1020/1168\n",
      "Training Step: 1586  | total loss: \u001b[1m\u001b[32m0.01613\u001b[0m\u001b[0m | time: 0.253s\n",
      "| SGD | epoch: 027 | loss: 0.01613 - R2: 0.9999 -- iter: 1040/1168\n",
      "Training Step: 1587  | total loss: \u001b[1m\u001b[32m0.01754\u001b[0m\u001b[0m | time: 0.258s\n",
      "| SGD | epoch: 027 | loss: 0.01754 - R2: 1.0014 -- iter: 1060/1168\n",
      "Training Step: 1588  | total loss: \u001b[1m\u001b[32m0.01798\u001b[0m\u001b[0m | time: 0.262s\n",
      "| SGD | epoch: 027 | loss: 0.01798 - R2: 1.0002 -- iter: 1080/1168\n",
      "Training Step: 1589  | total loss: \u001b[1m\u001b[32m0.01798\u001b[0m\u001b[0m | time: 0.264s\n",
      "| SGD | epoch: 027 | loss: 0.01798 - R2: 1.0002 -- iter: 1100/1168\n",
      "Training Step: 1590  | total loss: \u001b[1m\u001b[32m0.01703\u001b[0m\u001b[0m | time: 0.266s\n",
      "| SGD | epoch: 027 | loss: 0.01703 - R2: 1.0000 -- iter: 1120/1168\n",
      "Training Step: 1591  | total loss: \u001b[1m\u001b[32m0.01703\u001b[0m\u001b[0m | time: 0.274s\n",
      "| SGD | epoch: 027 | loss: 0.01703 - R2: 0.9999 -- iter: 1140/1168\n",
      "Training Step: 1592  | total loss: \u001b[1m\u001b[32m0.01736\u001b[0m\u001b[0m | time: 0.277s\n",
      "| SGD | epoch: 027 | loss: 0.01736 - R2: 0.9999 -- iter: 1160/1168\n",
      "Training Step: 1593  | total loss: \u001b[1m\u001b[32m0.02851\u001b[0m\u001b[0m | time: 1.283s\n",
      "| SGD | epoch: 027 | loss: 0.02851 - R2: 1.0023 | val_loss: 0.03217 - val_acc: 0.9991 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 1594  | total loss: \u001b[1m\u001b[32m0.02851\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 028 | loss: 0.02851 - R2: 1.0023 -- iter: 0020/1168\n",
      "Training Step: 1595  | total loss: \u001b[1m\u001b[32m0.02803\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 028 | loss: 0.02803 - R2: 1.0017 -- iter: 0040/1168\n",
      "Training Step: 1596  | total loss: \u001b[1m\u001b[32m0.02656\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 028 | loss: 0.02656 - R2: 1.0014 -- iter: 0060/1168\n",
      "Training Step: 1597  | total loss: \u001b[1m\u001b[32m0.02538\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 028 | loss: 0.02538 - R2: 1.0001 -- iter: 0080/1168\n",
      "Training Step: 1598  | total loss: \u001b[1m\u001b[32m0.02386\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 028 | loss: 0.02386 - R2: 1.0001 -- iter: 0100/1168\n",
      "Training Step: 1599  | total loss: \u001b[1m\u001b[32m0.02204\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 028 | loss: 0.02204 - R2: 1.0001 -- iter: 0120/1168\n",
      "Training Step: 1600  | total loss: \u001b[1m\u001b[32m0.02131\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 028 | loss: 0.02131 - R2: 0.9991 -- iter: 0140/1168\n",
      "Training Step: 1601  | total loss: \u001b[1m\u001b[32m0.02060\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 028 | loss: 0.02060 - R2: 0.9996 -- iter: 0160/1168\n",
      "Training Step: 1602  | total loss: \u001b[1m\u001b[32m0.02060\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 028 | loss: 0.02060 - R2: 0.9996 -- iter: 0180/1168\n",
      "Training Step: 1603  | total loss: \u001b[1m\u001b[32m0.01820\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 028 | loss: 0.01820 - R2: 0.9992 -- iter: 0200/1168\n",
      "Training Step: 1604  | total loss: \u001b[1m\u001b[32m0.01682\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 028 | loss: 0.01682 - R2: 0.9994 -- iter: 0220/1168\n",
      "Training Step: 1605  | total loss: \u001b[1m\u001b[32m0.01682\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 028 | loss: 0.01682 - R2: 0.9994 -- iter: 0240/1168\n",
      "Training Step: 1606  | total loss: \u001b[1m\u001b[32m0.01770\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 028 | loss: 0.01770 - R2: 0.9990 -- iter: 0260/1168\n",
      "Training Step: 1607  | total loss: \u001b[1m\u001b[32m0.01701\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 028 | loss: 0.01701 - R2: 0.9994 -- iter: 0280/1168\n",
      "Training Step: 1608  | total loss: \u001b[1m\u001b[32m0.01635\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 028 | loss: 0.01635 - R2: 0.9993 -- iter: 0300/1168\n",
      "Training Step: 1609  | total loss: \u001b[1m\u001b[32m0.01635\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 028 | loss: 0.01635 - R2: 0.9993 -- iter: 0320/1168\n",
      "Training Step: 1610  | total loss: \u001b[1m\u001b[32m0.01631\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 028 | loss: 0.01631 - R2: 0.9993 -- iter: 0340/1168\n",
      "Training Step: 1611  | total loss: \u001b[1m\u001b[32m0.01606\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 028 | loss: 0.01606 - R2: 0.9993 -- iter: 0360/1168\n",
      "Training Step: 1612  | total loss: \u001b[1m\u001b[32m0.01533\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 028 | loss: 0.01533 - R2: 0.9996 -- iter: 0380/1168\n",
      "Training Step: 1613  | total loss: \u001b[1m\u001b[32m0.01612\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 028 | loss: 0.01612 - R2: 0.9996 -- iter: 0400/1168\n",
      "Training Step: 1614  | total loss: \u001b[1m\u001b[32m0.01583\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 028 | loss: 0.01583 - R2: 0.9996 -- iter: 0420/1168\n",
      "Training Step: 1615  | total loss: \u001b[1m\u001b[32m0.01602\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 028 | loss: 0.01602 - R2: 1.0004 -- iter: 0440/1168\n",
      "Training Step: 1616  | total loss: \u001b[1m\u001b[32m0.01537\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 028 | loss: 0.01537 - R2: 1.0003 -- iter: 0460/1168\n",
      "Training Step: 1617  | total loss: \u001b[1m\u001b[32m0.01375\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 028 | loss: 0.01375 - R2: 0.9999 -- iter: 0480/1168\n",
      "Training Step: 1618  | total loss: \u001b[1m\u001b[32m0.01359\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 028 | loss: 0.01359 - R2: 0.9996 -- iter: 0500/1168\n",
      "Training Step: 1619  | total loss: \u001b[1m\u001b[32m0.01359\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 028 | loss: 0.01359 - R2: 1.0010 -- iter: 0520/1168\n",
      "Training Step: 1620  | total loss: \u001b[1m\u001b[32m0.02173\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 028 | loss: 0.02173 - R2: 1.0021 -- iter: 0540/1168\n",
      "Training Step: 1621  | total loss: \u001b[1m\u001b[32m0.02840\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 028 | loss: 0.02840 - R2: 1.0021 -- iter: 0560/1168\n",
      "Training Step: 1622  | total loss: \u001b[1m\u001b[32m0.02684\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 028 | loss: 0.02684 - R2: 1.0010 -- iter: 0580/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1623  | total loss: \u001b[1m\u001b[32m0.02605\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 028 | loss: 0.02605 - R2: 1.0023 -- iter: 0600/1168\n",
      "Training Step: 1624  | total loss: \u001b[1m\u001b[32m0.02437\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 028 | loss: 0.02437 - R2: 1.0026 -- iter: 0620/1168\n",
      "Training Step: 1625  | total loss: \u001b[1m\u001b[32m0.02580\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 028 | loss: 0.02580 - R2: 1.0025 -- iter: 0640/1168\n",
      "Training Step: 1626  | total loss: \u001b[1m\u001b[32m0.02569\u001b[0m\u001b[0m | time: 0.174s\n",
      "| SGD | epoch: 028 | loss: 0.02569 - R2: 1.0025 -- iter: 0660/1168\n",
      "Training Step: 1627  | total loss: \u001b[1m\u001b[32m0.02442\u001b[0m\u001b[0m | time: 0.176s\n",
      "| SGD | epoch: 028 | loss: 0.02442 - R2: 1.0016 -- iter: 0680/1168\n",
      "Training Step: 1628  | total loss: \u001b[1m\u001b[32m0.02325\u001b[0m\u001b[0m | time: 0.178s\n",
      "| SGD | epoch: 028 | loss: 0.02325 - R2: 1.0016 -- iter: 0700/1168\n",
      "Training Step: 1629  | total loss: \u001b[1m\u001b[32m0.02206\u001b[0m\u001b[0m | time: 0.180s\n",
      "| SGD | epoch: 028 | loss: 0.02206 - R2: 1.0006 -- iter: 0720/1168\n",
      "Training Step: 1630  | total loss: \u001b[1m\u001b[32m0.02176\u001b[0m\u001b[0m | time: 0.182s\n",
      "| SGD | epoch: 028 | loss: 0.02176 - R2: 1.0005 -- iter: 0740/1168\n",
      "Training Step: 1631  | total loss: \u001b[1m\u001b[32m0.02255\u001b[0m\u001b[0m | time: 0.185s\n",
      "| SGD | epoch: 028 | loss: 0.02255 - R2: 0.9987 -- iter: 0760/1168\n",
      "Training Step: 1632  | total loss: \u001b[1m\u001b[32m0.02177\u001b[0m\u001b[0m | time: 0.188s\n",
      "| SGD | epoch: 028 | loss: 0.02177 - R2: 0.9987 -- iter: 0780/1168\n",
      "Training Step: 1633  | total loss: \u001b[1m\u001b[32m0.02449\u001b[0m\u001b[0m | time: 0.193s\n",
      "| SGD | epoch: 028 | loss: 0.02449 - R2: 0.9993 -- iter: 0800/1168\n",
      "Training Step: 1634  | total loss: \u001b[1m\u001b[32m0.02449\u001b[0m\u001b[0m | time: 0.196s\n",
      "| SGD | epoch: 028 | loss: 0.02449 - R2: 0.9998 -- iter: 0820/1168\n",
      "Training Step: 1635  | total loss: \u001b[1m\u001b[32m0.02452\u001b[0m\u001b[0m | time: 0.200s\n",
      "| SGD | epoch: 028 | loss: 0.02452 - R2: 0.9996 -- iter: 0840/1168\n",
      "Training Step: 1636  | total loss: \u001b[1m\u001b[32m0.02452\u001b[0m\u001b[0m | time: 0.209s\n",
      "| SGD | epoch: 028 | loss: 0.02452 - R2: 0.9991 -- iter: 0860/1168\n",
      "Training Step: 1637  | total loss: \u001b[1m\u001b[32m0.02323\u001b[0m\u001b[0m | time: 0.212s\n",
      "| SGD | epoch: 028 | loss: 0.02323 - R2: 0.9991 -- iter: 0880/1168\n",
      "Training Step: 1638  | total loss: \u001b[1m\u001b[32m0.02160\u001b[0m\u001b[0m | time: 0.214s\n",
      "| SGD | epoch: 028 | loss: 0.02160 - R2: 0.9989 -- iter: 0900/1168\n",
      "Training Step: 1639  | total loss: \u001b[1m\u001b[32m0.02022\u001b[0m\u001b[0m | time: 0.218s\n",
      "| SGD | epoch: 028 | loss: 0.02022 - R2: 0.9991 -- iter: 0920/1168\n",
      "Training Step: 1640  | total loss: \u001b[1m\u001b[32m0.01944\u001b[0m\u001b[0m | time: 0.223s\n",
      "| SGD | epoch: 028 | loss: 0.01944 - R2: 0.9994 -- iter: 0940/1168\n",
      "Training Step: 1641  | total loss: \u001b[1m\u001b[32m0.01871\u001b[0m\u001b[0m | time: 0.227s\n",
      "| SGD | epoch: 028 | loss: 0.01871 - R2: 0.9997 -- iter: 0960/1168\n",
      "Training Step: 1642  | total loss: \u001b[1m\u001b[32m0.01900\u001b[0m\u001b[0m | time: 0.231s\n",
      "| SGD | epoch: 028 | loss: 0.01900 - R2: 0.9990 -- iter: 0980/1168\n",
      "Training Step: 1643  | total loss: \u001b[1m\u001b[32m0.01893\u001b[0m\u001b[0m | time: 0.235s\n",
      "| SGD | epoch: 028 | loss: 0.01893 - R2: 1.0005 -- iter: 1000/1168\n",
      "Training Step: 1644  | total loss: \u001b[1m\u001b[32m0.01893\u001b[0m\u001b[0m | time: 0.238s\n",
      "| SGD | epoch: 028 | loss: 0.01893 - R2: 1.0005 -- iter: 1020/1168\n",
      "Training Step: 1645  | total loss: \u001b[1m\u001b[32m0.02059\u001b[0m\u001b[0m | time: 0.241s\n",
      "| SGD | epoch: 028 | loss: 0.02059 - R2: 1.0006 -- iter: 1040/1168\n",
      "Training Step: 1646  | total loss: \u001b[1m\u001b[32m0.01914\u001b[0m\u001b[0m | time: 0.245s\n",
      "| SGD | epoch: 028 | loss: 0.01914 - R2: 0.9998 -- iter: 1060/1168\n",
      "Training Step: 1647  | total loss: \u001b[1m\u001b[32m0.01914\u001b[0m\u001b[0m | time: 0.250s\n",
      "| SGD | epoch: 028 | loss: 0.01914 - R2: 0.9998 -- iter: 1080/1168\n",
      "Training Step: 1648  | total loss: \u001b[1m\u001b[32m0.01821\u001b[0m\u001b[0m | time: 0.256s\n",
      "| SGD | epoch: 028 | loss: 0.01821 - R2: 0.9999 -- iter: 1100/1168\n",
      "Training Step: 1649  | total loss: \u001b[1m\u001b[32m0.01582\u001b[0m\u001b[0m | time: 0.260s\n",
      "| SGD | epoch: 028 | loss: 0.01582 - R2: 0.9999 -- iter: 1120/1168\n",
      "Training Step: 1650  | total loss: \u001b[1m\u001b[32m0.01534\u001b[0m\u001b[0m | time: 0.263s\n",
      "| SGD | epoch: 028 | loss: 0.01534 - R2: 0.9993 -- iter: 1140/1168\n",
      "Training Step: 1651  | total loss: \u001b[1m\u001b[32m0.01501\u001b[0m\u001b[0m | time: 0.267s\n",
      "| SGD | epoch: 028 | loss: 0.01501 - R2: 0.9992 -- iter: 1160/1168\n",
      "Training Step: 1652  | total loss: \u001b[1m\u001b[32m0.01484\u001b[0m\u001b[0m | time: 1.278s\n",
      "| SGD | epoch: 028 | loss: 0.01484 - R2: 0.9990 | val_loss: 0.03277 - val_acc: 1.0013 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 1653  | total loss: \u001b[1m\u001b[32m0.01484\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 029 | loss: 0.01484 - R2: 0.9990 -- iter: 0020/1168\n",
      "Training Step: 1654  | total loss: \u001b[1m\u001b[32m0.01650\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 029 | loss: 0.01650 - R2: 0.9981 -- iter: 0040/1168\n",
      "Training Step: 1655  | total loss: \u001b[1m\u001b[32m0.01908\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 029 | loss: 0.01908 - R2: 0.9980 -- iter: 0060/1168\n",
      "Training Step: 1656  | total loss: \u001b[1m\u001b[32m0.01810\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 029 | loss: 0.01810 - R2: 0.9985 -- iter: 0080/1168\n",
      "Training Step: 1657  | total loss: \u001b[1m\u001b[32m0.01729\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 029 | loss: 0.01729 - R2: 0.9986 -- iter: 0100/1168\n",
      "Training Step: 1658  | total loss: \u001b[1m\u001b[32m0.01633\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 029 | loss: 0.01633 - R2: 0.9986 -- iter: 0120/1168\n",
      "Training Step: 1659  | total loss: \u001b[1m\u001b[32m0.01606\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 029 | loss: 0.01606 - R2: 1.0002 -- iter: 0140/1168\n",
      "Training Step: 1660  | total loss: \u001b[1m\u001b[32m0.02200\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 029 | loss: 0.02200 - R2: 0.9999 -- iter: 0160/1168\n",
      "Training Step: 1661  | total loss: \u001b[1m\u001b[32m0.02200\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 029 | loss: 0.02200 - R2: 0.9999 -- iter: 0180/1168\n",
      "Training Step: 1662  | total loss: \u001b[1m\u001b[32m0.02203\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 029 | loss: 0.02203 - R2: 1.0006 -- iter: 0200/1168\n",
      "Training Step: 1663  | total loss: \u001b[1m\u001b[32m0.02203\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 029 | loss: 0.02203 - R2: 1.0006 -- iter: 0220/1168\n",
      "Training Step: 1664  | total loss: \u001b[1m\u001b[32m0.02140\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 029 | loss: 0.02140 - R2: 1.0001 -- iter: 0240/1168\n",
      "Training Step: 1665  | total loss: \u001b[1m\u001b[32m0.01960\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 029 | loss: 0.01960 - R2: 1.0002 -- iter: 0260/1168\n",
      "Training Step: 1666  | total loss: \u001b[1m\u001b[32m0.01976\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 029 | loss: 0.01976 - R2: 1.0001 -- iter: 0280/1168\n",
      "Training Step: 1667  | total loss: \u001b[1m\u001b[32m0.01976\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 029 | loss: 0.01976 - R2: 1.0001 -- iter: 0300/1168\n",
      "Training Step: 1668  | total loss: \u001b[1m\u001b[32m0.01865\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 029 | loss: 0.01865 - R2: 1.0009 -- iter: 0320/1168\n",
      "Training Step: 1669  | total loss: \u001b[1m\u001b[32m0.01865\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 029 | loss: 0.01865 - R2: 1.0012 -- iter: 0340/1168\n",
      "Training Step: 1670  | total loss: \u001b[1m\u001b[32m0.01851\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 029 | loss: 0.01851 - R2: 1.0013 -- iter: 0360/1168\n",
      "Training Step: 1671  | total loss: \u001b[1m\u001b[32m0.01796\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 029 | loss: 0.01796 - R2: 1.0009 -- iter: 0380/1168\n",
      "Training Step: 1672  | total loss: \u001b[1m\u001b[32m0.01741\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 029 | loss: 0.01741 - R2: 1.0007 -- iter: 0400/1168\n",
      "Training Step: 1673  | total loss: \u001b[1m\u001b[32m0.01790\u001b[0m\u001b[0m | time: 0.179s\n",
      "| SGD | epoch: 029 | loss: 0.01790 - R2: 1.0003 -- iter: 0420/1168\n",
      "Training Step: 1674  | total loss: \u001b[1m\u001b[32m0.01790\u001b[0m\u001b[0m | time: 0.181s\n",
      "| SGD | epoch: 029 | loss: 0.01790 - R2: 1.0003 -- iter: 0440/1168\n",
      "Training Step: 1675  | total loss: \u001b[1m\u001b[32m0.01672\u001b[0m\u001b[0m | time: 0.193s\n",
      "| SGD | epoch: 029 | loss: 0.01672 - R2: 1.0007 -- iter: 0460/1168\n",
      "Training Step: 1676  | total loss: \u001b[1m\u001b[32m0.01759\u001b[0m\u001b[0m | time: 0.194s\n",
      "| SGD | epoch: 029 | loss: 0.01759 - R2: 1.0004 -- iter: 0480/1168\n",
      "Training Step: 1677  | total loss: \u001b[1m\u001b[32m0.02299\u001b[0m\u001b[0m | time: 0.197s\n",
      "| SGD | epoch: 029 | loss: 0.02299 - R2: 0.9999 -- iter: 0500/1168\n",
      "Training Step: 1678  | total loss: \u001b[1m\u001b[32m0.02191\u001b[0m\u001b[0m | time: 0.199s\n",
      "| SGD | epoch: 029 | loss: 0.02191 - R2: 0.9995 -- iter: 0520/1168\n",
      "Training Step: 1679  | total loss: \u001b[1m\u001b[32m0.02191\u001b[0m\u001b[0m | time: 0.219s\n",
      "| SGD | epoch: 029 | loss: 0.02191 - R2: 0.9995 -- iter: 0540/1168\n",
      "Training Step: 1680  | total loss: \u001b[1m\u001b[32m0.01866\u001b[0m\u001b[0m | time: 0.225s\n",
      "| SGD | epoch: 029 | loss: 0.01866 - R2: 1.0007 -- iter: 0560/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1681  | total loss: \u001b[1m\u001b[32m0.01802\u001b[0m\u001b[0m | time: 0.230s\n",
      "| SGD | epoch: 029 | loss: 0.01802 - R2: 1.0001 -- iter: 0580/1168\n",
      "Training Step: 1682  | total loss: \u001b[1m\u001b[32m0.01767\u001b[0m\u001b[0m | time: 0.234s\n",
      "| SGD | epoch: 029 | loss: 0.01767 - R2: 1.0000 -- iter: 0600/1168\n",
      "Training Step: 1683  | total loss: \u001b[1m\u001b[32m0.01767\u001b[0m\u001b[0m | time: 0.237s\n",
      "| SGD | epoch: 029 | loss: 0.01767 - R2: 1.0000 -- iter: 0620/1168\n",
      "Training Step: 1684  | total loss: \u001b[1m\u001b[32m0.01755\u001b[0m\u001b[0m | time: 0.239s\n",
      "| SGD | epoch: 029 | loss: 0.01755 - R2: 0.9994 -- iter: 0640/1168\n",
      "Training Step: 1685  | total loss: \u001b[1m\u001b[32m0.01699\u001b[0m\u001b[0m | time: 0.245s\n",
      "| SGD | epoch: 029 | loss: 0.01699 - R2: 0.9996 -- iter: 0660/1168\n",
      "Training Step: 1686  | total loss: \u001b[1m\u001b[32m0.01610\u001b[0m\u001b[0m | time: 0.250s\n",
      "| SGD | epoch: 029 | loss: 0.01610 - R2: 1.0001 -- iter: 0680/1168\n",
      "Training Step: 1687  | total loss: \u001b[1m\u001b[32m0.01610\u001b[0m\u001b[0m | time: 0.258s\n",
      "| SGD | epoch: 029 | loss: 0.01610 - R2: 1.0001 -- iter: 0700/1168\n",
      "Training Step: 1688  | total loss: \u001b[1m\u001b[32m0.01645\u001b[0m\u001b[0m | time: 0.260s\n",
      "| SGD | epoch: 029 | loss: 0.01645 - R2: 0.9999 -- iter: 0720/1168\n",
      "Training Step: 1689  | total loss: \u001b[1m\u001b[32m0.01586\u001b[0m\u001b[0m | time: 0.263s\n",
      "| SGD | epoch: 029 | loss: 0.01586 - R2: 0.9999 -- iter: 0740/1168\n",
      "Training Step: 1690  | total loss: \u001b[1m\u001b[32m0.01555\u001b[0m\u001b[0m | time: 0.264s\n",
      "| SGD | epoch: 029 | loss: 0.01555 - R2: 0.9996 -- iter: 0760/1168\n",
      "Training Step: 1691  | total loss: \u001b[1m\u001b[32m0.01867\u001b[0m\u001b[0m | time: 0.267s\n",
      "| SGD | epoch: 029 | loss: 0.01867 - R2: 0.9994 -- iter: 0780/1168\n",
      "Training Step: 1692  | total loss: \u001b[1m\u001b[32m0.01885\u001b[0m\u001b[0m | time: 0.269s\n",
      "| SGD | epoch: 029 | loss: 0.01885 - R2: 0.9994 -- iter: 0800/1168\n",
      "Training Step: 1693  | total loss: \u001b[1m\u001b[32m0.01908\u001b[0m\u001b[0m | time: 0.271s\n",
      "| SGD | epoch: 029 | loss: 0.01908 - R2: 0.9992 -- iter: 0820/1168\n",
      "Training Step: 1694  | total loss: \u001b[1m\u001b[32m0.01768\u001b[0m\u001b[0m | time: 0.284s\n",
      "| SGD | epoch: 029 | loss: 0.01768 - R2: 0.9995 -- iter: 0840/1168\n",
      "Training Step: 1695  | total loss: \u001b[1m\u001b[32m0.01707\u001b[0m\u001b[0m | time: 0.290s\n",
      "| SGD | epoch: 029 | loss: 0.01707 - R2: 1.0000 -- iter: 0860/1168\n",
      "Training Step: 1696  | total loss: \u001b[1m\u001b[32m0.01771\u001b[0m\u001b[0m | time: 0.294s\n",
      "| SGD | epoch: 029 | loss: 0.01771 - R2: 0.9999 -- iter: 0880/1168\n",
      "Training Step: 1697  | total loss: \u001b[1m\u001b[32m0.01771\u001b[0m\u001b[0m | time: 0.307s\n",
      "| SGD | epoch: 029 | loss: 0.01771 - R2: 0.9999 -- iter: 0900/1168\n",
      "Training Step: 1698  | total loss: \u001b[1m\u001b[32m0.01977\u001b[0m\u001b[0m | time: 0.309s\n",
      "| SGD | epoch: 029 | loss: 0.01977 - R2: 0.9998 -- iter: 0920/1168\n",
      "Training Step: 1699  | total loss: \u001b[1m\u001b[32m0.01781\u001b[0m\u001b[0m | time: 0.312s\n",
      "| SGD | epoch: 029 | loss: 0.01781 - R2: 0.9991 -- iter: 0940/1168\n",
      "Training Step: 1700  | total loss: \u001b[1m\u001b[32m0.01781\u001b[0m\u001b[0m | time: 0.315s\n",
      "| SGD | epoch: 029 | loss: 0.01781 - R2: 0.9991 -- iter: 0960/1168\n",
      "Training Step: 1701  | total loss: \u001b[1m\u001b[32m0.01804\u001b[0m\u001b[0m | time: 0.317s\n",
      "| SGD | epoch: 029 | loss: 0.01804 - R2: 0.9996 -- iter: 0980/1168\n",
      "Training Step: 1702  | total loss: \u001b[1m\u001b[32m0.01710\u001b[0m\u001b[0m | time: 0.321s\n",
      "| SGD | epoch: 029 | loss: 0.01710 - R2: 0.9997 -- iter: 1000/1168\n",
      "Training Step: 1703  | total loss: \u001b[1m\u001b[32m0.01684\u001b[0m\u001b[0m | time: 0.326s\n",
      "| SGD | epoch: 029 | loss: 0.01684 - R2: 0.9995 -- iter: 1020/1168\n",
      "Training Step: 1704  | total loss: \u001b[1m\u001b[32m0.01684\u001b[0m\u001b[0m | time: 0.328s\n",
      "| SGD | epoch: 029 | loss: 0.01684 - R2: 0.9995 -- iter: 1040/1168\n",
      "Training Step: 1705  | total loss: \u001b[1m\u001b[32m0.01720\u001b[0m\u001b[0m | time: 0.332s\n",
      "| SGD | epoch: 029 | loss: 0.01720 - R2: 1.0000 -- iter: 1060/1168\n",
      "Training Step: 1706  | total loss: \u001b[1m\u001b[32m0.01664\u001b[0m\u001b[0m | time: 0.335s\n",
      "| SGD | epoch: 029 | loss: 0.01664 - R2: 1.0003 -- iter: 1080/1168\n",
      "Training Step: 1707  | total loss: \u001b[1m\u001b[32m0.01920\u001b[0m\u001b[0m | time: 0.338s\n",
      "| SGD | epoch: 029 | loss: 0.01920 - R2: 1.0006 -- iter: 1100/1168\n",
      "Training Step: 1708  | total loss: \u001b[1m\u001b[32m0.01920\u001b[0m\u001b[0m | time: 0.349s\n",
      "| SGD | epoch: 029 | loss: 0.01920 - R2: 1.0006 -- iter: 1120/1168\n",
      "Training Step: 1709  | total loss: \u001b[1m\u001b[32m0.01910\u001b[0m\u001b[0m | time: 0.353s\n",
      "| SGD | epoch: 029 | loss: 0.01910 - R2: 1.0006 -- iter: 1140/1168\n",
      "Training Step: 1710  | total loss: \u001b[1m\u001b[32m0.01882\u001b[0m\u001b[0m | time: 0.358s\n",
      "| SGD | epoch: 029 | loss: 0.01882 - R2: 0.9995 -- iter: 1160/1168\n",
      "Training Step: 1711  | total loss: \u001b[1m\u001b[32m0.01773\u001b[0m\u001b[0m | time: 1.371s\n",
      "| SGD | epoch: 029 | loss: 0.01773 - R2: 0.9993 | val_loss: 0.03262 - val_acc: 1.0011 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 1712  | total loss: \u001b[1m\u001b[32m0.01671\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 030 | loss: 0.01671 - R2: 0.9996 -- iter: 0020/1168\n",
      "Training Step: 1713  | total loss: \u001b[1m\u001b[32m0.01671\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 030 | loss: 0.01671 - R2: 0.9996 -- iter: 0040/1168\n",
      "Training Step: 1714  | total loss: \u001b[1m\u001b[32m0.01678\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 030 | loss: 0.01678 - R2: 0.9995 -- iter: 0060/1168\n",
      "Training Step: 1715  | total loss: \u001b[1m\u001b[32m0.01995\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 030 | loss: 0.01995 - R2: 1.0001 -- iter: 0080/1168\n",
      "Training Step: 1716  | total loss: \u001b[1m\u001b[32m0.02067\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 030 | loss: 0.02067 - R2: 1.0000 -- iter: 0100/1168\n",
      "Training Step: 1717  | total loss: \u001b[1m\u001b[32m0.01978\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 030 | loss: 0.01978 - R2: 0.9982 -- iter: 0120/1168\n",
      "Training Step: 1718  | total loss: \u001b[1m\u001b[32m0.01978\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 030 | loss: 0.01978 - R2: 0.9982 -- iter: 0140/1168\n",
      "Training Step: 1719  | total loss: \u001b[1m\u001b[32m0.01943\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 030 | loss: 0.01943 - R2: 0.9989 -- iter: 0160/1168\n",
      "Training Step: 1720  | total loss: \u001b[1m\u001b[32m0.01916\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 030 | loss: 0.01916 - R2: 0.9993 -- iter: 0180/1168\n",
      "Training Step: 1721  | total loss: \u001b[1m\u001b[32m0.01994\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 030 | loss: 0.01994 - R2: 0.9984 -- iter: 0200/1168\n",
      "Training Step: 1722  | total loss: \u001b[1m\u001b[32m0.01862\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 030 | loss: 0.01862 - R2: 0.9990 -- iter: 0220/1168\n",
      "Training Step: 1723  | total loss: \u001b[1m\u001b[32m0.01862\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 030 | loss: 0.01862 - R2: 0.9990 -- iter: 0240/1168\n",
      "Training Step: 1724  | total loss: \u001b[1m\u001b[32m0.01810\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 030 | loss: 0.01810 - R2: 0.9993 -- iter: 0260/1168\n",
      "Training Step: 1725  | total loss: \u001b[1m\u001b[32m0.01722\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 030 | loss: 0.01722 - R2: 0.9996 -- iter: 0280/1168\n",
      "Training Step: 1726  | total loss: \u001b[1m\u001b[32m0.01819\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 030 | loss: 0.01819 - R2: 1.0000 -- iter: 0300/1168\n",
      "Training Step: 1727  | total loss: \u001b[1m\u001b[32m0.01819\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 030 | loss: 0.01819 - R2: 1.0000 -- iter: 0320/1168\n",
      "Training Step: 1728  | total loss: \u001b[1m\u001b[32m0.01790\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 030 | loss: 0.01790 - R2: 1.0003 -- iter: 0340/1168\n",
      "Training Step: 1729  | total loss: \u001b[1m\u001b[32m0.01784\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 030 | loss: 0.01784 - R2: 1.0000 -- iter: 0360/1168\n",
      "Training Step: 1730  | total loss: \u001b[1m\u001b[32m0.01909\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 030 | loss: 0.01909 - R2: 0.9999 -- iter: 0380/1168\n",
      "Training Step: 1731  | total loss: \u001b[1m\u001b[32m0.02105\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 030 | loss: 0.02105 - R2: 0.9995 -- iter: 0400/1168\n",
      "Training Step: 1732  | total loss: \u001b[1m\u001b[32m0.02027\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 030 | loss: 0.02027 - R2: 1.0007 -- iter: 0420/1168\n",
      "Training Step: 1733  | total loss: \u001b[1m\u001b[32m0.01983\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 030 | loss: 0.01983 - R2: 1.0007 -- iter: 0440/1168\n",
      "Training Step: 1734  | total loss: \u001b[1m\u001b[32m0.01915\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 030 | loss: 0.01915 - R2: 1.0001 -- iter: 0460/1168\n",
      "Training Step: 1735  | total loss: \u001b[1m\u001b[32m0.01823\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 030 | loss: 0.01823 - R2: 1.0008 -- iter: 0480/1168\n",
      "Training Step: 1736  | total loss: \u001b[1m\u001b[32m0.01781\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 030 | loss: 0.01781 - R2: 1.0003 -- iter: 0500/1168\n",
      "Training Step: 1737  | total loss: \u001b[1m\u001b[32m0.02081\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 030 | loss: 0.02081 - R2: 1.0011 -- iter: 0520/1168\n",
      "Training Step: 1738  | total loss: \u001b[1m\u001b[32m0.01995\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 030 | loss: 0.01995 - R2: 1.0012 -- iter: 0540/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1739  | total loss: \u001b[1m\u001b[32m0.01920\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 030 | loss: 0.01920 - R2: 1.0003 -- iter: 0560/1168\n",
      "Training Step: 1740  | total loss: \u001b[1m\u001b[32m0.02004\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 030 | loss: 0.02004 - R2: 0.9991 -- iter: 0580/1168\n",
      "Training Step: 1741  | total loss: \u001b[1m\u001b[32m0.01963\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 030 | loss: 0.01963 - R2: 0.9984 -- iter: 0600/1168\n",
      "Training Step: 1742  | total loss: \u001b[1m\u001b[32m0.01963\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 030 | loss: 0.01963 - R2: 0.9984 -- iter: 0620/1168\n",
      "Training Step: 1743  | total loss: \u001b[1m\u001b[32m0.02014\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 030 | loss: 0.02014 - R2: 0.9993 -- iter: 0640/1168\n",
      "Training Step: 1744  | total loss: \u001b[1m\u001b[32m0.02069\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 030 | loss: 0.02069 - R2: 1.0002 -- iter: 0660/1168\n",
      "Training Step: 1745  | total loss: \u001b[1m\u001b[32m0.01899\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 030 | loss: 0.01899 - R2: 1.0002 -- iter: 0680/1168\n",
      "Training Step: 1746  | total loss: \u001b[1m\u001b[32m0.01811\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 030 | loss: 0.01811 - R2: 0.9999 -- iter: 0700/1168\n",
      "Training Step: 1747  | total loss: \u001b[1m\u001b[32m0.01811\u001b[0m\u001b[0m | time: 0.178s\n",
      "| SGD | epoch: 030 | loss: 0.01811 - R2: 0.9999 -- iter: 0720/1168\n",
      "Training Step: 1748  | total loss: \u001b[1m\u001b[32m0.01713\u001b[0m\u001b[0m | time: 0.182s\n",
      "| SGD | epoch: 030 | loss: 0.01713 - R2: 0.9994 -- iter: 0740/1168\n",
      "Training Step: 1749  | total loss: \u001b[1m\u001b[32m0.01618\u001b[0m\u001b[0m | time: 0.184s\n",
      "| SGD | epoch: 030 | loss: 0.01618 - R2: 0.9991 -- iter: 0760/1168\n",
      "Training Step: 1750  | total loss: \u001b[1m\u001b[32m0.01629\u001b[0m\u001b[0m | time: 0.186s\n",
      "| SGD | epoch: 030 | loss: 0.01629 - R2: 0.9992 -- iter: 0780/1168\n",
      "Training Step: 1751  | total loss: \u001b[1m\u001b[32m0.02234\u001b[0m\u001b[0m | time: 0.187s\n",
      "| SGD | epoch: 030 | loss: 0.02234 - R2: 1.0002 -- iter: 0800/1168\n",
      "Training Step: 1752  | total loss: \u001b[1m\u001b[32m0.02149\u001b[0m\u001b[0m | time: 0.189s\n",
      "| SGD | epoch: 030 | loss: 0.02149 - R2: 1.0008 -- iter: 0820/1168\n",
      "Training Step: 1753  | total loss: \u001b[1m\u001b[32m0.02144\u001b[0m\u001b[0m | time: 0.191s\n",
      "| SGD | epoch: 030 | loss: 0.02144 - R2: 1.0018 -- iter: 0840/1168\n",
      "Training Step: 1754  | total loss: \u001b[1m\u001b[32m0.02163\u001b[0m\u001b[0m | time: 0.198s\n",
      "| SGD | epoch: 030 | loss: 0.02163 - R2: 1.0018 -- iter: 0860/1168\n",
      "Training Step: 1755  | total loss: \u001b[1m\u001b[32m0.02347\u001b[0m\u001b[0m | time: 0.200s\n",
      "| SGD | epoch: 030 | loss: 0.02347 - R2: 1.0017 -- iter: 0880/1168\n",
      "Training Step: 1756  | total loss: \u001b[1m\u001b[32m0.02201\u001b[0m\u001b[0m | time: 0.202s\n",
      "| SGD | epoch: 030 | loss: 0.02201 - R2: 1.0006 -- iter: 0900/1168\n",
      "Training Step: 1757  | total loss: \u001b[1m\u001b[32m0.02201\u001b[0m\u001b[0m | time: 0.204s\n",
      "| SGD | epoch: 030 | loss: 0.02201 - R2: 1.0006 -- iter: 0920/1168\n",
      "Training Step: 1758  | total loss: \u001b[1m\u001b[32m0.02099\u001b[0m\u001b[0m | time: 0.206s\n",
      "| SGD | epoch: 030 | loss: 0.02099 - R2: 1.0012 -- iter: 0940/1168\n",
      "Training Step: 1759  | total loss: \u001b[1m\u001b[32m0.02118\u001b[0m\u001b[0m | time: 0.207s\n",
      "| SGD | epoch: 030 | loss: 0.02118 - R2: 1.0014 -- iter: 0960/1168\n",
      "Training Step: 1760  | total loss: \u001b[1m\u001b[32m0.02104\u001b[0m\u001b[0m | time: 0.210s\n",
      "| SGD | epoch: 030 | loss: 0.02104 - R2: 1.0007 -- iter: 0980/1168\n",
      "Training Step: 1761  | total loss: \u001b[1m\u001b[32m0.01877\u001b[0m\u001b[0m | time: 0.214s\n",
      "| SGD | epoch: 030 | loss: 0.01877 - R2: 1.0003 -- iter: 1000/1168\n",
      "Training Step: 1762  | total loss: \u001b[1m\u001b[32m0.01877\u001b[0m\u001b[0m | time: 0.216s\n",
      "| SGD | epoch: 030 | loss: 0.01877 - R2: 1.0003 -- iter: 1020/1168\n",
      "Training Step: 1763  | total loss: \u001b[1m\u001b[32m0.01765\u001b[0m\u001b[0m | time: 0.219s\n",
      "| SGD | epoch: 030 | loss: 0.01765 - R2: 1.0000 -- iter: 1040/1168\n",
      "Training Step: 1764  | total loss: \u001b[1m\u001b[32m0.01763\u001b[0m\u001b[0m | time: 0.223s\n",
      "| SGD | epoch: 030 | loss: 0.01763 - R2: 0.9995 -- iter: 1060/1168\n",
      "Training Step: 1765  | total loss: \u001b[1m\u001b[32m0.01703\u001b[0m\u001b[0m | time: 0.225s\n",
      "| SGD | epoch: 030 | loss: 0.01703 - R2: 0.9998 -- iter: 1080/1168\n",
      "Training Step: 1766  | total loss: \u001b[1m\u001b[32m0.01682\u001b[0m\u001b[0m | time: 0.234s\n",
      "| SGD | epoch: 030 | loss: 0.01682 - R2: 0.9993 -- iter: 1100/1168\n",
      "Training Step: 1767  | total loss: \u001b[1m\u001b[32m0.01666\u001b[0m\u001b[0m | time: 0.236s\n",
      "| SGD | epoch: 030 | loss: 0.01666 - R2: 0.9990 -- iter: 1120/1168\n",
      "Training Step: 1768  | total loss: \u001b[1m\u001b[32m0.01693\u001b[0m\u001b[0m | time: 0.242s\n",
      "| SGD | epoch: 030 | loss: 0.01693 - R2: 0.9984 -- iter: 1140/1168\n",
      "Training Step: 1769  | total loss: \u001b[1m\u001b[32m0.01744\u001b[0m\u001b[0m | time: 0.245s\n",
      "| SGD | epoch: 030 | loss: 0.01744 - R2: 0.9989 -- iter: 1160/1168\n",
      "Training Step: 1770  | total loss: \u001b[1m\u001b[32m0.01744\u001b[0m\u001b[0m | time: 1.249s\n",
      "| SGD | epoch: 030 | loss: 0.01744 - R2: 0.9993 | val_loss: 0.03205 - val_acc: 1.0011 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 1771  | total loss: \u001b[1m\u001b[32m0.01642\u001b[0m\u001b[0m | time: 0.019s\n",
      "| SGD | epoch: 031 | loss: 0.01642 - R2: 0.9993 -- iter: 0020/1168\n",
      "Training Step: 1772  | total loss: \u001b[1m\u001b[32m0.01931\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 031 | loss: 0.01931 - R2: 1.0001 -- iter: 0040/1168\n",
      "Training Step: 1773  | total loss: \u001b[1m\u001b[32m0.01883\u001b[0m\u001b[0m | time: 0.023s\n",
      "| SGD | epoch: 031 | loss: 0.01883 - R2: 1.0000 -- iter: 0060/1168\n",
      "Training Step: 1774  | total loss: \u001b[1m\u001b[32m0.02265\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 031 | loss: 0.02265 - R2: 0.9999 -- iter: 0080/1168\n",
      "Training Step: 1775  | total loss: \u001b[1m\u001b[32m0.02078\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 031 | loss: 0.02078 - R2: 0.9995 -- iter: 0100/1168\n",
      "Training Step: 1776  | total loss: \u001b[1m\u001b[32m0.02078\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 031 | loss: 0.02078 - R2: 0.9995 -- iter: 0120/1168\n",
      "Training Step: 1777  | total loss: \u001b[1m\u001b[32m0.01947\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 031 | loss: 0.01947 - R2: 1.0006 -- iter: 0140/1168\n",
      "Training Step: 1778  | total loss: \u001b[1m\u001b[32m0.01896\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 031 | loss: 0.01896 - R2: 1.0006 -- iter: 0160/1168\n",
      "Training Step: 1779  | total loss: \u001b[1m\u001b[32m0.01901\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 031 | loss: 0.01901 - R2: 1.0002 -- iter: 0180/1168\n",
      "Training Step: 1780  | total loss: \u001b[1m\u001b[32m0.01918\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 031 | loss: 0.01918 - R2: 1.0005 -- iter: 0200/1168\n",
      "Training Step: 1781  | total loss: \u001b[1m\u001b[32m0.01976\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 031 | loss: 0.01976 - R2: 0.9997 -- iter: 0220/1168\n",
      "Training Step: 1782  | total loss: \u001b[1m\u001b[32m0.01976\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 031 | loss: 0.01976 - R2: 0.9997 -- iter: 0240/1168\n",
      "Training Step: 1783  | total loss: \u001b[1m\u001b[32m0.01924\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 031 | loss: 0.01924 - R2: 0.9990 -- iter: 0260/1168\n",
      "Training Step: 1784  | total loss: \u001b[1m\u001b[32m0.02003\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 031 | loss: 0.02003 - R2: 0.9990 -- iter: 0280/1168\n",
      "Training Step: 1785  | total loss: \u001b[1m\u001b[32m0.02048\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 031 | loss: 0.02048 - R2: 0.9995 -- iter: 0300/1168\n",
      "Training Step: 1786  | total loss: \u001b[1m\u001b[32m0.01988\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 031 | loss: 0.01988 - R2: 0.9993 -- iter: 0320/1168\n",
      "Training Step: 1787  | total loss: \u001b[1m\u001b[32m0.01953\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 031 | loss: 0.01953 - R2: 0.9994 -- iter: 0340/1168\n",
      "Training Step: 1788  | total loss: \u001b[1m\u001b[32m0.01959\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 031 | loss: 0.01959 - R2: 0.9994 -- iter: 0360/1168\n",
      "Training Step: 1789  | total loss: \u001b[1m\u001b[32m0.01918\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 031 | loss: 0.01918 - R2: 0.9988 -- iter: 0380/1168\n",
      "Training Step: 1790  | total loss: \u001b[1m\u001b[32m0.01808\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 031 | loss: 0.01808 - R2: 0.9988 -- iter: 0400/1168\n",
      "Training Step: 1791  | total loss: \u001b[1m\u001b[32m0.01894\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 031 | loss: 0.01894 - R2: 0.9998 -- iter: 0420/1168\n",
      "Training Step: 1792  | total loss: \u001b[1m\u001b[32m0.01777\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 031 | loss: 0.01777 - R2: 0.9995 -- iter: 0440/1168\n",
      "Training Step: 1793  | total loss: \u001b[1m\u001b[32m0.01735\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 031 | loss: 0.01735 - R2: 0.9985 -- iter: 0460/1168\n",
      "Training Step: 1794  | total loss: \u001b[1m\u001b[32m0.01623\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 031 | loss: 0.01623 - R2: 0.9985 -- iter: 0480/1168\n",
      "Training Step: 1795  | total loss: \u001b[1m\u001b[32m0.01625\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 031 | loss: 0.01625 - R2: 0.9987 -- iter: 0500/1168\n",
      "Training Step: 1796  | total loss: \u001b[1m\u001b[32m0.01646\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 031 | loss: 0.01646 - R2: 0.9999 -- iter: 0520/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1797  | total loss: \u001b[1m\u001b[32m0.01556\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 031 | loss: 0.01556 - R2: 0.9994 -- iter: 0540/1168\n",
      "Training Step: 1798  | total loss: \u001b[1m\u001b[32m0.01614\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 031 | loss: 0.01614 - R2: 0.9991 -- iter: 0560/1168\n",
      "Training Step: 1799  | total loss: \u001b[1m\u001b[32m0.01736\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 031 | loss: 0.01736 - R2: 1.0002 -- iter: 0580/1168\n",
      "Training Step: 1800  | total loss: \u001b[1m\u001b[32m0.01736\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 031 | loss: 0.01736 - R2: 1.0002 -- iter: 0600/1168\n",
      "Training Step: 1801  | total loss: \u001b[1m\u001b[32m0.01823\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 031 | loss: 0.01823 - R2: 1.0010 -- iter: 0620/1168\n",
      "Training Step: 1802  | total loss: \u001b[1m\u001b[32m0.02440\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 031 | loss: 0.02440 - R2: 1.0015 -- iter: 0640/1168\n",
      "Training Step: 1803  | total loss: \u001b[1m\u001b[32m0.02479\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 031 | loss: 0.02479 - R2: 1.0015 -- iter: 0660/1168\n",
      "Training Step: 1804  | total loss: \u001b[1m\u001b[32m0.02356\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 031 | loss: 0.02356 - R2: 1.0010 -- iter: 0680/1168\n",
      "Training Step: 1805  | total loss: \u001b[1m\u001b[32m0.02356\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 031 | loss: 0.02356 - R2: 1.0010 -- iter: 0700/1168\n",
      "Training Step: 1806  | total loss: \u001b[1m\u001b[32m0.02234\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 031 | loss: 0.02234 - R2: 1.0014 -- iter: 0720/1168\n",
      "Training Step: 1807  | total loss: \u001b[1m\u001b[32m0.02076\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 031 | loss: 0.02076 - R2: 1.0014 -- iter: 0740/1168\n",
      "Training Step: 1808  | total loss: \u001b[1m\u001b[32m0.01966\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 031 | loss: 0.01966 - R2: 1.0007 -- iter: 0760/1168\n",
      "Training Step: 1809  | total loss: \u001b[1m\u001b[32m0.01956\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 031 | loss: 0.01956 - R2: 1.0009 -- iter: 0780/1168\n",
      "Training Step: 1810  | total loss: \u001b[1m\u001b[32m0.01861\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 031 | loss: 0.01861 - R2: 1.0005 -- iter: 0800/1168\n",
      "Training Step: 1811  | total loss: \u001b[1m\u001b[32m0.01828\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 031 | loss: 0.01828 - R2: 1.0005 -- iter: 0820/1168\n",
      "Training Step: 1812  | total loss: \u001b[1m\u001b[32m0.01661\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 031 | loss: 0.01661 - R2: 1.0011 -- iter: 0840/1168\n",
      "Training Step: 1813  | total loss: \u001b[1m\u001b[32m0.01641\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 031 | loss: 0.01641 - R2: 1.0007 -- iter: 0860/1168\n",
      "Training Step: 1814  | total loss: \u001b[1m\u001b[32m0.01633\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 031 | loss: 0.01633 - R2: 0.9997 -- iter: 0880/1168\n",
      "Training Step: 1815  | total loss: \u001b[1m\u001b[32m0.01633\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 031 | loss: 0.01633 - R2: 0.9996 -- iter: 0900/1168\n",
      "Training Step: 1816  | total loss: \u001b[1m\u001b[32m0.01561\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 031 | loss: 0.01561 - R2: 1.0002 -- iter: 0920/1168\n",
      "Training Step: 1817  | total loss: \u001b[1m\u001b[32m0.01720\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 031 | loss: 0.01720 - R2: 1.0002 -- iter: 0940/1168\n",
      "Training Step: 1818  | total loss: \u001b[1m\u001b[32m0.01704\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 031 | loss: 0.01704 - R2: 0.9997 -- iter: 0960/1168\n",
      "Training Step: 1819  | total loss: \u001b[1m\u001b[32m0.01626\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 031 | loss: 0.01626 - R2: 0.9998 -- iter: 0980/1168\n",
      "Training Step: 1820  | total loss: \u001b[1m\u001b[32m0.01626\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 031 | loss: 0.01626 - R2: 0.9998 -- iter: 1000/1168\n",
      "Training Step: 1821  | total loss: \u001b[1m\u001b[32m0.01597\u001b[0m\u001b[0m | time: 0.169s\n",
      "| SGD | epoch: 031 | loss: 0.01597 - R2: 1.0002 -- iter: 1020/1168\n",
      "Training Step: 1822  | total loss: \u001b[1m\u001b[32m0.01815\u001b[0m\u001b[0m | time: 0.171s\n",
      "| SGD | epoch: 031 | loss: 0.01815 - R2: 1.0002 -- iter: 1040/1168\n",
      "Training Step: 1823  | total loss: \u001b[1m\u001b[32m0.01787\u001b[0m\u001b[0m | time: 0.173s\n",
      "| SGD | epoch: 031 | loss: 0.01787 - R2: 1.0012 -- iter: 1060/1168\n",
      "Training Step: 1824  | total loss: \u001b[1m\u001b[32m0.01871\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 031 | loss: 0.01871 - R2: 1.0012 -- iter: 1080/1168\n",
      "Training Step: 1825  | total loss: \u001b[1m\u001b[32m0.01801\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 031 | loss: 0.01801 - R2: 1.0009 -- iter: 1100/1168\n",
      "Training Step: 1826  | total loss: \u001b[1m\u001b[32m0.01809\u001b[0m\u001b[0m | time: 0.179s\n",
      "| SGD | epoch: 031 | loss: 0.01809 - R2: 1.0002 -- iter: 1120/1168\n",
      "Training Step: 1827  | total loss: \u001b[1m\u001b[32m0.01825\u001b[0m\u001b[0m | time: 0.182s\n",
      "| SGD | epoch: 031 | loss: 0.01825 - R2: 1.0002 -- iter: 1140/1168\n",
      "Training Step: 1828  | total loss: \u001b[1m\u001b[32m0.01841\u001b[0m\u001b[0m | time: 0.186s\n",
      "| SGD | epoch: 031 | loss: 0.01841 - R2: 0.9998 -- iter: 1160/1168\n",
      "Training Step: 1829  | total loss: \u001b[1m\u001b[32m0.01758\u001b[0m\u001b[0m | time: 1.194s\n",
      "| SGD | epoch: 031 | loss: 0.01758 - R2: 0.9998 | val_loss: 0.03216 - val_acc: 1.0004 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 1830  | total loss: \u001b[1m\u001b[32m0.01758\u001b[0m\u001b[0m | time: 0.011s\n",
      "| SGD | epoch: 032 | loss: 0.01758 - R2: 0.9998 -- iter: 0020/1168\n",
      "Training Step: 1831  | total loss: \u001b[1m\u001b[32m0.01726\u001b[0m\u001b[0m | time: 0.019s\n",
      "| SGD | epoch: 032 | loss: 0.01726 - R2: 1.0002 -- iter: 0040/1168\n",
      "Training Step: 1832  | total loss: \u001b[1m\u001b[32m0.01568\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 032 | loss: 0.01568 - R2: 0.9997 -- iter: 0060/1168\n",
      "Training Step: 1833  | total loss: \u001b[1m\u001b[32m0.01533\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 032 | loss: 0.01533 - R2: 1.0002 -- iter: 0080/1168\n",
      "Training Step: 1834  | total loss: \u001b[1m\u001b[32m0.01452\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 032 | loss: 0.01452 - R2: 0.9995 -- iter: 0100/1168\n",
      "Training Step: 1835  | total loss: \u001b[1m\u001b[32m0.01496\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 032 | loss: 0.01496 - R2: 0.9990 -- iter: 0120/1168\n",
      "Training Step: 1836  | total loss: \u001b[1m\u001b[32m0.01496\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 032 | loss: 0.01496 - R2: 0.9990 -- iter: 0140/1168\n",
      "Training Step: 1837  | total loss: \u001b[1m\u001b[32m0.01521\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 032 | loss: 0.01521 - R2: 0.9993 -- iter: 0160/1168\n",
      "Training Step: 1838  | total loss: \u001b[1m\u001b[32m0.01564\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 032 | loss: 0.01564 - R2: 0.9999 -- iter: 0180/1168\n",
      "Training Step: 1839  | total loss: \u001b[1m\u001b[32m0.01684\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 032 | loss: 0.01684 - R2: 1.0006 -- iter: 0200/1168\n",
      "Training Step: 1840  | total loss: \u001b[1m\u001b[32m0.01646\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 032 | loss: 0.01646 - R2: 1.0003 -- iter: 0220/1168\n",
      "Training Step: 1841  | total loss: \u001b[1m\u001b[32m0.01646\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 032 | loss: 0.01646 - R2: 1.0005 -- iter: 0240/1168\n",
      "Training Step: 1842  | total loss: \u001b[1m\u001b[32m0.01539\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 032 | loss: 0.01539 - R2: 1.0004 -- iter: 0260/1168\n",
      "Training Step: 1843  | total loss: \u001b[1m\u001b[32m0.01502\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 032 | loss: 0.01502 - R2: 0.9999 -- iter: 0280/1168\n",
      "Training Step: 1844  | total loss: \u001b[1m\u001b[32m0.01502\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 032 | loss: 0.01502 - R2: 0.9999 -- iter: 0300/1168\n",
      "Training Step: 1845  | total loss: \u001b[1m\u001b[32m0.01627\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 032 | loss: 0.01627 - R2: 0.9999 -- iter: 0320/1168\n",
      "Training Step: 1846  | total loss: \u001b[1m\u001b[32m0.01611\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 032 | loss: 0.01611 - R2: 1.0005 -- iter: 0340/1168\n",
      "Training Step: 1847  | total loss: \u001b[1m\u001b[32m0.01650\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 032 | loss: 0.01650 - R2: 1.0002 -- iter: 0360/1168\n",
      "Training Step: 1848  | total loss: \u001b[1m\u001b[32m0.01650\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 032 | loss: 0.01650 - R2: 1.0006 -- iter: 0380/1168\n",
      "Training Step: 1849  | total loss: \u001b[1m\u001b[32m0.02214\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 032 | loss: 0.02214 - R2: 1.0022 -- iter: 0400/1168\n",
      "Training Step: 1850  | total loss: \u001b[1m\u001b[32m0.02298\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 032 | loss: 0.02298 - R2: 1.0009 -- iter: 0420/1168\n",
      "Training Step: 1851  | total loss: \u001b[1m\u001b[32m0.02298\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 032 | loss: 0.02298 - R2: 1.0009 -- iter: 0440/1168\n",
      "Training Step: 1852  | total loss: \u001b[1m\u001b[32m0.02190\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 032 | loss: 0.02190 - R2: 1.0002 -- iter: 0460/1168\n",
      "Training Step: 1853  | total loss: \u001b[1m\u001b[32m0.02090\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 032 | loss: 0.02090 - R2: 0.9994 -- iter: 0480/1168\n",
      "Training Step: 1854  | total loss: \u001b[1m\u001b[32m0.02090\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 032 | loss: 0.02090 - R2: 0.9994 -- iter: 0500/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1855  | total loss: \u001b[1m\u001b[32m0.02020\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 032 | loss: 0.02020 - R2: 0.9991 -- iter: 0520/1168\n",
      "Training Step: 1856  | total loss: \u001b[1m\u001b[32m0.01909\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 032 | loss: 0.01909 - R2: 0.9991 -- iter: 0540/1168\n",
      "Training Step: 1857  | total loss: \u001b[1m\u001b[32m0.01799\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 032 | loss: 0.01799 - R2: 0.9987 -- iter: 0560/1168\n",
      "Training Step: 1858  | total loss: \u001b[1m\u001b[32m0.01733\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 032 | loss: 0.01733 - R2: 0.9987 -- iter: 0580/1168\n",
      "Training Step: 1859  | total loss: \u001b[1m\u001b[32m0.01654\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 032 | loss: 0.01654 - R2: 0.9978 -- iter: 0600/1168\n",
      "Training Step: 1860  | total loss: \u001b[1m\u001b[32m0.01531\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 032 | loss: 0.01531 - R2: 0.9978 -- iter: 0620/1168\n",
      "Training Step: 1861  | total loss: \u001b[1m\u001b[32m0.01415\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 032 | loss: 0.01415 - R2: 0.9973 -- iter: 0640/1168\n",
      "Training Step: 1862  | total loss: \u001b[1m\u001b[32m0.01796\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 032 | loss: 0.01796 - R2: 0.9978 -- iter: 0660/1168\n",
      "Training Step: 1863  | total loss: \u001b[1m\u001b[32m0.01796\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 032 | loss: 0.01796 - R2: 0.9978 -- iter: 0680/1168\n",
      "Training Step: 1864  | total loss: \u001b[1m\u001b[32m0.01710\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 032 | loss: 0.01710 - R2: 0.9981 -- iter: 0700/1168\n",
      "Training Step: 1865  | total loss: \u001b[1m\u001b[32m0.01718\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 032 | loss: 0.01718 - R2: 0.9984 -- iter: 0720/1168\n",
      "Training Step: 1866  | total loss: \u001b[1m\u001b[32m0.01644\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 032 | loss: 0.01644 - R2: 0.9987 -- iter: 0740/1168\n",
      "Training Step: 1867  | total loss: \u001b[1m\u001b[32m0.01621\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 032 | loss: 0.01621 - R2: 0.9989 -- iter: 0760/1168\n",
      "Training Step: 1868  | total loss: \u001b[1m\u001b[32m0.01702\u001b[0m\u001b[0m | time: 0.171s\n",
      "| SGD | epoch: 032 | loss: 0.01702 - R2: 0.9997 -- iter: 0780/1168\n",
      "Training Step: 1869  | total loss: \u001b[1m\u001b[32m0.01702\u001b[0m\u001b[0m | time: 0.173s\n",
      "| SGD | epoch: 032 | loss: 0.01702 - R2: 1.0017 -- iter: 0800/1168\n",
      "Training Step: 1870  | total loss: \u001b[1m\u001b[32m0.02615\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 032 | loss: 0.02615 - R2: 1.0017 -- iter: 0820/1168\n",
      "Training Step: 1871  | total loss: \u001b[1m\u001b[32m0.02438\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 032 | loss: 0.02438 - R2: 1.0011 -- iter: 0840/1168\n",
      "Training Step: 1872  | total loss: \u001b[1m\u001b[32m0.02336\u001b[0m\u001b[0m | time: 0.179s\n",
      "| SGD | epoch: 032 | loss: 0.02336 - R2: 1.0010 -- iter: 0860/1168\n",
      "Training Step: 1873  | total loss: \u001b[1m\u001b[32m0.02188\u001b[0m\u001b[0m | time: 0.180s\n",
      "| SGD | epoch: 032 | loss: 0.02188 - R2: 1.0010 -- iter: 0880/1168\n",
      "Training Step: 1874  | total loss: \u001b[1m\u001b[32m0.02101\u001b[0m\u001b[0m | time: 0.183s\n",
      "| SGD | epoch: 032 | loss: 0.02101 - R2: 1.0008 -- iter: 0900/1168\n",
      "Training Step: 1875  | total loss: \u001b[1m\u001b[32m0.02095\u001b[0m\u001b[0m | time: 0.185s\n",
      "| SGD | epoch: 032 | loss: 0.02095 - R2: 0.9998 -- iter: 0920/1168\n",
      "Training Step: 1876  | total loss: \u001b[1m\u001b[32m0.02014\u001b[0m\u001b[0m | time: 0.187s\n",
      "| SGD | epoch: 032 | loss: 0.02014 - R2: 0.9992 -- iter: 0940/1168\n",
      "Training Step: 1877  | total loss: \u001b[1m\u001b[32m0.02014\u001b[0m\u001b[0m | time: 0.189s\n",
      "| SGD | epoch: 032 | loss: 0.02014 - R2: 0.9992 -- iter: 0960/1168\n",
      "Training Step: 1878  | total loss: \u001b[1m\u001b[32m0.02042\u001b[0m\u001b[0m | time: 0.192s\n",
      "| SGD | epoch: 032 | loss: 0.02042 - R2: 0.9989 -- iter: 0980/1168\n",
      "Training Step: 1879  | total loss: \u001b[1m\u001b[32m0.02042\u001b[0m\u001b[0m | time: 0.194s\n",
      "| SGD | epoch: 032 | loss: 0.02042 - R2: 0.9989 -- iter: 1000/1168\n",
      "Training Step: 1880  | total loss: \u001b[1m\u001b[32m0.01997\u001b[0m\u001b[0m | time: 0.195s\n",
      "| SGD | epoch: 032 | loss: 0.01997 - R2: 0.9996 -- iter: 1020/1168\n",
      "Training Step: 1881  | total loss: \u001b[1m\u001b[32m0.02110\u001b[0m\u001b[0m | time: 0.199s\n",
      "| SGD | epoch: 032 | loss: 0.02110 - R2: 0.9993 -- iter: 1040/1168\n",
      "Training Step: 1882  | total loss: \u001b[1m\u001b[32m0.02186\u001b[0m\u001b[0m | time: 0.202s\n",
      "| SGD | epoch: 032 | loss: 0.02186 - R2: 0.9991 -- iter: 1060/1168\n",
      "Training Step: 1883  | total loss: \u001b[1m\u001b[32m0.02062\u001b[0m\u001b[0m | time: 0.204s\n",
      "| SGD | epoch: 032 | loss: 0.02062 - R2: 0.9996 -- iter: 1080/1168\n",
      "Training Step: 1884  | total loss: \u001b[1m\u001b[32m0.01961\u001b[0m\u001b[0m | time: 0.208s\n",
      "| SGD | epoch: 032 | loss: 0.01961 - R2: 0.9998 -- iter: 1100/1168\n",
      "Training Step: 1885  | total loss: \u001b[1m\u001b[32m0.01888\u001b[0m\u001b[0m | time: 0.212s\n",
      "| SGD | epoch: 032 | loss: 0.01888 - R2: 0.9994 -- iter: 1120/1168\n",
      "Training Step: 1886  | total loss: \u001b[1m\u001b[32m0.01858\u001b[0m\u001b[0m | time: 0.218s\n",
      "| SGD | epoch: 032 | loss: 0.01858 - R2: 0.9990 -- iter: 1140/1168\n",
      "Training Step: 1887  | total loss: \u001b[1m\u001b[32m0.01722\u001b[0m\u001b[0m | time: 0.221s\n",
      "| SGD | epoch: 032 | loss: 0.01722 - R2: 0.9993 -- iter: 1160/1168\n",
      "Training Step: 1888  | total loss: \u001b[1m\u001b[32m0.01688\u001b[0m\u001b[0m | time: 1.227s\n",
      "| SGD | epoch: 032 | loss: 0.01688 - R2: 0.9994 | val_loss: 0.03246 - val_acc: 1.0016 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 1889  | total loss: \u001b[1m\u001b[32m0.01688\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 033 | loss: 0.01688 - R2: 0.9994 -- iter: 0020/1168\n",
      "Training Step: 1890  | total loss: \u001b[1m\u001b[32m0.01693\u001b[0m\u001b[0m | time: 0.007s\n",
      "| SGD | epoch: 033 | loss: 0.01693 - R2: 0.9995 -- iter: 0040/1168\n",
      "Training Step: 1891  | total loss: \u001b[1m\u001b[32m0.01693\u001b[0m\u001b[0m | time: 0.009s\n",
      "| SGD | epoch: 033 | loss: 0.01693 - R2: 0.9995 -- iter: 0060/1168\n",
      "Training Step: 1892  | total loss: \u001b[1m\u001b[32m0.01808\u001b[0m\u001b[0m | time: 0.013s\n",
      "| SGD | epoch: 033 | loss: 0.01808 - R2: 1.0005 -- iter: 0080/1168\n",
      "Training Step: 1893  | total loss: \u001b[1m\u001b[32m0.02117\u001b[0m\u001b[0m | time: 0.017s\n",
      "| SGD | epoch: 033 | loss: 0.02117 - R2: 1.0010 -- iter: 0100/1168\n",
      "Training Step: 1894  | total loss: \u001b[1m\u001b[32m0.02117\u001b[0m\u001b[0m | time: 0.019s\n",
      "| SGD | epoch: 033 | loss: 0.02117 - R2: 1.0010 -- iter: 0120/1168\n",
      "Training Step: 1895  | total loss: \u001b[1m\u001b[32m0.02103\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 033 | loss: 0.02103 - R2: 1.0008 -- iter: 0140/1168\n",
      "Training Step: 1896  | total loss: \u001b[1m\u001b[32m0.01881\u001b[0m\u001b[0m | time: 0.026s\n",
      "| SGD | epoch: 033 | loss: 0.01881 - R2: 1.0001 -- iter: 0160/1168\n",
      "Training Step: 1897  | total loss: \u001b[1m\u001b[32m0.01881\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 033 | loss: 0.01881 - R2: 0.9996 -- iter: 0180/1168\n",
      "Training Step: 1898  | total loss: \u001b[1m\u001b[32m0.01777\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 033 | loss: 0.01777 - R2: 0.9996 -- iter: 0200/1168\n",
      "Training Step: 1899  | total loss: \u001b[1m\u001b[32m0.01777\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 033 | loss: 0.01777 - R2: 0.9996 -- iter: 0220/1168\n",
      "Training Step: 1900  | total loss: \u001b[1m\u001b[32m0.01715\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 033 | loss: 0.01715 - R2: 0.9992 -- iter: 0240/1168\n",
      "Training Step: 1901  | total loss: \u001b[1m\u001b[32m0.01646\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 033 | loss: 0.01646 - R2: 1.0001 -- iter: 0260/1168\n",
      "Training Step: 1902  | total loss: \u001b[1m\u001b[32m0.01525\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 033 | loss: 0.01525 - R2: 1.0001 -- iter: 0280/1168\n",
      "Training Step: 1903  | total loss: \u001b[1m\u001b[32m0.01472\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 033 | loss: 0.01472 - R2: 1.0002 -- iter: 0300/1168\n",
      "Training Step: 1904  | total loss: \u001b[1m\u001b[32m0.01509\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 033 | loss: 0.01509 - R2: 0.9996 -- iter: 0320/1168\n",
      "Training Step: 1905  | total loss: \u001b[1m\u001b[32m0.01638\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 033 | loss: 0.01638 - R2: 1.0009 -- iter: 0340/1168\n",
      "Training Step: 1906  | total loss: \u001b[1m\u001b[32m0.01638\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 033 | loss: 0.01638 - R2: 1.0009 -- iter: 0360/1168\n",
      "Training Step: 1907  | total loss: \u001b[1m\u001b[32m0.01607\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 033 | loss: 0.01607 - R2: 1.0000 -- iter: 0380/1168\n",
      "Training Step: 1908  | total loss: \u001b[1m\u001b[32m0.01559\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 033 | loss: 0.01559 - R2: 1.0002 -- iter: 0400/1168\n",
      "Training Step: 1909  | total loss: \u001b[1m\u001b[32m0.01872\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 033 | loss: 0.01872 - R2: 1.0009 -- iter: 0420/1168\n",
      "Training Step: 1910  | total loss: \u001b[1m\u001b[32m0.01786\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 033 | loss: 0.01786 - R2: 1.0009 -- iter: 0440/1168\n",
      "Training Step: 1911  | total loss: \u001b[1m\u001b[32m0.01786\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 033 | loss: 0.01786 - R2: 1.0009 -- iter: 0460/1168\n",
      "Training Step: 1912  | total loss: \u001b[1m\u001b[32m0.01623\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 033 | loss: 0.01623 - R2: 1.0012 -- iter: 0480/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1913  | total loss: \u001b[1m\u001b[32m0.01605\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 033 | loss: 0.01605 - R2: 1.0008 -- iter: 0500/1168\n",
      "Training Step: 1914  | total loss: \u001b[1m\u001b[32m0.01605\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 033 | loss: 0.01605 - R2: 1.0008 -- iter: 0520/1168\n",
      "Training Step: 1915  | total loss: \u001b[1m\u001b[32m0.01576\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 033 | loss: 0.01576 - R2: 0.9994 -- iter: 0540/1168\n",
      "Training Step: 1916  | total loss: \u001b[1m\u001b[32m0.01525\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 033 | loss: 0.01525 - R2: 0.9991 -- iter: 0560/1168\n",
      "Training Step: 1917  | total loss: \u001b[1m\u001b[32m0.01451\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 033 | loss: 0.01451 - R2: 0.9991 -- iter: 0580/1168\n",
      "Training Step: 1918  | total loss: \u001b[1m\u001b[32m0.01602\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 033 | loss: 0.01602 - R2: 0.9991 -- iter: 0600/1168\n",
      "Training Step: 1919  | total loss: \u001b[1m\u001b[32m0.01534\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 033 | loss: 0.01534 - R2: 0.9996 -- iter: 0620/1168\n",
      "Training Step: 1920  | total loss: \u001b[1m\u001b[32m0.01534\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 033 | loss: 0.01534 - R2: 0.9996 -- iter: 0640/1168\n",
      "Training Step: 1921  | total loss: \u001b[1m\u001b[32m0.01532\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 033 | loss: 0.01532 - R2: 0.9997 -- iter: 0660/1168\n",
      "Training Step: 1922  | total loss: \u001b[1m\u001b[32m0.01857\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 033 | loss: 0.01857 - R2: 1.0004 -- iter: 0680/1168\n",
      "Training Step: 1923  | total loss: \u001b[1m\u001b[32m0.01845\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 033 | loss: 0.01845 - R2: 1.0001 -- iter: 0700/1168\n",
      "Training Step: 1924  | total loss: \u001b[1m\u001b[32m0.01845\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 033 | loss: 0.01845 - R2: 1.0001 -- iter: 0720/1168\n",
      "Training Step: 1925  | total loss: \u001b[1m\u001b[32m0.01753\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 033 | loss: 0.01753 - R2: 1.0005 -- iter: 0740/1168\n",
      "Training Step: 1926  | total loss: \u001b[1m\u001b[32m0.01840\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 033 | loss: 0.01840 - R2: 1.0003 -- iter: 0760/1168\n",
      "Training Step: 1927  | total loss: \u001b[1m\u001b[32m0.02538\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 033 | loss: 0.02538 - R2: 1.0010 -- iter: 0780/1168\n",
      "Training Step: 1928  | total loss: \u001b[1m\u001b[32m0.02333\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 033 | loss: 0.02333 - R2: 1.0009 -- iter: 0800/1168\n",
      "Training Step: 1929  | total loss: \u001b[1m\u001b[32m0.02304\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 033 | loss: 0.02304 - R2: 1.0011 -- iter: 0820/1168\n",
      "Training Step: 1930  | total loss: \u001b[1m\u001b[32m0.02304\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 033 | loss: 0.02304 - R2: 1.0006 -- iter: 0840/1168\n",
      "Training Step: 1931  | total loss: \u001b[1m\u001b[32m0.02319\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 033 | loss: 0.02319 - R2: 1.0006 -- iter: 0860/1168\n",
      "Training Step: 1932  | total loss: \u001b[1m\u001b[32m0.02192\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 033 | loss: 0.02192 - R2: 1.0008 -- iter: 0880/1168\n",
      "Training Step: 1933  | total loss: \u001b[1m\u001b[32m0.02044\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 033 | loss: 0.02044 - R2: 1.0005 -- iter: 0900/1168\n",
      "Training Step: 1934  | total loss: \u001b[1m\u001b[32m0.02114\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 033 | loss: 0.02114 - R2: 1.0013 -- iter: 0920/1168\n",
      "Training Step: 1935  | total loss: \u001b[1m\u001b[32m0.02059\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 033 | loss: 0.02059 - R2: 1.0009 -- iter: 0940/1168\n",
      "Training Step: 1936  | total loss: \u001b[1m\u001b[32m0.01972\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 033 | loss: 0.01972 - R2: 1.0009 -- iter: 0960/1168\n",
      "Training Step: 1937  | total loss: \u001b[1m\u001b[32m0.01804\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 033 | loss: 0.01804 - R2: 0.9999 -- iter: 0980/1168\n",
      "Training Step: 1938  | total loss: \u001b[1m\u001b[32m0.01768\u001b[0m\u001b[0m | time: 0.174s\n",
      "| SGD | epoch: 033 | loss: 0.01768 - R2: 0.9987 -- iter: 1000/1168\n",
      "Training Step: 1939  | total loss: \u001b[1m\u001b[32m0.01768\u001b[0m\u001b[0m | time: 0.179s\n",
      "| SGD | epoch: 033 | loss: 0.01768 - R2: 0.9987 -- iter: 1020/1168\n",
      "Training Step: 1940  | total loss: \u001b[1m\u001b[32m0.01738\u001b[0m\u001b[0m | time: 0.183s\n",
      "| SGD | epoch: 033 | loss: 0.01738 - R2: 0.9986 -- iter: 1040/1168\n",
      "Training Step: 1941  | total loss: \u001b[1m\u001b[32m0.01676\u001b[0m\u001b[0m | time: 0.188s\n",
      "| SGD | epoch: 033 | loss: 0.01676 - R2: 0.9991 -- iter: 1060/1168\n",
      "Training Step: 1942  | total loss: \u001b[1m\u001b[32m0.01771\u001b[0m\u001b[0m | time: 0.193s\n",
      "| SGD | epoch: 033 | loss: 0.01771 - R2: 0.9988 -- iter: 1080/1168\n",
      "Training Step: 1943  | total loss: \u001b[1m\u001b[32m0.01854\u001b[0m\u001b[0m | time: 0.199s\n",
      "| SGD | epoch: 033 | loss: 0.01854 - R2: 0.9983 -- iter: 1100/1168\n",
      "Training Step: 1944  | total loss: \u001b[1m\u001b[32m0.01958\u001b[0m\u001b[0m | time: 0.205s\n",
      "| SGD | epoch: 033 | loss: 0.01958 - R2: 0.9994 -- iter: 1120/1168\n",
      "Training Step: 1945  | total loss: \u001b[1m\u001b[32m0.01909\u001b[0m\u001b[0m | time: 0.208s\n",
      "| SGD | epoch: 033 | loss: 0.01909 - R2: 1.0000 -- iter: 1140/1168\n",
      "Training Step: 1946  | total loss: \u001b[1m\u001b[32m0.01909\u001b[0m\u001b[0m | time: 0.211s\n",
      "| SGD | epoch: 033 | loss: 0.01909 - R2: 1.0000 -- iter: 1160/1168\n",
      "Training Step: 1947  | total loss: \u001b[1m\u001b[32m0.02011\u001b[0m\u001b[0m | time: 1.217s\n",
      "| SGD | epoch: 033 | loss: 0.02011 - R2: 0.9994 | val_loss: 0.03217 - val_acc: 1.0011 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 1948  | total loss: \u001b[1m\u001b[32m0.01998\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 034 | loss: 0.01998 - R2: 0.9996 -- iter: 0020/1168\n",
      "Training Step: 1949  | total loss: \u001b[1m\u001b[32m0.01998\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 034 | loss: 0.01998 - R2: 0.9996 -- iter: 0040/1168\n",
      "Training Step: 1950  | total loss: \u001b[1m\u001b[32m0.01952\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 034 | loss: 0.01952 - R2: 1.0000 -- iter: 0060/1168\n",
      "Training Step: 1951  | total loss: \u001b[1m\u001b[32m0.02291\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 034 | loss: 0.02291 - R2: 1.0008 -- iter: 0080/1168\n",
      "Training Step: 1952  | total loss: \u001b[1m\u001b[32m0.02236\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 034 | loss: 0.02236 - R2: 1.0006 -- iter: 0100/1168\n",
      "Training Step: 1953  | total loss: \u001b[1m\u001b[32m0.02385\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 034 | loss: 0.02385 - R2: 0.9994 -- iter: 0120/1168\n",
      "Training Step: 1954  | total loss: \u001b[1m\u001b[32m0.02282\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 034 | loss: 0.02282 - R2: 0.9992 -- iter: 0140/1168\n",
      "Training Step: 1955  | total loss: \u001b[1m\u001b[32m0.02142\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 034 | loss: 0.02142 - R2: 0.9995 -- iter: 0160/1168\n",
      "Training Step: 1956  | total loss: \u001b[1m\u001b[32m0.02142\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 034 | loss: 0.02142 - R2: 0.9995 -- iter: 0180/1168\n",
      "Training Step: 1957  | total loss: \u001b[1m\u001b[32m0.02189\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 034 | loss: 0.02189 - R2: 0.9993 -- iter: 0200/1168\n",
      "Training Step: 1958  | total loss: \u001b[1m\u001b[32m0.02130\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 034 | loss: 0.02130 - R2: 0.9999 -- iter: 0220/1168\n",
      "Training Step: 1959  | total loss: \u001b[1m\u001b[32m0.02130\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 034 | loss: 0.02130 - R2: 0.9999 -- iter: 0240/1168\n",
      "Training Step: 1960  | total loss: \u001b[1m\u001b[32m0.01972\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 034 | loss: 0.01972 - R2: 0.9997 -- iter: 0260/1168\n",
      "Training Step: 1961  | total loss: \u001b[1m\u001b[32m0.02055\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 034 | loss: 0.02055 - R2: 0.9987 -- iter: 0280/1168\n",
      "Training Step: 1962  | total loss: \u001b[1m\u001b[32m0.02055\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 034 | loss: 0.02055 - R2: 0.9987 -- iter: 0300/1168\n",
      "Training Step: 1963  | total loss: \u001b[1m\u001b[32m0.01936\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 034 | loss: 0.01936 - R2: 0.9992 -- iter: 0320/1168\n",
      "Training Step: 1964  | total loss: \u001b[1m\u001b[32m0.02112\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 034 | loss: 0.02112 - R2: 0.9992 -- iter: 0340/1168\n",
      "Training Step: 1965  | total loss: \u001b[1m\u001b[32m0.02162\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 034 | loss: 0.02162 - R2: 1.0001 -- iter: 0360/1168\n",
      "Training Step: 1966  | total loss: \u001b[1m\u001b[32m0.01988\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 034 | loss: 0.01988 - R2: 0.9992 -- iter: 0380/1168\n",
      "Training Step: 1967  | total loss: \u001b[1m\u001b[32m0.01988\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 034 | loss: 0.01988 - R2: 0.9995 -- iter: 0400/1168\n",
      "Training Step: 1968  | total loss: \u001b[1m\u001b[32m0.01963\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 034 | loss: 0.01963 - R2: 0.9989 -- iter: 0420/1168\n",
      "Training Step: 1969  | total loss: \u001b[1m\u001b[32m0.01963\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 034 | loss: 0.01963 - R2: 0.9989 -- iter: 0440/1168\n",
      "Training Step: 1970  | total loss: \u001b[1m\u001b[32m0.01842\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 034 | loss: 0.01842 - R2: 0.9990 -- iter: 0460/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1971  | total loss: \u001b[1m\u001b[32m0.01788\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 034 | loss: 0.01788 - R2: 0.9990 -- iter: 0480/1168\n",
      "Training Step: 1972  | total loss: \u001b[1m\u001b[32m0.01793\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 034 | loss: 0.01793 - R2: 0.9987 -- iter: 0500/1168\n",
      "Training Step: 1973  | total loss: \u001b[1m\u001b[32m0.02076\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 034 | loss: 0.02076 - R2: 0.9998 -- iter: 0520/1168\n",
      "Training Step: 1974  | total loss: \u001b[1m\u001b[32m0.02001\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 034 | loss: 0.02001 - R2: 0.9993 -- iter: 0540/1168\n",
      "Training Step: 1975  | total loss: \u001b[1m\u001b[32m0.02001\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 034 | loss: 0.02001 - R2: 0.9993 -- iter: 0560/1168\n",
      "Training Step: 1976  | total loss: \u001b[1m\u001b[32m0.01921\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 034 | loss: 0.01921 - R2: 0.9992 -- iter: 0580/1168\n",
      "Training Step: 1977  | total loss: \u001b[1m\u001b[32m0.01921\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 034 | loss: 0.01921 - R2: 0.9992 -- iter: 0600/1168\n",
      "Training Step: 1978  | total loss: \u001b[1m\u001b[32m0.01863\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 034 | loss: 0.01863 - R2: 0.9993 -- iter: 0620/1168\n",
      "Training Step: 1979  | total loss: \u001b[1m\u001b[32m0.02456\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 034 | loss: 0.02456 - R2: 1.0002 -- iter: 0640/1168\n",
      "Training Step: 1980  | total loss: \u001b[1m\u001b[32m0.02262\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 034 | loss: 0.02262 - R2: 1.0010 -- iter: 0660/1168\n",
      "Training Step: 1981  | total loss: \u001b[1m\u001b[32m0.01973\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 034 | loss: 0.01973 - R2: 1.0011 -- iter: 0680/1168\n",
      "Training Step: 1982  | total loss: \u001b[1m\u001b[32m0.01973\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 034 | loss: 0.01973 - R2: 1.0011 -- iter: 0700/1168\n",
      "Training Step: 1983  | total loss: \u001b[1m\u001b[32m0.01833\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 034 | loss: 0.01833 - R2: 1.0013 -- iter: 0720/1168\n",
      "Training Step: 1984  | total loss: \u001b[1m\u001b[32m0.01862\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 034 | loss: 0.01862 - R2: 1.0009 -- iter: 0740/1168\n",
      "Training Step: 1985  | total loss: \u001b[1m\u001b[32m0.01796\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 034 | loss: 0.01796 - R2: 1.0004 -- iter: 0760/1168\n",
      "Training Step: 1986  | total loss: \u001b[1m\u001b[32m0.01889\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 034 | loss: 0.01889 - R2: 1.0015 -- iter: 0780/1168\n",
      "Training Step: 1987  | total loss: \u001b[1m\u001b[32m0.01827\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 034 | loss: 0.01827 - R2: 1.0017 -- iter: 0800/1168\n",
      "Training Step: 1988  | total loss: \u001b[1m\u001b[32m0.01827\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 034 | loss: 0.01827 - R2: 1.0017 -- iter: 0820/1168\n",
      "Training Step: 1989  | total loss: \u001b[1m\u001b[32m0.01613\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 034 | loss: 0.01613 - R2: 1.0015 -- iter: 0840/1168\n",
      "Training Step: 1990  | total loss: \u001b[1m\u001b[32m0.01606\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 034 | loss: 0.01606 - R2: 1.0013 -- iter: 0860/1168\n",
      "Training Step: 1991  | total loss: \u001b[1m\u001b[32m0.01586\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 034 | loss: 0.01586 - R2: 1.0016 -- iter: 0880/1168\n",
      "Training Step: 1992  | total loss: \u001b[1m\u001b[32m0.01659\u001b[0m\u001b[0m | time: 0.174s\n",
      "| SGD | epoch: 034 | loss: 0.01659 - R2: 1.0011 -- iter: 0900/1168\n",
      "Training Step: 1993  | total loss: \u001b[1m\u001b[32m0.01695\u001b[0m\u001b[0m | time: 0.178s\n",
      "| SGD | epoch: 034 | loss: 0.01695 - R2: 1.0011 -- iter: 0920/1168\n",
      "Training Step: 1994  | total loss: \u001b[1m\u001b[32m0.01654\u001b[0m\u001b[0m | time: 0.181s\n",
      "| SGD | epoch: 034 | loss: 0.01654 - R2: 1.0002 -- iter: 0940/1168\n",
      "Training Step: 1995  | total loss: \u001b[1m\u001b[32m0.01654\u001b[0m\u001b[0m | time: 0.185s\n",
      "| SGD | epoch: 034 | loss: 0.01654 - R2: 1.0002 -- iter: 0960/1168\n",
      "Training Step: 1996  | total loss: \u001b[1m\u001b[32m0.01604\u001b[0m\u001b[0m | time: 0.188s\n",
      "| SGD | epoch: 034 | loss: 0.01604 - R2: 0.9996 -- iter: 0980/1168\n",
      "Training Step: 1997  | total loss: \u001b[1m\u001b[32m0.01886\u001b[0m\u001b[0m | time: 0.193s\n",
      "| SGD | epoch: 034 | loss: 0.01886 - R2: 1.0000 -- iter: 1000/1168\n",
      "Training Step: 1998  | total loss: \u001b[1m\u001b[32m0.01886\u001b[0m\u001b[0m | time: 0.197s\n",
      "| SGD | epoch: 034 | loss: 0.01886 - R2: 1.0000 -- iter: 1020/1168\n",
      "Training Step: 1999  | total loss: \u001b[1m\u001b[32m0.01751\u001b[0m\u001b[0m | time: 0.199s\n",
      "| SGD | epoch: 034 | loss: 0.01751 - R2: 1.0001 -- iter: 1040/1168\n",
      "Training Step: 2000  | total loss: \u001b[1m\u001b[32m0.01726\u001b[0m\u001b[0m | time: 0.202s\n",
      "| SGD | epoch: 034 | loss: 0.01726 - R2: 1.0001 -- iter: 1060/1168\n",
      "Training Step: 2001  | total loss: \u001b[1m\u001b[32m0.01726\u001b[0m\u001b[0m | time: 0.204s\n",
      "| SGD | epoch: 034 | loss: 0.01726 - R2: 1.0001 -- iter: 1080/1168\n",
      "Training Step: 2002  | total loss: \u001b[1m\u001b[32m0.01632\u001b[0m\u001b[0m | time: 0.206s\n",
      "| SGD | epoch: 034 | loss: 0.01632 - R2: 0.9997 -- iter: 1100/1168\n",
      "Training Step: 2003  | total loss: \u001b[1m\u001b[32m0.01603\u001b[0m\u001b[0m | time: 0.208s\n",
      "| SGD | epoch: 034 | loss: 0.01603 - R2: 0.9997 -- iter: 1120/1168\n",
      "Training Step: 2004  | total loss: \u001b[1m\u001b[32m0.01508\u001b[0m\u001b[0m | time: 0.210s\n",
      "| SGD | epoch: 034 | loss: 0.01508 - R2: 0.9997 -- iter: 1140/1168\n",
      "Training Step: 2005  | total loss: \u001b[1m\u001b[32m0.01438\u001b[0m\u001b[0m | time: 0.217s\n",
      "| SGD | epoch: 034 | loss: 0.01438 - R2: 1.0001 -- iter: 1160/1168\n",
      "Training Step: 2006  | total loss: \u001b[1m\u001b[32m0.01508\u001b[0m\u001b[0m | time: 1.227s\n",
      "| SGD | epoch: 034 | loss: 0.01508 - R2: 0.9998 | val_loss: 0.03204 - val_acc: 1.0004 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 2007  | total loss: \u001b[1m\u001b[32m0.01460\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 035 | loss: 0.01460 - R2: 1.0001 -- iter: 0020/1168\n",
      "Training Step: 2008  | total loss: \u001b[1m\u001b[32m0.01460\u001b[0m\u001b[0m | time: 0.005s\n",
      "| SGD | epoch: 035 | loss: 0.01460 - R2: 1.0007 -- iter: 0040/1168\n",
      "Training Step: 2009  | total loss: \u001b[1m\u001b[32m0.01583\u001b[0m\u001b[0m | time: 0.007s\n",
      "| SGD | epoch: 035 | loss: 0.01583 - R2: 1.0007 -- iter: 0060/1168\n",
      "Training Step: 2010  | total loss: \u001b[1m\u001b[32m0.01605\u001b[0m\u001b[0m | time: 0.010s\n",
      "| SGD | epoch: 035 | loss: 0.01605 - R2: 1.0008 -- iter: 0080/1168\n",
      "Training Step: 2011  | total loss: \u001b[1m\u001b[32m0.01520\u001b[0m\u001b[0m | time: 0.013s\n",
      "| SGD | epoch: 035 | loss: 0.01520 - R2: 1.0013 -- iter: 0100/1168\n",
      "Training Step: 2012  | total loss: \u001b[1m\u001b[32m0.01516\u001b[0m\u001b[0m | time: 0.016s\n",
      "| SGD | epoch: 035 | loss: 0.01516 - R2: 1.0013 -- iter: 0120/1168\n",
      "Training Step: 2013  | total loss: \u001b[1m\u001b[32m0.01516\u001b[0m\u001b[0m | time: 0.019s\n",
      "| SGD | epoch: 035 | loss: 0.01516 - R2: 1.0013 -- iter: 0140/1168\n",
      "Training Step: 2014  | total loss: \u001b[1m\u001b[32m0.01486\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 035 | loss: 0.01486 - R2: 1.0005 -- iter: 0160/1168\n",
      "Training Step: 2015  | total loss: \u001b[1m\u001b[32m0.01433\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 035 | loss: 0.01433 - R2: 1.0005 -- iter: 0180/1168\n",
      "Training Step: 2016  | total loss: \u001b[1m\u001b[32m0.01406\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 035 | loss: 0.01406 - R2: 1.0004 -- iter: 0200/1168\n",
      "Training Step: 2017  | total loss: \u001b[1m\u001b[32m0.01486\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 035 | loss: 0.01486 - R2: 1.0002 -- iter: 0220/1168\n",
      "Training Step: 2018  | total loss: \u001b[1m\u001b[32m0.01486\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 035 | loss: 0.01486 - R2: 1.0002 -- iter: 0240/1168\n",
      "Training Step: 2019  | total loss: \u001b[1m\u001b[32m0.01651\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 035 | loss: 0.01651 - R2: 1.0000 -- iter: 0260/1168\n",
      "Training Step: 2020  | total loss: \u001b[1m\u001b[32m0.01651\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 035 | loss: 0.01651 - R2: 1.0000 -- iter: 0280/1168\n",
      "Training Step: 2021  | total loss: \u001b[1m\u001b[32m0.01800\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 035 | loss: 0.01800 - R2: 1.0000 -- iter: 0300/1168\n",
      "Training Step: 2022  | total loss: \u001b[1m\u001b[32m0.01800\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 035 | loss: 0.01800 - R2: 0.9987 -- iter: 0320/1168\n",
      "Training Step: 2023  | total loss: \u001b[1m\u001b[32m0.01822\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 035 | loss: 0.01822 - R2: 0.9987 -- iter: 0340/1168\n",
      "Training Step: 2024  | total loss: \u001b[1m\u001b[32m0.01691\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 035 | loss: 0.01691 - R2: 0.9984 -- iter: 0360/1168\n",
      "Training Step: 2025  | total loss: \u001b[1m\u001b[32m0.01691\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 035 | loss: 0.01691 - R2: 0.9979 -- iter: 0380/1168\n",
      "Training Step: 2026  | total loss: \u001b[1m\u001b[32m0.01847\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 035 | loss: 0.01847 - R2: 0.9982 -- iter: 0400/1168\n",
      "Training Step: 2027  | total loss: \u001b[1m\u001b[32m0.01804\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 035 | loss: 0.01804 - R2: 0.9986 -- iter: 0420/1168\n",
      "Training Step: 2028  | total loss: \u001b[1m\u001b[32m0.01746\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 035 | loss: 0.01746 - R2: 0.9986 -- iter: 0440/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2029  | total loss: \u001b[1m\u001b[32m0.01663\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 035 | loss: 0.01663 - R2: 0.9982 -- iter: 0460/1168\n",
      "Training Step: 2030  | total loss: \u001b[1m\u001b[32m0.01669\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 035 | loss: 0.01669 - R2: 0.9980 -- iter: 0480/1168\n",
      "Training Step: 2031  | total loss: \u001b[1m\u001b[32m0.01602\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 035 | loss: 0.01602 - R2: 0.9984 -- iter: 0500/1168\n",
      "Training Step: 2032  | total loss: \u001b[1m\u001b[32m0.01602\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 035 | loss: 0.01602 - R2: 0.9988 -- iter: 0520/1168\n",
      "Training Step: 2033  | total loss: \u001b[1m\u001b[32m0.01530\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 035 | loss: 0.01530 - R2: 0.9992 -- iter: 0540/1168\n",
      "Training Step: 2034  | total loss: \u001b[1m\u001b[32m0.01526\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 035 | loss: 0.01526 - R2: 0.9992 -- iter: 0560/1168\n",
      "Training Step: 2035  | total loss: \u001b[1m\u001b[32m0.01453\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 035 | loss: 0.01453 - R2: 0.9995 -- iter: 0580/1168\n",
      "Training Step: 2036  | total loss: \u001b[1m\u001b[32m0.01418\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 035 | loss: 0.01418 - R2: 0.9991 -- iter: 0600/1168\n",
      "Training Step: 2037  | total loss: \u001b[1m\u001b[32m0.01420\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 035 | loss: 0.01420 - R2: 0.9991 -- iter: 0620/1168\n",
      "Training Step: 2038  | total loss: \u001b[1m\u001b[32m0.01340\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 035 | loss: 0.01340 - R2: 0.9992 -- iter: 0640/1168\n",
      "Training Step: 2039  | total loss: \u001b[1m\u001b[32m0.01422\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 035 | loss: 0.01422 - R2: 0.9998 -- iter: 0660/1168\n",
      "Training Step: 2040  | total loss: \u001b[1m\u001b[32m0.01368\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 035 | loss: 0.01368 - R2: 0.9992 -- iter: 0680/1168\n",
      "Training Step: 2041  | total loss: \u001b[1m\u001b[32m0.01316\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 035 | loss: 0.01316 - R2: 0.9987 -- iter: 0700/1168\n",
      "Training Step: 2042  | total loss: \u001b[1m\u001b[32m0.01368\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 035 | loss: 0.01368 - R2: 0.9989 -- iter: 0720/1168\n",
      "Training Step: 2043  | total loss: \u001b[1m\u001b[32m0.01808\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 035 | loss: 0.01808 - R2: 0.9994 -- iter: 0740/1168\n",
      "Training Step: 2044  | total loss: \u001b[1m\u001b[32m0.01808\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 035 | loss: 0.01808 - R2: 0.9994 -- iter: 0760/1168\n",
      "Training Step: 2045  | total loss: \u001b[1m\u001b[32m0.01792\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 035 | loss: 0.01792 - R2: 0.9989 -- iter: 0780/1168\n",
      "Training Step: 2046  | total loss: \u001b[1m\u001b[32m0.01797\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 035 | loss: 0.01797 - R2: 0.9990 -- iter: 0800/1168\n",
      "Training Step: 2047  | total loss: \u001b[1m\u001b[32m0.01850\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 035 | loss: 0.01850 - R2: 0.9991 -- iter: 0820/1168\n",
      "Training Step: 2048  | total loss: \u001b[1m\u001b[32m0.01799\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 035 | loss: 0.01799 - R2: 0.9988 -- iter: 0840/1168\n",
      "Training Step: 2049  | total loss: \u001b[1m\u001b[32m0.01799\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 035 | loss: 0.01799 - R2: 0.9997 -- iter: 0860/1168\n",
      "Training Step: 2050  | total loss: \u001b[1m\u001b[32m0.01774\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 035 | loss: 0.01774 - R2: 0.9990 -- iter: 0880/1168\n",
      "Training Step: 2051  | total loss: \u001b[1m\u001b[32m0.01774\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 035 | loss: 0.01774 - R2: 0.9990 -- iter: 0900/1168\n",
      "Training Step: 2052  | total loss: \u001b[1m\u001b[32m0.01760\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 035 | loss: 0.01760 - R2: 0.9989 -- iter: 0920/1168\n",
      "Training Step: 2053  | total loss: \u001b[1m\u001b[32m0.01748\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 035 | loss: 0.01748 - R2: 0.9988 -- iter: 0940/1168\n",
      "Training Step: 2054  | total loss: \u001b[1m\u001b[32m0.01748\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 035 | loss: 0.01748 - R2: 0.9994 -- iter: 0960/1168\n",
      "Training Step: 2055  | total loss: \u001b[1m\u001b[32m0.02435\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 035 | loss: 0.02435 - R2: 1.0010 -- iter: 0980/1168\n",
      "Training Step: 2056  | total loss: \u001b[1m\u001b[32m0.02316\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 035 | loss: 0.02316 - R2: 1.0013 -- iter: 1000/1168\n",
      "Training Step: 2057  | total loss: \u001b[1m\u001b[32m0.02241\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 035 | loss: 0.02241 - R2: 1.0005 -- iter: 1020/1168\n",
      "Training Step: 2058  | total loss: \u001b[1m\u001b[32m0.02115\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 035 | loss: 0.02115 - R2: 1.0005 -- iter: 1040/1168\n",
      "Training Step: 2059  | total loss: \u001b[1m\u001b[32m0.02047\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 035 | loss: 0.02047 - R2: 1.0005 -- iter: 1060/1168\n",
      "Training Step: 2060  | total loss: \u001b[1m\u001b[32m0.01873\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 035 | loss: 0.01873 - R2: 1.0004 -- iter: 1080/1168\n",
      "Training Step: 2061  | total loss: \u001b[1m\u001b[32m0.01873\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 035 | loss: 0.01873 - R2: 1.0004 -- iter: 1100/1168\n",
      "Training Step: 2062  | total loss: \u001b[1m\u001b[32m0.01976\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 035 | loss: 0.01976 - R2: 1.0003 -- iter: 1120/1168\n",
      "Training Step: 2063  | total loss: \u001b[1m\u001b[32m0.01848\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 035 | loss: 0.01848 - R2: 0.9998 -- iter: 1140/1168\n",
      "Training Step: 2064  | total loss: \u001b[1m\u001b[32m0.02023\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 035 | loss: 0.02023 - R2: 1.0008 -- iter: 1160/1168\n",
      "Training Step: 2065  | total loss: \u001b[1m\u001b[32m0.01912\u001b[0m\u001b[0m | time: 1.164s\n",
      "| SGD | epoch: 035 | loss: 0.01912 - R2: 1.0008 | val_loss: 0.03229 - val_acc: 1.0012 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 2066  | total loss: \u001b[1m\u001b[32m0.01864\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 036 | loss: 0.01864 - R2: 1.0005 -- iter: 0020/1168\n",
      "Training Step: 2067  | total loss: \u001b[1m\u001b[32m0.01834\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 036 | loss: 0.01834 - R2: 1.0005 -- iter: 0040/1168\n",
      "Training Step: 2068  | total loss: \u001b[1m\u001b[32m0.04159\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 036 | loss: 0.04159 - R2: 0.9999 -- iter: 0060/1168\n",
      "Training Step: 2069  | total loss: \u001b[1m\u001b[32m0.04159\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 036 | loss: 0.04159 - R2: 0.9999 -- iter: 0080/1168\n",
      "Training Step: 2070  | total loss: \u001b[1m\u001b[32m0.03872\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 036 | loss: 0.03872 - R2: 1.0007 -- iter: 0100/1168\n",
      "Training Step: 2071  | total loss: \u001b[1m\u001b[32m0.03310\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 036 | loss: 0.03310 - R2: 1.0004 -- iter: 0120/1168\n",
      "Training Step: 2072  | total loss: \u001b[1m\u001b[32m0.03725\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 036 | loss: 0.03725 - R2: 1.0013 -- iter: 0140/1168\n",
      "Training Step: 2073  | total loss: \u001b[1m\u001b[32m0.03725\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 036 | loss: 0.03725 - R2: 1.0013 -- iter: 0160/1168\n",
      "Training Step: 2074  | total loss: \u001b[1m\u001b[32m0.03436\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 036 | loss: 0.03436 - R2: 1.0006 -- iter: 0180/1168\n",
      "Training Step: 2075  | total loss: \u001b[1m\u001b[32m0.03958\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 036 | loss: 0.03958 - R2: 1.0016 -- iter: 0200/1168\n",
      "Training Step: 2076  | total loss: \u001b[1m\u001b[32m0.03429\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 036 | loss: 0.03429 - R2: 1.0020 -- iter: 0220/1168\n",
      "Training Step: 2077  | total loss: \u001b[1m\u001b[32m0.03429\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 036 | loss: 0.03429 - R2: 1.0020 -- iter: 0240/1168\n",
      "Training Step: 2078  | total loss: \u001b[1m\u001b[32m0.03290\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 036 | loss: 0.03290 - R2: 1.0008 -- iter: 0260/1168\n",
      "Training Step: 2079  | total loss: \u001b[1m\u001b[32m0.03072\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 036 | loss: 0.03072 - R2: 1.0005 -- iter: 0280/1168\n",
      "Training Step: 2080  | total loss: \u001b[1m\u001b[32m0.02887\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 036 | loss: 0.02887 - R2: 1.0005 -- iter: 0300/1168\n",
      "Training Step: 2081  | total loss: \u001b[1m\u001b[32m0.02500\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 036 | loss: 0.02500 - R2: 1.0005 -- iter: 0320/1168\n",
      "Training Step: 2082  | total loss: \u001b[1m\u001b[32m0.02500\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 036 | loss: 0.02500 - R2: 1.0005 -- iter: 0340/1168\n",
      "Training Step: 2083  | total loss: \u001b[1m\u001b[32m0.02391\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 036 | loss: 0.02391 - R2: 1.0007 -- iter: 0360/1168\n",
      "Training Step: 2084  | total loss: \u001b[1m\u001b[32m0.02221\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 036 | loss: 0.02221 - R2: 1.0003 -- iter: 0380/1168\n",
      "Training Step: 2085  | total loss: \u001b[1m\u001b[32m0.02163\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 036 | loss: 0.02163 - R2: 1.0004 -- iter: 0400/1168\n",
      "Training Step: 2086  | total loss: \u001b[1m\u001b[32m0.02027\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 036 | loss: 0.02027 - R2: 1.0001 -- iter: 0420/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2087  | total loss: \u001b[1m\u001b[32m0.01993\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 036 | loss: 0.01993 - R2: 1.0001 -- iter: 0440/1168\n",
      "Training Step: 2088  | total loss: \u001b[1m\u001b[32m0.01907\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 036 | loss: 0.01907 - R2: 0.9999 -- iter: 0460/1168\n",
      "Training Step: 2089  | total loss: \u001b[1m\u001b[32m0.01898\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 036 | loss: 0.01898 - R2: 1.0010 -- iter: 0480/1168\n",
      "Training Step: 2090  | total loss: \u001b[1m\u001b[32m0.01898\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 036 | loss: 0.01898 - R2: 1.0004 -- iter: 0500/1168\n",
      "Training Step: 2091  | total loss: \u001b[1m\u001b[32m0.01853\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 036 | loss: 0.01853 - R2: 1.0004 -- iter: 0520/1168\n",
      "Training Step: 2092  | total loss: \u001b[1m\u001b[32m0.01780\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 036 | loss: 0.01780 - R2: 0.9998 -- iter: 0540/1168\n",
      "Training Step: 2093  | total loss: \u001b[1m\u001b[32m0.02087\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 036 | loss: 0.02087 - R2: 0.9999 -- iter: 0560/1168\n",
      "Training Step: 2094  | total loss: \u001b[1m\u001b[32m0.02113\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 036 | loss: 0.02113 - R2: 0.9999 -- iter: 0580/1168\n",
      "Training Step: 2095  | total loss: \u001b[1m\u001b[32m0.02080\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 036 | loss: 0.02080 - R2: 1.0002 -- iter: 0600/1168\n",
      "Training Step: 2096  | total loss: \u001b[1m\u001b[32m0.02311\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 036 | loss: 0.02311 - R2: 1.0007 -- iter: 0620/1168\n",
      "Training Step: 2097  | total loss: \u001b[1m\u001b[32m0.02120\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 036 | loss: 0.02120 - R2: 0.9998 -- iter: 0640/1168\n",
      "Training Step: 2098  | total loss: \u001b[1m\u001b[32m0.01992\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 036 | loss: 0.01992 - R2: 0.9997 -- iter: 0660/1168\n",
      "Training Step: 2099  | total loss: \u001b[1m\u001b[32m0.01888\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 036 | loss: 0.01888 - R2: 0.9998 -- iter: 0680/1168\n",
      "Training Step: 2100  | total loss: \u001b[1m\u001b[32m0.01888\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 036 | loss: 0.01888 - R2: 0.9998 -- iter: 0700/1168\n",
      "Training Step: 2101  | total loss: \u001b[1m\u001b[32m0.01793\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 036 | loss: 0.01793 - R2: 0.9999 -- iter: 0720/1168\n",
      "Training Step: 2102  | total loss: \u001b[1m\u001b[32m0.01751\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 036 | loss: 0.01751 - R2: 0.9994 -- iter: 0740/1168\n",
      "Training Step: 2103  | total loss: \u001b[1m\u001b[32m0.01709\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 036 | loss: 0.01709 - R2: 0.9994 -- iter: 0760/1168\n",
      "Training Step: 2104  | total loss: \u001b[1m\u001b[32m0.01716\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 036 | loss: 0.01716 - R2: 0.9987 -- iter: 0780/1168\n",
      "Training Step: 2105  | total loss: \u001b[1m\u001b[32m0.01736\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 036 | loss: 0.01736 - R2: 0.9980 -- iter: 0800/1168\n",
      "Training Step: 2106  | total loss: \u001b[1m\u001b[32m0.01736\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 036 | loss: 0.01736 - R2: 0.9980 -- iter: 0820/1168\n",
      "Training Step: 2107  | total loss: \u001b[1m\u001b[32m0.01661\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 036 | loss: 0.01661 - R2: 0.9981 -- iter: 0840/1168\n",
      "Training Step: 2108  | total loss: \u001b[1m\u001b[32m0.01660\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 036 | loss: 0.01660 - R2: 0.9987 -- iter: 0860/1168\n",
      "Training Step: 2109  | total loss: \u001b[1m\u001b[32m0.01660\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 036 | loss: 0.01660 - R2: 0.9987 -- iter: 0880/1168\n",
      "Training Step: 2110  | total loss: \u001b[1m\u001b[32m0.01709\u001b[0m\u001b[0m | time: 0.174s\n",
      "| SGD | epoch: 036 | loss: 0.01709 - R2: 0.9983 -- iter: 0900/1168\n",
      "Training Step: 2111  | total loss: \u001b[1m\u001b[32m0.01843\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 036 | loss: 0.01843 - R2: 0.9984 -- iter: 0920/1168\n",
      "Training Step: 2112  | total loss: \u001b[1m\u001b[32m0.01866\u001b[0m\u001b[0m | time: 0.180s\n",
      "| SGD | epoch: 036 | loss: 0.01866 - R2: 0.9994 -- iter: 0940/1168\n",
      "Training Step: 2113  | total loss: \u001b[1m\u001b[32m0.01866\u001b[0m\u001b[0m | time: 0.184s\n",
      "| SGD | epoch: 036 | loss: 0.01866 - R2: 0.9994 -- iter: 0960/1168\n",
      "Training Step: 2114  | total loss: \u001b[1m\u001b[32m0.01788\u001b[0m\u001b[0m | time: 0.188s\n",
      "| SGD | epoch: 036 | loss: 0.01788 - R2: 0.9993 -- iter: 0980/1168\n",
      "Training Step: 2115  | total loss: \u001b[1m\u001b[32m0.02021\u001b[0m\u001b[0m | time: 0.189s\n",
      "| SGD | epoch: 036 | loss: 0.02021 - R2: 0.9998 -- iter: 1000/1168\n",
      "Training Step: 2116  | total loss: \u001b[1m\u001b[32m0.01958\u001b[0m\u001b[0m | time: 0.191s\n",
      "| SGD | epoch: 036 | loss: 0.01958 - R2: 1.0002 -- iter: 1020/1168\n",
      "Training Step: 2117  | total loss: \u001b[1m\u001b[32m0.01868\u001b[0m\u001b[0m | time: 0.193s\n",
      "| SGD | epoch: 036 | loss: 0.01868 - R2: 1.0002 -- iter: 1040/1168\n",
      "Training Step: 2118  | total loss: \u001b[1m\u001b[32m0.01893\u001b[0m\u001b[0m | time: 0.196s\n",
      "| SGD | epoch: 036 | loss: 0.01893 - R2: 0.9997 -- iter: 1060/1168\n",
      "Training Step: 2119  | total loss: \u001b[1m\u001b[32m0.01893\u001b[0m\u001b[0m | time: 0.203s\n",
      "| SGD | epoch: 036 | loss: 0.01893 - R2: 0.9997 -- iter: 1080/1168\n",
      "Training Step: 2120  | total loss: \u001b[1m\u001b[32m0.01817\u001b[0m\u001b[0m | time: 0.204s\n",
      "| SGD | epoch: 036 | loss: 0.01817 - R2: 0.9998 -- iter: 1100/1168\n",
      "Training Step: 2121  | total loss: \u001b[1m\u001b[32m0.01794\u001b[0m\u001b[0m | time: 0.207s\n",
      "| SGD | epoch: 036 | loss: 0.01794 - R2: 0.9998 -- iter: 1120/1168\n",
      "Training Step: 2122  | total loss: \u001b[1m\u001b[32m0.01779\u001b[0m\u001b[0m | time: 0.209s\n",
      "| SGD | epoch: 036 | loss: 0.01779 - R2: 0.9989 -- iter: 1140/1168\n",
      "Training Step: 2123  | total loss: \u001b[1m\u001b[32m0.01785\u001b[0m\u001b[0m | time: 0.211s\n",
      "| SGD | epoch: 036 | loss: 0.01785 - R2: 0.9989 -- iter: 1160/1168\n",
      "Training Step: 2124  | total loss: \u001b[1m\u001b[32m0.01737\u001b[0m\u001b[0m | time: 1.217s\n",
      "| SGD | epoch: 036 | loss: 0.01737 - R2: 0.9987 | val_loss: 0.03231 - val_acc: 1.0017 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 2125  | total loss: \u001b[1m\u001b[32m0.01707\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 037 | loss: 0.01707 - R2: 0.9987 -- iter: 0020/1168\n",
      "Training Step: 2126  | total loss: \u001b[1m\u001b[32m0.01614\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 037 | loss: 0.01614 - R2: 0.9986 -- iter: 0040/1168\n",
      "Training Step: 2127  | total loss: \u001b[1m\u001b[32m0.01711\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 037 | loss: 0.01711 - R2: 0.9987 -- iter: 0060/1168\n",
      "Training Step: 2128  | total loss: \u001b[1m\u001b[32m0.05627\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 037 | loss: 0.05627 - R2: 0.9982 -- iter: 0080/1168\n",
      "Training Step: 2129  | total loss: \u001b[1m\u001b[32m0.05187\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 037 | loss: 0.05187 - R2: 0.9981 -- iter: 0100/1168\n",
      "Training Step: 2130  | total loss: \u001b[1m\u001b[32m0.04824\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 037 | loss: 0.04824 - R2: 0.9983 -- iter: 0120/1168\n",
      "Training Step: 2131  | total loss: \u001b[1m\u001b[32m0.04443\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 037 | loss: 0.04443 - R2: 0.9989 -- iter: 0140/1168\n",
      "Training Step: 2132  | total loss: \u001b[1m\u001b[32m0.04054\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 037 | loss: 0.04054 - R2: 0.9990 -- iter: 0160/1168\n",
      "Training Step: 2133  | total loss: \u001b[1m\u001b[32m0.04077\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 037 | loss: 0.04077 - R2: 0.9998 -- iter: 0180/1168\n",
      "Training Step: 2134  | total loss: \u001b[1m\u001b[32m0.03762\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 037 | loss: 0.03762 - R2: 0.9997 -- iter: 0200/1168\n",
      "Training Step: 2135  | total loss: \u001b[1m\u001b[32m0.03527\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 037 | loss: 0.03527 - R2: 0.9989 -- iter: 0220/1168\n",
      "Training Step: 2136  | total loss: \u001b[1m\u001b[32m0.03400\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 037 | loss: 0.03400 - R2: 0.9996 -- iter: 0240/1168\n",
      "Training Step: 2137  | total loss: \u001b[1m\u001b[32m0.03207\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 037 | loss: 0.03207 - R2: 0.9998 -- iter: 0260/1168\n",
      "Training Step: 2138  | total loss: \u001b[1m\u001b[32m0.03207\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 037 | loss: 0.03207 - R2: 0.9998 -- iter: 0280/1168\n",
      "Training Step: 2139  | total loss: \u001b[1m\u001b[32m0.03188\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 037 | loss: 0.03188 - R2: 1.0002 -- iter: 0300/1168\n",
      "Training Step: 2140  | total loss: \u001b[1m\u001b[32m0.03010\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 037 | loss: 0.03010 - R2: 0.9991 -- iter: 0320/1168\n",
      "Training Step: 2141  | total loss: \u001b[1m\u001b[32m0.03010\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 037 | loss: 0.03010 - R2: 0.9991 -- iter: 0340/1168\n",
      "Training Step: 2142  | total loss: \u001b[1m\u001b[32m0.02923\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 037 | loss: 0.02923 - R2: 0.9989 -- iter: 0360/1168\n",
      "Training Step: 2143  | total loss: \u001b[1m\u001b[32m0.02745\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 037 | loss: 0.02745 - R2: 0.9986 -- iter: 0380/1168\n",
      "Training Step: 2144  | total loss: \u001b[1m\u001b[32m0.02599\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 037 | loss: 0.02599 - R2: 0.9991 -- iter: 0400/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2145  | total loss: \u001b[1m\u001b[32m0.02623\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 037 | loss: 0.02623 - R2: 0.9987 -- iter: 0420/1168\n",
      "Training Step: 2146  | total loss: \u001b[1m\u001b[32m0.02492\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 037 | loss: 0.02492 - R2: 0.9995 -- iter: 0440/1168\n",
      "Training Step: 2147  | total loss: \u001b[1m\u001b[32m0.02506\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 037 | loss: 0.02506 - R2: 0.9995 -- iter: 0460/1168\n",
      "Training Step: 2148  | total loss: \u001b[1m\u001b[32m0.02684\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 037 | loss: 0.02684 - R2: 0.9996 -- iter: 0480/1168\n",
      "Training Step: 2149  | total loss: \u001b[1m\u001b[32m0.02476\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 037 | loss: 0.02476 - R2: 1.0007 -- iter: 0500/1168\n",
      "Training Step: 2150  | total loss: \u001b[1m\u001b[32m0.02735\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 037 | loss: 0.02735 - R2: 1.0007 -- iter: 0520/1168\n",
      "Training Step: 2151  | total loss: \u001b[1m\u001b[32m0.02635\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 037 | loss: 0.02635 - R2: 1.0010 -- iter: 0540/1168\n",
      "Training Step: 2152  | total loss: \u001b[1m\u001b[32m0.02478\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 037 | loss: 0.02478 - R2: 1.0007 -- iter: 0560/1168\n",
      "Training Step: 2153  | total loss: \u001b[1m\u001b[32m0.02338\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 037 | loss: 0.02338 - R2: 1.0011 -- iter: 0580/1168\n",
      "Training Step: 2154  | total loss: \u001b[1m\u001b[32m0.02195\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 037 | loss: 0.02195 - R2: 1.0012 -- iter: 0600/1168\n",
      "Training Step: 2155  | total loss: \u001b[1m\u001b[32m0.02112\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 037 | loss: 0.02112 - R2: 1.0019 -- iter: 0620/1168\n",
      "Training Step: 2156  | total loss: \u001b[1m\u001b[32m0.02074\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 037 | loss: 0.02074 - R2: 1.0011 -- iter: 0640/1168\n",
      "Training Step: 2157  | total loss: \u001b[1m\u001b[32m0.02032\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 037 | loss: 0.02032 - R2: 1.0003 -- iter: 0660/1168\n",
      "Training Step: 2158  | total loss: \u001b[1m\u001b[32m0.02032\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 037 | loss: 0.02032 - R2: 1.0003 -- iter: 0680/1168\n",
      "Training Step: 2159  | total loss: \u001b[1m\u001b[32m0.02012\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 037 | loss: 0.02012 - R2: 1.0003 -- iter: 0700/1168\n",
      "Training Step: 2160  | total loss: \u001b[1m\u001b[32m0.01839\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 037 | loss: 0.01839 - R2: 1.0010 -- iter: 0720/1168\n",
      "Training Step: 2161  | total loss: \u001b[1m\u001b[32m0.01844\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 037 | loss: 0.01844 - R2: 1.0015 -- iter: 0740/1168\n",
      "Training Step: 2162  | total loss: \u001b[1m\u001b[32m0.01844\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 037 | loss: 0.01844 - R2: 1.0015 -- iter: 0760/1168\n",
      "Training Step: 2163  | total loss: \u001b[1m\u001b[32m0.01765\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 037 | loss: 0.01765 - R2: 1.0015 -- iter: 0780/1168\n",
      "Training Step: 2164  | total loss: \u001b[1m\u001b[32m0.01912\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 037 | loss: 0.01912 - R2: 1.0017 -- iter: 0800/1168\n",
      "Training Step: 2165  | total loss: \u001b[1m\u001b[32m0.01819\u001b[0m\u001b[0m | time: 0.169s\n",
      "| SGD | epoch: 037 | loss: 0.01819 - R2: 1.0008 -- iter: 0820/1168\n",
      "Training Step: 2166  | total loss: \u001b[1m\u001b[32m0.01819\u001b[0m\u001b[0m | time: 0.171s\n",
      "| SGD | epoch: 037 | loss: 0.01819 - R2: 1.0008 -- iter: 0840/1168\n",
      "Training Step: 2167  | total loss: \u001b[1m\u001b[32m0.01818\u001b[0m\u001b[0m | time: 0.174s\n",
      "| SGD | epoch: 037 | loss: 0.01818 - R2: 1.0010 -- iter: 0860/1168\n",
      "Training Step: 2168  | total loss: \u001b[1m\u001b[32m0.01617\u001b[0m\u001b[0m | time: 0.176s\n",
      "| SGD | epoch: 037 | loss: 0.01617 - R2: 1.0008 -- iter: 0880/1168\n",
      "Training Step: 2169  | total loss: \u001b[1m\u001b[32m0.01617\u001b[0m\u001b[0m | time: 0.186s\n",
      "| SGD | epoch: 037 | loss: 0.01617 - R2: 1.0008 -- iter: 0900/1168\n",
      "Training Step: 2170  | total loss: \u001b[1m\u001b[32m0.01553\u001b[0m\u001b[0m | time: 0.188s\n",
      "| SGD | epoch: 037 | loss: 0.01553 - R2: 1.0009 -- iter: 0920/1168\n",
      "Training Step: 2171  | total loss: \u001b[1m\u001b[32m0.01553\u001b[0m\u001b[0m | time: 0.191s\n",
      "| SGD | epoch: 037 | loss: 0.01553 - R2: 1.0009 -- iter: 0940/1168\n",
      "Training Step: 2172  | total loss: \u001b[1m\u001b[32m0.01560\u001b[0m\u001b[0m | time: 0.193s\n",
      "| SGD | epoch: 037 | loss: 0.01560 - R2: 1.0009 -- iter: 0960/1168\n",
      "Training Step: 2173  | total loss: \u001b[1m\u001b[32m0.01627\u001b[0m\u001b[0m | time: 0.195s\n",
      "| SGD | epoch: 037 | loss: 0.01627 - R2: 1.0007 -- iter: 0980/1168\n",
      "Training Step: 2174  | total loss: \u001b[1m\u001b[32m0.01789\u001b[0m\u001b[0m | time: 0.206s\n",
      "| SGD | epoch: 037 | loss: 0.01789 - R2: 1.0002 -- iter: 1000/1168\n",
      "Training Step: 2175  | total loss: \u001b[1m\u001b[32m0.01770\u001b[0m\u001b[0m | time: 0.208s\n",
      "| SGD | epoch: 037 | loss: 0.01770 - R2: 1.0003 -- iter: 1020/1168\n",
      "Training Step: 2176  | total loss: \u001b[1m\u001b[32m0.01723\u001b[0m\u001b[0m | time: 0.210s\n",
      "| SGD | epoch: 037 | loss: 0.01723 - R2: 1.0004 -- iter: 1040/1168\n",
      "Training Step: 2177  | total loss: \u001b[1m\u001b[32m0.01555\u001b[0m\u001b[0m | time: 0.213s\n",
      "| SGD | epoch: 037 | loss: 0.01555 - R2: 1.0005 -- iter: 1060/1168\n",
      "Training Step: 2178  | total loss: \u001b[1m\u001b[32m0.01555\u001b[0m\u001b[0m | time: 0.215s\n",
      "| SGD | epoch: 037 | loss: 0.01555 - R2: 1.0005 -- iter: 1080/1168\n",
      "Training Step: 2179  | total loss: \u001b[1m\u001b[32m0.01615\u001b[0m\u001b[0m | time: 0.219s\n",
      "| SGD | epoch: 037 | loss: 0.01615 - R2: 0.9994 -- iter: 1100/1168\n",
      "Training Step: 2180  | total loss: \u001b[1m\u001b[32m0.01528\u001b[0m\u001b[0m | time: 0.221s\n",
      "| SGD | epoch: 037 | loss: 0.01528 - R2: 0.9992 -- iter: 1120/1168\n",
      "Training Step: 2181  | total loss: \u001b[1m\u001b[32m0.01470\u001b[0m\u001b[0m | time: 0.223s\n",
      "| SGD | epoch: 037 | loss: 0.01470 - R2: 0.9985 -- iter: 1140/1168\n",
      "Training Step: 2182  | total loss: \u001b[1m\u001b[32m0.01567\u001b[0m\u001b[0m | time: 0.226s\n",
      "| SGD | epoch: 037 | loss: 0.01567 - R2: 0.9985 -- iter: 1160/1168\n",
      "Training Step: 2183  | total loss: \u001b[1m\u001b[32m0.01537\u001b[0m\u001b[0m | time: 1.232s\n",
      "| SGD | epoch: 037 | loss: 0.01537 - R2: 0.9983 | val_loss: 0.03215 - val_acc: 1.0015 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 2184  | total loss: \u001b[1m\u001b[32m0.01502\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 038 | loss: 0.01502 - R2: 0.9985 -- iter: 0020/1168\n",
      "Training Step: 2185  | total loss: \u001b[1m\u001b[32m0.01462\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 038 | loss: 0.01462 - R2: 0.9988 -- iter: 0040/1168\n",
      "Training Step: 2186  | total loss: \u001b[1m\u001b[32m0.01439\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 038 | loss: 0.01439 - R2: 0.9997 -- iter: 0060/1168\n",
      "Training Step: 2187  | total loss: \u001b[1m\u001b[32m0.02124\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 038 | loss: 0.02124 - R2: 1.0006 -- iter: 0080/1168\n",
      "Training Step: 2188  | total loss: \u001b[1m\u001b[32m0.02124\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 038 | loss: 0.02124 - R2: 1.0006 -- iter: 0100/1168\n",
      "Training Step: 2189  | total loss: \u001b[1m\u001b[32m0.02041\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 038 | loss: 0.02041 - R2: 0.9996 -- iter: 0120/1168\n",
      "Training Step: 2190  | total loss: \u001b[1m\u001b[32m0.02041\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 038 | loss: 0.02041 - R2: 0.9996 -- iter: 0140/1168\n",
      "Training Step: 2191  | total loss: \u001b[1m\u001b[32m0.01964\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 038 | loss: 0.01964 - R2: 0.9995 -- iter: 0160/1168\n",
      "Training Step: 2192  | total loss: \u001b[1m\u001b[32m0.02226\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 038 | loss: 0.02226 - R2: 1.0001 -- iter: 0180/1168\n",
      "Training Step: 2193  | total loss: \u001b[1m\u001b[32m0.02226\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 038 | loss: 0.02226 - R2: 1.0001 -- iter: 0200/1168\n",
      "Training Step: 2194  | total loss: \u001b[1m\u001b[32m0.02237\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 038 | loss: 0.02237 - R2: 1.0013 -- iter: 0220/1168\n",
      "Training Step: 2195  | total loss: \u001b[1m\u001b[32m0.02176\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 038 | loss: 0.02176 - R2: 1.0008 -- iter: 0240/1168\n",
      "Training Step: 2196  | total loss: \u001b[1m\u001b[32m0.02176\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 038 | loss: 0.02176 - R2: 1.0008 -- iter: 0260/1168\n",
      "Training Step: 2197  | total loss: \u001b[1m\u001b[32m0.01966\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 038 | loss: 0.01966 - R2: 1.0010 -- iter: 0280/1168\n",
      "Training Step: 2198  | total loss: \u001b[1m\u001b[32m0.01966\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 038 | loss: 0.01966 - R2: 1.0010 -- iter: 0300/1168\n",
      "Training Step: 2199  | total loss: \u001b[1m\u001b[32m0.01927\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 038 | loss: 0.01927 - R2: 1.0007 -- iter: 0320/1168\n",
      "Training Step: 2200  | total loss: \u001b[1m\u001b[32m0.01940\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 038 | loss: 0.01940 - R2: 1.0016 -- iter: 0340/1168\n",
      "Training Step: 2201  | total loss: \u001b[1m\u001b[32m0.01940\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 038 | loss: 0.01940 - R2: 1.0016 -- iter: 0360/1168\n",
      "Training Step: 2202  | total loss: \u001b[1m\u001b[32m0.01841\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 038 | loss: 0.01841 - R2: 1.0009 -- iter: 0380/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2203  | total loss: \u001b[1m\u001b[32m0.01900\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 038 | loss: 0.01900 - R2: 0.9998 -- iter: 0400/1168\n",
      "Training Step: 2204  | total loss: \u001b[1m\u001b[32m0.02211\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 038 | loss: 0.02211 - R2: 1.0002 -- iter: 0420/1168\n",
      "Training Step: 2205  | total loss: \u001b[1m\u001b[32m0.02105\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 038 | loss: 0.02105 - R2: 1.0000 -- iter: 0440/1168\n",
      "Training Step: 2206  | total loss: \u001b[1m\u001b[32m0.02105\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 038 | loss: 0.02105 - R2: 1.0000 -- iter: 0460/1168\n",
      "Training Step: 2207  | total loss: \u001b[1m\u001b[32m0.01927\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 038 | loss: 0.01927 - R2: 0.9998 -- iter: 0480/1168\n",
      "Training Step: 2208  | total loss: \u001b[1m\u001b[32m0.01927\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 038 | loss: 0.01927 - R2: 0.9998 -- iter: 0500/1168\n",
      "Training Step: 2209  | total loss: \u001b[1m\u001b[32m0.01742\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 038 | loss: 0.01742 - R2: 0.9999 -- iter: 0520/1168\n",
      "Training Step: 2210  | total loss: \u001b[1m\u001b[32m0.02455\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 038 | loss: 0.02455 - R2: 1.0011 -- iter: 0540/1168\n",
      "Training Step: 2211  | total loss: \u001b[1m\u001b[32m0.02393\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 038 | loss: 0.02393 - R2: 1.0002 -- iter: 0560/1168\n",
      "Training Step: 2212  | total loss: \u001b[1m\u001b[32m0.02288\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 038 | loss: 0.02288 - R2: 0.9996 -- iter: 0580/1168\n",
      "Training Step: 2213  | total loss: \u001b[1m\u001b[32m0.02288\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 038 | loss: 0.02288 - R2: 0.9996 -- iter: 0600/1168\n",
      "Training Step: 2214  | total loss: \u001b[1m\u001b[32m0.02169\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 038 | loss: 0.02169 - R2: 0.9999 -- iter: 0620/1168\n",
      "Training Step: 2215  | total loss: \u001b[1m\u001b[32m0.02037\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 038 | loss: 0.02037 - R2: 0.9996 -- iter: 0640/1168\n",
      "Training Step: 2216  | total loss: \u001b[1m\u001b[32m0.01940\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 038 | loss: 0.01940 - R2: 0.9992 -- iter: 0660/1168\n",
      "Training Step: 2217  | total loss: \u001b[1m\u001b[32m0.01902\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 038 | loss: 0.01902 - R2: 0.9993 -- iter: 0680/1168\n",
      "Training Step: 2218  | total loss: \u001b[1m\u001b[32m0.01750\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 038 | loss: 0.01750 - R2: 0.9998 -- iter: 0700/1168\n",
      "Training Step: 2219  | total loss: \u001b[1m\u001b[32m0.01750\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 038 | loss: 0.01750 - R2: 0.9998 -- iter: 0720/1168\n",
      "Training Step: 2220  | total loss: \u001b[1m\u001b[32m0.01607\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 038 | loss: 0.01607 - R2: 0.9990 -- iter: 0740/1168\n",
      "Training Step: 2221  | total loss: \u001b[1m\u001b[32m0.01580\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 038 | loss: 0.01580 - R2: 0.9986 -- iter: 0760/1168\n",
      "Training Step: 2222  | total loss: \u001b[1m\u001b[32m0.01580\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 038 | loss: 0.01580 - R2: 0.9993 -- iter: 0780/1168\n",
      "Training Step: 2223  | total loss: \u001b[1m\u001b[32m0.01521\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 038 | loss: 0.01521 - R2: 0.9993 -- iter: 0800/1168\n",
      "Training Step: 2224  | total loss: \u001b[1m\u001b[32m0.01360\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 038 | loss: 0.01360 - R2: 0.9989 -- iter: 0820/1168\n",
      "Training Step: 2225  | total loss: \u001b[1m\u001b[32m0.01484\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 038 | loss: 0.01484 - R2: 0.9987 -- iter: 0840/1168\n",
      "Training Step: 2226  | total loss: \u001b[1m\u001b[32m0.01484\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 038 | loss: 0.01484 - R2: 0.9987 -- iter: 0860/1168\n",
      "Training Step: 2227  | total loss: \u001b[1m\u001b[32m0.01544\u001b[0m\u001b[0m | time: 0.172s\n",
      "| SGD | epoch: 038 | loss: 0.01544 - R2: 0.9990 -- iter: 0880/1168\n",
      "Training Step: 2228  | total loss: \u001b[1m\u001b[32m0.01544\u001b[0m\u001b[0m | time: 0.174s\n",
      "| SGD | epoch: 038 | loss: 0.01544 - R2: 0.9990 -- iter: 0900/1168\n",
      "Training Step: 2229  | total loss: \u001b[1m\u001b[32m0.01528\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 038 | loss: 0.01528 - R2: 0.9983 -- iter: 0920/1168\n",
      "Training Step: 2230  | total loss: \u001b[1m\u001b[32m0.01635\u001b[0m\u001b[0m | time: 0.181s\n",
      "| SGD | epoch: 038 | loss: 0.01635 - R2: 0.9982 -- iter: 0940/1168\n",
      "Training Step: 2231  | total loss: \u001b[1m\u001b[32m0.01552\u001b[0m\u001b[0m | time: 0.183s\n",
      "| SGD | epoch: 038 | loss: 0.01552 - R2: 0.9982 -- iter: 0960/1168\n",
      "Training Step: 2232  | total loss: \u001b[1m\u001b[32m0.01614\u001b[0m\u001b[0m | time: 0.186s\n",
      "| SGD | epoch: 038 | loss: 0.01614 - R2: 0.9993 -- iter: 0980/1168\n",
      "Training Step: 2233  | total loss: \u001b[1m\u001b[32m0.01614\u001b[0m\u001b[0m | time: 0.187s\n",
      "| SGD | epoch: 038 | loss: 0.01614 - R2: 0.9993 -- iter: 1000/1168\n",
      "Training Step: 2234  | total loss: \u001b[1m\u001b[32m0.01536\u001b[0m\u001b[0m | time: 0.190s\n",
      "| SGD | epoch: 038 | loss: 0.01536 - R2: 0.9998 -- iter: 1020/1168\n",
      "Training Step: 2235  | total loss: \u001b[1m\u001b[32m0.01536\u001b[0m\u001b[0m | time: 0.193s\n",
      "| SGD | epoch: 038 | loss: 0.01536 - R2: 0.9998 -- iter: 1040/1168\n",
      "Training Step: 2236  | total loss: \u001b[1m\u001b[32m0.01556\u001b[0m\u001b[0m | time: 0.195s\n",
      "| SGD | epoch: 038 | loss: 0.01556 - R2: 0.9998 -- iter: 1060/1168\n",
      "Training Step: 2237  | total loss: \u001b[1m\u001b[32m0.01508\u001b[0m\u001b[0m | time: 0.197s\n",
      "| SGD | epoch: 038 | loss: 0.01508 - R2: 1.0000 -- iter: 1080/1168\n",
      "Training Step: 2238  | total loss: \u001b[1m\u001b[32m0.01730\u001b[0m\u001b[0m | time: 0.199s\n",
      "| SGD | epoch: 038 | loss: 0.01730 - R2: 0.9998 -- iter: 1100/1168\n",
      "Training Step: 2239  | total loss: \u001b[1m\u001b[32m0.01901\u001b[0m\u001b[0m | time: 0.212s\n",
      "| SGD | epoch: 038 | loss: 0.01901 - R2: 1.0001 -- iter: 1120/1168\n",
      "Training Step: 2240  | total loss: \u001b[1m\u001b[32m0.01809\u001b[0m\u001b[0m | time: 0.215s\n",
      "| SGD | epoch: 038 | loss: 0.01809 - R2: 1.0004 -- iter: 1140/1168\n",
      "Training Step: 2241  | total loss: \u001b[1m\u001b[32m0.01761\u001b[0m\u001b[0m | time: 0.232s\n",
      "| SGD | epoch: 038 | loss: 0.01761 - R2: 1.0006 -- iter: 1160/1168\n",
      "Training Step: 2242  | total loss: \u001b[1m\u001b[32m0.02039\u001b[0m\u001b[0m | time: 1.238s\n",
      "| SGD | epoch: 038 | loss: 0.02039 - R2: 1.0007 | val_loss: 0.03204 - val_acc: 1.0008 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 2243  | total loss: \u001b[1m\u001b[32m0.02039\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 039 | loss: 0.02039 - R2: 1.0007 -- iter: 0020/1168\n",
      "Training Step: 2244  | total loss: \u001b[1m\u001b[32m0.02022\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 039 | loss: 0.02022 - R2: 1.0007 -- iter: 0040/1168\n",
      "Training Step: 2245  | total loss: \u001b[1m\u001b[32m0.02009\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 039 | loss: 0.02009 - R2: 0.9996 -- iter: 0060/1168\n",
      "Training Step: 2246  | total loss: \u001b[1m\u001b[32m0.01896\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 039 | loss: 0.01896 - R2: 0.9995 -- iter: 0080/1168\n",
      "Training Step: 2247  | total loss: \u001b[1m\u001b[32m0.01896\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 039 | loss: 0.01896 - R2: 0.9995 -- iter: 0100/1168\n",
      "Training Step: 2248  | total loss: \u001b[1m\u001b[32m0.01879\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 039 | loss: 0.01879 - R2: 1.0001 -- iter: 0120/1168\n",
      "Training Step: 2249  | total loss: \u001b[1m\u001b[32m0.01951\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 039 | loss: 0.01951 - R2: 1.0008 -- iter: 0140/1168\n",
      "Training Step: 2250  | total loss: \u001b[1m\u001b[32m0.01951\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 039 | loss: 0.01951 - R2: 1.0008 -- iter: 0160/1168\n",
      "Training Step: 2251  | total loss: \u001b[1m\u001b[32m0.01892\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 039 | loss: 0.01892 - R2: 1.0012 -- iter: 0180/1168\n",
      "Training Step: 2252  | total loss: \u001b[1m\u001b[32m0.01786\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 039 | loss: 0.01786 - R2: 1.0011 -- iter: 0200/1168\n",
      "Training Step: 2253  | total loss: \u001b[1m\u001b[32m0.01986\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 039 | loss: 0.01986 - R2: 1.0011 -- iter: 0220/1168\n",
      "Training Step: 2254  | total loss: \u001b[1m\u001b[32m0.01844\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 039 | loss: 0.01844 - R2: 1.0016 -- iter: 0240/1168\n",
      "Training Step: 2255  | total loss: \u001b[1m\u001b[32m0.01844\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 039 | loss: 0.01844 - R2: 1.0016 -- iter: 0260/1168\n",
      "Training Step: 2256  | total loss: \u001b[1m\u001b[32m0.01905\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 039 | loss: 0.01905 - R2: 1.0023 -- iter: 0280/1168\n",
      "Training Step: 2257  | total loss: \u001b[1m\u001b[32m0.01857\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 039 | loss: 0.01857 - R2: 1.0020 -- iter: 0300/1168\n",
      "Training Step: 2258  | total loss: \u001b[1m\u001b[32m0.02017\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 039 | loss: 0.02017 - R2: 1.0015 -- iter: 0320/1168\n",
      "Training Step: 2259  | total loss: \u001b[1m\u001b[32m0.02017\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 039 | loss: 0.02017 - R2: 1.0015 -- iter: 0340/1168\n",
      "Training Step: 2260  | total loss: \u001b[1m\u001b[32m0.02325\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 039 | loss: 0.02325 - R2: 1.0024 -- iter: 0360/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2261  | total loss: \u001b[1m\u001b[32m0.02074\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 039 | loss: 0.02074 - R2: 1.0013 -- iter: 0380/1168\n",
      "Training Step: 2262  | total loss: \u001b[1m\u001b[32m0.02074\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 039 | loss: 0.02074 - R2: 1.0004 -- iter: 0400/1168\n",
      "Training Step: 2263  | total loss: \u001b[1m\u001b[32m0.01978\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 039 | loss: 0.01978 - R2: 1.0004 -- iter: 0420/1168\n",
      "Training Step: 2264  | total loss: \u001b[1m\u001b[32m0.01964\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 039 | loss: 0.01964 - R2: 0.9998 -- iter: 0440/1168\n",
      "Training Step: 2265  | total loss: \u001b[1m\u001b[32m0.01964\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 039 | loss: 0.01964 - R2: 0.9998 -- iter: 0460/1168\n",
      "Training Step: 2266  | total loss: \u001b[1m\u001b[32m0.01822\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 039 | loss: 0.01822 - R2: 0.9994 -- iter: 0480/1168\n",
      "Training Step: 2267  | total loss: \u001b[1m\u001b[32m0.01822\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 039 | loss: 0.01822 - R2: 0.9994 -- iter: 0500/1168\n",
      "Training Step: 2268  | total loss: \u001b[1m\u001b[32m0.01959\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 039 | loss: 0.01959 - R2: 0.9998 -- iter: 0520/1168\n",
      "Training Step: 2269  | total loss: \u001b[1m\u001b[32m0.01876\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 039 | loss: 0.01876 - R2: 0.9995 -- iter: 0540/1168\n",
      "Training Step: 2270  | total loss: \u001b[1m\u001b[32m0.01991\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 039 | loss: 0.01991 - R2: 0.9999 -- iter: 0560/1168\n",
      "Training Step: 2271  | total loss: \u001b[1m\u001b[32m0.01901\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 039 | loss: 0.01901 - R2: 0.9996 -- iter: 0580/1168\n",
      "Training Step: 2272  | total loss: \u001b[1m\u001b[32m0.01840\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 039 | loss: 0.01840 - R2: 0.9996 -- iter: 0600/1168\n",
      "Training Step: 2273  | total loss: \u001b[1m\u001b[32m0.01840\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 039 | loss: 0.01840 - R2: 0.9996 -- iter: 0620/1168\n",
      "Training Step: 2274  | total loss: \u001b[1m\u001b[32m0.02112\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 039 | loss: 0.02112 - R2: 0.9996 -- iter: 0640/1168\n",
      "Training Step: 2275  | total loss: \u001b[1m\u001b[32m0.02112\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 039 | loss: 0.02112 - R2: 1.0003 -- iter: 0660/1168\n",
      "Training Step: 2276  | total loss: \u001b[1m\u001b[32m0.02060\u001b[0m\u001b[0m | time: 0.179s\n",
      "| SGD | epoch: 039 | loss: 0.02060 - R2: 1.0005 -- iter: 0680/1168\n",
      "Training Step: 2277  | total loss: \u001b[1m\u001b[32m0.02060\u001b[0m\u001b[0m | time: 0.181s\n",
      "| SGD | epoch: 039 | loss: 0.02060 - R2: 1.0005 -- iter: 0700/1168\n",
      "Training Step: 2278  | total loss: \u001b[1m\u001b[32m0.01874\u001b[0m\u001b[0m | time: 0.183s\n",
      "| SGD | epoch: 039 | loss: 0.01874 - R2: 1.0009 -- iter: 0720/1168\n",
      "Training Step: 2279  | total loss: \u001b[1m\u001b[32m0.01874\u001b[0m\u001b[0m | time: 0.185s\n",
      "| SGD | epoch: 039 | loss: 0.01874 - R2: 1.0009 -- iter: 0740/1168\n",
      "Training Step: 2280  | total loss: \u001b[1m\u001b[32m0.01757\u001b[0m\u001b[0m | time: 0.187s\n",
      "| SGD | epoch: 039 | loss: 0.01757 - R2: 1.0009 -- iter: 0760/1168\n",
      "Training Step: 2281  | total loss: \u001b[1m\u001b[32m0.01649\u001b[0m\u001b[0m | time: 0.189s\n",
      "| SGD | epoch: 039 | loss: 0.01649 - R2: 1.0009 -- iter: 0780/1168\n",
      "Training Step: 2282  | total loss: \u001b[1m\u001b[32m0.01582\u001b[0m\u001b[0m | time: 0.192s\n",
      "| SGD | epoch: 039 | loss: 0.01582 - R2: 1.0009 -- iter: 0800/1168\n",
      "Training Step: 2283  | total loss: \u001b[1m\u001b[32m0.01547\u001b[0m\u001b[0m | time: 0.194s\n",
      "| SGD | epoch: 039 | loss: 0.01547 - R2: 1.0011 -- iter: 0820/1168\n",
      "Training Step: 2284  | total loss: \u001b[1m\u001b[32m0.01635\u001b[0m\u001b[0m | time: 0.196s\n",
      "| SGD | epoch: 039 | loss: 0.01635 - R2: 1.0006 -- iter: 0840/1168\n",
      "Training Step: 2285  | total loss: \u001b[1m\u001b[32m0.01526\u001b[0m\u001b[0m | time: 0.199s\n",
      "| SGD | epoch: 039 | loss: 0.01526 - R2: 0.9997 -- iter: 0860/1168\n",
      "Training Step: 2286  | total loss: \u001b[1m\u001b[32m0.01593\u001b[0m\u001b[0m | time: 0.207s\n",
      "| SGD | epoch: 039 | loss: 0.01593 - R2: 0.9997 -- iter: 0880/1168\n",
      "Training Step: 2287  | total loss: \u001b[1m\u001b[32m0.01635\u001b[0m\u001b[0m | time: 0.222s\n",
      "| SGD | epoch: 039 | loss: 0.01635 - R2: 0.9992 -- iter: 0900/1168\n",
      "Training Step: 2288  | total loss: \u001b[1m\u001b[32m0.01694\u001b[0m\u001b[0m | time: 0.224s\n",
      "| SGD | epoch: 039 | loss: 0.01694 - R2: 0.9989 -- iter: 0920/1168\n",
      "Training Step: 2289  | total loss: \u001b[1m\u001b[32m0.02324\u001b[0m\u001b[0m | time: 0.227s\n",
      "| SGD | epoch: 039 | loss: 0.02324 - R2: 1.0006 -- iter: 0940/1168\n",
      "Training Step: 2290  | total loss: \u001b[1m\u001b[32m0.02239\u001b[0m\u001b[0m | time: 0.229s\n",
      "| SGD | epoch: 039 | loss: 0.02239 - R2: 1.0006 -- iter: 0960/1168\n",
      "Training Step: 2291  | total loss: \u001b[1m\u001b[32m0.02065\u001b[0m\u001b[0m | time: 0.231s\n",
      "| SGD | epoch: 039 | loss: 0.02065 - R2: 1.0006 -- iter: 0980/1168\n",
      "Training Step: 2292  | total loss: \u001b[1m\u001b[32m0.02014\u001b[0m\u001b[0m | time: 0.234s\n",
      "| SGD | epoch: 039 | loss: 0.02014 - R2: 1.0001 -- iter: 1000/1168\n",
      "Training Step: 2293  | total loss: \u001b[1m\u001b[32m0.01921\u001b[0m\u001b[0m | time: 0.236s\n",
      "| SGD | epoch: 039 | loss: 0.01921 - R2: 0.9997 -- iter: 1020/1168\n",
      "Training Step: 2294  | total loss: \u001b[1m\u001b[32m0.01891\u001b[0m\u001b[0m | time: 0.238s\n",
      "| SGD | epoch: 039 | loss: 0.01891 - R2: 0.9989 -- iter: 1040/1168\n",
      "Training Step: 2295  | total loss: \u001b[1m\u001b[32m0.01758\u001b[0m\u001b[0m | time: 0.240s\n",
      "| SGD | epoch: 039 | loss: 0.01758 - R2: 0.9986 -- iter: 1060/1168\n",
      "Training Step: 2296  | total loss: \u001b[1m\u001b[32m0.01667\u001b[0m\u001b[0m | time: 0.242s\n",
      "| SGD | epoch: 039 | loss: 0.01667 - R2: 0.9986 -- iter: 1080/1168\n",
      "Training Step: 2297  | total loss: \u001b[1m\u001b[32m0.01594\u001b[0m\u001b[0m | time: 0.244s\n",
      "| SGD | epoch: 039 | loss: 0.01594 - R2: 0.9986 -- iter: 1100/1168\n",
      "Training Step: 2298  | total loss: \u001b[1m\u001b[32m0.01576\u001b[0m\u001b[0m | time: 0.258s\n",
      "| SGD | epoch: 039 | loss: 0.01576 - R2: 0.9983 -- iter: 1120/1168\n",
      "Training Step: 2299  | total loss: \u001b[1m\u001b[32m0.01826\u001b[0m\u001b[0m | time: 0.261s\n",
      "| SGD | epoch: 039 | loss: 0.01826 - R2: 0.9979 -- iter: 1140/1168\n",
      "Training Step: 2300  | total loss: \u001b[1m\u001b[32m0.01770\u001b[0m\u001b[0m | time: 0.277s\n",
      "| SGD | epoch: 039 | loss: 0.01770 - R2: 0.9977 -- iter: 1160/1168\n",
      "Training Step: 2301  | total loss: \u001b[1m\u001b[32m0.01828\u001b[0m\u001b[0m | time: 1.287s\n",
      "| SGD | epoch: 039 | loss: 0.01828 - R2: 0.9976 | val_loss: 0.03213 - val_acc: 1.0014 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 2302  | total loss: \u001b[1m\u001b[32m0.01847\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 040 | loss: 0.01847 - R2: 0.9979 -- iter: 0020/1168\n",
      "Training Step: 2303  | total loss: \u001b[1m\u001b[32m0.01759\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 040 | loss: 0.01759 - R2: 0.9977 -- iter: 0040/1168\n",
      "Training Step: 2304  | total loss: \u001b[1m\u001b[32m0.01683\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 040 | loss: 0.01683 - R2: 0.9977 -- iter: 0060/1168\n",
      "Training Step: 2305  | total loss: \u001b[1m\u001b[32m0.01695\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 040 | loss: 0.01695 - R2: 0.9976 -- iter: 0080/1168\n",
      "Training Step: 2306  | total loss: \u001b[1m\u001b[32m0.01702\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 040 | loss: 0.01702 - R2: 0.9974 -- iter: 0100/1168\n",
      "Training Step: 2307  | total loss: \u001b[1m\u001b[32m0.01702\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 040 | loss: 0.01702 - R2: 0.9974 -- iter: 0120/1168\n",
      "Training Step: 2308  | total loss: \u001b[1m\u001b[32m0.01580\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 040 | loss: 0.01580 - R2: 0.9975 -- iter: 0140/1168\n",
      "Training Step: 2309  | total loss: \u001b[1m\u001b[32m0.01549\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 040 | loss: 0.01549 - R2: 0.9978 -- iter: 0160/1168\n",
      "Training Step: 2310  | total loss: \u001b[1m\u001b[32m0.01526\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 040 | loss: 0.01526 - R2: 0.9982 -- iter: 0180/1168\n",
      "Training Step: 2311  | total loss: \u001b[1m\u001b[32m0.01526\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 040 | loss: 0.01526 - R2: 0.9982 -- iter: 0200/1168\n",
      "Training Step: 2312  | total loss: \u001b[1m\u001b[32m0.01496\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 040 | loss: 0.01496 - R2: 0.9978 -- iter: 0220/1168\n",
      "Training Step: 2313  | total loss: \u001b[1m\u001b[32m0.01551\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 040 | loss: 0.01551 - R2: 0.9978 -- iter: 0240/1168\n",
      "Training Step: 2314  | total loss: \u001b[1m\u001b[32m0.01594\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 040 | loss: 0.01594 - R2: 0.9986 -- iter: 0260/1168\n",
      "Training Step: 2315  | total loss: \u001b[1m\u001b[32m0.01553\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 040 | loss: 0.01553 - R2: 0.9989 -- iter: 0280/1168\n",
      "Training Step: 2316  | total loss: \u001b[1m\u001b[32m0.01513\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 040 | loss: 0.01513 - R2: 0.9990 -- iter: 0300/1168\n",
      "Training Step: 2317  | total loss: \u001b[1m\u001b[32m0.01472\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 040 | loss: 0.01472 - R2: 0.9981 -- iter: 0320/1168\n",
      "Training Step: 2318  | total loss: \u001b[1m\u001b[32m0.01507\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 040 | loss: 0.01507 - R2: 0.9981 -- iter: 0340/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2319  | total loss: \u001b[1m\u001b[32m0.01491\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 040 | loss: 0.01491 - R2: 0.9983 -- iter: 0360/1168\n",
      "Training Step: 2320  | total loss: \u001b[1m\u001b[32m0.01503\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 040 | loss: 0.01503 - R2: 0.9990 -- iter: 0380/1168\n",
      "Training Step: 2321  | total loss: \u001b[1m\u001b[32m0.01503\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 040 | loss: 0.01503 - R2: 0.9990 -- iter: 0400/1168\n",
      "Training Step: 2322  | total loss: \u001b[1m\u001b[32m0.01899\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 040 | loss: 0.01899 - R2: 1.0000 -- iter: 0420/1168\n",
      "Training Step: 2323  | total loss: \u001b[1m\u001b[32m0.01908\u001b[0m\u001b[0m | time: 0.165s\n",
      "| SGD | epoch: 040 | loss: 0.01908 - R2: 1.0000 -- iter: 0440/1168\n",
      "Training Step: 2324  | total loss: \u001b[1m\u001b[32m0.01923\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 040 | loss: 0.01923 - R2: 0.9991 -- iter: 0460/1168\n",
      "Training Step: 2325  | total loss: \u001b[1m\u001b[32m0.02348\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 040 | loss: 0.02348 - R2: 0.9992 -- iter: 0480/1168\n",
      "Training Step: 2326  | total loss: \u001b[1m\u001b[32m0.02401\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 040 | loss: 0.02401 - R2: 0.9992 -- iter: 0500/1168\n",
      "Training Step: 2327  | total loss: \u001b[1m\u001b[32m0.02247\u001b[0m\u001b[0m | time: 0.171s\n",
      "| SGD | epoch: 040 | loss: 0.02247 - R2: 1.0001 -- iter: 0520/1168\n",
      "Training Step: 2328  | total loss: \u001b[1m\u001b[32m0.02135\u001b[0m\u001b[0m | time: 0.173s\n",
      "| SGD | epoch: 040 | loss: 0.02135 - R2: 1.0002 -- iter: 0540/1168\n",
      "Training Step: 2329  | total loss: \u001b[1m\u001b[32m0.02057\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 040 | loss: 0.02057 - R2: 1.0005 -- iter: 0560/1168\n",
      "Training Step: 2330  | total loss: \u001b[1m\u001b[32m0.01943\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 040 | loss: 0.01943 - R2: 1.0004 -- iter: 0580/1168\n",
      "Training Step: 2331  | total loss: \u001b[1m\u001b[32m0.01857\u001b[0m\u001b[0m | time: 0.179s\n",
      "| SGD | epoch: 040 | loss: 0.01857 - R2: 1.0012 -- iter: 0600/1168\n",
      "Training Step: 2332  | total loss: \u001b[1m\u001b[32m0.01725\u001b[0m\u001b[0m | time: 0.181s\n",
      "| SGD | epoch: 040 | loss: 0.01725 - R2: 1.0012 -- iter: 0620/1168\n",
      "Training Step: 2333  | total loss: \u001b[1m\u001b[32m0.01688\u001b[0m\u001b[0m | time: 0.183s\n",
      "| SGD | epoch: 040 | loss: 0.01688 - R2: 1.0010 -- iter: 0640/1168\n",
      "Training Step: 2334  | total loss: \u001b[1m\u001b[32m0.01709\u001b[0m\u001b[0m | time: 0.186s\n",
      "| SGD | epoch: 040 | loss: 0.01709 - R2: 1.0017 -- iter: 0660/1168\n",
      "Training Step: 2335  | total loss: \u001b[1m\u001b[32m0.01593\u001b[0m\u001b[0m | time: 0.188s\n",
      "| SGD | epoch: 040 | loss: 0.01593 - R2: 1.0012 -- iter: 0680/1168\n",
      "Training Step: 2336  | total loss: \u001b[1m\u001b[32m0.01593\u001b[0m\u001b[0m | time: 0.190s\n",
      "| SGD | epoch: 040 | loss: 0.01593 - R2: 1.0011 -- iter: 0700/1168\n",
      "Training Step: 2337  | total loss: \u001b[1m\u001b[32m0.01562\u001b[0m\u001b[0m | time: 0.199s\n",
      "| SGD | epoch: 040 | loss: 0.01562 - R2: 1.0011 -- iter: 0720/1168\n",
      "Training Step: 2338  | total loss: \u001b[1m\u001b[32m0.01540\u001b[0m\u001b[0m | time: 0.203s\n",
      "| SGD | epoch: 040 | loss: 0.01540 - R2: 1.0006 -- iter: 0740/1168\n",
      "Training Step: 2339  | total loss: \u001b[1m\u001b[32m0.01570\u001b[0m\u001b[0m | time: 0.206s\n",
      "| SGD | epoch: 040 | loss: 0.01570 - R2: 1.0009 -- iter: 0760/1168\n",
      "Training Step: 2340  | total loss: \u001b[1m\u001b[32m0.01570\u001b[0m\u001b[0m | time: 0.208s\n",
      "| SGD | epoch: 040 | loss: 0.01570 - R2: 1.0009 -- iter: 0780/1168\n",
      "Training Step: 2341  | total loss: \u001b[1m\u001b[32m0.01476\u001b[0m\u001b[0m | time: 0.213s\n",
      "| SGD | epoch: 040 | loss: 0.01476 - R2: 1.0009 -- iter: 0800/1168\n",
      "Training Step: 2342  | total loss: \u001b[1m\u001b[32m0.01681\u001b[0m\u001b[0m | time: 0.215s\n",
      "| SGD | epoch: 040 | loss: 0.01681 - R2: 1.0003 -- iter: 0820/1168\n",
      "Training Step: 2343  | total loss: \u001b[1m\u001b[32m0.01641\u001b[0m\u001b[0m | time: 0.218s\n",
      "| SGD | epoch: 040 | loss: 0.01641 - R2: 0.9999 -- iter: 0840/1168\n",
      "Training Step: 2344  | total loss: \u001b[1m\u001b[32m0.01697\u001b[0m\u001b[0m | time: 0.221s\n",
      "| SGD | epoch: 040 | loss: 0.01697 - R2: 0.9999 -- iter: 0860/1168\n",
      "Training Step: 2345  | total loss: \u001b[1m\u001b[32m0.01640\u001b[0m\u001b[0m | time: 0.225s\n",
      "| SGD | epoch: 040 | loss: 0.01640 - R2: 0.9999 -- iter: 0880/1168\n",
      "Training Step: 2346  | total loss: \u001b[1m\u001b[32m0.02469\u001b[0m\u001b[0m | time: 0.228s\n",
      "| SGD | epoch: 040 | loss: 0.02469 - R2: 1.0017 -- iter: 0900/1168\n",
      "Training Step: 2347  | total loss: \u001b[1m\u001b[32m0.02559\u001b[0m\u001b[0m | time: 0.232s\n",
      "| SGD | epoch: 040 | loss: 0.02559 - R2: 1.0018 -- iter: 0920/1168\n",
      "Training Step: 2348  | total loss: \u001b[1m\u001b[32m0.02427\u001b[0m\u001b[0m | time: 0.236s\n",
      "| SGD | epoch: 040 | loss: 0.02427 - R2: 1.0020 -- iter: 0940/1168\n",
      "Training Step: 2349  | total loss: \u001b[1m\u001b[32m0.02283\u001b[0m\u001b[0m | time: 0.239s\n",
      "| SGD | epoch: 040 | loss: 0.02283 - R2: 1.0012 -- iter: 0960/1168\n",
      "Training Step: 2350  | total loss: \u001b[1m\u001b[32m0.02144\u001b[0m\u001b[0m | time: 0.248s\n",
      "| SGD | epoch: 040 | loss: 0.02144 - R2: 1.0009 -- iter: 0980/1168\n",
      "Training Step: 2351  | total loss: \u001b[1m\u001b[32m0.02069\u001b[0m\u001b[0m | time: 0.253s\n",
      "| SGD | epoch: 040 | loss: 0.02069 - R2: 1.0016 -- iter: 1000/1168\n",
      "Training Step: 2352  | total loss: \u001b[1m\u001b[32m0.02069\u001b[0m\u001b[0m | time: 0.256s\n",
      "| SGD | epoch: 040 | loss: 0.02069 - R2: 1.0016 -- iter: 1020/1168\n",
      "Training Step: 2353  | total loss: \u001b[1m\u001b[32m0.02260\u001b[0m\u001b[0m | time: 0.261s\n",
      "| SGD | epoch: 040 | loss: 0.02260 - R2: 1.0007 -- iter: 1040/1168\n",
      "Training Step: 2354  | total loss: \u001b[1m\u001b[32m0.02260\u001b[0m\u001b[0m | time: 0.263s\n",
      "| SGD | epoch: 040 | loss: 0.02260 - R2: 1.0007 -- iter: 1060/1168\n",
      "Training Step: 2355  | total loss: \u001b[1m\u001b[32m0.02099\u001b[0m\u001b[0m | time: 0.266s\n",
      "| SGD | epoch: 040 | loss: 0.02099 - R2: 1.0000 -- iter: 1080/1168\n",
      "Training Step: 2356  | total loss: \u001b[1m\u001b[32m0.02070\u001b[0m\u001b[0m | time: 0.268s\n",
      "| SGD | epoch: 040 | loss: 0.02070 - R2: 1.0000 -- iter: 1100/1168\n",
      "Training Step: 2357  | total loss: \u001b[1m\u001b[32m0.02008\u001b[0m\u001b[0m | time: 0.273s\n",
      "| SGD | epoch: 040 | loss: 0.02008 - R2: 0.9993 -- iter: 1120/1168\n",
      "Training Step: 2358  | total loss: \u001b[1m\u001b[32m0.01863\u001b[0m\u001b[0m | time: 0.278s\n",
      "| SGD | epoch: 040 | loss: 0.01863 - R2: 0.9998 -- iter: 1140/1168\n",
      "Training Step: 2359  | total loss: \u001b[1m\u001b[32m0.01863\u001b[0m\u001b[0m | time: 0.282s\n",
      "| SGD | epoch: 040 | loss: 0.01863 - R2: 0.9998 -- iter: 1160/1168\n",
      "Training Step: 2360  | total loss: \u001b[1m\u001b[32m0.01876\u001b[0m\u001b[0m | time: 1.288s\n",
      "| SGD | epoch: 040 | loss: 0.01876 - R2: 0.9995 | val_loss: 0.03206 - val_acc: 1.0012 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 2361  | total loss: \u001b[1m\u001b[32m0.01749\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 041 | loss: 0.01749 - R2: 1.0000 -- iter: 0020/1168\n",
      "Training Step: 2362  | total loss: \u001b[1m\u001b[32m0.01749\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 041 | loss: 0.01749 - R2: 1.0000 -- iter: 0040/1168\n",
      "Training Step: 2363  | total loss: \u001b[1m\u001b[32m0.01762\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 041 | loss: 0.01762 - R2: 1.0001 -- iter: 0060/1168\n",
      "Training Step: 2364  | total loss: \u001b[1m\u001b[32m0.01652\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 041 | loss: 0.01652 - R2: 0.9999 -- iter: 0080/1168\n",
      "Training Step: 2365  | total loss: \u001b[1m\u001b[32m0.01679\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 041 | loss: 0.01679 - R2: 1.0006 -- iter: 0100/1168\n",
      "Training Step: 2366  | total loss: \u001b[1m\u001b[32m0.01639\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 041 | loss: 0.01639 - R2: 1.0007 -- iter: 0120/1168\n",
      "Training Step: 2367  | total loss: \u001b[1m\u001b[32m0.03623\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 041 | loss: 0.03623 - R2: 1.0003 -- iter: 0140/1168\n",
      "Training Step: 2368  | total loss: \u001b[1m\u001b[32m0.03380\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 041 | loss: 0.03380 - R2: 1.0001 -- iter: 0160/1168\n",
      "Training Step: 2369  | total loss: \u001b[1m\u001b[32m0.03117\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 041 | loss: 0.03117 - R2: 0.9998 -- iter: 0180/1168\n",
      "Training Step: 2370  | total loss: \u001b[1m\u001b[32m0.02916\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 041 | loss: 0.02916 - R2: 0.9998 -- iter: 0200/1168\n",
      "Training Step: 2371  | total loss: \u001b[1m\u001b[32m0.02751\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 041 | loss: 0.02751 - R2: 0.9995 -- iter: 0220/1168\n",
      "Training Step: 2372  | total loss: \u001b[1m\u001b[32m0.02663\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 041 | loss: 0.02663 - R2: 0.9986 -- iter: 0240/1168\n",
      "Training Step: 2373  | total loss: \u001b[1m\u001b[32m0.02493\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 041 | loss: 0.02493 - R2: 0.9983 -- iter: 0260/1168\n",
      "Training Step: 2374  | total loss: \u001b[1m\u001b[32m0.02302\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 041 | loss: 0.02302 - R2: 0.9982 -- iter: 0280/1168\n",
      "Training Step: 2375  | total loss: \u001b[1m\u001b[32m0.02248\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 041 | loss: 0.02248 - R2: 0.9980 -- iter: 0300/1168\n",
      "Training Step: 2376  | total loss: \u001b[1m\u001b[32m0.02176\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 041 | loss: 0.02176 - R2: 0.9979 -- iter: 0320/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2377  | total loss: \u001b[1m\u001b[32m0.02176\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 041 | loss: 0.02176 - R2: 0.9979 -- iter: 0340/1168\n",
      "Training Step: 2378  | total loss: \u001b[1m\u001b[32m0.02772\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 041 | loss: 0.02772 - R2: 0.9999 -- iter: 0360/1168\n",
      "Training Step: 2379  | total loss: \u001b[1m\u001b[32m0.02772\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 041 | loss: 0.02772 - R2: 0.9999 -- iter: 0380/1168\n",
      "Training Step: 2380  | total loss: \u001b[1m\u001b[32m0.02580\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 041 | loss: 0.02580 - R2: 1.0000 -- iter: 0400/1168\n",
      "Training Step: 2381  | total loss: \u001b[1m\u001b[32m0.02471\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 041 | loss: 0.02471 - R2: 0.9997 -- iter: 0420/1168\n",
      "Training Step: 2382  | total loss: \u001b[1m\u001b[32m0.02471\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 041 | loss: 0.02471 - R2: 0.9997 -- iter: 0440/1168\n",
      "Training Step: 2383  | total loss: \u001b[1m\u001b[32m0.02609\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 041 | loss: 0.02609 - R2: 1.0010 -- iter: 0460/1168\n",
      "Training Step: 2384  | total loss: \u001b[1m\u001b[32m0.02512\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 041 | loss: 0.02512 - R2: 1.0006 -- iter: 0480/1168\n",
      "Training Step: 2385  | total loss: \u001b[1m\u001b[32m0.02387\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 041 | loss: 0.02387 - R2: 1.0003 -- iter: 0500/1168\n",
      "Training Step: 2386  | total loss: \u001b[1m\u001b[32m0.02387\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 041 | loss: 0.02387 - R2: 1.0003 -- iter: 0520/1168\n",
      "Training Step: 2387  | total loss: \u001b[1m\u001b[32m0.02197\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 041 | loss: 0.02197 - R2: 0.9995 -- iter: 0540/1168\n",
      "Training Step: 2388  | total loss: \u001b[1m\u001b[32m0.02197\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 041 | loss: 0.02197 - R2: 0.9995 -- iter: 0560/1168\n",
      "Training Step: 2389  | total loss: \u001b[1m\u001b[32m0.02132\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 041 | loss: 0.02132 - R2: 0.9991 -- iter: 0580/1168\n",
      "Training Step: 2390  | total loss: \u001b[1m\u001b[32m0.02108\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 041 | loss: 0.02108 - R2: 1.0000 -- iter: 0600/1168\n",
      "Training Step: 2391  | total loss: \u001b[1m\u001b[32m0.02152\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 041 | loss: 0.02152 - R2: 0.9997 -- iter: 0620/1168\n",
      "Training Step: 2392  | total loss: \u001b[1m\u001b[32m0.02032\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 041 | loss: 0.02032 - R2: 1.0000 -- iter: 0640/1168\n",
      "Training Step: 2393  | total loss: \u001b[1m\u001b[32m0.02029\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 041 | loss: 0.02029 - R2: 1.0006 -- iter: 0660/1168\n",
      "Training Step: 2394  | total loss: \u001b[1m\u001b[32m0.02366\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 041 | loss: 0.02366 - R2: 1.0007 -- iter: 0680/1168\n",
      "Training Step: 2395  | total loss: \u001b[1m\u001b[32m0.02366\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 041 | loss: 0.02366 - R2: 1.0007 -- iter: 0700/1168\n",
      "Training Step: 2396  | total loss: \u001b[1m\u001b[32m0.02099\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 041 | loss: 0.02099 - R2: 1.0003 -- iter: 0720/1168\n",
      "Training Step: 2397  | total loss: \u001b[1m\u001b[32m0.02099\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 041 | loss: 0.02099 - R2: 1.0003 -- iter: 0740/1168\n",
      "Training Step: 2398  | total loss: \u001b[1m\u001b[32m0.02018\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 041 | loss: 0.02018 - R2: 1.0010 -- iter: 0760/1168\n",
      "Training Step: 2399  | total loss: \u001b[1m\u001b[32m0.02106\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 041 | loss: 0.02106 - R2: 1.0011 -- iter: 0780/1168\n",
      "Training Step: 2400  | total loss: \u001b[1m\u001b[32m0.02106\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 041 | loss: 0.02106 - R2: 1.0011 -- iter: 0800/1168\n",
      "Training Step: 2401  | total loss: \u001b[1m\u001b[32m0.02169\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 041 | loss: 0.02169 - R2: 1.0014 -- iter: 0820/1168\n",
      "Training Step: 2402  | total loss: \u001b[1m\u001b[32m0.02196\u001b[0m\u001b[0m | time: 0.172s\n",
      "| SGD | epoch: 041 | loss: 0.02196 - R2: 1.0005 -- iter: 0840/1168\n",
      "Training Step: 2403  | total loss: \u001b[1m\u001b[32m0.02037\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 041 | loss: 0.02037 - R2: 1.0001 -- iter: 0860/1168\n",
      "Training Step: 2404  | total loss: \u001b[1m\u001b[32m0.02037\u001b[0m\u001b[0m | time: 0.178s\n",
      "| SGD | epoch: 041 | loss: 0.02037 - R2: 1.0001 -- iter: 0880/1168\n",
      "Training Step: 2405  | total loss: \u001b[1m\u001b[32m0.02057\u001b[0m\u001b[0m | time: 0.182s\n",
      "| SGD | epoch: 041 | loss: 0.02057 - R2: 1.0001 -- iter: 0900/1168\n",
      "Training Step: 2406  | total loss: \u001b[1m\u001b[32m0.01921\u001b[0m\u001b[0m | time: 0.185s\n",
      "| SGD | epoch: 041 | loss: 0.01921 - R2: 1.0005 -- iter: 0920/1168\n",
      "Training Step: 2407  | total loss: \u001b[1m\u001b[32m0.01921\u001b[0m\u001b[0m | time: 0.187s\n",
      "| SGD | epoch: 041 | loss: 0.01921 - R2: 1.0005 -- iter: 0940/1168\n",
      "Training Step: 2408  | total loss: \u001b[1m\u001b[32m0.02183\u001b[0m\u001b[0m | time: 0.189s\n",
      "| SGD | epoch: 041 | loss: 0.02183 - R2: 1.0017 -- iter: 0960/1168\n",
      "Training Step: 2409  | total loss: \u001b[1m\u001b[32m0.02219\u001b[0m\u001b[0m | time: 0.191s\n",
      "| SGD | epoch: 041 | loss: 0.02219 - R2: 1.0012 -- iter: 0980/1168\n",
      "Training Step: 2410  | total loss: \u001b[1m\u001b[32m0.02169\u001b[0m\u001b[0m | time: 0.194s\n",
      "| SGD | epoch: 041 | loss: 0.02169 - R2: 1.0004 -- iter: 1000/1168\n",
      "Training Step: 2411  | total loss: \u001b[1m\u001b[32m0.02169\u001b[0m\u001b[0m | time: 0.196s\n",
      "| SGD | epoch: 041 | loss: 0.02169 - R2: 1.0001 -- iter: 1020/1168\n",
      "Training Step: 2412  | total loss: \u001b[1m\u001b[32m0.02124\u001b[0m\u001b[0m | time: 0.207s\n",
      "| SGD | epoch: 041 | loss: 0.02124 - R2: 1.0001 -- iter: 1040/1168\n",
      "Training Step: 2413  | total loss: \u001b[1m\u001b[32m0.02092\u001b[0m\u001b[0m | time: 0.210s\n",
      "| SGD | epoch: 041 | loss: 0.02092 - R2: 1.0001 -- iter: 1060/1168\n",
      "Training Step: 2414  | total loss: \u001b[1m\u001b[32m0.02008\u001b[0m\u001b[0m | time: 0.212s\n",
      "| SGD | epoch: 041 | loss: 0.02008 - R2: 0.9998 -- iter: 1080/1168\n",
      "Training Step: 2415  | total loss: \u001b[1m\u001b[32m0.01872\u001b[0m\u001b[0m | time: 0.216s\n",
      "| SGD | epoch: 041 | loss: 0.01872 - R2: 0.9998 -- iter: 1100/1168\n",
      "Training Step: 2416  | total loss: \u001b[1m\u001b[32m0.01977\u001b[0m\u001b[0m | time: 0.223s\n",
      "| SGD | epoch: 041 | loss: 0.01977 - R2: 0.9992 -- iter: 1120/1168\n",
      "Training Step: 2417  | total loss: \u001b[1m\u001b[32m0.01895\u001b[0m\u001b[0m | time: 0.227s\n",
      "| SGD | epoch: 041 | loss: 0.01895 - R2: 0.9992 -- iter: 1140/1168\n",
      "Training Step: 2418  | total loss: \u001b[1m\u001b[32m0.01715\u001b[0m\u001b[0m | time: 0.230s\n",
      "| SGD | epoch: 041 | loss: 0.01715 - R2: 0.9994 -- iter: 1160/1168\n",
      "Training Step: 2419  | total loss: \u001b[1m\u001b[32m0.01715\u001b[0m\u001b[0m | time: 1.238s\n",
      "| SGD | epoch: 041 | loss: 0.01715 - R2: 0.9994 | val_loss: 0.03193 - val_acc: 1.0009 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 2420  | total loss: \u001b[1m\u001b[32m0.01647\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 042 | loss: 0.01647 - R2: 0.9984 -- iter: 0020/1168\n",
      "Training Step: 2421  | total loss: \u001b[1m\u001b[32m0.01647\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 042 | loss: 0.01647 - R2: 0.9984 -- iter: 0040/1168\n",
      "Training Step: 2422  | total loss: \u001b[1m\u001b[32m0.01700\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 042 | loss: 0.01700 - R2: 0.9994 -- iter: 0060/1168\n",
      "Training Step: 2423  | total loss: \u001b[1m\u001b[32m0.01788\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 042 | loss: 0.01788 - R2: 1.0003 -- iter: 0080/1168\n",
      "Training Step: 2424  | total loss: \u001b[1m\u001b[32m0.01788\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 042 | loss: 0.01788 - R2: 1.0003 -- iter: 0100/1168\n",
      "Training Step: 2425  | total loss: \u001b[1m\u001b[32m0.01806\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 042 | loss: 0.01806 - R2: 1.0009 -- iter: 0120/1168\n",
      "Training Step: 2426  | total loss: \u001b[1m\u001b[32m0.01806\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 042 | loss: 0.01806 - R2: 1.0009 -- iter: 0140/1168\n",
      "Training Step: 2427  | total loss: \u001b[1m\u001b[32m0.03542\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 042 | loss: 0.03542 - R2: 1.0026 -- iter: 0160/1168\n",
      "Training Step: 2428  | total loss: \u001b[1m\u001b[32m0.03284\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 042 | loss: 0.03284 - R2: 1.0021 -- iter: 0180/1168\n",
      "Training Step: 2429  | total loss: \u001b[1m\u001b[32m0.03284\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 042 | loss: 0.03284 - R2: 1.0021 -- iter: 0200/1168\n",
      "Training Step: 2430  | total loss: \u001b[1m\u001b[32m0.02988\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 042 | loss: 0.02988 - R2: 1.0022 -- iter: 0220/1168\n",
      "Training Step: 2431  | total loss: \u001b[1m\u001b[32m0.02836\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 042 | loss: 0.02836 - R2: 1.0014 -- iter: 0240/1168\n",
      "Training Step: 2432  | total loss: \u001b[1m\u001b[32m0.03453\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 042 | loss: 0.03453 - R2: 1.0026 -- iter: 0260/1168\n",
      "Training Step: 2433  | total loss: \u001b[1m\u001b[32m0.03453\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 042 | loss: 0.03453 - R2: 1.0026 -- iter: 0280/1168\n",
      "Training Step: 2434  | total loss: \u001b[1m\u001b[32m0.03392\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 042 | loss: 0.03392 - R2: 1.0017 -- iter: 0300/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2435  | total loss: \u001b[1m\u001b[32m0.03392\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 042 | loss: 0.03392 - R2: 1.0017 -- iter: 0320/1168\n",
      "Training Step: 2436  | total loss: \u001b[1m\u001b[32m0.03067\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 042 | loss: 0.03067 - R2: 1.0020 -- iter: 0340/1168\n",
      "Training Step: 2437  | total loss: \u001b[1m\u001b[32m0.03067\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 042 | loss: 0.03067 - R2: 1.0020 -- iter: 0360/1168\n",
      "Training Step: 2438  | total loss: \u001b[1m\u001b[32m0.02787\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 042 | loss: 0.02787 - R2: 1.0011 -- iter: 0380/1168\n",
      "Training Step: 2439  | total loss: \u001b[1m\u001b[32m0.02787\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 042 | loss: 0.02787 - R2: 1.0011 -- iter: 0400/1168\n",
      "Training Step: 2440  | total loss: \u001b[1m\u001b[32m0.02485\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 042 | loss: 0.02485 - R2: 1.0001 -- iter: 0420/1168\n",
      "Training Step: 2441  | total loss: \u001b[1m\u001b[32m0.02485\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 042 | loss: 0.02485 - R2: 1.0001 -- iter: 0440/1168\n",
      "Training Step: 2442  | total loss: \u001b[1m\u001b[32m0.02297\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 042 | loss: 0.02297 - R2: 0.9987 -- iter: 0460/1168\n",
      "Training Step: 2443  | total loss: \u001b[1m\u001b[32m0.02297\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 042 | loss: 0.02297 - R2: 0.9987 -- iter: 0480/1168\n",
      "Training Step: 2444  | total loss: \u001b[1m\u001b[32m0.02135\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 042 | loss: 0.02135 - R2: 0.9990 -- iter: 0500/1168\n",
      "Training Step: 2445  | total loss: \u001b[1m\u001b[32m0.02113\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 042 | loss: 0.02113 - R2: 0.9981 -- iter: 0520/1168\n",
      "Training Step: 2446  | total loss: \u001b[1m\u001b[32m0.01950\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 042 | loss: 0.01950 - R2: 0.9969 -- iter: 0540/1168\n",
      "Training Step: 2447  | total loss: \u001b[1m\u001b[32m0.01950\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 042 | loss: 0.01950 - R2: 0.9969 -- iter: 0560/1168\n",
      "Training Step: 2448  | total loss: \u001b[1m\u001b[32m0.01873\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 042 | loss: 0.01873 - R2: 0.9976 -- iter: 0580/1168\n",
      "Training Step: 2449  | total loss: \u001b[1m\u001b[32m0.01841\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 042 | loss: 0.01841 - R2: 0.9977 -- iter: 0600/1168\n",
      "Training Step: 2450  | total loss: \u001b[1m\u001b[32m0.01881\u001b[0m\u001b[0m | time: 0.171s\n",
      "| SGD | epoch: 042 | loss: 0.01881 - R2: 0.9978 -- iter: 0620/1168\n",
      "Training Step: 2451  | total loss: \u001b[1m\u001b[32m0.01881\u001b[0m\u001b[0m | time: 0.173s\n",
      "| SGD | epoch: 042 | loss: 0.01881 - R2: 0.9978 -- iter: 0640/1168\n",
      "Training Step: 2452  | total loss: \u001b[1m\u001b[32m0.01829\u001b[0m\u001b[0m | time: 0.183s\n",
      "| SGD | epoch: 042 | loss: 0.01829 - R2: 0.9993 -- iter: 0660/1168\n",
      "Training Step: 2453  | total loss: \u001b[1m\u001b[32m0.01851\u001b[0m\u001b[0m | time: 0.198s\n",
      "| SGD | epoch: 042 | loss: 0.01851 - R2: 0.9990 -- iter: 0680/1168\n",
      "Training Step: 2454  | total loss: \u001b[1m\u001b[32m0.01851\u001b[0m\u001b[0m | time: 0.200s\n",
      "| SGD | epoch: 042 | loss: 0.01851 - R2: 0.9990 -- iter: 0700/1168\n",
      "Training Step: 2455  | total loss: \u001b[1m\u001b[32m0.01773\u001b[0m\u001b[0m | time: 0.210s\n",
      "| SGD | epoch: 042 | loss: 0.01773 - R2: 0.9992 -- iter: 0720/1168\n",
      "Training Step: 2456  | total loss: \u001b[1m\u001b[32m0.01933\u001b[0m\u001b[0m | time: 0.212s\n",
      "| SGD | epoch: 042 | loss: 0.01933 - R2: 0.9997 -- iter: 0740/1168\n",
      "Training Step: 2457  | total loss: \u001b[1m\u001b[32m0.02111\u001b[0m\u001b[0m | time: 0.214s\n",
      "| SGD | epoch: 042 | loss: 0.02111 - R2: 1.0001 -- iter: 0760/1168\n",
      "Training Step: 2458  | total loss: \u001b[1m\u001b[32m0.02072\u001b[0m\u001b[0m | time: 0.217s\n",
      "| SGD | epoch: 042 | loss: 0.02072 - R2: 0.9996 -- iter: 0780/1168\n",
      "Training Step: 2459  | total loss: \u001b[1m\u001b[32m0.02313\u001b[0m\u001b[0m | time: 0.220s\n",
      "| SGD | epoch: 042 | loss: 0.02313 - R2: 1.0007 -- iter: 0800/1168\n",
      "Training Step: 2460  | total loss: \u001b[1m\u001b[32m0.02519\u001b[0m\u001b[0m | time: 0.224s\n",
      "| SGD | epoch: 042 | loss: 0.02519 - R2: 1.0017 -- iter: 0820/1168\n",
      "Training Step: 2461  | total loss: \u001b[1m\u001b[32m0.02411\u001b[0m\u001b[0m | time: 0.226s\n",
      "| SGD | epoch: 042 | loss: 0.02411 - R2: 1.0004 -- iter: 0840/1168\n",
      "Training Step: 2462  | total loss: \u001b[1m\u001b[32m0.02424\u001b[0m\u001b[0m | time: 0.229s\n",
      "| SGD | epoch: 042 | loss: 0.02424 - R2: 0.9992 -- iter: 0860/1168\n",
      "Training Step: 2463  | total loss: \u001b[1m\u001b[32m0.02587\u001b[0m\u001b[0m | time: 0.231s\n",
      "| SGD | epoch: 042 | loss: 0.02587 - R2: 0.9999 -- iter: 0880/1168\n",
      "Training Step: 2464  | total loss: \u001b[1m\u001b[32m0.02587\u001b[0m\u001b[0m | time: 0.233s\n",
      "| SGD | epoch: 042 | loss: 0.02587 - R2: 0.9999 -- iter: 0900/1168\n",
      "Training Step: 2465  | total loss: \u001b[1m\u001b[32m0.02503\u001b[0m\u001b[0m | time: 0.235s\n",
      "| SGD | epoch: 042 | loss: 0.02503 - R2: 1.0000 -- iter: 0920/1168\n",
      "Training Step: 2466  | total loss: \u001b[1m\u001b[32m0.02343\u001b[0m\u001b[0m | time: 0.237s\n",
      "| SGD | epoch: 042 | loss: 0.02343 - R2: 1.0000 -- iter: 0940/1168\n",
      "Training Step: 2467  | total loss: \u001b[1m\u001b[32m0.02339\u001b[0m\u001b[0m | time: 0.240s\n",
      "| SGD | epoch: 042 | loss: 0.02339 - R2: 1.0008 -- iter: 0960/1168\n",
      "Training Step: 2468  | total loss: \u001b[1m\u001b[32m0.02373\u001b[0m\u001b[0m | time: 0.249s\n",
      "| SGD | epoch: 042 | loss: 0.02373 - R2: 0.9998 -- iter: 0980/1168\n",
      "Training Step: 2469  | total loss: \u001b[1m\u001b[32m0.02373\u001b[0m\u001b[0m | time: 0.252s\n",
      "| SGD | epoch: 042 | loss: 0.02373 - R2: 0.9998 -- iter: 1000/1168\n",
      "Training Step: 2470  | total loss: \u001b[1m\u001b[32m0.02197\u001b[0m\u001b[0m | time: 0.256s\n",
      "| SGD | epoch: 042 | loss: 0.02197 - R2: 1.0000 -- iter: 1020/1168\n",
      "Training Step: 2471  | total loss: \u001b[1m\u001b[32m0.02197\u001b[0m\u001b[0m | time: 0.260s\n",
      "| SGD | epoch: 042 | loss: 0.02197 - R2: 1.0002 -- iter: 1040/1168\n",
      "Training Step: 2472  | total loss: \u001b[1m\u001b[32m0.02086\u001b[0m\u001b[0m | time: 0.262s\n",
      "| SGD | epoch: 042 | loss: 0.02086 - R2: 1.0002 -- iter: 1060/1168\n",
      "Training Step: 2473  | total loss: \u001b[1m\u001b[32m0.01946\u001b[0m\u001b[0m | time: 0.264s\n",
      "| SGD | epoch: 042 | loss: 0.01946 - R2: 0.9995 -- iter: 1080/1168\n",
      "Training Step: 2474  | total loss: \u001b[1m\u001b[32m0.01873\u001b[0m\u001b[0m | time: 0.270s\n",
      "| SGD | epoch: 042 | loss: 0.01873 - R2: 0.9992 -- iter: 1100/1168\n",
      "Training Step: 2475  | total loss: \u001b[1m\u001b[32m0.01873\u001b[0m\u001b[0m | time: 0.274s\n",
      "| SGD | epoch: 042 | loss: 0.01873 - R2: 0.9992 -- iter: 1120/1168\n",
      "Training Step: 2476  | total loss: \u001b[1m\u001b[32m0.01802\u001b[0m\u001b[0m | time: 0.281s\n",
      "| SGD | epoch: 042 | loss: 0.01802 - R2: 0.9994 -- iter: 1140/1168\n",
      "Training Step: 2477  | total loss: \u001b[1m\u001b[32m0.01772\u001b[0m\u001b[0m | time: 0.283s\n",
      "| SGD | epoch: 042 | loss: 0.01772 - R2: 0.9991 -- iter: 1160/1168\n",
      "Training Step: 2478  | total loss: \u001b[1m\u001b[32m0.01713\u001b[0m\u001b[0m | time: 1.292s\n",
      "| SGD | epoch: 042 | loss: 0.01713 - R2: 0.9985 | val_loss: 0.03193 - val_acc: 1.0009 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 2479  | total loss: \u001b[1m\u001b[32m0.01802\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 043 | loss: 0.01802 - R2: 0.9986 -- iter: 0020/1168\n",
      "Training Step: 2480  | total loss: \u001b[1m\u001b[32m0.01594\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 043 | loss: 0.01594 - R2: 0.9999 -- iter: 0040/1168\n",
      "Training Step: 2481  | total loss: \u001b[1m\u001b[32m0.01594\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 043 | loss: 0.01594 - R2: 0.9999 -- iter: 0060/1168\n",
      "Training Step: 2482  | total loss: \u001b[1m\u001b[32m0.01748\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 043 | loss: 0.01748 - R2: 0.9990 -- iter: 0080/1168\n",
      "Training Step: 2483  | total loss: \u001b[1m\u001b[32m0.01631\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 043 | loss: 0.01631 - R2: 1.0000 -- iter: 0100/1168\n",
      "Training Step: 2484  | total loss: \u001b[1m\u001b[32m0.01602\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 043 | loss: 0.01602 - R2: 1.0000 -- iter: 0120/1168\n",
      "Training Step: 2485  | total loss: \u001b[1m\u001b[32m0.01509\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 043 | loss: 0.01509 - R2: 0.9998 -- iter: 0140/1168\n",
      "Training Step: 2486  | total loss: \u001b[1m\u001b[32m0.01501\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 043 | loss: 0.01501 - R2: 1.0003 -- iter: 0160/1168\n",
      "Training Step: 2487  | total loss: \u001b[1m\u001b[32m0.01501\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 043 | loss: 0.01501 - R2: 1.0003 -- iter: 0180/1168\n",
      "Training Step: 2488  | total loss: \u001b[1m\u001b[32m0.01463\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 043 | loss: 0.01463 - R2: 1.0008 -- iter: 0200/1168\n",
      "Training Step: 2489  | total loss: \u001b[1m\u001b[32m0.01475\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 043 | loss: 0.01475 - R2: 1.0005 -- iter: 0220/1168\n",
      "Training Step: 2490  | total loss: \u001b[1m\u001b[32m0.01482\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 043 | loss: 0.01482 - R2: 1.0006 -- iter: 0240/1168\n",
      "Training Step: 2491  | total loss: \u001b[1m\u001b[32m0.01482\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 043 | loss: 0.01482 - R2: 1.0006 -- iter: 0260/1168\n",
      "Training Step: 2492  | total loss: \u001b[1m\u001b[32m0.01412\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 043 | loss: 0.01412 - R2: 1.0004 -- iter: 0280/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2493  | total loss: \u001b[1m\u001b[32m0.01514\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 043 | loss: 0.01514 - R2: 1.0005 -- iter: 0300/1168\n",
      "Training Step: 2494  | total loss: \u001b[1m\u001b[32m0.01566\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 043 | loss: 0.01566 - R2: 1.0003 -- iter: 0320/1168\n",
      "Training Step: 2495  | total loss: \u001b[1m\u001b[32m0.01566\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 043 | loss: 0.01566 - R2: 0.9999 -- iter: 0340/1168\n",
      "Training Step: 2496  | total loss: \u001b[1m\u001b[32m0.01612\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 043 | loss: 0.01612 - R2: 1.0003 -- iter: 0360/1168\n",
      "Training Step: 2497  | total loss: \u001b[1m\u001b[32m0.01840\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 043 | loss: 0.01840 - R2: 1.0005 -- iter: 0380/1168\n",
      "Training Step: 2498  | total loss: \u001b[1m\u001b[32m0.01747\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 043 | loss: 0.01747 - R2: 1.0005 -- iter: 0400/1168\n",
      "Training Step: 2499  | total loss: \u001b[1m\u001b[32m0.01676\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 043 | loss: 0.01676 - R2: 1.0007 -- iter: 0420/1168\n",
      "Training Step: 2500  | total loss: \u001b[1m\u001b[32m0.01792\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 043 | loss: 0.01792 - R2: 0.9995 -- iter: 0440/1168\n",
      "Training Step: 2501  | total loss: \u001b[1m\u001b[32m0.01792\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 043 | loss: 0.01792 - R2: 0.9995 -- iter: 0460/1168\n",
      "Training Step: 2502  | total loss: \u001b[1m\u001b[32m0.01707\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 043 | loss: 0.01707 - R2: 0.9997 -- iter: 0480/1168\n",
      "Training Step: 2503  | total loss: \u001b[1m\u001b[32m0.01707\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 043 | loss: 0.01707 - R2: 0.9997 -- iter: 0500/1168\n",
      "Training Step: 2504  | total loss: \u001b[1m\u001b[32m0.01726\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 043 | loss: 0.01726 - R2: 1.0002 -- iter: 0520/1168\n",
      "Training Step: 2505  | total loss: \u001b[1m\u001b[32m0.01714\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 043 | loss: 0.01714 - R2: 0.9998 -- iter: 0540/1168\n",
      "Training Step: 2506  | total loss: \u001b[1m\u001b[32m0.01613\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 043 | loss: 0.01613 - R2: 0.9996 -- iter: 0560/1168\n",
      "Training Step: 2507  | total loss: \u001b[1m\u001b[32m0.01544\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 043 | loss: 0.01544 - R2: 0.9999 -- iter: 0580/1168\n",
      "Training Step: 2508  | total loss: \u001b[1m\u001b[32m0.01866\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 043 | loss: 0.01866 - R2: 1.0003 -- iter: 0600/1168\n",
      "Training Step: 2509  | total loss: \u001b[1m\u001b[32m0.01846\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 043 | loss: 0.01846 - R2: 1.0001 -- iter: 0620/1168\n",
      "Training Step: 2510  | total loss: \u001b[1m\u001b[32m0.01648\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 043 | loss: 0.01648 - R2: 0.9997 -- iter: 0640/1168\n",
      "Training Step: 2511  | total loss: \u001b[1m\u001b[32m0.01648\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 043 | loss: 0.01648 - R2: 0.9993 -- iter: 0660/1168\n",
      "Training Step: 2512  | total loss: \u001b[1m\u001b[32m0.01583\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 043 | loss: 0.01583 - R2: 0.9984 -- iter: 0680/1168\n",
      "Training Step: 2513  | total loss: \u001b[1m\u001b[32m0.01478\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 043 | loss: 0.01478 - R2: 0.9980 -- iter: 0700/1168\n",
      "Training Step: 2514  | total loss: \u001b[1m\u001b[32m0.01478\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 043 | loss: 0.01478 - R2: 0.9980 -- iter: 0720/1168\n",
      "Training Step: 2515  | total loss: \u001b[1m\u001b[32m0.01391\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 043 | loss: 0.01391 - R2: 0.9977 -- iter: 0740/1168\n",
      "Training Step: 2516  | total loss: \u001b[1m\u001b[32m0.01391\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 043 | loss: 0.01391 - R2: 0.9977 -- iter: 0760/1168\n",
      "Training Step: 2517  | total loss: \u001b[1m\u001b[32m0.01491\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 043 | loss: 0.01491 - R2: 0.9977 -- iter: 0780/1168\n",
      "Training Step: 2518  | total loss: \u001b[1m\u001b[32m0.01452\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 043 | loss: 0.01452 - R2: 0.9980 -- iter: 0800/1168\n",
      "Training Step: 2519  | total loss: \u001b[1m\u001b[32m0.01446\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 043 | loss: 0.01446 - R2: 0.9985 -- iter: 0820/1168\n",
      "Training Step: 2520  | total loss: \u001b[1m\u001b[32m0.01432\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 043 | loss: 0.01432 - R2: 0.9987 -- iter: 0840/1168\n",
      "Training Step: 2521  | total loss: \u001b[1m\u001b[32m0.01432\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 043 | loss: 0.01432 - R2: 0.9987 -- iter: 0860/1168\n",
      "Training Step: 2522  | total loss: \u001b[1m\u001b[32m0.01595\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 043 | loss: 0.01595 - R2: 0.9988 -- iter: 0880/1168\n",
      "Training Step: 2523  | total loss: \u001b[1m\u001b[32m0.01595\u001b[0m\u001b[0m | time: 0.176s\n",
      "| SGD | epoch: 043 | loss: 0.01595 - R2: 0.9988 -- iter: 0900/1168\n",
      "Training Step: 2524  | total loss: \u001b[1m\u001b[32m0.01561\u001b[0m\u001b[0m | time: 0.181s\n",
      "| SGD | epoch: 043 | loss: 0.01561 - R2: 0.9989 -- iter: 0920/1168\n",
      "Training Step: 2525  | total loss: \u001b[1m\u001b[32m0.01554\u001b[0m\u001b[0m | time: 0.183s\n",
      "| SGD | epoch: 043 | loss: 0.01554 - R2: 0.9992 -- iter: 0940/1168\n",
      "Training Step: 2526  | total loss: \u001b[1m\u001b[32m0.01558\u001b[0m\u001b[0m | time: 0.186s\n",
      "| SGD | epoch: 043 | loss: 0.01558 - R2: 0.9997 -- iter: 0960/1168\n",
      "Training Step: 2527  | total loss: \u001b[1m\u001b[32m0.01536\u001b[0m\u001b[0m | time: 0.188s\n",
      "| SGD | epoch: 043 | loss: 0.01536 - R2: 1.0002 -- iter: 0980/1168\n",
      "Training Step: 2528  | total loss: \u001b[1m\u001b[32m0.01462\u001b[0m\u001b[0m | time: 0.191s\n",
      "| SGD | epoch: 043 | loss: 0.01462 - R2: 0.9998 -- iter: 1000/1168\n",
      "Training Step: 2529  | total loss: \u001b[1m\u001b[32m0.02160\u001b[0m\u001b[0m | time: 0.194s\n",
      "| SGD | epoch: 043 | loss: 0.02160 - R2: 1.0003 -- iter: 1020/1168\n",
      "Training Step: 2530  | total loss: \u001b[1m\u001b[32m0.02160\u001b[0m\u001b[0m | time: 0.196s\n",
      "| SGD | epoch: 043 | loss: 0.02160 - R2: 1.0006 -- iter: 1040/1168\n",
      "Training Step: 2531  | total loss: \u001b[1m\u001b[32m0.02026\u001b[0m\u001b[0m | time: 0.198s\n",
      "| SGD | epoch: 043 | loss: 0.02026 - R2: 1.0006 -- iter: 1060/1168\n",
      "Training Step: 2532  | total loss: \u001b[1m\u001b[32m0.01963\u001b[0m\u001b[0m | time: 0.204s\n",
      "| SGD | epoch: 043 | loss: 0.01963 - R2: 1.0000 -- iter: 1080/1168\n",
      "Training Step: 2533  | total loss: \u001b[1m\u001b[32m0.01906\u001b[0m\u001b[0m | time: 0.212s\n",
      "| SGD | epoch: 043 | loss: 0.01906 - R2: 0.9991 -- iter: 1100/1168\n",
      "Training Step: 2534  | total loss: \u001b[1m\u001b[32m0.01906\u001b[0m\u001b[0m | time: 0.215s\n",
      "| SGD | epoch: 043 | loss: 0.01906 - R2: 0.9991 -- iter: 1120/1168\n",
      "Training Step: 2535  | total loss: \u001b[1m\u001b[32m0.01936\u001b[0m\u001b[0m | time: 0.217s\n",
      "| SGD | epoch: 043 | loss: 0.01936 - R2: 0.9998 -- iter: 1140/1168\n",
      "Training Step: 2536  | total loss: \u001b[1m\u001b[32m0.01847\u001b[0m\u001b[0m | time: 0.221s\n",
      "| SGD | epoch: 043 | loss: 0.01847 - R2: 1.0001 -- iter: 1160/1168\n",
      "Training Step: 2537  | total loss: \u001b[1m\u001b[32m0.01936\u001b[0m\u001b[0m | time: 1.227s\n",
      "| SGD | epoch: 043 | loss: 0.01936 - R2: 1.0009 | val_loss: 0.03206 - val_acc: 1.0008 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 2538  | total loss: \u001b[1m\u001b[32m0.01936\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 044 | loss: 0.01936 - R2: 1.0009 -- iter: 0020/1168\n",
      "Training Step: 2539  | total loss: \u001b[1m\u001b[32m0.02128\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 044 | loss: 0.02128 - R2: 1.0012 -- iter: 0040/1168\n",
      "Training Step: 2540  | total loss: \u001b[1m\u001b[32m0.02025\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 044 | loss: 0.02025 - R2: 1.0010 -- iter: 0060/1168\n",
      "Training Step: 2541  | total loss: \u001b[1m\u001b[32m0.02094\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 044 | loss: 0.02094 - R2: 1.0004 -- iter: 0080/1168\n",
      "Training Step: 2542  | total loss: \u001b[1m\u001b[32m0.02017\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 044 | loss: 0.02017 - R2: 1.0004 -- iter: 0100/1168\n",
      "Training Step: 2543  | total loss: \u001b[1m\u001b[32m0.02353\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 044 | loss: 0.02353 - R2: 1.0017 -- iter: 0120/1168\n",
      "Training Step: 2544  | total loss: \u001b[1m\u001b[32m0.02353\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 044 | loss: 0.02353 - R2: 1.0017 -- iter: 0140/1168\n",
      "Training Step: 2545  | total loss: \u001b[1m\u001b[32m0.02242\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 044 | loss: 0.02242 - R2: 1.0017 -- iter: 0160/1168\n",
      "Training Step: 2546  | total loss: \u001b[1m\u001b[32m0.02110\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 044 | loss: 0.02110 - R2: 1.0017 -- iter: 0180/1168\n",
      "Training Step: 2547  | total loss: \u001b[1m\u001b[32m0.02110\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 044 | loss: 0.02110 - R2: 1.0005 -- iter: 0200/1168\n",
      "Training Step: 2548  | total loss: \u001b[1m\u001b[32m0.02110\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 044 | loss: 0.02110 - R2: 1.0005 -- iter: 0220/1168\n",
      "Training Step: 2549  | total loss: \u001b[1m\u001b[32m0.02110\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 044 | loss: 0.02110 - R2: 1.0007 -- iter: 0240/1168\n",
      "Training Step: 2550  | total loss: \u001b[1m\u001b[32m0.01975\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 044 | loss: 0.01975 - R2: 1.0008 -- iter: 0260/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2551  | total loss: \u001b[1m\u001b[32m0.01979\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 044 | loss: 0.01979 - R2: 0.9996 -- iter: 0280/1168\n",
      "Training Step: 2552  | total loss: \u001b[1m\u001b[32m0.01979\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 044 | loss: 0.01979 - R2: 0.9997 -- iter: 0300/1168\n",
      "Training Step: 2553  | total loss: \u001b[1m\u001b[32m0.01771\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 044 | loss: 0.01771 - R2: 0.9996 -- iter: 0320/1168\n",
      "Training Step: 2554  | total loss: \u001b[1m\u001b[32m0.01771\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 044 | loss: 0.01771 - R2: 0.9996 -- iter: 0340/1168\n",
      "Training Step: 2555  | total loss: \u001b[1m\u001b[32m0.01721\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 044 | loss: 0.01721 - R2: 0.9991 -- iter: 0360/1168\n",
      "Training Step: 2556  | total loss: \u001b[1m\u001b[32m0.01716\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 044 | loss: 0.01716 - R2: 0.9992 -- iter: 0380/1168\n",
      "Training Step: 2557  | total loss: \u001b[1m\u001b[32m0.01716\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 044 | loss: 0.01716 - R2: 0.9992 -- iter: 0400/1168\n",
      "Training Step: 2558  | total loss: \u001b[1m\u001b[32m0.01677\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 044 | loss: 0.01677 - R2: 0.9992 -- iter: 0420/1168\n",
      "Training Step: 2559  | total loss: \u001b[1m\u001b[32m0.01685\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 044 | loss: 0.01685 - R2: 0.9993 -- iter: 0440/1168\n",
      "Training Step: 2560  | total loss: \u001b[1m\u001b[32m0.01606\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 044 | loss: 0.01606 - R2: 1.0003 -- iter: 0460/1168\n",
      "Training Step: 2561  | total loss: \u001b[1m\u001b[32m0.01729\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 044 | loss: 0.01729 - R2: 1.0002 -- iter: 0480/1168\n",
      "Training Step: 2562  | total loss: \u001b[1m\u001b[32m0.01729\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 044 | loss: 0.01729 - R2: 1.0002 -- iter: 0500/1168\n",
      "Training Step: 2563  | total loss: \u001b[1m\u001b[32m0.01648\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 044 | loss: 0.01648 - R2: 1.0000 -- iter: 0520/1168\n",
      "Training Step: 2564  | total loss: \u001b[1m\u001b[32m0.01673\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 044 | loss: 0.01673 - R2: 0.9992 -- iter: 0540/1168\n",
      "Training Step: 2565  | total loss: \u001b[1m\u001b[32m0.01650\u001b[0m\u001b[0m | time: 0.171s\n",
      "| SGD | epoch: 044 | loss: 0.01650 - R2: 0.9997 -- iter: 0560/1168\n",
      "Training Step: 2566  | total loss: \u001b[1m\u001b[32m0.01636\u001b[0m\u001b[0m | time: 0.176s\n",
      "| SGD | epoch: 044 | loss: 0.01636 - R2: 0.9987 -- iter: 0580/1168\n",
      "Training Step: 2567  | total loss: \u001b[1m\u001b[32m0.01636\u001b[0m\u001b[0m | time: 0.178s\n",
      "| SGD | epoch: 044 | loss: 0.01636 - R2: 0.9987 -- iter: 0600/1168\n",
      "Training Step: 2568  | total loss: \u001b[1m\u001b[32m0.01610\u001b[0m\u001b[0m | time: 0.180s\n",
      "| SGD | epoch: 044 | loss: 0.01610 - R2: 0.9986 -- iter: 0620/1168\n",
      "Training Step: 2569  | total loss: \u001b[1m\u001b[32m0.01654\u001b[0m\u001b[0m | time: 0.182s\n",
      "| SGD | epoch: 044 | loss: 0.01654 - R2: 0.9986 -- iter: 0640/1168\n",
      "Training Step: 2570  | total loss: \u001b[1m\u001b[32m0.02836\u001b[0m\u001b[0m | time: 0.185s\n",
      "| SGD | epoch: 044 | loss: 0.02836 - R2: 1.0011 -- iter: 0660/1168\n",
      "Training Step: 2571  | total loss: \u001b[1m\u001b[32m0.02836\u001b[0m\u001b[0m | time: 0.189s\n",
      "| SGD | epoch: 044 | loss: 0.02836 - R2: 1.0011 -- iter: 0680/1168\n",
      "Training Step: 2572  | total loss: \u001b[1m\u001b[32m0.02427\u001b[0m\u001b[0m | time: 0.192s\n",
      "| SGD | epoch: 044 | loss: 0.02427 - R2: 1.0010 -- iter: 0700/1168\n",
      "Training Step: 2573  | total loss: \u001b[1m\u001b[32m0.02373\u001b[0m\u001b[0m | time: 0.196s\n",
      "| SGD | epoch: 044 | loss: 0.02373 - R2: 1.0006 -- iter: 0720/1168\n",
      "Training Step: 2574  | total loss: \u001b[1m\u001b[32m0.02287\u001b[0m\u001b[0m | time: 0.200s\n",
      "| SGD | epoch: 044 | loss: 0.02287 - R2: 1.0005 -- iter: 0740/1168\n",
      "Training Step: 2575  | total loss: \u001b[1m\u001b[32m0.02299\u001b[0m\u001b[0m | time: 0.204s\n",
      "| SGD | epoch: 044 | loss: 0.02299 - R2: 1.0002 -- iter: 0760/1168\n",
      "Training Step: 2576  | total loss: \u001b[1m\u001b[32m0.02250\u001b[0m\u001b[0m | time: 0.208s\n",
      "| SGD | epoch: 044 | loss: 0.02250 - R2: 1.0002 -- iter: 0780/1168\n",
      "Training Step: 2577  | total loss: \u001b[1m\u001b[32m0.02275\u001b[0m\u001b[0m | time: 0.212s\n",
      "| SGD | epoch: 044 | loss: 0.02275 - R2: 1.0007 -- iter: 0800/1168\n",
      "Training Step: 2578  | total loss: \u001b[1m\u001b[32m0.02275\u001b[0m\u001b[0m | time: 0.215s\n",
      "| SGD | epoch: 044 | loss: 0.02275 - R2: 1.0007 -- iter: 0820/1168\n",
      "Training Step: 2579  | total loss: \u001b[1m\u001b[32m0.02340\u001b[0m\u001b[0m | time: 0.220s\n",
      "| SGD | epoch: 044 | loss: 0.02340 - R2: 1.0000 -- iter: 0840/1168\n",
      "Training Step: 2580  | total loss: \u001b[1m\u001b[32m0.02453\u001b[0m\u001b[0m | time: 0.223s\n",
      "| SGD | epoch: 044 | loss: 0.02453 - R2: 1.0020 -- iter: 0860/1168\n",
      "Training Step: 2581  | total loss: \u001b[1m\u001b[32m0.02267\u001b[0m\u001b[0m | time: 0.227s\n",
      "| SGD | epoch: 044 | loss: 0.02267 - R2: 1.0016 -- iter: 0880/1168\n",
      "Training Step: 2582  | total loss: \u001b[1m\u001b[32m0.02267\u001b[0m\u001b[0m | time: 0.230s\n",
      "| SGD | epoch: 044 | loss: 0.02267 - R2: 1.0016 -- iter: 0900/1168\n",
      "Training Step: 2583  | total loss: \u001b[1m\u001b[32m0.02159\u001b[0m\u001b[0m | time: 0.232s\n",
      "| SGD | epoch: 044 | loss: 0.02159 - R2: 1.0008 -- iter: 0920/1168\n",
      "Training Step: 2584  | total loss: \u001b[1m\u001b[32m0.02017\u001b[0m\u001b[0m | time: 0.234s\n",
      "| SGD | epoch: 044 | loss: 0.02017 - R2: 0.9998 -- iter: 0940/1168\n",
      "Training Step: 2585  | total loss: \u001b[1m\u001b[32m0.02017\u001b[0m\u001b[0m | time: 0.247s\n",
      "| SGD | epoch: 044 | loss: 0.02017 - R2: 0.9998 -- iter: 0960/1168\n",
      "Training Step: 2586  | total loss: \u001b[1m\u001b[32m0.01877\u001b[0m\u001b[0m | time: 0.250s\n",
      "| SGD | epoch: 044 | loss: 0.01877 - R2: 0.9993 -- iter: 0980/1168\n",
      "Training Step: 2587  | total loss: \u001b[1m\u001b[32m0.01877\u001b[0m\u001b[0m | time: 0.253s\n",
      "| SGD | epoch: 044 | loss: 0.01877 - R2: 0.9995 -- iter: 1000/1168\n",
      "Training Step: 2588  | total loss: \u001b[1m\u001b[32m0.01841\u001b[0m\u001b[0m | time: 0.255s\n",
      "| SGD | epoch: 044 | loss: 0.01841 - R2: 0.9995 -- iter: 1020/1168\n",
      "Training Step: 2589  | total loss: \u001b[1m\u001b[32m0.01806\u001b[0m\u001b[0m | time: 0.257s\n",
      "| SGD | epoch: 044 | loss: 0.01806 - R2: 0.9996 -- iter: 1040/1168\n",
      "Training Step: 2590  | total loss: \u001b[1m\u001b[32m0.02027\u001b[0m\u001b[0m | time: 0.263s\n",
      "| SGD | epoch: 044 | loss: 0.02027 - R2: 1.0008 -- iter: 1060/1168\n",
      "Training Step: 2591  | total loss: \u001b[1m\u001b[32m0.02334\u001b[0m\u001b[0m | time: 0.269s\n",
      "| SGD | epoch: 044 | loss: 0.02334 - R2: 1.0008 -- iter: 1080/1168\n",
      "Training Step: 2592  | total loss: \u001b[1m\u001b[32m0.02201\u001b[0m\u001b[0m | time: 0.273s\n",
      "| SGD | epoch: 044 | loss: 0.02201 - R2: 1.0012 -- iter: 1100/1168\n",
      "Training Step: 2593  | total loss: \u001b[1m\u001b[32m0.02057\u001b[0m\u001b[0m | time: 0.275s\n",
      "| SGD | epoch: 044 | loss: 0.02057 - R2: 1.0008 -- iter: 1120/1168\n",
      "Training Step: 2594  | total loss: \u001b[1m\u001b[32m0.01946\u001b[0m\u001b[0m | time: 0.277s\n",
      "| SGD | epoch: 044 | loss: 0.01946 - R2: 1.0009 -- iter: 1140/1168\n",
      "Training Step: 2595  | total loss: \u001b[1m\u001b[32m0.01842\u001b[0m\u001b[0m | time: 0.279s\n",
      "| SGD | epoch: 044 | loss: 0.01842 - R2: 1.0005 -- iter: 1160/1168\n",
      "Training Step: 2596  | total loss: \u001b[1m\u001b[32m0.01770\u001b[0m\u001b[0m | time: 1.286s\n",
      "| SGD | epoch: 044 | loss: 0.01770 - R2: 1.0003 | val_loss: 0.03219 - val_acc: 1.0001 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 2597  | total loss: \u001b[1m\u001b[32m0.01743\u001b[0m\u001b[0m | time: 0.010s\n",
      "| SGD | epoch: 045 | loss: 0.01743 - R2: 1.0010 -- iter: 0020/1168\n",
      "Training Step: 2598  | total loss: \u001b[1m\u001b[32m0.01794\u001b[0m\u001b[0m | time: 0.013s\n",
      "| SGD | epoch: 045 | loss: 0.01794 - R2: 1.0016 -- iter: 0040/1168\n",
      "Training Step: 2599  | total loss: \u001b[1m\u001b[32m0.01679\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 045 | loss: 0.01679 - R2: 1.0015 -- iter: 0060/1168\n",
      "Training Step: 2600  | total loss: \u001b[1m\u001b[32m0.01652\u001b[0m\u001b[0m | time: 0.017s\n",
      "| SGD | epoch: 045 | loss: 0.01652 - R2: 1.0011 -- iter: 0080/1168\n",
      "Training Step: 2601  | total loss: \u001b[1m\u001b[32m0.01701\u001b[0m\u001b[0m | time: 0.019s\n",
      "| SGD | epoch: 045 | loss: 0.01701 - R2: 1.0006 -- iter: 0100/1168\n",
      "Training Step: 2602  | total loss: \u001b[1m\u001b[32m0.01691\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 045 | loss: 0.01691 - R2: 1.0001 -- iter: 0120/1168\n",
      "Training Step: 2603  | total loss: \u001b[1m\u001b[32m0.01653\u001b[0m\u001b[0m | time: 0.023s\n",
      "| SGD | epoch: 045 | loss: 0.01653 - R2: 1.0003 -- iter: 0140/1168\n",
      "Training Step: 2604  | total loss: \u001b[1m\u001b[32m0.01842\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 045 | loss: 0.01842 - R2: 0.9997 -- iter: 0160/1168\n",
      "Training Step: 2605  | total loss: \u001b[1m\u001b[32m0.01842\u001b[0m\u001b[0m | time: 0.026s\n",
      "| SGD | epoch: 045 | loss: 0.01842 - R2: 0.9997 -- iter: 0180/1168\n",
      "Training Step: 2606  | total loss: \u001b[1m\u001b[32m0.01783\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 045 | loss: 0.01783 - R2: 0.9997 -- iter: 0200/1168\n",
      "Training Step: 2607  | total loss: \u001b[1m\u001b[32m0.01789\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 045 | loss: 0.01789 - R2: 0.9997 -- iter: 0220/1168\n",
      "Training Step: 2608  | total loss: \u001b[1m\u001b[32m0.01832\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 045 | loss: 0.01832 - R2: 0.9990 -- iter: 0240/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2609  | total loss: \u001b[1m\u001b[32m0.01865\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 045 | loss: 0.01865 - R2: 0.9993 -- iter: 0260/1168\n",
      "Training Step: 2610  | total loss: \u001b[1m\u001b[32m0.02251\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 045 | loss: 0.02251 - R2: 1.0001 -- iter: 0280/1168\n",
      "Training Step: 2611  | total loss: \u001b[1m\u001b[32m0.02243\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 045 | loss: 0.02243 - R2: 0.9996 -- iter: 0300/1168\n",
      "Training Step: 2612  | total loss: \u001b[1m\u001b[32m0.02159\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 045 | loss: 0.02159 - R2: 0.9990 -- iter: 0320/1168\n",
      "Training Step: 2613  | total loss: \u001b[1m\u001b[32m0.02071\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 045 | loss: 0.02071 - R2: 0.9995 -- iter: 0340/1168\n",
      "Training Step: 2614  | total loss: \u001b[1m\u001b[32m0.02715\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 045 | loss: 0.02715 - R2: 0.9997 -- iter: 0360/1168\n",
      "Training Step: 2615  | total loss: \u001b[1m\u001b[32m0.02715\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 045 | loss: 0.02715 - R2: 0.9997 -- iter: 0380/1168\n",
      "Training Step: 2616  | total loss: \u001b[1m\u001b[32m0.02409\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 045 | loss: 0.02409 - R2: 0.9996 -- iter: 0400/1168\n",
      "Training Step: 2617  | total loss: \u001b[1m\u001b[32m0.02246\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 045 | loss: 0.02246 - R2: 0.9999 -- iter: 0420/1168\n",
      "Training Step: 2618  | total loss: \u001b[1m\u001b[32m0.02135\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 045 | loss: 0.02135 - R2: 0.9999 -- iter: 0440/1168\n",
      "Training Step: 2619  | total loss: \u001b[1m\u001b[32m0.02135\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 045 | loss: 0.02135 - R2: 0.9999 -- iter: 0460/1168\n",
      "Training Step: 2620  | total loss: \u001b[1m\u001b[32m0.02197\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 045 | loss: 0.02197 - R2: 0.9990 -- iter: 0480/1168\n",
      "Training Step: 2621  | total loss: \u001b[1m\u001b[32m0.02124\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 045 | loss: 0.02124 - R2: 0.9987 -- iter: 0500/1168\n",
      "Training Step: 2622  | total loss: \u001b[1m\u001b[32m0.01990\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 045 | loss: 0.01990 - R2: 0.9986 -- iter: 0520/1168\n",
      "Training Step: 2623  | total loss: \u001b[1m\u001b[32m0.01999\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 045 | loss: 0.01999 - R2: 0.9985 -- iter: 0540/1168\n",
      "Training Step: 2624  | total loss: \u001b[1m\u001b[32m0.02052\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 045 | loss: 0.02052 - R2: 0.9992 -- iter: 0560/1168\n",
      "Training Step: 2625  | total loss: \u001b[1m\u001b[32m0.01971\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 045 | loss: 0.01971 - R2: 0.9994 -- iter: 0580/1168\n",
      "Training Step: 2626  | total loss: \u001b[1m\u001b[32m0.02020\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 045 | loss: 0.02020 - R2: 0.9998 -- iter: 0600/1168\n",
      "Training Step: 2627  | total loss: \u001b[1m\u001b[32m0.01964\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 045 | loss: 0.01964 - R2: 0.9995 -- iter: 0620/1168\n",
      "Training Step: 2628  | total loss: \u001b[1m\u001b[32m0.01964\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 045 | loss: 0.01964 - R2: 0.9995 -- iter: 0640/1168\n",
      "Training Step: 2629  | total loss: \u001b[1m\u001b[32m0.02014\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 045 | loss: 0.02014 - R2: 1.0000 -- iter: 0660/1168\n",
      "Training Step: 2630  | total loss: \u001b[1m\u001b[32m0.01919\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 045 | loss: 0.01919 - R2: 1.0006 -- iter: 0680/1168\n",
      "Training Step: 2631  | total loss: \u001b[1m\u001b[32m0.01866\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 045 | loss: 0.01866 - R2: 1.0001 -- iter: 0700/1168\n",
      "Training Step: 2632  | total loss: \u001b[1m\u001b[32m0.01871\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 045 | loss: 0.01871 - R2: 0.9995 -- iter: 0720/1168\n",
      "Training Step: 2633  | total loss: \u001b[1m\u001b[32m0.01829\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 045 | loss: 0.01829 - R2: 0.9995 -- iter: 0740/1168\n",
      "Training Step: 2634  | total loss: \u001b[1m\u001b[32m0.01702\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 045 | loss: 0.01702 - R2: 0.9995 -- iter: 0760/1168\n",
      "Training Step: 2635  | total loss: \u001b[1m\u001b[32m0.01646\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 045 | loss: 0.01646 - R2: 0.9997 -- iter: 0780/1168\n",
      "Training Step: 2636  | total loss: \u001b[1m\u001b[32m0.01580\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 045 | loss: 0.01580 - R2: 0.9998 -- iter: 0800/1168\n",
      "Training Step: 2637  | total loss: \u001b[1m\u001b[32m0.01573\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 045 | loss: 0.01573 - R2: 0.9994 -- iter: 0820/1168\n",
      "Training Step: 2638  | total loss: \u001b[1m\u001b[32m0.01523\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 045 | loss: 0.01523 - R2: 0.9990 -- iter: 0840/1168\n",
      "Training Step: 2639  | total loss: \u001b[1m\u001b[32m0.01471\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 045 | loss: 0.01471 - R2: 0.9990 -- iter: 0860/1168\n",
      "Training Step: 2640  | total loss: \u001b[1m\u001b[32m0.01385\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 045 | loss: 0.01385 - R2: 0.9985 -- iter: 0880/1168\n",
      "Training Step: 2641  | total loss: \u001b[1m\u001b[32m0.01364\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 045 | loss: 0.01364 - R2: 0.9982 -- iter: 0900/1168\n",
      "Training Step: 2642  | total loss: \u001b[1m\u001b[32m0.01364\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 045 | loss: 0.01364 - R2: 0.9982 -- iter: 0920/1168\n",
      "Training Step: 2643  | total loss: \u001b[1m\u001b[32m0.01406\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 045 | loss: 0.01406 - R2: 0.9987 -- iter: 0940/1168\n",
      "Training Step: 2644  | total loss: \u001b[1m\u001b[32m0.01406\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 045 | loss: 0.01406 - R2: 0.9987 -- iter: 0960/1168\n",
      "Training Step: 2645  | total loss: \u001b[1m\u001b[32m0.01355\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 045 | loss: 0.01355 - R2: 0.9989 -- iter: 0980/1168\n",
      "Training Step: 2646  | total loss: \u001b[1m\u001b[32m0.01499\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 045 | loss: 0.01499 - R2: 0.9982 -- iter: 1000/1168\n",
      "Training Step: 2647  | total loss: \u001b[1m\u001b[32m0.01499\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 045 | loss: 0.01499 - R2: 0.9982 -- iter: 1020/1168\n",
      "Training Step: 2648  | total loss: \u001b[1m\u001b[32m0.01622\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 045 | loss: 0.01622 - R2: 0.9992 -- iter: 1040/1168\n",
      "Training Step: 2649  | total loss: \u001b[1m\u001b[32m0.01865\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 045 | loss: 0.01865 - R2: 1.0000 -- iter: 1060/1168\n",
      "Training Step: 2650  | total loss: \u001b[1m\u001b[32m0.01758\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 045 | loss: 0.01758 - R2: 0.9997 -- iter: 1080/1168\n",
      "Training Step: 2651  | total loss: \u001b[1m\u001b[32m0.01785\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 045 | loss: 0.01785 - R2: 0.9995 -- iter: 1100/1168\n",
      "Training Step: 2652  | total loss: \u001b[1m\u001b[32m0.01716\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 045 | loss: 0.01716 - R2: 0.9999 -- iter: 1120/1168\n",
      "Training Step: 2653  | total loss: \u001b[1m\u001b[32m0.01752\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 045 | loss: 0.01752 - R2: 0.9993 -- iter: 1140/1168\n",
      "Training Step: 2654  | total loss: \u001b[1m\u001b[32m0.01702\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 045 | loss: 0.01702 - R2: 0.9995 -- iter: 1160/1168\n",
      "Training Step: 2655  | total loss: \u001b[1m\u001b[32m0.01702\u001b[0m\u001b[0m | time: 1.152s\n",
      "| SGD | epoch: 045 | loss: 0.01702 - R2: 0.9995 | val_loss: 0.03205 - val_acc: 1.0011 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 2656  | total loss: \u001b[1m\u001b[32m0.01606\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 046 | loss: 0.01606 - R2: 0.9996 -- iter: 0020/1168\n",
      "Training Step: 2657  | total loss: \u001b[1m\u001b[32m0.01558\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 046 | loss: 0.01558 - R2: 0.9999 -- iter: 0040/1168\n",
      "Training Step: 2658  | total loss: \u001b[1m\u001b[32m0.01490\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 046 | loss: 0.01490 - R2: 0.9999 -- iter: 0060/1168\n",
      "Training Step: 2659  | total loss: \u001b[1m\u001b[32m0.01363\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 046 | loss: 0.01363 - R2: 1.0004 -- iter: 0080/1168\n",
      "Training Step: 2660  | total loss: \u001b[1m\u001b[32m0.01363\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 046 | loss: 0.01363 - R2: 1.0004 -- iter: 0100/1168\n",
      "Training Step: 2661  | total loss: \u001b[1m\u001b[32m0.01499\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 046 | loss: 0.01499 - R2: 1.0011 -- iter: 0120/1168\n",
      "Training Step: 2662  | total loss: \u001b[1m\u001b[32m0.01653\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 046 | loss: 0.01653 - R2: 1.0013 -- iter: 0140/1168\n",
      "Training Step: 2663  | total loss: \u001b[1m\u001b[32m0.01653\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 046 | loss: 0.01653 - R2: 1.0002 -- iter: 0160/1168\n",
      "Training Step: 2664  | total loss: \u001b[1m\u001b[32m0.01693\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 046 | loss: 0.01693 - R2: 1.0002 -- iter: 0180/1168\n",
      "Training Step: 2665  | total loss: \u001b[1m\u001b[32m0.01847\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 046 | loss: 0.01847 - R2: 0.9997 -- iter: 0200/1168\n",
      "Training Step: 2666  | total loss: \u001b[1m\u001b[32m0.01812\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 046 | loss: 0.01812 - R2: 1.0002 -- iter: 0220/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2667  | total loss: \u001b[1m\u001b[32m0.01757\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 046 | loss: 0.01757 - R2: 1.0000 -- iter: 0240/1168\n",
      "Training Step: 2668  | total loss: \u001b[1m\u001b[32m0.04863\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 046 | loss: 0.04863 - R2: 0.9967 -- iter: 0260/1168\n",
      "Training Step: 2669  | total loss: \u001b[1m\u001b[32m0.04498\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 046 | loss: 0.04498 - R2: 0.9966 -- iter: 0280/1168\n",
      "Training Step: 2670  | total loss: \u001b[1m\u001b[32m0.03882\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 046 | loss: 0.03882 - R2: 0.9970 -- iter: 0300/1168\n",
      "Training Step: 2671  | total loss: \u001b[1m\u001b[32m0.03576\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 046 | loss: 0.03576 - R2: 0.9968 -- iter: 0320/1168\n",
      "Training Step: 2672  | total loss: \u001b[1m\u001b[32m0.03576\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 046 | loss: 0.03576 - R2: 0.9968 -- iter: 0340/1168\n",
      "Training Step: 2673  | total loss: \u001b[1m\u001b[32m0.03297\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 046 | loss: 0.03297 - R2: 0.9976 -- iter: 0360/1168\n",
      "Training Step: 2674  | total loss: \u001b[1m\u001b[32m0.03083\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 046 | loss: 0.03083 - R2: 0.9976 -- iter: 0380/1168\n",
      "Training Step: 2675  | total loss: \u001b[1m\u001b[32m0.02855\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 046 | loss: 0.02855 - R2: 0.9982 -- iter: 0400/1168\n",
      "Training Step: 2676  | total loss: \u001b[1m\u001b[32m0.02666\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 046 | loss: 0.02666 - R2: 0.9987 -- iter: 0420/1168\n",
      "Training Step: 2677  | total loss: \u001b[1m\u001b[32m0.02662\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 046 | loss: 0.02662 - R2: 0.9994 -- iter: 0440/1168\n",
      "Training Step: 2678  | total loss: \u001b[1m\u001b[32m0.02553\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 046 | loss: 0.02553 - R2: 0.9992 -- iter: 0460/1168\n",
      "Training Step: 2679  | total loss: \u001b[1m\u001b[32m0.02553\u001b[0m\u001b[0m | time: 0.183s\n",
      "| SGD | epoch: 046 | loss: 0.02553 - R2: 0.9992 -- iter: 0480/1168\n",
      "Training Step: 2680  | total loss: \u001b[1m\u001b[32m0.02469\u001b[0m\u001b[0m | time: 0.185s\n",
      "| SGD | epoch: 046 | loss: 0.02469 - R2: 0.9986 -- iter: 0500/1168\n",
      "Training Step: 2681  | total loss: \u001b[1m\u001b[32m0.02207\u001b[0m\u001b[0m | time: 0.189s\n",
      "| SGD | epoch: 046 | loss: 0.02207 - R2: 0.9986 -- iter: 0520/1168\n",
      "Training Step: 2682  | total loss: \u001b[1m\u001b[32m0.02150\u001b[0m\u001b[0m | time: 0.193s\n",
      "| SGD | epoch: 046 | loss: 0.02150 - R2: 0.9986 -- iter: 0540/1168\n",
      "Training Step: 2683  | total loss: \u001b[1m\u001b[32m0.02150\u001b[0m\u001b[0m | time: 0.196s\n",
      "| SGD | epoch: 046 | loss: 0.02150 - R2: 0.9986 -- iter: 0560/1168\n",
      "Training Step: 2684  | total loss: \u001b[1m\u001b[32m0.02057\u001b[0m\u001b[0m | time: 0.199s\n",
      "| SGD | epoch: 046 | loss: 0.02057 - R2: 0.9994 -- iter: 0580/1168\n",
      "Training Step: 2685  | total loss: \u001b[1m\u001b[32m0.02007\u001b[0m\u001b[0m | time: 0.200s\n",
      "| SGD | epoch: 046 | loss: 0.02007 - R2: 0.9996 -- iter: 0600/1168\n",
      "Training Step: 2686  | total loss: \u001b[1m\u001b[32m0.01917\u001b[0m\u001b[0m | time: 0.202s\n",
      "| SGD | epoch: 046 | loss: 0.01917 - R2: 1.0000 -- iter: 0620/1168\n",
      "Training Step: 2687  | total loss: \u001b[1m\u001b[32m0.02558\u001b[0m\u001b[0m | time: 0.207s\n",
      "| SGD | epoch: 046 | loss: 0.02558 - R2: 1.0008 -- iter: 0640/1168\n",
      "Training Step: 2688  | total loss: \u001b[1m\u001b[32m0.02558\u001b[0m\u001b[0m | time: 0.212s\n",
      "| SGD | epoch: 046 | loss: 0.02558 - R2: 1.0006 -- iter: 0660/1168\n",
      "Training Step: 2689  | total loss: \u001b[1m\u001b[32m0.02423\u001b[0m\u001b[0m | time: 0.216s\n",
      "| SGD | epoch: 046 | loss: 0.02423 - R2: 1.0006 -- iter: 0680/1168\n",
      "Training Step: 2690  | total loss: \u001b[1m\u001b[32m0.02144\u001b[0m\u001b[0m | time: 0.218s\n",
      "| SGD | epoch: 046 | loss: 0.02144 - R2: 1.0007 -- iter: 0700/1168\n",
      "Training Step: 2691  | total loss: \u001b[1m\u001b[32m0.02144\u001b[0m\u001b[0m | time: 0.228s\n",
      "| SGD | epoch: 046 | loss: 0.02144 - R2: 1.0007 -- iter: 0720/1168\n",
      "Training Step: 2692  | total loss: \u001b[1m\u001b[32m0.02187\u001b[0m\u001b[0m | time: 0.232s\n",
      "| SGD | epoch: 046 | loss: 0.02187 - R2: 1.0009 -- iter: 0740/1168\n",
      "Training Step: 2693  | total loss: \u001b[1m\u001b[32m0.02396\u001b[0m\u001b[0m | time: 0.236s\n",
      "| SGD | epoch: 046 | loss: 0.02396 - R2: 1.0020 -- iter: 0760/1168\n",
      "Training Step: 2694  | total loss: \u001b[1m\u001b[32m0.02396\u001b[0m\u001b[0m | time: 0.241s\n",
      "| SGD | epoch: 046 | loss: 0.02396 - R2: 1.0020 -- iter: 0780/1168\n",
      "Training Step: 2695  | total loss: \u001b[1m\u001b[32m0.02421\u001b[0m\u001b[0m | time: 0.245s\n",
      "| SGD | epoch: 046 | loss: 0.02421 - R2: 1.0009 -- iter: 0800/1168\n",
      "Training Step: 2696  | total loss: \u001b[1m\u001b[32m0.02361\u001b[0m\u001b[0m | time: 0.249s\n",
      "| SGD | epoch: 046 | loss: 0.02361 - R2: 1.0004 -- iter: 0820/1168\n",
      "Training Step: 2697  | total loss: \u001b[1m\u001b[32m0.02361\u001b[0m\u001b[0m | time: 0.251s\n",
      "| SGD | epoch: 046 | loss: 0.02361 - R2: 1.0015 -- iter: 0840/1168\n",
      "Training Step: 2698  | total loss: \u001b[1m\u001b[32m0.02232\u001b[0m\u001b[0m | time: 0.254s\n",
      "| SGD | epoch: 046 | loss: 0.02232 - R2: 1.0014 -- iter: 0860/1168\n",
      "Training Step: 2699  | total loss: \u001b[1m\u001b[32m0.02467\u001b[0m\u001b[0m | time: 0.258s\n",
      "| SGD | epoch: 046 | loss: 0.02467 - R2: 1.0004 -- iter: 0880/1168\n",
      "Training Step: 2700  | total loss: \u001b[1m\u001b[32m0.02467\u001b[0m\u001b[0m | time: 0.260s\n",
      "| SGD | epoch: 046 | loss: 0.02467 - R2: 1.0004 -- iter: 0900/1168\n",
      "Training Step: 2701  | total loss: \u001b[1m\u001b[32m0.02671\u001b[0m\u001b[0m | time: 0.262s\n",
      "| SGD | epoch: 046 | loss: 0.02671 - R2: 0.9995 -- iter: 0920/1168\n",
      "Training Step: 2702  | total loss: \u001b[1m\u001b[32m0.02696\u001b[0m\u001b[0m | time: 0.265s\n",
      "| SGD | epoch: 046 | loss: 0.02696 - R2: 0.9999 -- iter: 0940/1168\n",
      "Training Step: 2703  | total loss: \u001b[1m\u001b[32m0.02451\u001b[0m\u001b[0m | time: 0.269s\n",
      "| SGD | epoch: 046 | loss: 0.02451 - R2: 1.0005 -- iter: 0960/1168\n",
      "Training Step: 2704  | total loss: \u001b[1m\u001b[32m0.02350\u001b[0m\u001b[0m | time: 0.275s\n",
      "| SGD | epoch: 046 | loss: 0.02350 - R2: 0.9999 -- iter: 0980/1168\n",
      "Training Step: 2705  | total loss: \u001b[1m\u001b[32m0.02222\u001b[0m\u001b[0m | time: 0.282s\n",
      "| SGD | epoch: 046 | loss: 0.02222 - R2: 0.9996 -- iter: 1000/1168\n",
      "Training Step: 2706  | total loss: \u001b[1m\u001b[32m0.02136\u001b[0m\u001b[0m | time: 0.287s\n",
      "| SGD | epoch: 046 | loss: 0.02136 - R2: 0.9999 -- iter: 1020/1168\n",
      "Training Step: 2707  | total loss: \u001b[1m\u001b[32m0.02073\u001b[0m\u001b[0m | time: 0.293s\n",
      "| SGD | epoch: 046 | loss: 0.02073 - R2: 0.9999 -- iter: 1040/1168\n",
      "Training Step: 2708  | total loss: \u001b[1m\u001b[32m0.02073\u001b[0m\u001b[0m | time: 0.297s\n",
      "| SGD | epoch: 046 | loss: 0.02073 - R2: 0.9999 -- iter: 1060/1168\n",
      "Training Step: 2709  | total loss: \u001b[1m\u001b[32m0.01918\u001b[0m\u001b[0m | time: 0.301s\n",
      "| SGD | epoch: 046 | loss: 0.01918 - R2: 1.0002 -- iter: 1080/1168\n",
      "Training Step: 2710  | total loss: \u001b[1m\u001b[32m0.01918\u001b[0m\u001b[0m | time: 0.303s\n",
      "| SGD | epoch: 046 | loss: 0.01918 - R2: 1.0002 -- iter: 1100/1168\n",
      "Training Step: 2711  | total loss: \u001b[1m\u001b[32m0.01816\u001b[0m\u001b[0m | time: 0.307s\n",
      "| SGD | epoch: 046 | loss: 0.01816 - R2: 1.0000 -- iter: 1120/1168\n",
      "Training Step: 2712  | total loss: \u001b[1m\u001b[32m0.01795\u001b[0m\u001b[0m | time: 0.314s\n",
      "| SGD | epoch: 046 | loss: 0.01795 - R2: 0.9999 -- iter: 1140/1168\n",
      "Training Step: 2713  | total loss: \u001b[1m\u001b[32m0.01713\u001b[0m\u001b[0m | time: 0.316s\n",
      "| SGD | epoch: 046 | loss: 0.01713 - R2: 0.9996 -- iter: 1160/1168\n",
      "Training Step: 2714  | total loss: \u001b[1m\u001b[32m0.01713\u001b[0m\u001b[0m | time: 1.322s\n",
      "| SGD | epoch: 046 | loss: 0.01713 - R2: 0.9996 | val_loss: 0.03205 - val_acc: 1.0012 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 2715  | total loss: \u001b[1m\u001b[32m0.01968\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 047 | loss: 0.01968 - R2: 0.9999 -- iter: 0020/1168\n",
      "Training Step: 2716  | total loss: \u001b[1m\u001b[32m0.01900\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 047 | loss: 0.01900 - R2: 0.9995 -- iter: 0040/1168\n",
      "Training Step: 2717  | total loss: \u001b[1m\u001b[32m0.01935\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 047 | loss: 0.01935 - R2: 0.9993 -- iter: 0060/1168\n",
      "Training Step: 2718  | total loss: \u001b[1m\u001b[32m0.01935\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 047 | loss: 0.01935 - R2: 0.9993 -- iter: 0080/1168\n",
      "Training Step: 2719  | total loss: \u001b[1m\u001b[32m0.01886\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 047 | loss: 0.01886 - R2: 0.9992 -- iter: 0100/1168\n",
      "Training Step: 2720  | total loss: \u001b[1m\u001b[32m0.01807\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 047 | loss: 0.01807 - R2: 0.9990 -- iter: 0120/1168\n",
      "Training Step: 2721  | total loss: \u001b[1m\u001b[32m0.01740\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 047 | loss: 0.01740 - R2: 0.9987 -- iter: 0140/1168\n",
      "Training Step: 2722  | total loss: \u001b[1m\u001b[32m0.01772\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 047 | loss: 0.01772 - R2: 0.9983 -- iter: 0160/1168\n",
      "Training Step: 2723  | total loss: \u001b[1m\u001b[32m0.01656\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 047 | loss: 0.01656 - R2: 0.9987 -- iter: 0180/1168\n",
      "Training Step: 2724  | total loss: \u001b[1m\u001b[32m0.01986\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 047 | loss: 0.01986 - R2: 0.9993 -- iter: 0200/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2725  | total loss: \u001b[1m\u001b[32m0.01986\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 047 | loss: 0.01986 - R2: 0.9993 -- iter: 0220/1168\n",
      "Training Step: 2726  | total loss: \u001b[1m\u001b[32m0.01886\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 047 | loss: 0.01886 - R2: 0.9998 -- iter: 0240/1168\n",
      "Training Step: 2727  | total loss: \u001b[1m\u001b[32m0.02217\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 047 | loss: 0.02217 - R2: 1.0005 -- iter: 0260/1168\n",
      "Training Step: 2728  | total loss: \u001b[1m\u001b[32m0.02142\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 047 | loss: 0.02142 - R2: 0.9992 -- iter: 0280/1168\n",
      "Training Step: 2729  | total loss: \u001b[1m\u001b[32m0.02142\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 047 | loss: 0.02142 - R2: 0.9992 -- iter: 0300/1168\n",
      "Training Step: 2730  | total loss: \u001b[1m\u001b[32m0.02164\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 047 | loss: 0.02164 - R2: 0.9995 -- iter: 0320/1168\n",
      "Training Step: 2731  | total loss: \u001b[1m\u001b[32m0.02132\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 047 | loss: 0.02132 - R2: 0.9993 -- iter: 0340/1168\n",
      "Training Step: 2732  | total loss: \u001b[1m\u001b[32m0.02008\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 047 | loss: 0.02008 - R2: 0.9993 -- iter: 0360/1168\n",
      "Training Step: 2733  | total loss: \u001b[1m\u001b[32m0.01900\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 047 | loss: 0.01900 - R2: 0.9995 -- iter: 0380/1168\n",
      "Training Step: 2734  | total loss: \u001b[1m\u001b[32m0.01806\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 047 | loss: 0.01806 - R2: 0.9995 -- iter: 0400/1168\n",
      "Training Step: 2735  | total loss: \u001b[1m\u001b[32m0.01914\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 047 | loss: 0.01914 - R2: 0.9997 -- iter: 0420/1168\n",
      "Training Step: 2736  | total loss: \u001b[1m\u001b[32m0.01914\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 047 | loss: 0.01914 - R2: 0.9996 -- iter: 0440/1168\n",
      "Training Step: 2737  | total loss: \u001b[1m\u001b[32m0.01797\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 047 | loss: 0.01797 - R2: 0.9996 -- iter: 0460/1168\n",
      "Training Step: 2738  | total loss: \u001b[1m\u001b[32m0.02072\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 047 | loss: 0.02072 - R2: 1.0000 -- iter: 0480/1168\n",
      "Training Step: 2739  | total loss: \u001b[1m\u001b[32m0.02084\u001b[0m\u001b[0m | time: 0.165s\n",
      "| SGD | epoch: 047 | loss: 0.02084 - R2: 1.0003 -- iter: 0500/1168\n",
      "Training Step: 2740  | total loss: \u001b[1m\u001b[32m0.02137\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 047 | loss: 0.02137 - R2: 1.0012 -- iter: 0520/1168\n",
      "Training Step: 2741  | total loss: \u001b[1m\u001b[32m0.02458\u001b[0m\u001b[0m | time: 0.169s\n",
      "| SGD | epoch: 047 | loss: 0.02458 - R2: 1.0017 -- iter: 0540/1168\n",
      "Training Step: 2742  | total loss: \u001b[1m\u001b[32m0.02343\u001b[0m\u001b[0m | time: 0.171s\n",
      "| SGD | epoch: 047 | loss: 0.02343 - R2: 1.0021 -- iter: 0560/1168\n",
      "Training Step: 2743  | total loss: \u001b[1m\u001b[32m0.02189\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 047 | loss: 0.02189 - R2: 1.0021 -- iter: 0580/1168\n",
      "Training Step: 2744  | total loss: \u001b[1m\u001b[32m0.02189\u001b[0m\u001b[0m | time: 0.184s\n",
      "| SGD | epoch: 047 | loss: 0.02189 - R2: 1.0021 -- iter: 0600/1168\n",
      "Training Step: 2745  | total loss: \u001b[1m\u001b[32m0.02124\u001b[0m\u001b[0m | time: 0.188s\n",
      "| SGD | epoch: 047 | loss: 0.02124 - R2: 1.0021 -- iter: 0620/1168\n",
      "Training Step: 2746  | total loss: \u001b[1m\u001b[32m0.02052\u001b[0m\u001b[0m | time: 0.192s\n",
      "| SGD | epoch: 047 | loss: 0.02052 - R2: 1.0008 -- iter: 0640/1168\n",
      "Training Step: 2747  | total loss: \u001b[1m\u001b[32m0.02052\u001b[0m\u001b[0m | time: 0.195s\n",
      "| SGD | epoch: 047 | loss: 0.02052 - R2: 1.0007 -- iter: 0660/1168\n",
      "Training Step: 2748  | total loss: \u001b[1m\u001b[32m0.02052\u001b[0m\u001b[0m | time: 0.197s\n",
      "| SGD | epoch: 047 | loss: 0.02052 - R2: 1.0007 -- iter: 0680/1168\n",
      "Training Step: 2749  | total loss: \u001b[1m\u001b[32m0.02044\u001b[0m\u001b[0m | time: 0.200s\n",
      "| SGD | epoch: 047 | loss: 0.02044 - R2: 0.9996 -- iter: 0700/1168\n",
      "Training Step: 2750  | total loss: \u001b[1m\u001b[32m0.01851\u001b[0m\u001b[0m | time: 0.203s\n",
      "| SGD | epoch: 047 | loss: 0.01851 - R2: 0.9996 -- iter: 0720/1168\n",
      "Training Step: 2751  | total loss: \u001b[1m\u001b[32m0.01851\u001b[0m\u001b[0m | time: 0.207s\n",
      "| SGD | epoch: 047 | loss: 0.01851 - R2: 0.9996 -- iter: 0740/1168\n",
      "Training Step: 2752  | total loss: \u001b[1m\u001b[32m0.02354\u001b[0m\u001b[0m | time: 0.211s\n",
      "| SGD | epoch: 047 | loss: 0.02354 - R2: 1.0011 -- iter: 0760/1168\n",
      "Training Step: 2753  | total loss: \u001b[1m\u001b[32m0.02354\u001b[0m\u001b[0m | time: 0.221s\n",
      "| SGD | epoch: 047 | loss: 0.02354 - R2: 1.0011 -- iter: 0780/1168\n",
      "Training Step: 2754  | total loss: \u001b[1m\u001b[32m0.02247\u001b[0m\u001b[0m | time: 0.224s\n",
      "| SGD | epoch: 047 | loss: 0.02247 - R2: 1.0007 -- iter: 0800/1168\n",
      "Training Step: 2755  | total loss: \u001b[1m\u001b[32m0.02282\u001b[0m\u001b[0m | time: 0.225s\n",
      "| SGD | epoch: 047 | loss: 0.02282 - R2: 1.0018 -- iter: 0820/1168\n",
      "Training Step: 2756  | total loss: \u001b[1m\u001b[32m0.02119\u001b[0m\u001b[0m | time: 0.228s\n",
      "| SGD | epoch: 047 | loss: 0.02119 - R2: 1.0025 -- iter: 0840/1168\n",
      "Training Step: 2757  | total loss: \u001b[1m\u001b[32m0.02208\u001b[0m\u001b[0m | time: 0.231s\n",
      "| SGD | epoch: 047 | loss: 0.02208 - R2: 1.0022 -- iter: 0860/1168\n",
      "Training Step: 2758  | total loss: \u001b[1m\u001b[32m0.02208\u001b[0m\u001b[0m | time: 0.234s\n",
      "| SGD | epoch: 047 | loss: 0.02208 - R2: 1.0022 -- iter: 0880/1168\n",
      "Training Step: 2759  | total loss: \u001b[1m\u001b[32m0.02140\u001b[0m\u001b[0m | time: 0.236s\n",
      "| SGD | epoch: 047 | loss: 0.02140 - R2: 1.0020 -- iter: 0900/1168\n",
      "Training Step: 2760  | total loss: \u001b[1m\u001b[32m0.02168\u001b[0m\u001b[0m | time: 0.243s\n",
      "| SGD | epoch: 047 | loss: 0.02168 - R2: 1.0030 -- iter: 0920/1168\n",
      "Training Step: 2761  | total loss: \u001b[1m\u001b[32m0.02181\u001b[0m\u001b[0m | time: 0.245s\n",
      "| SGD | epoch: 047 | loss: 0.02181 - R2: 1.0039 -- iter: 0940/1168\n",
      "Training Step: 2762  | total loss: \u001b[1m\u001b[32m0.02054\u001b[0m\u001b[0m | time: 0.247s\n",
      "| SGD | epoch: 047 | loss: 0.02054 - R2: 1.0035 -- iter: 0960/1168\n",
      "Training Step: 2763  | total loss: \u001b[1m\u001b[32m0.01933\u001b[0m\u001b[0m | time: 0.251s\n",
      "| SGD | epoch: 047 | loss: 0.01933 - R2: 1.0027 -- iter: 0980/1168\n",
      "Training Step: 2764  | total loss: \u001b[1m\u001b[32m0.01816\u001b[0m\u001b[0m | time: 0.253s\n",
      "| SGD | epoch: 047 | loss: 0.01816 - R2: 1.0025 -- iter: 1000/1168\n",
      "Training Step: 2765  | total loss: \u001b[1m\u001b[32m0.01764\u001b[0m\u001b[0m | time: 0.256s\n",
      "| SGD | epoch: 047 | loss: 0.01764 - R2: 1.0026 -- iter: 1020/1168\n",
      "Training Step: 2766  | total loss: \u001b[1m\u001b[32m0.01729\u001b[0m\u001b[0m | time: 0.260s\n",
      "| SGD | epoch: 047 | loss: 0.01729 - R2: 1.0016 -- iter: 1040/1168\n",
      "Training Step: 2767  | total loss: \u001b[1m\u001b[32m0.01729\u001b[0m\u001b[0m | time: 0.261s\n",
      "| SGD | epoch: 047 | loss: 0.01729 - R2: 1.0016 -- iter: 1060/1168\n",
      "Training Step: 2768  | total loss: \u001b[1m\u001b[32m0.01649\u001b[0m\u001b[0m | time: 0.264s\n",
      "| SGD | epoch: 047 | loss: 0.01649 - R2: 1.0009 -- iter: 1080/1168\n",
      "Training Step: 2769  | total loss: \u001b[1m\u001b[32m0.01814\u001b[0m\u001b[0m | time: 0.269s\n",
      "| SGD | epoch: 047 | loss: 0.01814 - R2: 1.0000 -- iter: 1100/1168\n",
      "Training Step: 2770  | total loss: \u001b[1m\u001b[32m0.01783\u001b[0m\u001b[0m | time: 0.271s\n",
      "| SGD | epoch: 047 | loss: 0.01783 - R2: 1.0005 -- iter: 1120/1168\n",
      "Training Step: 2771  | total loss: \u001b[1m\u001b[32m0.01790\u001b[0m\u001b[0m | time: 0.274s\n",
      "| SGD | epoch: 047 | loss: 0.01790 - R2: 0.9999 -- iter: 1140/1168\n",
      "Training Step: 2772  | total loss: \u001b[1m\u001b[32m0.01748\u001b[0m\u001b[0m | time: 0.277s\n",
      "| SGD | epoch: 047 | loss: 0.01748 - R2: 0.9997 -- iter: 1160/1168\n",
      "Training Step: 2773  | total loss: \u001b[1m\u001b[32m0.01684\u001b[0m\u001b[0m | time: 1.283s\n",
      "| SGD | epoch: 047 | loss: 0.01684 - R2: 0.9996 | val_loss: 0.03169 - val_acc: 1.0002 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 2774  | total loss: \u001b[1m\u001b[32m0.01869\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 048 | loss: 0.01869 - R2: 0.9999 -- iter: 0020/1168\n",
      "Training Step: 2775  | total loss: \u001b[1m\u001b[32m0.01814\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 048 | loss: 0.01814 - R2: 1.0002 -- iter: 0040/1168\n",
      "Training Step: 2776  | total loss: \u001b[1m\u001b[32m0.01775\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 048 | loss: 0.01775 - R2: 1.0002 -- iter: 0060/1168\n",
      "Training Step: 2777  | total loss: \u001b[1m\u001b[32m0.01741\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 048 | loss: 0.01741 - R2: 0.9996 -- iter: 0080/1168\n",
      "Training Step: 2778  | total loss: \u001b[1m\u001b[32m0.01743\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 048 | loss: 0.01743 - R2: 0.9984 -- iter: 0100/1168\n",
      "Training Step: 2779  | total loss: \u001b[1m\u001b[32m0.01697\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 048 | loss: 0.01697 - R2: 0.9987 -- iter: 0120/1168\n",
      "Training Step: 2780  | total loss: \u001b[1m\u001b[32m0.01616\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 048 | loss: 0.01616 - R2: 0.9984 -- iter: 0140/1168\n",
      "Training Step: 2781  | total loss: \u001b[1m\u001b[32m0.01603\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 048 | loss: 0.01603 - R2: 0.9984 -- iter: 0160/1168\n",
      "Training Step: 2782  | total loss: \u001b[1m\u001b[32m0.01833\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 048 | loss: 0.01833 - R2: 0.9991 -- iter: 0180/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2783  | total loss: \u001b[1m\u001b[32m0.01788\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 048 | loss: 0.01788 - R2: 0.9987 -- iter: 0200/1168\n",
      "Training Step: 2784  | total loss: \u001b[1m\u001b[32m0.01694\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 048 | loss: 0.01694 - R2: 0.9983 -- iter: 0220/1168\n",
      "Training Step: 2785  | total loss: \u001b[1m\u001b[32m0.01594\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 048 | loss: 0.01594 - R2: 0.9981 -- iter: 0240/1168\n",
      "Training Step: 2786  | total loss: \u001b[1m\u001b[32m0.01652\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 048 | loss: 0.01652 - R2: 0.9983 -- iter: 0260/1168\n",
      "Training Step: 2787  | total loss: \u001b[1m\u001b[32m0.01597\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 048 | loss: 0.01597 - R2: 0.9983 -- iter: 0280/1168\n",
      "Training Step: 2788  | total loss: \u001b[1m\u001b[32m0.04044\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 048 | loss: 0.04044 - R2: 1.0001 -- iter: 0300/1168\n",
      "Training Step: 2789  | total loss: \u001b[1m\u001b[32m0.03741\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 048 | loss: 0.03741 - R2: 1.0001 -- iter: 0320/1168\n",
      "Training Step: 2790  | total loss: \u001b[1m\u001b[32m0.03460\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 048 | loss: 0.03460 - R2: 0.9992 -- iter: 0340/1168\n",
      "Training Step: 2791  | total loss: \u001b[1m\u001b[32m0.03319\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 048 | loss: 0.03319 - R2: 0.9992 -- iter: 0360/1168\n",
      "Training Step: 2792  | total loss: \u001b[1m\u001b[32m0.03463\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 048 | loss: 0.03463 - R2: 0.9995 -- iter: 0380/1168\n",
      "Training Step: 2793  | total loss: \u001b[1m\u001b[32m0.03186\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 048 | loss: 0.03186 - R2: 0.9995 -- iter: 0400/1168\n",
      "Training Step: 2794  | total loss: \u001b[1m\u001b[32m0.02829\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 048 | loss: 0.02829 - R2: 0.9996 -- iter: 0420/1168\n",
      "Training Step: 2795  | total loss: \u001b[1m\u001b[32m0.02669\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 048 | loss: 0.02669 - R2: 0.9996 -- iter: 0440/1168\n",
      "Training Step: 2796  | total loss: \u001b[1m\u001b[32m0.02581\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 048 | loss: 0.02581 - R2: 0.9991 -- iter: 0460/1168\n",
      "Training Step: 2797  | total loss: \u001b[1m\u001b[32m0.02581\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 048 | loss: 0.02581 - R2: 0.9991 -- iter: 0480/1168\n",
      "Training Step: 2798  | total loss: \u001b[1m\u001b[32m0.02555\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 048 | loss: 0.02555 - R2: 0.9990 -- iter: 0500/1168\n",
      "Training Step: 2799  | total loss: \u001b[1m\u001b[32m0.02371\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 048 | loss: 0.02371 - R2: 0.9993 -- iter: 0520/1168\n",
      "Training Step: 2800  | total loss: \u001b[1m\u001b[32m0.02309\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 048 | loss: 0.02309 - R2: 0.9989 -- iter: 0540/1168\n",
      "Training Step: 2801  | total loss: \u001b[1m\u001b[32m0.02788\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 048 | loss: 0.02788 - R2: 1.0011 -- iter: 0560/1168\n",
      "Training Step: 2802  | total loss: \u001b[1m\u001b[32m0.02703\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 048 | loss: 0.02703 - R2: 1.0011 -- iter: 0580/1168\n",
      "Training Step: 2803  | total loss: \u001b[1m\u001b[32m0.02703\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 048 | loss: 0.02703 - R2: 1.0011 -- iter: 0600/1168\n",
      "Training Step: 2804  | total loss: \u001b[1m\u001b[32m0.02509\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 048 | loss: 0.02509 - R2: 1.0003 -- iter: 0620/1168\n",
      "Training Step: 2805  | total loss: \u001b[1m\u001b[32m0.02404\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 048 | loss: 0.02404 - R2: 0.9995 -- iter: 0640/1168\n",
      "Training Step: 2806  | total loss: \u001b[1m\u001b[32m0.02271\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 048 | loss: 0.02271 - R2: 0.9990 -- iter: 0660/1168\n",
      "Training Step: 2807  | total loss: \u001b[1m\u001b[32m0.02201\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 048 | loss: 0.02201 - R2: 0.9990 -- iter: 0680/1168\n",
      "Training Step: 2808  | total loss: \u001b[1m\u001b[32m0.02032\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 048 | loss: 0.02032 - R2: 0.9994 -- iter: 0700/1168\n",
      "Training Step: 2809  | total loss: \u001b[1m\u001b[32m0.01980\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 048 | loss: 0.01980 - R2: 0.9991 -- iter: 0720/1168\n",
      "Training Step: 2810  | total loss: \u001b[1m\u001b[32m0.01905\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 048 | loss: 0.01905 - R2: 0.9991 -- iter: 0740/1168\n",
      "Training Step: 2811  | total loss: \u001b[1m\u001b[32m0.01972\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 048 | loss: 0.01972 - R2: 0.9998 -- iter: 0760/1168\n",
      "Training Step: 2812  | total loss: \u001b[1m\u001b[32m0.01979\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 048 | loss: 0.01979 - R2: 0.9992 -- iter: 0780/1168\n",
      "Training Step: 2813  | total loss: \u001b[1m\u001b[32m0.01863\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 048 | loss: 0.01863 - R2: 0.9995 -- iter: 0800/1168\n",
      "Training Step: 2814  | total loss: \u001b[1m\u001b[32m0.01859\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 048 | loss: 0.01859 - R2: 0.9988 -- iter: 0820/1168\n",
      "Training Step: 2815  | total loss: \u001b[1m\u001b[32m0.01736\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 048 | loss: 0.01736 - R2: 0.9991 -- iter: 0840/1168\n",
      "Training Step: 2816  | total loss: \u001b[1m\u001b[32m0.01690\u001b[0m\u001b[0m | time: 0.172s\n",
      "| SGD | epoch: 048 | loss: 0.01690 - R2: 0.9989 -- iter: 0860/1168\n",
      "Training Step: 2817  | total loss: \u001b[1m\u001b[32m0.01690\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 048 | loss: 0.01690 - R2: 0.9989 -- iter: 0880/1168\n",
      "Training Step: 2818  | total loss: \u001b[1m\u001b[32m0.01771\u001b[0m\u001b[0m | time: 0.182s\n",
      "| SGD | epoch: 048 | loss: 0.01771 - R2: 0.9998 -- iter: 0900/1168\n",
      "Training Step: 2819  | total loss: \u001b[1m\u001b[32m0.01739\u001b[0m\u001b[0m | time: 0.189s\n",
      "| SGD | epoch: 048 | loss: 0.01739 - R2: 0.9997 -- iter: 0920/1168\n",
      "Training Step: 2820  | total loss: \u001b[1m\u001b[32m0.02019\u001b[0m\u001b[0m | time: 0.191s\n",
      "| SGD | epoch: 048 | loss: 0.02019 - R2: 1.0004 -- iter: 0940/1168\n",
      "Training Step: 2821  | total loss: \u001b[1m\u001b[32m0.02261\u001b[0m\u001b[0m | time: 0.195s\n",
      "| SGD | epoch: 048 | loss: 0.02261 - R2: 1.0011 -- iter: 0960/1168\n",
      "Training Step: 2822  | total loss: \u001b[1m\u001b[32m0.02142\u001b[0m\u001b[0m | time: 0.197s\n",
      "| SGD | epoch: 048 | loss: 0.02142 - R2: 1.0015 -- iter: 0980/1168\n",
      "Training Step: 2823  | total loss: \u001b[1m\u001b[32m0.02042\u001b[0m\u001b[0m | time: 0.200s\n",
      "| SGD | epoch: 048 | loss: 0.02042 - R2: 1.0008 -- iter: 1000/1168\n",
      "Training Step: 2824  | total loss: \u001b[1m\u001b[32m0.02060\u001b[0m\u001b[0m | time: 0.203s\n",
      "| SGD | epoch: 048 | loss: 0.02060 - R2: 0.9998 -- iter: 1020/1168\n",
      "Training Step: 2825  | total loss: \u001b[1m\u001b[32m0.02177\u001b[0m\u001b[0m | time: 0.205s\n",
      "| SGD | epoch: 048 | loss: 0.02177 - R2: 0.9997 -- iter: 1040/1168\n",
      "Training Step: 2826  | total loss: \u001b[1m\u001b[32m0.02199\u001b[0m\u001b[0m | time: 0.209s\n",
      "| SGD | epoch: 048 | loss: 0.02199 - R2: 1.0000 -- iter: 1060/1168\n",
      "Training Step: 2827  | total loss: \u001b[1m\u001b[32m0.02160\u001b[0m\u001b[0m | time: 0.219s\n",
      "| SGD | epoch: 048 | loss: 0.02160 - R2: 0.9997 -- iter: 1080/1168\n",
      "Training Step: 2828  | total loss: \u001b[1m\u001b[32m0.02160\u001b[0m\u001b[0m | time: 0.221s\n",
      "| SGD | epoch: 048 | loss: 0.02160 - R2: 0.9997 -- iter: 1100/1168\n",
      "Training Step: 2829  | total loss: \u001b[1m\u001b[32m0.02341\u001b[0m\u001b[0m | time: 0.225s\n",
      "| SGD | epoch: 048 | loss: 0.02341 - R2: 1.0000 -- iter: 1120/1168\n",
      "Training Step: 2830  | total loss: \u001b[1m\u001b[32m0.02102\u001b[0m\u001b[0m | time: 0.229s\n",
      "| SGD | epoch: 048 | loss: 0.02102 - R2: 1.0005 -- iter: 1140/1168\n",
      "Training Step: 2831  | total loss: \u001b[1m\u001b[32m0.02018\u001b[0m\u001b[0m | time: 0.240s\n",
      "| SGD | epoch: 048 | loss: 0.02018 - R2: 1.0004 -- iter: 1160/1168\n",
      "Training Step: 2832  | total loss: \u001b[1m\u001b[32m0.01965\u001b[0m\u001b[0m | time: 1.247s\n",
      "| SGD | epoch: 048 | loss: 0.01965 - R2: 0.9999 | val_loss: 0.03188 - val_acc: 1.0008 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 2833  | total loss: \u001b[1m\u001b[32m0.02023\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 049 | loss: 0.02023 - R2: 0.9991 -- iter: 0020/1168\n",
      "Training Step: 2834  | total loss: \u001b[1m\u001b[32m0.02023\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 049 | loss: 0.02023 - R2: 0.9991 -- iter: 0040/1168\n",
      "Training Step: 2835  | total loss: \u001b[1m\u001b[32m0.01941\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 049 | loss: 0.01941 - R2: 0.9990 -- iter: 0060/1168\n",
      "Training Step: 2836  | total loss: \u001b[1m\u001b[32m0.01861\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 049 | loss: 0.01861 - R2: 0.9984 -- iter: 0080/1168\n",
      "Training Step: 2837  | total loss: \u001b[1m\u001b[32m0.01927\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 049 | loss: 0.01927 - R2: 0.9995 -- iter: 0100/1168\n",
      "Training Step: 2838  | total loss: \u001b[1m\u001b[32m0.01859\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 049 | loss: 0.01859 - R2: 0.9998 -- iter: 0120/1168\n",
      "Training Step: 2839  | total loss: \u001b[1m\u001b[32m0.01890\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 049 | loss: 0.01890 - R2: 1.0009 -- iter: 0140/1168\n",
      "Training Step: 2840  | total loss: \u001b[1m\u001b[32m0.01773\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 049 | loss: 0.01773 - R2: 1.0008 -- iter: 0160/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2841  | total loss: \u001b[1m\u001b[32m0.01721\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 049 | loss: 0.01721 - R2: 1.0004 -- iter: 0180/1168\n",
      "Training Step: 2842  | total loss: \u001b[1m\u001b[32m0.01711\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 049 | loss: 0.01711 - R2: 1.0004 -- iter: 0200/1168\n",
      "Training Step: 2843  | total loss: \u001b[1m\u001b[32m0.01624\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 049 | loss: 0.01624 - R2: 1.0004 -- iter: 0220/1168\n",
      "Training Step: 2844  | total loss: \u001b[1m\u001b[32m0.01531\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 049 | loss: 0.01531 - R2: 1.0000 -- iter: 0240/1168\n",
      "Training Step: 2845  | total loss: \u001b[1m\u001b[32m0.01546\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 049 | loss: 0.01546 - R2: 0.9997 -- iter: 0260/1168\n",
      "Training Step: 2846  | total loss: \u001b[1m\u001b[32m0.01500\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 049 | loss: 0.01500 - R2: 1.0001 -- iter: 0280/1168\n",
      "Training Step: 2847  | total loss: \u001b[1m\u001b[32m0.01500\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 049 | loss: 0.01500 - R2: 1.0001 -- iter: 0300/1168\n",
      "Training Step: 2848  | total loss: \u001b[1m\u001b[32m0.02184\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 049 | loss: 0.02184 - R2: 1.0014 -- iter: 0320/1168\n",
      "Training Step: 2849  | total loss: \u001b[1m\u001b[32m0.02113\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 049 | loss: 0.02113 - R2: 1.0010 -- iter: 0340/1168\n",
      "Training Step: 2850  | total loss: \u001b[1m\u001b[32m0.01912\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 049 | loss: 0.01912 - R2: 1.0009 -- iter: 0360/1168\n",
      "Training Step: 2851  | total loss: \u001b[1m\u001b[32m0.01917\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 049 | loss: 0.01917 - R2: 1.0007 -- iter: 0380/1168\n",
      "Training Step: 2852  | total loss: \u001b[1m\u001b[32m0.02123\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 049 | loss: 0.02123 - R2: 1.0015 -- iter: 0400/1168\n",
      "Training Step: 2853  | total loss: \u001b[1m\u001b[32m0.02068\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 049 | loss: 0.02068 - R2: 1.0012 -- iter: 0420/1168\n",
      "Training Step: 2854  | total loss: \u001b[1m\u001b[32m0.02068\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 049 | loss: 0.02068 - R2: 1.0012 -- iter: 0440/1168\n",
      "Training Step: 2855  | total loss: \u001b[1m\u001b[32m0.02780\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 049 | loss: 0.02780 - R2: 1.0012 -- iter: 0460/1168\n",
      "Training Step: 2856  | total loss: \u001b[1m\u001b[32m0.02592\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 049 | loss: 0.02592 - R2: 1.0009 -- iter: 0480/1168\n",
      "Training Step: 2857  | total loss: \u001b[1m\u001b[32m0.02367\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 049 | loss: 0.02367 - R2: 1.0001 -- iter: 0500/1168\n",
      "Training Step: 2858  | total loss: \u001b[1m\u001b[32m0.02292\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 049 | loss: 0.02292 - R2: 1.0004 -- iter: 0520/1168\n",
      "Training Step: 2859  | total loss: \u001b[1m\u001b[32m0.02239\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 049 | loss: 0.02239 - R2: 1.0004 -- iter: 0540/1168\n",
      "Training Step: 2860  | total loss: \u001b[1m\u001b[32m0.02239\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 049 | loss: 0.02239 - R2: 0.9999 -- iter: 0560/1168\n",
      "Training Step: 2861  | total loss: \u001b[1m\u001b[32m0.01987\u001b[0m\u001b[0m | time: 0.165s\n",
      "| SGD | epoch: 049 | loss: 0.01987 - R2: 1.0000 -- iter: 0580/1168\n",
      "Training Step: 2862  | total loss: \u001b[1m\u001b[32m0.01940\u001b[0m\u001b[0m | time: 0.169s\n",
      "| SGD | epoch: 049 | loss: 0.01940 - R2: 1.0002 -- iter: 0600/1168\n",
      "Training Step: 2863  | total loss: \u001b[1m\u001b[32m0.01940\u001b[0m\u001b[0m | time: 0.172s\n",
      "| SGD | epoch: 049 | loss: 0.01940 - R2: 1.0002 -- iter: 0620/1168\n",
      "Training Step: 2864  | total loss: \u001b[1m\u001b[32m0.01846\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 049 | loss: 0.01846 - R2: 1.0007 -- iter: 0640/1168\n",
      "Training Step: 2865  | total loss: \u001b[1m\u001b[32m0.01953\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 049 | loss: 0.01953 - R2: 0.9993 -- iter: 0660/1168\n",
      "Training Step: 2866  | total loss: \u001b[1m\u001b[32m0.01858\u001b[0m\u001b[0m | time: 0.179s\n",
      "| SGD | epoch: 049 | loss: 0.01858 - R2: 0.9992 -- iter: 0680/1168\n",
      "Training Step: 2867  | total loss: \u001b[1m\u001b[32m0.01758\u001b[0m\u001b[0m | time: 0.181s\n",
      "| SGD | epoch: 049 | loss: 0.01758 - R2: 0.9999 -- iter: 0700/1168\n",
      "Training Step: 2868  | total loss: \u001b[1m\u001b[32m0.01758\u001b[0m\u001b[0m | time: 0.184s\n",
      "| SGD | epoch: 049 | loss: 0.01758 - R2: 0.9999 -- iter: 0720/1168\n",
      "Training Step: 2869  | total loss: \u001b[1m\u001b[32m0.01765\u001b[0m\u001b[0m | time: 0.191s\n",
      "| SGD | epoch: 049 | loss: 0.01765 - R2: 0.9991 -- iter: 0740/1168\n",
      "Training Step: 2870  | total loss: \u001b[1m\u001b[32m0.01653\u001b[0m\u001b[0m | time: 0.195s\n",
      "| SGD | epoch: 049 | loss: 0.01653 - R2: 0.9992 -- iter: 0760/1168\n",
      "Training Step: 2871  | total loss: \u001b[1m\u001b[32m0.01653\u001b[0m\u001b[0m | time: 0.197s\n",
      "| SGD | epoch: 049 | loss: 0.01653 - R2: 0.9992 -- iter: 0780/1168\n",
      "Training Step: 2872  | total loss: \u001b[1m\u001b[32m0.01589\u001b[0m\u001b[0m | time: 0.199s\n",
      "| SGD | epoch: 049 | loss: 0.01589 - R2: 0.9991 -- iter: 0800/1168\n",
      "Training Step: 2873  | total loss: \u001b[1m\u001b[32m0.01507\u001b[0m\u001b[0m | time: 0.201s\n",
      "| SGD | epoch: 049 | loss: 0.01507 - R2: 0.9995 -- iter: 0820/1168\n",
      "Training Step: 2874  | total loss: \u001b[1m\u001b[32m0.01439\u001b[0m\u001b[0m | time: 0.204s\n",
      "| SGD | epoch: 049 | loss: 0.01439 - R2: 0.9997 -- iter: 0840/1168\n",
      "Training Step: 2875  | total loss: \u001b[1m\u001b[32m0.01498\u001b[0m\u001b[0m | time: 0.207s\n",
      "| SGD | epoch: 049 | loss: 0.01498 - R2: 1.0000 -- iter: 0860/1168\n",
      "Training Step: 2876  | total loss: \u001b[1m\u001b[32m0.01482\u001b[0m\u001b[0m | time: 0.211s\n",
      "| SGD | epoch: 049 | loss: 0.01482 - R2: 1.0003 -- iter: 0880/1168\n",
      "Training Step: 2877  | total loss: \u001b[1m\u001b[32m0.01482\u001b[0m\u001b[0m | time: 0.215s\n",
      "| SGD | epoch: 049 | loss: 0.01482 - R2: 1.0003 -- iter: 0900/1168\n",
      "Training Step: 2878  | total loss: \u001b[1m\u001b[32m0.01623\u001b[0m\u001b[0m | time: 0.219s\n",
      "| SGD | epoch: 049 | loss: 0.01623 - R2: 0.9996 -- iter: 0920/1168\n",
      "Training Step: 2879  | total loss: \u001b[1m\u001b[32m0.01516\u001b[0m\u001b[0m | time: 0.224s\n",
      "| SGD | epoch: 049 | loss: 0.01516 - R2: 0.9992 -- iter: 0940/1168\n",
      "Training Step: 2880  | total loss: \u001b[1m\u001b[32m0.01516\u001b[0m\u001b[0m | time: 0.227s\n",
      "| SGD | epoch: 049 | loss: 0.01516 - R2: 0.9992 -- iter: 0960/1168\n",
      "Training Step: 2881  | total loss: \u001b[1m\u001b[32m0.01410\u001b[0m\u001b[0m | time: 0.234s\n",
      "| SGD | epoch: 049 | loss: 0.01410 - R2: 0.9992 -- iter: 0980/1168\n",
      "Training Step: 2882  | total loss: \u001b[1m\u001b[32m0.01410\u001b[0m\u001b[0m | time: 0.236s\n",
      "| SGD | epoch: 049 | loss: 0.01410 - R2: 0.9992 -- iter: 1000/1168\n",
      "Training Step: 2883  | total loss: \u001b[1m\u001b[32m0.01687\u001b[0m\u001b[0m | time: 0.238s\n",
      "| SGD | epoch: 049 | loss: 0.01687 - R2: 0.9981 -- iter: 1020/1168\n",
      "Training Step: 2884  | total loss: \u001b[1m\u001b[32m0.01599\u001b[0m\u001b[0m | time: 0.242s\n",
      "| SGD | epoch: 049 | loss: 0.01599 - R2: 0.9990 -- iter: 1040/1168\n",
      "Training Step: 2885  | total loss: \u001b[1m\u001b[32m0.01860\u001b[0m\u001b[0m | time: 0.246s\n",
      "| SGD | epoch: 049 | loss: 0.01860 - R2: 0.9995 -- iter: 1060/1168\n",
      "Training Step: 2886  | total loss: \u001b[1m\u001b[32m0.01860\u001b[0m\u001b[0m | time: 0.248s\n",
      "| SGD | epoch: 049 | loss: 0.01860 - R2: 1.0001 -- iter: 1080/1168\n",
      "Training Step: 2887  | total loss: \u001b[1m\u001b[32m0.01809\u001b[0m\u001b[0m | time: 0.255s\n",
      "| SGD | epoch: 049 | loss: 0.01809 - R2: 0.9998 -- iter: 1100/1168\n",
      "Training Step: 2888  | total loss: \u001b[1m\u001b[32m0.01761\u001b[0m\u001b[0m | time: 0.263s\n",
      "| SGD | epoch: 049 | loss: 0.01761 - R2: 1.0003 -- iter: 1120/1168\n",
      "Training Step: 2889  | total loss: \u001b[1m\u001b[32m0.01661\u001b[0m\u001b[0m | time: 0.284s\n",
      "| SGD | epoch: 049 | loss: 0.01661 - R2: 1.0008 -- iter: 1140/1168\n",
      "Training Step: 2890  | total loss: \u001b[1m\u001b[32m0.01663\u001b[0m\u001b[0m | time: 0.291s\n",
      "| SGD | epoch: 049 | loss: 0.01663 - R2: 1.0002 -- iter: 1160/1168\n",
      "Training Step: 2891  | total loss: \u001b[1m\u001b[32m0.01663\u001b[0m\u001b[0m | time: 1.302s\n",
      "| SGD | epoch: 049 | loss: 0.01663 - R2: 1.0002 | val_loss: 0.03182 - val_acc: 1.0005 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 2892  | total loss: \u001b[1m\u001b[32m0.01673\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 050 | loss: 0.01673 - R2: 1.0006 -- iter: 0020/1168\n",
      "Training Step: 2893  | total loss: \u001b[1m\u001b[32m0.01979\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 050 | loss: 0.01979 - R2: 1.0009 -- iter: 0040/1168\n",
      "Training Step: 2894  | total loss: \u001b[1m\u001b[32m0.01905\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 050 | loss: 0.01905 - R2: 0.9995 -- iter: 0060/1168\n",
      "Training Step: 2895  | total loss: \u001b[1m\u001b[32m0.01905\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 050 | loss: 0.01905 - R2: 0.9996 -- iter: 0080/1168\n",
      "Training Step: 2896  | total loss: \u001b[1m\u001b[32m0.01791\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 050 | loss: 0.01791 - R2: 0.9988 -- iter: 0100/1168\n",
      "Training Step: 2897  | total loss: \u001b[1m\u001b[32m0.01691\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 050 | loss: 0.01691 - R2: 0.9989 -- iter: 0120/1168\n",
      "Training Step: 2898  | total loss: \u001b[1m\u001b[32m0.01691\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 050 | loss: 0.01691 - R2: 0.9989 -- iter: 0140/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2899  | total loss: \u001b[1m\u001b[32m0.01592\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 050 | loss: 0.01592 - R2: 0.9986 -- iter: 0160/1168\n",
      "Training Step: 2900  | total loss: \u001b[1m\u001b[32m0.01973\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 050 | loss: 0.01973 - R2: 0.9991 -- iter: 0180/1168\n",
      "Training Step: 2901  | total loss: \u001b[1m\u001b[32m0.01888\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 050 | loss: 0.01888 - R2: 0.9995 -- iter: 0200/1168\n",
      "Training Step: 2902  | total loss: \u001b[1m\u001b[32m0.01891\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 050 | loss: 0.01891 - R2: 0.9990 -- iter: 0220/1168\n",
      "Training Step: 2903  | total loss: \u001b[1m\u001b[32m0.01842\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 050 | loss: 0.01842 - R2: 0.9987 -- iter: 0240/1168\n",
      "Training Step: 2904  | total loss: \u001b[1m\u001b[32m0.01915\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 050 | loss: 0.01915 - R2: 0.9993 -- iter: 0260/1168\n",
      "Training Step: 2905  | total loss: \u001b[1m\u001b[32m0.01826\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 050 | loss: 0.01826 - R2: 0.9988 -- iter: 0280/1168\n",
      "Training Step: 2906  | total loss: \u001b[1m\u001b[32m0.02085\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 050 | loss: 0.02085 - R2: 1.0002 -- iter: 0300/1168\n",
      "Training Step: 2907  | total loss: \u001b[1m\u001b[32m0.01873\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 050 | loss: 0.01873 - R2: 0.9994 -- iter: 0320/1168\n",
      "Training Step: 2908  | total loss: \u001b[1m\u001b[32m0.01853\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 050 | loss: 0.01853 - R2: 0.9997 -- iter: 0340/1168\n",
      "Training Step: 2909  | total loss: \u001b[1m\u001b[32m0.01919\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 050 | loss: 0.01919 - R2: 0.9996 -- iter: 0360/1168\n",
      "Training Step: 2910  | total loss: \u001b[1m\u001b[32m0.01919\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 050 | loss: 0.01919 - R2: 0.9996 -- iter: 0380/1168\n",
      "Training Step: 2911  | total loss: \u001b[1m\u001b[32m0.02216\u001b[0m\u001b[0m | time: 0.169s\n",
      "| SGD | epoch: 050 | loss: 0.02216 - R2: 1.0011 -- iter: 0400/1168\n",
      "Training Step: 2912  | total loss: \u001b[1m\u001b[32m0.02266\u001b[0m\u001b[0m | time: 0.171s\n",
      "| SGD | epoch: 050 | loss: 0.02266 - R2: 1.0012 -- iter: 0420/1168\n",
      "Training Step: 2913  | total loss: \u001b[1m\u001b[32m0.02159\u001b[0m\u001b[0m | time: 0.176s\n",
      "| SGD | epoch: 050 | loss: 0.02159 - R2: 1.0012 -- iter: 0440/1168\n",
      "Training Step: 2914  | total loss: \u001b[1m\u001b[32m0.02159\u001b[0m\u001b[0m | time: 0.196s\n",
      "| SGD | epoch: 050 | loss: 0.02159 - R2: 1.0012 -- iter: 0460/1168\n",
      "Training Step: 2915  | total loss: \u001b[1m\u001b[32m0.02707\u001b[0m\u001b[0m | time: 0.202s\n",
      "| SGD | epoch: 050 | loss: 0.02707 - R2: 1.0013 -- iter: 0480/1168\n",
      "Training Step: 2916  | total loss: \u001b[1m\u001b[32m0.02707\u001b[0m\u001b[0m | time: 0.205s\n",
      "| SGD | epoch: 050 | loss: 0.02707 - R2: 1.0013 -- iter: 0500/1168\n",
      "Training Step: 2917  | total loss: \u001b[1m\u001b[32m0.02497\u001b[0m\u001b[0m | time: 0.214s\n",
      "| SGD | epoch: 050 | loss: 0.02497 - R2: 1.0011 -- iter: 0520/1168\n",
      "Training Step: 2918  | total loss: \u001b[1m\u001b[32m0.02346\u001b[0m\u001b[0m | time: 0.217s\n",
      "| SGD | epoch: 050 | loss: 0.02346 - R2: 1.0013 -- iter: 0540/1168\n",
      "Training Step: 2919  | total loss: \u001b[1m\u001b[32m0.02314\u001b[0m\u001b[0m | time: 0.220s\n",
      "| SGD | epoch: 050 | loss: 0.02314 - R2: 1.0009 -- iter: 0560/1168\n",
      "Training Step: 2920  | total loss: \u001b[1m\u001b[32m0.02182\u001b[0m\u001b[0m | time: 0.223s\n",
      "| SGD | epoch: 050 | loss: 0.02182 - R2: 1.0009 -- iter: 0580/1168\n",
      "Training Step: 2921  | total loss: \u001b[1m\u001b[32m0.02091\u001b[0m\u001b[0m | time: 0.226s\n",
      "| SGD | epoch: 050 | loss: 0.02091 - R2: 1.0000 -- iter: 0600/1168\n",
      "Training Step: 2922  | total loss: \u001b[1m\u001b[32m0.02208\u001b[0m\u001b[0m | time: 0.230s\n",
      "| SGD | epoch: 050 | loss: 0.02208 - R2: 1.0000 -- iter: 0620/1168\n",
      "Training Step: 2923  | total loss: \u001b[1m\u001b[32m0.02159\u001b[0m\u001b[0m | time: 0.234s\n",
      "| SGD | epoch: 050 | loss: 0.02159 - R2: 0.9993 -- iter: 0640/1168\n",
      "Training Step: 2924  | total loss: \u001b[1m\u001b[32m0.02020\u001b[0m\u001b[0m | time: 0.236s\n",
      "| SGD | epoch: 050 | loss: 0.02020 - R2: 0.9992 -- iter: 0660/1168\n",
      "Training Step: 2925  | total loss: \u001b[1m\u001b[32m0.01954\u001b[0m\u001b[0m | time: 0.242s\n",
      "| SGD | epoch: 050 | loss: 0.01954 - R2: 0.9984 -- iter: 0680/1168\n",
      "Training Step: 2926  | total loss: \u001b[1m\u001b[32m0.01896\u001b[0m\u001b[0m | time: 0.257s\n",
      "| SGD | epoch: 050 | loss: 0.01896 - R2: 0.9989 -- iter: 0700/1168\n",
      "Training Step: 2927  | total loss: \u001b[1m\u001b[32m0.01854\u001b[0m\u001b[0m | time: 0.263s\n",
      "| SGD | epoch: 050 | loss: 0.01854 - R2: 0.9989 -- iter: 0720/1168\n",
      "Training Step: 2928  | total loss: \u001b[1m\u001b[32m0.01854\u001b[0m\u001b[0m | time: 0.267s\n",
      "| SGD | epoch: 050 | loss: 0.01854 - R2: 0.9989 -- iter: 0740/1168\n",
      "Training Step: 2929  | total loss: \u001b[1m\u001b[32m0.01961\u001b[0m\u001b[0m | time: 0.270s\n",
      "| SGD | epoch: 050 | loss: 0.01961 - R2: 0.9999 -- iter: 0760/1168\n",
      "Training Step: 2930  | total loss: \u001b[1m\u001b[32m0.01867\u001b[0m\u001b[0m | time: 0.274s\n",
      "| SGD | epoch: 050 | loss: 0.01867 - R2: 0.9999 -- iter: 0780/1168\n",
      "Training Step: 2931  | total loss: \u001b[1m\u001b[32m0.02260\u001b[0m\u001b[0m | time: 0.276s\n",
      "| SGD | epoch: 050 | loss: 0.02260 - R2: 1.0005 -- iter: 0800/1168\n",
      "Training Step: 2932  | total loss: \u001b[1m\u001b[32m0.02165\u001b[0m\u001b[0m | time: 0.278s\n",
      "| SGD | epoch: 050 | loss: 0.02165 - R2: 1.0001 -- iter: 0820/1168\n",
      "Training Step: 2933  | total loss: \u001b[1m\u001b[32m0.02041\u001b[0m\u001b[0m | time: 0.281s\n",
      "| SGD | epoch: 050 | loss: 0.02041 - R2: 1.0002 -- iter: 0840/1168\n",
      "Training Step: 2934  | total loss: \u001b[1m\u001b[32m0.02134\u001b[0m\u001b[0m | time: 0.285s\n",
      "| SGD | epoch: 050 | loss: 0.02134 - R2: 1.0010 -- iter: 0860/1168\n",
      "Training Step: 2935  | total loss: \u001b[1m\u001b[32m0.01990\u001b[0m\u001b[0m | time: 0.291s\n",
      "| SGD | epoch: 050 | loss: 0.01990 - R2: 1.0005 -- iter: 0880/1168\n",
      "Training Step: 2936  | total loss: \u001b[1m\u001b[32m0.01846\u001b[0m\u001b[0m | time: 0.297s\n",
      "| SGD | epoch: 050 | loss: 0.01846 - R2: 1.0002 -- iter: 0900/1168\n",
      "Training Step: 2937  | total loss: \u001b[1m\u001b[32m0.01797\u001b[0m\u001b[0m | time: 0.301s\n",
      "| SGD | epoch: 050 | loss: 0.01797 - R2: 0.9998 -- iter: 0920/1168\n",
      "Training Step: 2938  | total loss: \u001b[1m\u001b[32m0.01797\u001b[0m\u001b[0m | time: 0.304s\n",
      "| SGD | epoch: 050 | loss: 0.01797 - R2: 0.9998 -- iter: 0940/1168\n",
      "Training Step: 2939  | total loss: \u001b[1m\u001b[32m0.01614\u001b[0m\u001b[0m | time: 0.308s\n",
      "| SGD | epoch: 050 | loss: 0.01614 - R2: 1.0000 -- iter: 0960/1168\n",
      "Training Step: 2940  | total loss: \u001b[1m\u001b[32m0.01614\u001b[0m\u001b[0m | time: 0.319s\n",
      "| SGD | epoch: 050 | loss: 0.01614 - R2: 1.0000 -- iter: 0980/1168\n",
      "Training Step: 2941  | total loss: \u001b[1m\u001b[32m0.01528\u001b[0m\u001b[0m | time: 0.323s\n",
      "| SGD | epoch: 050 | loss: 0.01528 - R2: 1.0005 -- iter: 1000/1168\n",
      "Training Step: 2942  | total loss: \u001b[1m\u001b[32m0.01480\u001b[0m\u001b[0m | time: 0.327s\n",
      "| SGD | epoch: 050 | loss: 0.01480 - R2: 1.0002 -- iter: 1020/1168\n",
      "Training Step: 2943  | total loss: \u001b[1m\u001b[32m0.01674\u001b[0m\u001b[0m | time: 0.331s\n",
      "| SGD | epoch: 050 | loss: 0.01674 - R2: 0.9998 -- iter: 1040/1168\n",
      "Training Step: 2944  | total loss: \u001b[1m\u001b[32m0.01668\u001b[0m\u001b[0m | time: 0.334s\n",
      "| SGD | epoch: 050 | loss: 0.01668 - R2: 0.9998 -- iter: 1060/1168\n",
      "Training Step: 2945  | total loss: \u001b[1m\u001b[32m0.01629\u001b[0m\u001b[0m | time: 0.337s\n",
      "| SGD | epoch: 050 | loss: 0.01629 - R2: 1.0006 -- iter: 1080/1168\n",
      "Training Step: 2946  | total loss: \u001b[1m\u001b[32m0.01629\u001b[0m\u001b[0m | time: 0.342s\n",
      "| SGD | epoch: 050 | loss: 0.01629 - R2: 1.0010 -- iter: 1100/1168\n",
      "Training Step: 2947  | total loss: \u001b[1m\u001b[32m0.01581\u001b[0m\u001b[0m | time: 0.349s\n",
      "| SGD | epoch: 050 | loss: 0.01581 - R2: 1.0004 -- iter: 1120/1168\n",
      "Training Step: 2948  | total loss: \u001b[1m\u001b[32m0.01530\u001b[0m\u001b[0m | time: 0.353s\n",
      "| SGD | epoch: 050 | loss: 0.01530 - R2: 1.0003 -- iter: 1140/1168\n",
      "Training Step: 2949  | total loss: \u001b[1m\u001b[32m0.01686\u001b[0m\u001b[0m | time: 0.357s\n",
      "| SGD | epoch: 050 | loss: 0.01686 - R2: 1.0006 -- iter: 1160/1168\n",
      "Training Step: 2950  | total loss: \u001b[1m\u001b[32m0.01621\u001b[0m\u001b[0m | time: 1.366s\n",
      "| SGD | epoch: 050 | loss: 0.01621 - R2: 1.0003 | val_loss: 0.03189 - val_acc: 1.0005 -- iter: 1168/1168\n",
      "--\n",
      "---------------------------------\n",
      "Run id: LFP4L8\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "---------------------------------\n",
      "Training samples: 1168\n",
      "Validation samples: 292\n",
      "--\n",
      "Training Step: 2951  | total loss: \u001b[1m\u001b[32m0.03582\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 051 | loss: 0.03582 - R2: 1.0016 -- iter: 0020/1168\n",
      "Training Step: 2952  | total loss: \u001b[1m\u001b[32m0.03293\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 051 | loss: 0.03293 - R2: 1.0010 -- iter: 0040/1168\n",
      "Training Step: 2953  | total loss: \u001b[1m\u001b[32m0.03075\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 051 | loss: 0.03075 - R2: 1.0004 -- iter: 0060/1168\n",
      "Training Step: 2954  | total loss: \u001b[1m\u001b[32m0.03069\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 051 | loss: 0.03069 - R2: 1.0001 -- iter: 0080/1168\n",
      "Training Step: 2955  | total loss: \u001b[1m\u001b[32m0.02902\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 051 | loss: 0.02902 - R2: 1.0006 -- iter: 0100/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2956  | total loss: \u001b[1m\u001b[32m0.02836\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 051 | loss: 0.02836 - R2: 1.0007 -- iter: 0120/1168\n",
      "Training Step: 2957  | total loss: \u001b[1m\u001b[32m0.02836\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 051 | loss: 0.02836 - R2: 1.0007 -- iter: 0140/1168\n",
      "Training Step: 2958  | total loss: \u001b[1m\u001b[32m0.02654\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 051 | loss: 0.02654 - R2: 1.0005 -- iter: 0160/1168\n",
      "Training Step: 2959  | total loss: \u001b[1m\u001b[32m0.02547\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 051 | loss: 0.02547 - R2: 1.0005 -- iter: 0180/1168\n",
      "Training Step: 2960  | total loss: \u001b[1m\u001b[32m0.02696\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 051 | loss: 0.02696 - R2: 1.0008 -- iter: 0200/1168\n",
      "Training Step: 2961  | total loss: \u001b[1m\u001b[32m0.02696\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 051 | loss: 0.02696 - R2: 1.0008 -- iter: 0220/1168\n",
      "Training Step: 2962  | total loss: \u001b[1m\u001b[32m0.02538\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 051 | loss: 0.02538 - R2: 1.0005 -- iter: 0240/1168\n",
      "Training Step: 2963  | total loss: \u001b[1m\u001b[32m0.02538\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 051 | loss: 0.02538 - R2: 1.0005 -- iter: 0260/1168\n",
      "Training Step: 2964  | total loss: \u001b[1m\u001b[32m0.02321\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 051 | loss: 0.02321 - R2: 1.0001 -- iter: 0280/1168\n",
      "Training Step: 2965  | total loss: \u001b[1m\u001b[32m0.02321\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 051 | loss: 0.02321 - R2: 1.0001 -- iter: 0300/1168\n",
      "Training Step: 2966  | total loss: \u001b[1m\u001b[32m0.02216\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 051 | loss: 0.02216 - R2: 1.0003 -- iter: 0320/1168\n",
      "Training Step: 2967  | total loss: \u001b[1m\u001b[32m0.02156\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 051 | loss: 0.02156 - R2: 1.0008 -- iter: 0340/1168\n",
      "Training Step: 2968  | total loss: \u001b[1m\u001b[32m0.02059\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 051 | loss: 0.02059 - R2: 1.0011 -- iter: 0360/1168\n",
      "Training Step: 2969  | total loss: \u001b[1m\u001b[32m0.01968\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 051 | loss: 0.01968 - R2: 1.0015 -- iter: 0380/1168\n",
      "Training Step: 2970  | total loss: \u001b[1m\u001b[32m0.01872\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 051 | loss: 0.01872 - R2: 1.0008 -- iter: 0400/1168\n",
      "Training Step: 2971  | total loss: \u001b[1m\u001b[32m0.01913\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 051 | loss: 0.01913 - R2: 1.0000 -- iter: 0420/1168\n",
      "Training Step: 2972  | total loss: \u001b[1m\u001b[32m0.01913\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 051 | loss: 0.01913 - R2: 1.0000 -- iter: 0440/1168\n",
      "Training Step: 2973  | total loss: \u001b[1m\u001b[32m0.01876\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 051 | loss: 0.01876 - R2: 0.9995 -- iter: 0460/1168\n",
      "Training Step: 2974  | total loss: \u001b[1m\u001b[32m0.01838\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 051 | loss: 0.01838 - R2: 0.9995 -- iter: 0480/1168\n",
      "Training Step: 2975  | total loss: \u001b[1m\u001b[32m0.01758\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 051 | loss: 0.01758 - R2: 0.9996 -- iter: 0500/1168\n",
      "Training Step: 2976  | total loss: \u001b[1m\u001b[32m0.01723\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 051 | loss: 0.01723 - R2: 1.0000 -- iter: 0520/1168\n",
      "Training Step: 2977  | total loss: \u001b[1m\u001b[32m0.01691\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 051 | loss: 0.01691 - R2: 0.9993 -- iter: 0540/1168\n",
      "Training Step: 2978  | total loss: \u001b[1m\u001b[32m0.01617\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 051 | loss: 0.01617 - R2: 0.9991 -- iter: 0560/1168\n",
      "Training Step: 2979  | total loss: \u001b[1m\u001b[32m0.01552\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 051 | loss: 0.01552 - R2: 0.9990 -- iter: 0580/1168\n",
      "Training Step: 2980  | total loss: \u001b[1m\u001b[32m0.01552\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 051 | loss: 0.01552 - R2: 0.9990 -- iter: 0600/1168\n",
      "Training Step: 2981  | total loss: \u001b[1m\u001b[32m0.01439\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 051 | loss: 0.01439 - R2: 0.9988 -- iter: 0620/1168\n",
      "Training Step: 2982  | total loss: \u001b[1m\u001b[32m0.01763\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 051 | loss: 0.01763 - R2: 0.9997 -- iter: 0640/1168\n",
      "Training Step: 2983  | total loss: \u001b[1m\u001b[32m0.01757\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 051 | loss: 0.01757 - R2: 1.0004 -- iter: 0660/1168\n",
      "Training Step: 2984  | total loss: \u001b[1m\u001b[32m0.01671\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 051 | loss: 0.01671 - R2: 1.0001 -- iter: 0680/1168\n",
      "Training Step: 2985  | total loss: \u001b[1m\u001b[32m0.01601\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 051 | loss: 0.01601 - R2: 1.0003 -- iter: 0700/1168\n",
      "Training Step: 2986  | total loss: \u001b[1m\u001b[32m0.01672\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 051 | loss: 0.01672 - R2: 0.9994 -- iter: 0720/1168\n",
      "Training Step: 2987  | total loss: \u001b[1m\u001b[32m0.01831\u001b[0m\u001b[0m | time: 0.165s\n",
      "| SGD | epoch: 051 | loss: 0.01831 - R2: 1.0004 -- iter: 0740/1168\n",
      "Training Step: 2988  | total loss: \u001b[1m\u001b[32m0.01831\u001b[0m\u001b[0m | time: 0.169s\n",
      "| SGD | epoch: 051 | loss: 0.01831 - R2: 1.0004 -- iter: 0760/1168\n",
      "Training Step: 2989  | total loss: \u001b[1m\u001b[32m0.01943\u001b[0m\u001b[0m | time: 0.174s\n",
      "| SGD | epoch: 051 | loss: 0.01943 - R2: 1.0017 -- iter: 0780/1168\n",
      "Training Step: 2990  | total loss: \u001b[1m\u001b[32m0.02570\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 051 | loss: 0.02570 - R2: 1.0027 -- iter: 0800/1168\n",
      "Training Step: 2991  | total loss: \u001b[1m\u001b[32m0.02495\u001b[0m\u001b[0m | time: 0.180s\n",
      "| SGD | epoch: 051 | loss: 0.02495 - R2: 1.0031 -- iter: 0820/1168\n",
      "Training Step: 2992  | total loss: \u001b[1m\u001b[32m0.02495\u001b[0m\u001b[0m | time: 0.183s\n",
      "| SGD | epoch: 051 | loss: 0.02495 - R2: 1.0031 -- iter: 0840/1168\n",
      "Training Step: 2993  | total loss: \u001b[1m\u001b[32m0.02366\u001b[0m\u001b[0m | time: 0.186s\n",
      "| SGD | epoch: 051 | loss: 0.02366 - R2: 1.0025 -- iter: 0860/1168\n",
      "Training Step: 2994  | total loss: \u001b[1m\u001b[32m0.02319\u001b[0m\u001b[0m | time: 0.190s\n",
      "| SGD | epoch: 051 | loss: 0.02319 - R2: 1.0014 -- iter: 0880/1168\n",
      "Training Step: 2995  | total loss: \u001b[1m\u001b[32m0.02281\u001b[0m\u001b[0m | time: 0.200s\n",
      "| SGD | epoch: 051 | loss: 0.02281 - R2: 1.0007 -- iter: 0900/1168\n",
      "Training Step: 2996  | total loss: \u001b[1m\u001b[32m0.02216\u001b[0m\u001b[0m | time: 0.205s\n",
      "| SGD | epoch: 051 | loss: 0.02216 - R2: 1.0006 -- iter: 0920/1168\n",
      "Training Step: 2997  | total loss: \u001b[1m\u001b[32m0.02095\u001b[0m\u001b[0m | time: 0.219s\n",
      "| SGD | epoch: 051 | loss: 0.02095 - R2: 1.0007 -- iter: 0940/1168\n",
      "Training Step: 2998  | total loss: \u001b[1m\u001b[32m0.01991\u001b[0m\u001b[0m | time: 0.223s\n",
      "| SGD | epoch: 051 | loss: 0.01991 - R2: 1.0009 -- iter: 0960/1168\n",
      "Training Step: 2999  | total loss: \u001b[1m\u001b[32m0.02087\u001b[0m\u001b[0m | time: 0.230s\n",
      "| SGD | epoch: 051 | loss: 0.02087 - R2: 1.0014 -- iter: 0980/1168\n",
      "Training Step: 3000  | total loss: \u001b[1m\u001b[32m0.01986\u001b[0m\u001b[0m | time: 0.233s\n",
      "| SGD | epoch: 051 | loss: 0.01986 - R2: 1.0009 -- iter: 1000/1168\n",
      "Training Step: 3001  | total loss: \u001b[1m\u001b[32m0.01986\u001b[0m\u001b[0m | time: 0.238s\n",
      "| SGD | epoch: 051 | loss: 0.01986 - R2: 1.0002 -- iter: 1020/1168\n",
      "Training Step: 3002  | total loss: \u001b[1m\u001b[32m0.01908\u001b[0m\u001b[0m | time: 0.242s\n",
      "| SGD | epoch: 051 | loss: 0.01908 - R2: 0.9994 -- iter: 1040/1168\n",
      "Training Step: 3003  | total loss: \u001b[1m\u001b[32m0.01853\u001b[0m\u001b[0m | time: 0.245s\n",
      "| SGD | epoch: 051 | loss: 0.01853 - R2: 0.9992 -- iter: 1060/1168\n",
      "Training Step: 3004  | total loss: \u001b[1m\u001b[32m0.01784\u001b[0m\u001b[0m | time: 0.248s\n",
      "| SGD | epoch: 051 | loss: 0.01784 - R2: 0.9988 -- iter: 1080/1168\n",
      "Training Step: 3005  | total loss: \u001b[1m\u001b[32m0.01698\u001b[0m\u001b[0m | time: 0.251s\n",
      "| SGD | epoch: 051 | loss: 0.01698 - R2: 0.9990 -- iter: 1100/1168\n",
      "Training Step: 3006  | total loss: \u001b[1m\u001b[32m0.01734\u001b[0m\u001b[0m | time: 0.254s\n",
      "| SGD | epoch: 051 | loss: 0.01734 - R2: 0.9981 -- iter: 1120/1168\n",
      "Training Step: 3007  | total loss: \u001b[1m\u001b[32m0.01667\u001b[0m\u001b[0m | time: 0.259s\n",
      "| SGD | epoch: 051 | loss: 0.01667 - R2: 0.9982 -- iter: 1140/1168\n",
      "Training Step: 3008  | total loss: \u001b[1m\u001b[32m0.01663\u001b[0m\u001b[0m | time: 0.263s\n",
      "| SGD | epoch: 051 | loss: 0.01663 - R2: 0.9984 -- iter: 1160/1168\n",
      "Training Step: 3009  | total loss: \u001b[1m\u001b[32m0.01663\u001b[0m\u001b[0m | time: 1.271s\n",
      "| SGD | epoch: 051 | loss: 0.01663 - R2: 0.9984 | val_loss: 0.02226 - val_acc: 0.9982 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 3010  | total loss: \u001b[1m\u001b[32m0.01566\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 052 | loss: 0.01566 - R2: 0.9977 -- iter: 0020/1168\n",
      "Training Step: 3011  | total loss: \u001b[1m\u001b[32m0.01476\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 052 | loss: 0.01476 - R2: 0.9971 -- iter: 0040/1168\n",
      "Training Step: 3012  | total loss: \u001b[1m\u001b[32m0.01607\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 052 | loss: 0.01607 - R2: 0.9977 -- iter: 0060/1168\n",
      "Training Step: 3013  | total loss: \u001b[1m\u001b[32m0.01607\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 052 | loss: 0.01607 - R2: 0.9977 -- iter: 0080/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3014  | total loss: \u001b[1m\u001b[32m0.01556\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 052 | loss: 0.01556 - R2: 0.9979 -- iter: 0100/1168\n",
      "Training Step: 3015  | total loss: \u001b[1m\u001b[32m0.01556\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 052 | loss: 0.01556 - R2: 0.9979 -- iter: 0120/1168\n",
      "Training Step: 3016  | total loss: \u001b[1m\u001b[32m0.01550\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 052 | loss: 0.01550 - R2: 0.9977 -- iter: 0140/1168\n",
      "Training Step: 3017  | total loss: \u001b[1m\u001b[32m0.01649\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 052 | loss: 0.01649 - R2: 0.9972 -- iter: 0160/1168\n",
      "Training Step: 3018  | total loss: \u001b[1m\u001b[32m0.01662\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 052 | loss: 0.01662 - R2: 0.9977 -- iter: 0180/1168\n",
      "Training Step: 3019  | total loss: \u001b[1m\u001b[32m0.01660\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 052 | loss: 0.01660 - R2: 0.9985 -- iter: 0200/1168\n",
      "Training Step: 3020  | total loss: \u001b[1m\u001b[32m0.01696\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 052 | loss: 0.01696 - R2: 0.9986 -- iter: 0220/1168\n",
      "Training Step: 3021  | total loss: \u001b[1m\u001b[32m0.01706\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 052 | loss: 0.01706 - R2: 0.9986 -- iter: 0240/1168\n",
      "Training Step: 3022  | total loss: \u001b[1m\u001b[32m0.01668\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 052 | loss: 0.01668 - R2: 0.9983 -- iter: 0260/1168\n",
      "Training Step: 3023  | total loss: \u001b[1m\u001b[32m0.01646\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 052 | loss: 0.01646 - R2: 0.9991 -- iter: 0280/1168\n",
      "Training Step: 3024  | total loss: \u001b[1m\u001b[32m0.01631\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 052 | loss: 0.01631 - R2: 0.9998 -- iter: 0300/1168\n",
      "Training Step: 3025  | total loss: \u001b[1m\u001b[32m0.01631\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 052 | loss: 0.01631 - R2: 0.9998 -- iter: 0320/1168\n",
      "Training Step: 3026  | total loss: \u001b[1m\u001b[32m0.01533\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 052 | loss: 0.01533 - R2: 0.9996 -- iter: 0340/1168\n",
      "Training Step: 3027  | total loss: \u001b[1m\u001b[32m0.01639\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 052 | loss: 0.01639 - R2: 0.9991 -- iter: 0360/1168\n",
      "Training Step: 3028  | total loss: \u001b[1m\u001b[32m0.01711\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 052 | loss: 0.01711 - R2: 0.9998 -- iter: 0380/1168\n",
      "Training Step: 3029  | total loss: \u001b[1m\u001b[32m0.01711\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 052 | loss: 0.01711 - R2: 0.9998 -- iter: 0400/1168\n",
      "Training Step: 3030  | total loss: \u001b[1m\u001b[32m0.01696\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 052 | loss: 0.01696 - R2: 0.9993 -- iter: 0420/1168\n",
      "Training Step: 3031  | total loss: \u001b[1m\u001b[32m0.01626\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 052 | loss: 0.01626 - R2: 0.9994 -- iter: 0440/1168\n",
      "Training Step: 3032  | total loss: \u001b[1m\u001b[32m0.01626\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 052 | loss: 0.01626 - R2: 0.9999 -- iter: 0460/1168\n",
      "Training Step: 3033  | total loss: \u001b[1m\u001b[32m0.03149\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 052 | loss: 0.03149 - R2: 1.0013 -- iter: 0480/1168\n",
      "Training Step: 3034  | total loss: \u001b[1m\u001b[32m0.02993\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 052 | loss: 0.02993 - R2: 1.0000 -- iter: 0500/1168\n",
      "Training Step: 3035  | total loss: \u001b[1m\u001b[32m0.02993\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 052 | loss: 0.02993 - R2: 0.9999 -- iter: 0520/1168\n",
      "Training Step: 3036  | total loss: \u001b[1m\u001b[32m0.02792\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 052 | loss: 0.02792 - R2: 0.9998 -- iter: 0540/1168\n",
      "Training Step: 3037  | total loss: \u001b[1m\u001b[32m0.02669\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 052 | loss: 0.02669 - R2: 0.9998 -- iter: 0560/1168\n",
      "Training Step: 3038  | total loss: \u001b[1m\u001b[32m0.02383\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 052 | loss: 0.02383 - R2: 1.0001 -- iter: 0580/1168\n",
      "Training Step: 3039  | total loss: \u001b[1m\u001b[32m0.02290\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 052 | loss: 0.02290 - R2: 1.0001 -- iter: 0600/1168\n",
      "Training Step: 3040  | total loss: \u001b[1m\u001b[32m0.02147\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 052 | loss: 0.02147 - R2: 0.9996 -- iter: 0620/1168\n",
      "Training Step: 3041  | total loss: \u001b[1m\u001b[32m0.02147\u001b[0m\u001b[0m | time: 0.169s\n",
      "| SGD | epoch: 052 | loss: 0.02147 - R2: 0.9996 -- iter: 0640/1168\n",
      "Training Step: 3042  | total loss: \u001b[1m\u001b[32m0.02062\u001b[0m\u001b[0m | time: 0.172s\n",
      "| SGD | epoch: 052 | loss: 0.02062 - R2: 1.0002 -- iter: 0660/1168\n",
      "Training Step: 3043  | total loss: \u001b[1m\u001b[32m0.02276\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 052 | loss: 0.02276 - R2: 1.0009 -- iter: 0680/1168\n",
      "Training Step: 3044  | total loss: \u001b[1m\u001b[32m0.02693\u001b[0m\u001b[0m | time: 0.178s\n",
      "| SGD | epoch: 052 | loss: 0.02693 - R2: 1.0012 -- iter: 0700/1168\n",
      "Training Step: 3045  | total loss: \u001b[1m\u001b[32m0.02693\u001b[0m\u001b[0m | time: 0.180s\n",
      "| SGD | epoch: 052 | loss: 0.02693 - R2: 1.0012 -- iter: 0720/1168\n",
      "Training Step: 3046  | total loss: \u001b[1m\u001b[32m0.02520\u001b[0m\u001b[0m | time: 0.183s\n",
      "| SGD | epoch: 052 | loss: 0.02520 - R2: 1.0009 -- iter: 0740/1168\n",
      "Training Step: 3047  | total loss: \u001b[1m\u001b[32m0.02311\u001b[0m\u001b[0m | time: 0.185s\n",
      "| SGD | epoch: 052 | loss: 0.02311 - R2: 1.0003 -- iter: 0760/1168\n",
      "Training Step: 3048  | total loss: \u001b[1m\u001b[32m0.02311\u001b[0m\u001b[0m | time: 0.187s\n",
      "| SGD | epoch: 052 | loss: 0.02311 - R2: 0.9997 -- iter: 0780/1168\n",
      "Training Step: 3049  | total loss: \u001b[1m\u001b[32m0.02317\u001b[0m\u001b[0m | time: 0.188s\n",
      "| SGD | epoch: 052 | loss: 0.02317 - R2: 0.9997 -- iter: 0800/1168\n",
      "Training Step: 3050  | total loss: \u001b[1m\u001b[32m0.02351\u001b[0m\u001b[0m | time: 0.190s\n",
      "| SGD | epoch: 052 | loss: 0.02351 - R2: 1.0005 -- iter: 0820/1168\n",
      "Training Step: 3051  | total loss: \u001b[1m\u001b[32m0.02393\u001b[0m\u001b[0m | time: 0.192s\n",
      "| SGD | epoch: 052 | loss: 0.02393 - R2: 0.9997 -- iter: 0840/1168\n",
      "Training Step: 3052  | total loss: \u001b[1m\u001b[32m0.02393\u001b[0m\u001b[0m | time: 0.196s\n",
      "| SGD | epoch: 052 | loss: 0.02393 - R2: 0.9995 -- iter: 0860/1168\n",
      "Training Step: 3053  | total loss: \u001b[1m\u001b[32m0.02307\u001b[0m\u001b[0m | time: 0.212s\n",
      "| SGD | epoch: 052 | loss: 0.02307 - R2: 0.9991 -- iter: 0880/1168\n",
      "Training Step: 3054  | total loss: \u001b[1m\u001b[32m0.02307\u001b[0m\u001b[0m | time: 0.214s\n",
      "| SGD | epoch: 052 | loss: 0.02307 - R2: 0.9991 -- iter: 0900/1168\n",
      "Training Step: 3055  | total loss: \u001b[1m\u001b[32m0.02076\u001b[0m\u001b[0m | time: 0.217s\n",
      "| SGD | epoch: 052 | loss: 0.02076 - R2: 0.9993 -- iter: 0920/1168\n",
      "Training Step: 3056  | total loss: \u001b[1m\u001b[32m0.02004\u001b[0m\u001b[0m | time: 0.221s\n",
      "| SGD | epoch: 052 | loss: 0.02004 - R2: 0.9990 -- iter: 0940/1168\n",
      "Training Step: 3057  | total loss: \u001b[1m\u001b[32m0.01879\u001b[0m\u001b[0m | time: 0.225s\n",
      "| SGD | epoch: 052 | loss: 0.01879 - R2: 0.9996 -- iter: 0960/1168\n",
      "Training Step: 3058  | total loss: \u001b[1m\u001b[32m0.01892\u001b[0m\u001b[0m | time: 0.230s\n",
      "| SGD | epoch: 052 | loss: 0.01892 - R2: 0.9988 -- iter: 0980/1168\n",
      "Training Step: 3059  | total loss: \u001b[1m\u001b[32m0.01892\u001b[0m\u001b[0m | time: 0.233s\n",
      "| SGD | epoch: 052 | loss: 0.01892 - R2: 0.9988 -- iter: 1000/1168\n",
      "Training Step: 3060  | total loss: \u001b[1m\u001b[32m0.01935\u001b[0m\u001b[0m | time: 0.237s\n",
      "| SGD | epoch: 052 | loss: 0.01935 - R2: 0.9994 -- iter: 1020/1168\n",
      "Training Step: 3061  | total loss: \u001b[1m\u001b[32m0.01946\u001b[0m\u001b[0m | time: 0.239s\n",
      "| SGD | epoch: 052 | loss: 0.01946 - R2: 0.9993 -- iter: 1040/1168\n",
      "Training Step: 3062  | total loss: \u001b[1m\u001b[32m0.02071\u001b[0m\u001b[0m | time: 0.242s\n",
      "| SGD | epoch: 052 | loss: 0.02071 - R2: 0.9993 -- iter: 1060/1168\n",
      "Training Step: 3063  | total loss: \u001b[1m\u001b[32m0.02004\u001b[0m\u001b[0m | time: 0.245s\n",
      "| SGD | epoch: 052 | loss: 0.02004 - R2: 0.9987 -- iter: 1080/1168\n",
      "Training Step: 3064  | total loss: \u001b[1m\u001b[32m0.01912\u001b[0m\u001b[0m | time: 0.248s\n",
      "| SGD | epoch: 052 | loss: 0.01912 - R2: 0.9981 -- iter: 1100/1168\n",
      "Training Step: 3065  | total loss: \u001b[1m\u001b[32m0.01893\u001b[0m\u001b[0m | time: 0.252s\n",
      "| SGD | epoch: 052 | loss: 0.01893 - R2: 0.9980 -- iter: 1120/1168\n",
      "Training Step: 3066  | total loss: \u001b[1m\u001b[32m0.01827\u001b[0m\u001b[0m | time: 0.255s\n",
      "| SGD | epoch: 052 | loss: 0.01827 - R2: 0.9971 -- iter: 1140/1168\n",
      "Training Step: 3067  | total loss: \u001b[1m\u001b[32m0.01821\u001b[0m\u001b[0m | time: 0.258s\n",
      "| SGD | epoch: 052 | loss: 0.01821 - R2: 0.9981 -- iter: 1160/1168\n",
      "Training Step: 3068  | total loss: \u001b[1m\u001b[32m0.01770\u001b[0m\u001b[0m | time: 1.270s\n",
      "| SGD | epoch: 052 | loss: 0.01770 - R2: 0.9980 | val_loss: 0.02237 - val_acc: 0.9988 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 3069  | total loss: \u001b[1m\u001b[32m0.01770\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 053 | loss: 0.01770 - R2: 0.9980 -- iter: 0020/1168\n",
      "Training Step: 3070  | total loss: \u001b[1m\u001b[32m0.01922\u001b[0m\u001b[0m | time: 0.008s\n",
      "| SGD | epoch: 053 | loss: 0.01922 - R2: 1.0002 -- iter: 0040/1168\n",
      "Training Step: 3071  | total loss: \u001b[1m\u001b[32m0.01811\u001b[0m\u001b[0m | time: 0.012s\n",
      "| SGD | epoch: 053 | loss: 0.01811 - R2: 1.0005 -- iter: 0060/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3072  | total loss: \u001b[1m\u001b[32m0.01811\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 053 | loss: 0.01811 - R2: 1.0005 -- iter: 0080/1168\n",
      "Training Step: 3073  | total loss: \u001b[1m\u001b[32m0.01794\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 053 | loss: 0.01794 - R2: 1.0004 -- iter: 0100/1168\n",
      "Training Step: 3074  | total loss: \u001b[1m\u001b[32m0.03338\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 053 | loss: 0.03338 - R2: 1.0028 -- iter: 0120/1168\n",
      "Training Step: 3075  | total loss: \u001b[1m\u001b[32m0.03338\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 053 | loss: 0.03338 - R2: 1.0022 -- iter: 0140/1168\n",
      "Training Step: 3076  | total loss: \u001b[1m\u001b[32m0.03356\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 053 | loss: 0.03356 - R2: 1.0022 -- iter: 0160/1168\n",
      "Training Step: 3077  | total loss: \u001b[1m\u001b[32m0.03157\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 053 | loss: 0.03157 - R2: 1.0024 -- iter: 0180/1168\n",
      "Training Step: 3078  | total loss: \u001b[1m\u001b[32m0.02714\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 053 | loss: 0.02714 - R2: 1.0027 -- iter: 0200/1168\n",
      "Training Step: 3079  | total loss: \u001b[1m\u001b[32m0.02714\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 053 | loss: 0.02714 - R2: 1.0025 -- iter: 0220/1168\n",
      "Training Step: 3080  | total loss: \u001b[1m\u001b[32m0.02676\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 053 | loss: 0.02676 - R2: 1.0027 -- iter: 0240/1168\n",
      "Training Step: 3081  | total loss: \u001b[1m\u001b[32m0.02430\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 053 | loss: 0.02430 - R2: 1.0026 -- iter: 0260/1168\n",
      "Training Step: 3082  | total loss: \u001b[1m\u001b[32m0.02430\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 053 | loss: 0.02430 - R2: 1.0018 -- iter: 0280/1168\n",
      "Training Step: 3083  | total loss: \u001b[1m\u001b[32m0.02241\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 053 | loss: 0.02241 - R2: 1.0016 -- iter: 0300/1168\n",
      "Training Step: 3084  | total loss: \u001b[1m\u001b[32m0.02189\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 053 | loss: 0.02189 - R2: 1.0004 -- iter: 0320/1168\n",
      "Training Step: 3085  | total loss: \u001b[1m\u001b[32m0.02189\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 053 | loss: 0.02189 - R2: 1.0004 -- iter: 0340/1168\n",
      "Training Step: 3086  | total loss: \u001b[1m\u001b[32m0.02230\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 053 | loss: 0.02230 - R2: 0.9999 -- iter: 0360/1168\n",
      "Training Step: 3087  | total loss: \u001b[1m\u001b[32m0.02208\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 053 | loss: 0.02208 - R2: 1.0002 -- iter: 0380/1168\n",
      "Training Step: 3088  | total loss: \u001b[1m\u001b[32m0.02051\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 053 | loss: 0.02051 - R2: 1.0004 -- iter: 0400/1168\n",
      "Training Step: 3089  | total loss: \u001b[1m\u001b[32m0.01957\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 053 | loss: 0.01957 - R2: 1.0003 -- iter: 0420/1168\n",
      "Training Step: 3090  | total loss: \u001b[1m\u001b[32m0.02111\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 053 | loss: 0.02111 - R2: 1.0010 -- iter: 0440/1168\n",
      "Training Step: 3091  | total loss: \u001b[1m\u001b[32m0.02044\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 053 | loss: 0.02044 - R2: 1.0015 -- iter: 0460/1168\n",
      "Training Step: 3092  | total loss: \u001b[1m\u001b[32m0.02000\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 053 | loss: 0.02000 - R2: 1.0010 -- iter: 0480/1168\n",
      "Training Step: 3093  | total loss: \u001b[1m\u001b[32m0.01936\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 053 | loss: 0.01936 - R2: 1.0002 -- iter: 0500/1168\n",
      "Training Step: 3094  | total loss: \u001b[1m\u001b[32m0.01824\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 053 | loss: 0.01824 - R2: 1.0007 -- iter: 0520/1168\n",
      "Training Step: 3095  | total loss: \u001b[1m\u001b[32m0.01824\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 053 | loss: 0.01824 - R2: 1.0007 -- iter: 0540/1168\n",
      "Training Step: 3096  | total loss: \u001b[1m\u001b[32m0.01594\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 053 | loss: 0.01594 - R2: 0.9997 -- iter: 0560/1168\n",
      "Training Step: 3097  | total loss: \u001b[1m\u001b[32m0.04396\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 053 | loss: 0.04396 - R2: 0.9997 -- iter: 0580/1168\n",
      "Training Step: 3098  | total loss: \u001b[1m\u001b[32m0.04071\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 053 | loss: 0.04071 - R2: 0.9997 -- iter: 0600/1168\n",
      "Training Step: 3099  | total loss: \u001b[1m\u001b[32m0.04071\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 053 | loss: 0.04071 - R2: 0.9994 -- iter: 0620/1168\n",
      "Training Step: 3100  | total loss: \u001b[1m\u001b[32m0.03486\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 053 | loss: 0.03486 - R2: 0.9988 -- iter: 0640/1168\n",
      "Training Step: 3101  | total loss: \u001b[1m\u001b[32m0.03217\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 053 | loss: 0.03217 - R2: 0.9987 -- iter: 0660/1168\n",
      "Training Step: 3102  | total loss: \u001b[1m\u001b[32m0.03014\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 053 | loss: 0.03014 - R2: 0.9994 -- iter: 0680/1168\n",
      "Training Step: 3103  | total loss: \u001b[1m\u001b[32m0.02883\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 053 | loss: 0.02883 - R2: 0.9990 -- iter: 0700/1168\n",
      "Training Step: 3104  | total loss: \u001b[1m\u001b[32m0.02883\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 053 | loss: 0.02883 - R2: 0.9990 -- iter: 0720/1168\n",
      "Training Step: 3105  | total loss: \u001b[1m\u001b[32m0.02900\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 053 | loss: 0.02900 - R2: 1.0004 -- iter: 0740/1168\n",
      "Training Step: 3106  | total loss: \u001b[1m\u001b[32m0.02761\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 053 | loss: 0.02761 - R2: 0.9996 -- iter: 0760/1168\n",
      "Training Step: 3107  | total loss: \u001b[1m\u001b[32m0.02761\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 053 | loss: 0.02761 - R2: 0.9996 -- iter: 0780/1168\n",
      "Training Step: 3108  | total loss: \u001b[1m\u001b[32m0.02643\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 053 | loss: 0.02643 - R2: 0.9991 -- iter: 0800/1168\n",
      "Training Step: 3109  | total loss: \u001b[1m\u001b[32m0.02516\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 053 | loss: 0.02516 - R2: 0.9989 -- iter: 0820/1168\n",
      "Training Step: 3110  | total loss: \u001b[1m\u001b[32m0.02432\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 053 | loss: 0.02432 - R2: 0.9985 -- iter: 0840/1168\n",
      "Training Step: 3111  | total loss: \u001b[1m\u001b[32m0.02432\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 053 | loss: 0.02432 - R2: 0.9985 -- iter: 0860/1168\n",
      "Training Step: 3112  | total loss: \u001b[1m\u001b[32m0.02303\u001b[0m\u001b[0m | time: 0.172s\n",
      "| SGD | epoch: 053 | loss: 0.02303 - R2: 0.9987 -- iter: 0880/1168\n",
      "Training Step: 3113  | total loss: \u001b[1m\u001b[32m0.02086\u001b[0m\u001b[0m | time: 0.176s\n",
      "| SGD | epoch: 053 | loss: 0.02086 - R2: 0.9988 -- iter: 0900/1168\n",
      "Training Step: 3114  | total loss: \u001b[1m\u001b[32m0.01992\u001b[0m\u001b[0m | time: 0.179s\n",
      "| SGD | epoch: 053 | loss: 0.01992 - R2: 0.9989 -- iter: 0920/1168\n",
      "Training Step: 3115  | total loss: \u001b[1m\u001b[32m0.01875\u001b[0m\u001b[0m | time: 0.184s\n",
      "| SGD | epoch: 053 | loss: 0.01875 - R2: 0.9990 -- iter: 0940/1168\n",
      "Training Step: 3116  | total loss: \u001b[1m\u001b[32m0.01755\u001b[0m\u001b[0m | time: 0.188s\n",
      "| SGD | epoch: 053 | loss: 0.01755 - R2: 0.9988 -- iter: 0960/1168\n",
      "Training Step: 3117  | total loss: \u001b[1m\u001b[32m0.03207\u001b[0m\u001b[0m | time: 0.194s\n",
      "| SGD | epoch: 053 | loss: 0.03207 - R2: 1.0007 -- iter: 0980/1168\n",
      "Training Step: 3118  | total loss: \u001b[1m\u001b[32m0.03027\u001b[0m\u001b[0m | time: 0.198s\n",
      "| SGD | epoch: 053 | loss: 0.03027 - R2: 0.9997 -- iter: 1000/1168\n",
      "Training Step: 3119  | total loss: \u001b[1m\u001b[32m0.02855\u001b[0m\u001b[0m | time: 0.200s\n",
      "| SGD | epoch: 053 | loss: 0.02855 - R2: 0.9993 -- iter: 1020/1168\n",
      "Training Step: 3120  | total loss: \u001b[1m\u001b[32m0.02855\u001b[0m\u001b[0m | time: 0.203s\n",
      "| SGD | epoch: 053 | loss: 0.02855 - R2: 0.9994 -- iter: 1040/1168\n",
      "Training Step: 3121  | total loss: \u001b[1m\u001b[32m0.02755\u001b[0m\u001b[0m | time: 0.205s\n",
      "| SGD | epoch: 053 | loss: 0.02755 - R2: 0.9994 -- iter: 1060/1168\n",
      "Training Step: 3122  | total loss: \u001b[1m\u001b[32m0.02601\u001b[0m\u001b[0m | time: 0.207s\n",
      "| SGD | epoch: 053 | loss: 0.02601 - R2: 0.9994 -- iter: 1080/1168\n",
      "Training Step: 3123  | total loss: \u001b[1m\u001b[32m0.02281\u001b[0m\u001b[0m | time: 0.209s\n",
      "| SGD | epoch: 053 | loss: 0.02281 - R2: 0.9994 -- iter: 1100/1168\n",
      "Training Step: 3124  | total loss: \u001b[1m\u001b[32m0.02281\u001b[0m\u001b[0m | time: 0.211s\n",
      "| SGD | epoch: 053 | loss: 0.02281 - R2: 0.9994 -- iter: 1120/1168\n",
      "Training Step: 3125  | total loss: \u001b[1m\u001b[32m0.02262\u001b[0m\u001b[0m | time: 0.214s\n",
      "| SGD | epoch: 053 | loss: 0.02262 - R2: 0.9995 -- iter: 1140/1168\n",
      "Training Step: 3126  | total loss: \u001b[1m\u001b[32m0.02243\u001b[0m\u001b[0m | time: 0.219s\n",
      "| SGD | epoch: 053 | loss: 0.02243 - R2: 0.9984 -- iter: 1160/1168\n",
      "Training Step: 3127  | total loss: \u001b[1m\u001b[32m0.02282\u001b[0m\u001b[0m | time: 1.228s\n",
      "| SGD | epoch: 053 | loss: 0.02282 - R2: 0.9993 | val_loss: 0.02249 - val_acc: 0.9982 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 3128  | total loss: \u001b[1m\u001b[32m0.02282\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 054 | loss: 0.02282 - R2: 0.9993 -- iter: 0020/1168\n",
      "Training Step: 3129  | total loss: \u001b[1m\u001b[32m0.02395\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 054 | loss: 0.02395 - R2: 0.9996 -- iter: 0040/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3130  | total loss: \u001b[1m\u001b[32m0.02343\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 054 | loss: 0.02343 - R2: 0.9988 -- iter: 0060/1168\n",
      "Training Step: 3131  | total loss: \u001b[1m\u001b[32m0.02285\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 054 | loss: 0.02285 - R2: 0.9980 -- iter: 0080/1168\n",
      "Training Step: 3132  | total loss: \u001b[1m\u001b[32m0.02396\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 054 | loss: 0.02396 - R2: 0.9983 -- iter: 0100/1168\n",
      "Training Step: 3133  | total loss: \u001b[1m\u001b[32m0.02303\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 054 | loss: 0.02303 - R2: 0.9983 -- iter: 0120/1168\n",
      "Training Step: 3134  | total loss: \u001b[1m\u001b[32m0.02160\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 054 | loss: 0.02160 - R2: 0.9983 -- iter: 0140/1168\n",
      "Training Step: 3135  | total loss: \u001b[1m\u001b[32m0.02160\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 054 | loss: 0.02160 - R2: 0.9986 -- iter: 0160/1168\n",
      "Training Step: 3136  | total loss: \u001b[1m\u001b[32m0.02074\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 054 | loss: 0.02074 - R2: 0.9984 -- iter: 0180/1168\n",
      "Training Step: 3137  | total loss: \u001b[1m\u001b[32m0.02149\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 054 | loss: 0.02149 - R2: 0.9999 -- iter: 0200/1168\n",
      "Training Step: 3138  | total loss: \u001b[1m\u001b[32m0.02149\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 054 | loss: 0.02149 - R2: 0.9999 -- iter: 0220/1168\n",
      "Training Step: 3139  | total loss: \u001b[1m\u001b[32m0.02064\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 054 | loss: 0.02064 - R2: 0.9999 -- iter: 0240/1168\n",
      "Training Step: 3140  | total loss: \u001b[1m\u001b[32m0.01871\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 054 | loss: 0.01871 - R2: 0.9996 -- iter: 0260/1168\n",
      "Training Step: 3141  | total loss: \u001b[1m\u001b[32m0.01871\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 054 | loss: 0.01871 - R2: 1.0003 -- iter: 0280/1168\n",
      "Training Step: 3142  | total loss: \u001b[1m\u001b[32m0.01835\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 054 | loss: 0.01835 - R2: 1.0003 -- iter: 0300/1168\n",
      "Training Step: 3143  | total loss: \u001b[1m\u001b[32m0.01715\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 054 | loss: 0.01715 - R2: 0.9995 -- iter: 0320/1168\n",
      "Training Step: 3144  | total loss: \u001b[1m\u001b[32m0.01627\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 054 | loss: 0.01627 - R2: 0.9995 -- iter: 0340/1168\n",
      "Training Step: 3145  | total loss: \u001b[1m\u001b[32m0.01658\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 054 | loss: 0.01658 - R2: 0.9995 -- iter: 0360/1168\n",
      "Training Step: 3146  | total loss: \u001b[1m\u001b[32m0.01658\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 054 | loss: 0.01658 - R2: 0.9995 -- iter: 0380/1168\n",
      "Training Step: 3147  | total loss: \u001b[1m\u001b[32m0.01624\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 054 | loss: 0.01624 - R2: 0.9994 -- iter: 0400/1168\n",
      "Training Step: 3148  | total loss: \u001b[1m\u001b[32m0.01848\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 054 | loss: 0.01848 - R2: 0.9994 -- iter: 0420/1168\n",
      "Training Step: 3149  | total loss: \u001b[1m\u001b[32m0.01869\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 054 | loss: 0.01869 - R2: 0.9994 -- iter: 0440/1168\n",
      "Training Step: 3150  | total loss: \u001b[1m\u001b[32m0.02093\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 054 | loss: 0.02093 - R2: 1.0000 -- iter: 0460/1168\n",
      "Training Step: 3151  | total loss: \u001b[1m\u001b[32m0.02093\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 054 | loss: 0.02093 - R2: 1.0000 -- iter: 0480/1168\n",
      "Training Step: 3152  | total loss: \u001b[1m\u001b[32m0.02345\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 054 | loss: 0.02345 - R2: 1.0011 -- iter: 0500/1168\n",
      "Training Step: 3153  | total loss: \u001b[1m\u001b[32m0.02185\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 054 | loss: 0.02185 - R2: 1.0015 -- iter: 0520/1168\n",
      "Training Step: 3154  | total loss: \u001b[1m\u001b[32m0.02185\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 054 | loss: 0.02185 - R2: 1.0012 -- iter: 0540/1168\n",
      "Training Step: 3155  | total loss: \u001b[1m\u001b[32m0.02100\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 054 | loss: 0.02100 - R2: 1.0014 -- iter: 0560/1168\n",
      "Training Step: 3156  | total loss: \u001b[1m\u001b[32m0.02100\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 054 | loss: 0.02100 - R2: 1.0014 -- iter: 0580/1168\n",
      "Training Step: 3157  | total loss: \u001b[1m\u001b[32m0.02049\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 054 | loss: 0.02049 - R2: 1.0009 -- iter: 0600/1168\n",
      "Training Step: 3158  | total loss: \u001b[1m\u001b[32m0.02048\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 054 | loss: 0.02048 - R2: 1.0012 -- iter: 0620/1168\n",
      "Training Step: 3159  | total loss: \u001b[1m\u001b[32m0.01924\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 054 | loss: 0.01924 - R2: 1.0006 -- iter: 0640/1168\n",
      "Training Step: 3160  | total loss: \u001b[1m\u001b[32m0.03283\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 054 | loss: 0.03283 - R2: 1.0027 -- iter: 0660/1168\n",
      "Training Step: 3161  | total loss: \u001b[1m\u001b[32m0.03125\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 054 | loss: 0.03125 - R2: 1.0027 -- iter: 0680/1168\n",
      "Training Step: 3162  | total loss: \u001b[1m\u001b[32m0.02864\u001b[0m\u001b[0m | time: 0.169s\n",
      "| SGD | epoch: 054 | loss: 0.02864 - R2: 1.0010 -- iter: 0700/1168\n",
      "Training Step: 3163  | total loss: \u001b[1m\u001b[32m0.02746\u001b[0m\u001b[0m | time: 0.173s\n",
      "| SGD | epoch: 054 | loss: 0.02746 - R2: 1.0005 -- iter: 0720/1168\n",
      "Training Step: 3164  | total loss: \u001b[1m\u001b[32m0.02595\u001b[0m\u001b[0m | time: 0.178s\n",
      "| SGD | epoch: 054 | loss: 0.02595 - R2: 1.0005 -- iter: 0740/1168\n",
      "Training Step: 3165  | total loss: \u001b[1m\u001b[32m0.02507\u001b[0m\u001b[0m | time: 0.182s\n",
      "| SGD | epoch: 054 | loss: 0.02507 - R2: 0.9997 -- iter: 0760/1168\n",
      "Training Step: 3166  | total loss: \u001b[1m\u001b[32m0.02370\u001b[0m\u001b[0m | time: 0.186s\n",
      "| SGD | epoch: 054 | loss: 0.02370 - R2: 0.9994 -- iter: 0780/1168\n",
      "Training Step: 3167  | total loss: \u001b[1m\u001b[32m0.02280\u001b[0m\u001b[0m | time: 0.189s\n",
      "| SGD | epoch: 054 | loss: 0.02280 - R2: 0.9994 -- iter: 0800/1168\n",
      "Training Step: 3168  | total loss: \u001b[1m\u001b[32m0.02280\u001b[0m\u001b[0m | time: 0.193s\n",
      "| SGD | epoch: 054 | loss: 0.02280 - R2: 0.9995 -- iter: 0820/1168\n",
      "Training Step: 3169  | total loss: \u001b[1m\u001b[32m0.02245\u001b[0m\u001b[0m | time: 0.195s\n",
      "| SGD | epoch: 054 | loss: 0.02245 - R2: 0.9995 -- iter: 0840/1168\n",
      "Training Step: 3170  | total loss: \u001b[1m\u001b[32m0.02083\u001b[0m\u001b[0m | time: 0.199s\n",
      "| SGD | epoch: 054 | loss: 0.02083 - R2: 0.9996 -- iter: 0860/1168\n",
      "Training Step: 3171  | total loss: \u001b[1m\u001b[32m0.02065\u001b[0m\u001b[0m | time: 0.201s\n",
      "| SGD | epoch: 054 | loss: 0.02065 - R2: 0.9997 -- iter: 0880/1168\n",
      "Training Step: 3172  | total loss: \u001b[1m\u001b[32m0.01948\u001b[0m\u001b[0m | time: 0.203s\n",
      "| SGD | epoch: 054 | loss: 0.01948 - R2: 0.9997 -- iter: 0900/1168\n",
      "Training Step: 3173  | total loss: \u001b[1m\u001b[32m0.02027\u001b[0m\u001b[0m | time: 0.206s\n",
      "| SGD | epoch: 054 | loss: 0.02027 - R2: 0.9997 -- iter: 0920/1168\n",
      "Training Step: 3174  | total loss: \u001b[1m\u001b[32m0.01928\u001b[0m\u001b[0m | time: 0.208s\n",
      "| SGD | epoch: 054 | loss: 0.01928 - R2: 0.9992 -- iter: 0940/1168\n",
      "Training Step: 3175  | total loss: \u001b[1m\u001b[32m0.01907\u001b[0m\u001b[0m | time: 0.211s\n",
      "| SGD | epoch: 054 | loss: 0.01907 - R2: 0.9992 -- iter: 0960/1168\n",
      "Training Step: 3176  | total loss: \u001b[1m\u001b[32m0.02110\u001b[0m\u001b[0m | time: 0.214s\n",
      "| SGD | epoch: 054 | loss: 0.02110 - R2: 0.9999 -- iter: 0980/1168\n",
      "Training Step: 3177  | total loss: \u001b[1m\u001b[32m0.02232\u001b[0m\u001b[0m | time: 0.216s\n",
      "| SGD | epoch: 054 | loss: 0.02232 - R2: 1.0003 -- iter: 1000/1168\n",
      "Training Step: 3178  | total loss: \u001b[1m\u001b[32m0.02159\u001b[0m\u001b[0m | time: 0.218s\n",
      "| SGD | epoch: 054 | loss: 0.02159 - R2: 1.0006 -- iter: 1020/1168\n",
      "Training Step: 3179  | total loss: \u001b[1m\u001b[32m0.02159\u001b[0m\u001b[0m | time: 0.243s\n",
      "| SGD | epoch: 054 | loss: 0.02159 - R2: 1.0006 -- iter: 1040/1168\n",
      "Training Step: 3180  | total loss: \u001b[1m\u001b[32m0.02077\u001b[0m\u001b[0m | time: 0.248s\n",
      "| SGD | epoch: 054 | loss: 0.02077 - R2: 1.0004 -- iter: 1060/1168\n",
      "Training Step: 3181  | total loss: \u001b[1m\u001b[32m0.01866\u001b[0m\u001b[0m | time: 0.251s\n",
      "| SGD | epoch: 054 | loss: 0.01866 - R2: 0.9994 -- iter: 1080/1168\n",
      "Training Step: 3182  | total loss: \u001b[1m\u001b[32m0.01827\u001b[0m\u001b[0m | time: 0.255s\n",
      "| SGD | epoch: 054 | loss: 0.01827 - R2: 1.0000 -- iter: 1100/1168\n",
      "Training Step: 3183  | total loss: \u001b[1m\u001b[32m0.01827\u001b[0m\u001b[0m | time: 0.256s\n",
      "| SGD | epoch: 054 | loss: 0.01827 - R2: 1.0000 -- iter: 1120/1168\n",
      "Training Step: 3184  | total loss: \u001b[1m\u001b[32m0.01751\u001b[0m\u001b[0m | time: 0.258s\n",
      "| SGD | epoch: 054 | loss: 0.01751 - R2: 1.0010 -- iter: 1140/1168\n",
      "Training Step: 3185  | total loss: \u001b[1m\u001b[32m0.01972\u001b[0m\u001b[0m | time: 0.260s\n",
      "| SGD | epoch: 054 | loss: 0.01972 - R2: 1.0010 -- iter: 1160/1168\n",
      "Training Step: 3186  | total loss: \u001b[1m\u001b[32m0.02030\u001b[0m\u001b[0m | time: 1.265s\n",
      "| SGD | epoch: 054 | loss: 0.02030 - R2: 1.0009 | val_loss: 0.02273 - val_acc: 0.9980 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 3187  | total loss: \u001b[1m\u001b[32m0.01940\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 055 | loss: 0.01940 - R2: 1.0013 -- iter: 0020/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3188  | total loss: \u001b[1m\u001b[32m0.01887\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 055 | loss: 0.01887 - R2: 1.0003 -- iter: 0040/1168\n",
      "Training Step: 3189  | total loss: \u001b[1m\u001b[32m0.01887\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 055 | loss: 0.01887 - R2: 1.0003 -- iter: 0060/1168\n",
      "Training Step: 3190  | total loss: \u001b[1m\u001b[32m0.02146\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 055 | loss: 0.02146 - R2: 0.9992 -- iter: 0080/1168\n",
      "Training Step: 3191  | total loss: \u001b[1m\u001b[32m0.02356\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 055 | loss: 0.02356 - R2: 0.9982 -- iter: 0100/1168\n",
      "Training Step: 3192  | total loss: \u001b[1m\u001b[32m0.02140\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 055 | loss: 0.02140 - R2: 0.9981 -- iter: 0120/1168\n",
      "Training Step: 3193  | total loss: \u001b[1m\u001b[32m0.02077\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 055 | loss: 0.02077 - R2: 0.9993 -- iter: 0140/1168\n",
      "Training Step: 3194  | total loss: \u001b[1m\u001b[32m0.02029\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 055 | loss: 0.02029 - R2: 0.9992 -- iter: 0160/1168\n",
      "Training Step: 3195  | total loss: \u001b[1m\u001b[32m0.02029\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 055 | loss: 0.02029 - R2: 0.9992 -- iter: 0180/1168\n",
      "Training Step: 3196  | total loss: \u001b[1m\u001b[32m0.02115\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 055 | loss: 0.02115 - R2: 0.9989 -- iter: 0200/1168\n",
      "Training Step: 3197  | total loss: \u001b[1m\u001b[32m0.02002\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 055 | loss: 0.02002 - R2: 0.9997 -- iter: 0220/1168\n",
      "Training Step: 3198  | total loss: \u001b[1m\u001b[32m0.02002\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 055 | loss: 0.02002 - R2: 0.9999 -- iter: 0240/1168\n",
      "Training Step: 3199  | total loss: \u001b[1m\u001b[32m0.01920\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 055 | loss: 0.01920 - R2: 0.9989 -- iter: 0260/1168\n",
      "Training Step: 3200  | total loss: \u001b[1m\u001b[32m0.01872\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 055 | loss: 0.01872 - R2: 0.9990 -- iter: 0280/1168\n",
      "Training Step: 3201  | total loss: \u001b[1m\u001b[32m0.01872\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 055 | loss: 0.01872 - R2: 0.9990 -- iter: 0300/1168\n",
      "Training Step: 3202  | total loss: \u001b[1m\u001b[32m0.01856\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 055 | loss: 0.01856 - R2: 0.9991 -- iter: 0320/1168\n",
      "Training Step: 3203  | total loss: \u001b[1m\u001b[32m0.01856\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 055 | loss: 0.01856 - R2: 0.9991 -- iter: 0340/1168\n",
      "Training Step: 3204  | total loss: \u001b[1m\u001b[32m0.01801\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 055 | loss: 0.01801 - R2: 0.9988 -- iter: 0360/1168\n",
      "Training Step: 3205  | total loss: \u001b[1m\u001b[32m0.02016\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 055 | loss: 0.02016 - R2: 0.9995 -- iter: 0380/1168\n",
      "Training Step: 3206  | total loss: \u001b[1m\u001b[32m0.02070\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 055 | loss: 0.02070 - R2: 0.9991 -- iter: 0400/1168\n",
      "Training Step: 3207  | total loss: \u001b[1m\u001b[32m0.02070\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 055 | loss: 0.02070 - R2: 0.9991 -- iter: 0420/1168\n",
      "Training Step: 3208  | total loss: \u001b[1m\u001b[32m0.02091\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 055 | loss: 0.02091 - R2: 0.9996 -- iter: 0440/1168\n",
      "Training Step: 3209  | total loss: \u001b[1m\u001b[32m0.02080\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 055 | loss: 0.02080 - R2: 1.0000 -- iter: 0460/1168\n",
      "Training Step: 3210  | total loss: \u001b[1m\u001b[32m0.01911\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 055 | loss: 0.01911 - R2: 1.0003 -- iter: 0480/1168\n",
      "Training Step: 3211  | total loss: \u001b[1m\u001b[32m0.01911\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 055 | loss: 0.01911 - R2: 1.0003 -- iter: 0500/1168\n",
      "Training Step: 3212  | total loss: \u001b[1m\u001b[32m0.01889\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 055 | loss: 0.01889 - R2: 1.0007 -- iter: 0520/1168\n",
      "Training Step: 3213  | total loss: \u001b[1m\u001b[32m0.01837\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 055 | loss: 0.01837 - R2: 1.0004 -- iter: 0540/1168\n",
      "Training Step: 3214  | total loss: \u001b[1m\u001b[32m0.01757\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 055 | loss: 0.01757 - R2: 0.9997 -- iter: 0560/1168\n",
      "Training Step: 3215  | total loss: \u001b[1m\u001b[32m0.01700\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 055 | loss: 0.01700 - R2: 0.9996 -- iter: 0580/1168\n",
      "Training Step: 3216  | total loss: \u001b[1m\u001b[32m0.01788\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 055 | loss: 0.01788 - R2: 0.9996 -- iter: 0600/1168\n",
      "Training Step: 3217  | total loss: \u001b[1m\u001b[32m0.01788\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 055 | loss: 0.01788 - R2: 0.9987 -- iter: 0620/1168\n",
      "Training Step: 3218  | total loss: \u001b[1m\u001b[32m0.05061\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 055 | loss: 0.05061 - R2: 0.9987 -- iter: 0640/1168\n",
      "Training Step: 3219  | total loss: \u001b[1m\u001b[32m0.04699\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 055 | loss: 0.04699 - R2: 0.9994 -- iter: 0660/1168\n",
      "Training Step: 3220  | total loss: \u001b[1m\u001b[32m0.04483\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 055 | loss: 0.04483 - R2: 1.0005 -- iter: 0680/1168\n",
      "Training Step: 3221  | total loss: \u001b[1m\u001b[32m0.03848\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 055 | loss: 0.03848 - R2: 1.0010 -- iter: 0700/1168\n",
      "Training Step: 3222  | total loss: \u001b[1m\u001b[32m0.03571\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 055 | loss: 0.03571 - R2: 1.0008 -- iter: 0720/1168\n",
      "Training Step: 3223  | total loss: \u001b[1m\u001b[32m0.03571\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 055 | loss: 0.03571 - R2: 1.0008 -- iter: 0740/1168\n",
      "Training Step: 3224  | total loss: \u001b[1m\u001b[32m0.03350\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 055 | loss: 0.03350 - R2: 1.0002 -- iter: 0760/1168\n",
      "Training Step: 3225  | total loss: \u001b[1m\u001b[32m0.03128\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 055 | loss: 0.03128 - R2: 0.9998 -- iter: 0780/1168\n",
      "Training Step: 3226  | total loss: \u001b[1m\u001b[32m0.02785\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 055 | loss: 0.02785 - R2: 0.9988 -- iter: 0800/1168\n",
      "Training Step: 3227  | total loss: \u001b[1m\u001b[32m0.02788\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 055 | loss: 0.02788 - R2: 0.9995 -- iter: 0820/1168\n",
      "Training Step: 3228  | total loss: \u001b[1m\u001b[32m0.02788\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 055 | loss: 0.02788 - R2: 1.0003 -- iter: 0840/1168\n",
      "Training Step: 3229  | total loss: \u001b[1m\u001b[32m0.02708\u001b[0m\u001b[0m | time: 0.172s\n",
      "| SGD | epoch: 055 | loss: 0.02708 - R2: 1.0003 -- iter: 0860/1168\n",
      "Training Step: 3230  | total loss: \u001b[1m\u001b[32m0.02578\u001b[0m\u001b[0m | time: 0.179s\n",
      "| SGD | epoch: 055 | loss: 0.02578 - R2: 1.0000 -- iter: 0880/1168\n",
      "Training Step: 3231  | total loss: \u001b[1m\u001b[32m0.02336\u001b[0m\u001b[0m | time: 0.184s\n",
      "| SGD | epoch: 055 | loss: 0.02336 - R2: 1.0001 -- iter: 0900/1168\n",
      "Training Step: 3232  | total loss: \u001b[1m\u001b[32m0.02336\u001b[0m\u001b[0m | time: 0.186s\n",
      "| SGD | epoch: 055 | loss: 0.02336 - R2: 1.0000 -- iter: 0920/1168\n",
      "Training Step: 3233  | total loss: \u001b[1m\u001b[32m0.02045\u001b[0m\u001b[0m | time: 0.190s\n",
      "| SGD | epoch: 055 | loss: 0.02045 - R2: 0.9996 -- iter: 0940/1168\n",
      "Training Step: 3234  | total loss: \u001b[1m\u001b[32m0.02045\u001b[0m\u001b[0m | time: 0.192s\n",
      "| SGD | epoch: 055 | loss: 0.02045 - R2: 0.9996 -- iter: 0960/1168\n",
      "Training Step: 3235  | total loss: \u001b[1m\u001b[32m0.02049\u001b[0m\u001b[0m | time: 0.194s\n",
      "| SGD | epoch: 055 | loss: 0.02049 - R2: 0.9989 -- iter: 0980/1168\n",
      "Training Step: 3236  | total loss: \u001b[1m\u001b[32m0.02055\u001b[0m\u001b[0m | time: 0.197s\n",
      "| SGD | epoch: 055 | loss: 0.02055 - R2: 0.9984 -- iter: 1000/1168\n",
      "Training Step: 3237  | total loss: \u001b[1m\u001b[32m0.02055\u001b[0m\u001b[0m | time: 0.201s\n",
      "| SGD | epoch: 055 | loss: 0.02055 - R2: 0.9984 -- iter: 1020/1168\n",
      "Training Step: 3238  | total loss: \u001b[1m\u001b[32m0.03257\u001b[0m\u001b[0m | time: 0.202s\n",
      "| SGD | epoch: 055 | loss: 0.03257 - R2: 0.9995 -- iter: 1040/1168\n",
      "Training Step: 3239  | total loss: \u001b[1m\u001b[32m0.02862\u001b[0m\u001b[0m | time: 0.206s\n",
      "| SGD | epoch: 055 | loss: 0.02862 - R2: 1.0003 -- iter: 1060/1168\n",
      "Training Step: 3240  | total loss: \u001b[1m\u001b[32m0.02862\u001b[0m\u001b[0m | time: 0.209s\n",
      "| SGD | epoch: 055 | loss: 0.02862 - R2: 1.0004 -- iter: 1080/1168\n",
      "Training Step: 3241  | total loss: \u001b[1m\u001b[32m0.02539\u001b[0m\u001b[0m | time: 0.211s\n",
      "| SGD | epoch: 055 | loss: 0.02539 - R2: 1.0004 -- iter: 1100/1168\n",
      "Training Step: 3242  | total loss: \u001b[1m\u001b[32m0.02539\u001b[0m\u001b[0m | time: 0.213s\n",
      "| SGD | epoch: 055 | loss: 0.02539 - R2: 1.0004 -- iter: 1120/1168\n",
      "Training Step: 3243  | total loss: \u001b[1m\u001b[32m0.02494\u001b[0m\u001b[0m | time: 0.216s\n",
      "| SGD | epoch: 055 | loss: 0.02494 - R2: 0.9993 -- iter: 1140/1168\n",
      "Training Step: 3244  | total loss: \u001b[1m\u001b[32m0.02337\u001b[0m\u001b[0m | time: 0.223s\n",
      "| SGD | epoch: 055 | loss: 0.02337 - R2: 0.9991 -- iter: 1160/1168\n",
      "Training Step: 3245  | total loss: \u001b[1m\u001b[32m0.02219\u001b[0m\u001b[0m | time: 1.229s\n",
      "| SGD | epoch: 055 | loss: 0.02219 - R2: 0.9991 | val_loss: 0.02280 - val_acc: 0.9982 -- iter: 1168/1168\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3246  | total loss: \u001b[1m\u001b[32m0.02127\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 056 | loss: 0.02127 - R2: 0.9995 -- iter: 0020/1168\n",
      "Training Step: 3247  | total loss: \u001b[1m\u001b[32m0.02042\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 056 | loss: 0.02042 - R2: 0.9995 -- iter: 0040/1168\n",
      "Training Step: 3248  | total loss: \u001b[1m\u001b[32m0.02050\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 056 | loss: 0.02050 - R2: 0.9993 -- iter: 0060/1168\n",
      "Training Step: 3249  | total loss: \u001b[1m\u001b[32m0.02050\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 056 | loss: 0.02050 - R2: 0.9993 -- iter: 0080/1168\n",
      "Training Step: 3250  | total loss: \u001b[1m\u001b[32m0.01791\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 056 | loss: 0.01791 - R2: 0.9995 -- iter: 0100/1168\n",
      "Training Step: 3251  | total loss: \u001b[1m\u001b[32m0.02215\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 056 | loss: 0.02215 - R2: 0.9991 -- iter: 0120/1168\n",
      "Training Step: 3252  | total loss: \u001b[1m\u001b[32m0.02215\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 056 | loss: 0.02215 - R2: 0.9991 -- iter: 0140/1168\n",
      "Training Step: 3253  | total loss: \u001b[1m\u001b[32m0.02167\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 056 | loss: 0.02167 - R2: 0.9984 -- iter: 0160/1168\n",
      "Training Step: 3254  | total loss: \u001b[1m\u001b[32m0.02121\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 056 | loss: 0.02121 - R2: 0.9988 -- iter: 0180/1168\n",
      "Training Step: 3255  | total loss: \u001b[1m\u001b[32m0.02057\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 056 | loss: 0.02057 - R2: 0.9992 -- iter: 0200/1168\n",
      "Training Step: 3256  | total loss: \u001b[1m\u001b[32m0.01947\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 056 | loss: 0.01947 - R2: 0.9998 -- iter: 0220/1168\n",
      "Training Step: 3257  | total loss: \u001b[1m\u001b[32m0.01947\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 056 | loss: 0.01947 - R2: 0.9998 -- iter: 0240/1168\n",
      "Training Step: 3258  | total loss: \u001b[1m\u001b[32m0.01969\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 056 | loss: 0.01969 - R2: 0.9995 -- iter: 0260/1168\n",
      "Training Step: 3259  | total loss: \u001b[1m\u001b[32m0.01985\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 056 | loss: 0.01985 - R2: 0.9999 -- iter: 0280/1168\n",
      "Training Step: 3260  | total loss: \u001b[1m\u001b[32m0.01940\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 056 | loss: 0.01940 - R2: 0.9998 -- iter: 0300/1168\n",
      "Training Step: 3261  | total loss: \u001b[1m\u001b[32m0.01984\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 056 | loss: 0.01984 - R2: 0.9998 -- iter: 0320/1168\n",
      "Training Step: 3262  | total loss: \u001b[1m\u001b[32m0.01984\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 056 | loss: 0.01984 - R2: 0.9993 -- iter: 0340/1168\n",
      "Training Step: 3263  | total loss: \u001b[1m\u001b[32m0.01946\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 056 | loss: 0.01946 - R2: 0.9997 -- iter: 0360/1168\n",
      "Training Step: 3264  | total loss: \u001b[1m\u001b[32m0.01861\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 056 | loss: 0.01861 - R2: 1.0003 -- iter: 0380/1168\n",
      "Training Step: 3265  | total loss: \u001b[1m\u001b[32m0.01861\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 056 | loss: 0.01861 - R2: 1.0005 -- iter: 0400/1168\n",
      "Training Step: 3266  | total loss: \u001b[1m\u001b[32m0.01692\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 056 | loss: 0.01692 - R2: 1.0001 -- iter: 0420/1168\n",
      "Training Step: 3267  | total loss: \u001b[1m\u001b[32m0.01795\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 056 | loss: 0.01795 - R2: 1.0005 -- iter: 0440/1168\n",
      "Training Step: 3268  | total loss: \u001b[1m\u001b[32m0.01795\u001b[0m\u001b[0m | time: 0.169s\n",
      "| SGD | epoch: 056 | loss: 0.01795 - R2: 1.0005 -- iter: 0460/1168\n",
      "Training Step: 3269  | total loss: \u001b[1m\u001b[32m0.01687\u001b[0m\u001b[0m | time: 0.171s\n",
      "| SGD | epoch: 056 | loss: 0.01687 - R2: 0.9999 -- iter: 0480/1168\n",
      "Training Step: 3270  | total loss: \u001b[1m\u001b[32m0.01687\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 056 | loss: 0.01687 - R2: 0.9999 -- iter: 0500/1168\n",
      "Training Step: 3271  | total loss: \u001b[1m\u001b[32m0.01605\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 056 | loss: 0.01605 - R2: 0.9997 -- iter: 0520/1168\n",
      "Training Step: 3272  | total loss: \u001b[1m\u001b[32m0.01757\u001b[0m\u001b[0m | time: 0.179s\n",
      "| SGD | epoch: 056 | loss: 0.01757 - R2: 1.0002 -- iter: 0540/1168\n",
      "Training Step: 3273  | total loss: \u001b[1m\u001b[32m0.01757\u001b[0m\u001b[0m | time: 0.181s\n",
      "| SGD | epoch: 056 | loss: 0.01757 - R2: 1.0002 -- iter: 0560/1168\n",
      "Training Step: 3274  | total loss: \u001b[1m\u001b[32m0.02174\u001b[0m\u001b[0m | time: 0.184s\n",
      "| SGD | epoch: 056 | loss: 0.02174 - R2: 1.0002 -- iter: 0580/1168\n",
      "Training Step: 3275  | total loss: \u001b[1m\u001b[32m0.02269\u001b[0m\u001b[0m | time: 0.187s\n",
      "| SGD | epoch: 056 | loss: 0.02269 - R2: 1.0002 -- iter: 0600/1168\n",
      "Training Step: 3276  | total loss: \u001b[1m\u001b[32m0.02269\u001b[0m\u001b[0m | time: 0.189s\n",
      "| SGD | epoch: 056 | loss: 0.02269 - R2: 1.0002 -- iter: 0620/1168\n",
      "Training Step: 3277  | total loss: \u001b[1m\u001b[32m0.02175\u001b[0m\u001b[0m | time: 0.192s\n",
      "| SGD | epoch: 056 | loss: 0.02175 - R2: 0.9997 -- iter: 0640/1168\n",
      "Training Step: 3278  | total loss: \u001b[1m\u001b[32m0.02189\u001b[0m\u001b[0m | time: 0.207s\n",
      "| SGD | epoch: 056 | loss: 0.02189 - R2: 0.9997 -- iter: 0660/1168\n",
      "Training Step: 3279  | total loss: \u001b[1m\u001b[32m0.02054\u001b[0m\u001b[0m | time: 0.218s\n",
      "| SGD | epoch: 056 | loss: 0.02054 - R2: 0.9990 -- iter: 0680/1168\n",
      "Training Step: 3280  | total loss: \u001b[1m\u001b[32m0.01997\u001b[0m\u001b[0m | time: 0.220s\n",
      "| SGD | epoch: 056 | loss: 0.01997 - R2: 0.9981 -- iter: 0700/1168\n",
      "Training Step: 3281  | total loss: \u001b[1m\u001b[32m0.01969\u001b[0m\u001b[0m | time: 0.222s\n",
      "| SGD | epoch: 056 | loss: 0.01969 - R2: 0.9981 -- iter: 0720/1168\n",
      "Training Step: 3282  | total loss: \u001b[1m\u001b[32m0.01864\u001b[0m\u001b[0m | time: 0.224s\n",
      "| SGD | epoch: 056 | loss: 0.01864 - R2: 0.9986 -- iter: 0740/1168\n",
      "Training Step: 3283  | total loss: \u001b[1m\u001b[32m0.01852\u001b[0m\u001b[0m | time: 0.225s\n",
      "| SGD | epoch: 056 | loss: 0.01852 - R2: 0.9992 -- iter: 0760/1168\n",
      "Training Step: 3284  | total loss: \u001b[1m\u001b[32m0.01728\u001b[0m\u001b[0m | time: 0.227s\n",
      "| SGD | epoch: 056 | loss: 0.01728 - R2: 0.9994 -- iter: 0780/1168\n",
      "Training Step: 3285  | total loss: \u001b[1m\u001b[32m0.01735\u001b[0m\u001b[0m | time: 0.228s\n",
      "| SGD | epoch: 056 | loss: 0.01735 - R2: 0.9999 -- iter: 0800/1168\n",
      "Training Step: 3286  | total loss: \u001b[1m\u001b[32m0.01632\u001b[0m\u001b[0m | time: 0.238s\n",
      "| SGD | epoch: 056 | loss: 0.01632 - R2: 0.9995 -- iter: 0820/1168\n",
      "Training Step: 3287  | total loss: \u001b[1m\u001b[32m0.01610\u001b[0m\u001b[0m | time: 0.240s\n",
      "| SGD | epoch: 056 | loss: 0.01610 - R2: 0.9988 -- iter: 0840/1168\n",
      "Training Step: 3288  | total loss: \u001b[1m\u001b[32m0.01524\u001b[0m\u001b[0m | time: 0.242s\n",
      "| SGD | epoch: 056 | loss: 0.01524 - R2: 0.9990 -- iter: 0860/1168\n",
      "Training Step: 3289  | total loss: \u001b[1m\u001b[32m0.01512\u001b[0m\u001b[0m | time: 0.244s\n",
      "| SGD | epoch: 056 | loss: 0.01512 - R2: 0.9981 -- iter: 0880/1168\n",
      "Training Step: 3290  | total loss: \u001b[1m\u001b[32m0.01429\u001b[0m\u001b[0m | time: 0.247s\n",
      "| SGD | epoch: 056 | loss: 0.01429 - R2: 0.9984 -- iter: 0900/1168\n",
      "Training Step: 3291  | total loss: \u001b[1m\u001b[32m0.01623\u001b[0m\u001b[0m | time: 0.251s\n",
      "| SGD | epoch: 056 | loss: 0.01623 - R2: 0.9989 -- iter: 0920/1168\n",
      "Training Step: 3292  | total loss: \u001b[1m\u001b[32m0.01623\u001b[0m\u001b[0m | time: 0.254s\n",
      "| SGD | epoch: 056 | loss: 0.01623 - R2: 0.9989 -- iter: 0940/1168\n",
      "Training Step: 3293  | total loss: \u001b[1m\u001b[32m0.01639\u001b[0m\u001b[0m | time: 0.258s\n",
      "| SGD | epoch: 056 | loss: 0.01639 - R2: 0.9996 -- iter: 0960/1168\n",
      "Training Step: 3294  | total loss: \u001b[1m\u001b[32m0.01639\u001b[0m\u001b[0m | time: 0.260s\n",
      "| SGD | epoch: 056 | loss: 0.01639 - R2: 0.9996 -- iter: 0980/1168\n",
      "Training Step: 3295  | total loss: \u001b[1m\u001b[32m0.01594\u001b[0m\u001b[0m | time: 0.280s\n",
      "| SGD | epoch: 056 | loss: 0.01594 - R2: 0.9994 -- iter: 1000/1168\n",
      "Training Step: 3296  | total loss: \u001b[1m\u001b[32m0.01897\u001b[0m\u001b[0m | time: 0.282s\n",
      "| SGD | epoch: 056 | loss: 0.01897 - R2: 0.9997 -- iter: 1020/1168\n",
      "Training Step: 3297  | total loss: \u001b[1m\u001b[32m0.01779\u001b[0m\u001b[0m | time: 0.284s\n",
      "| SGD | epoch: 056 | loss: 0.01779 - R2: 0.9999 -- iter: 1040/1168\n",
      "Training Step: 3298  | total loss: \u001b[1m\u001b[32m0.01720\u001b[0m\u001b[0m | time: 0.287s\n",
      "| SGD | epoch: 056 | loss: 0.01720 - R2: 1.0001 -- iter: 1060/1168\n",
      "Training Step: 3299  | total loss: \u001b[1m\u001b[32m0.01729\u001b[0m\u001b[0m | time: 0.289s\n",
      "| SGD | epoch: 056 | loss: 0.01729 - R2: 0.9989 -- iter: 1080/1168\n",
      "Training Step: 3300  | total loss: \u001b[1m\u001b[32m0.01735\u001b[0m\u001b[0m | time: 0.290s\n",
      "| SGD | epoch: 056 | loss: 0.01735 - R2: 0.9989 -- iter: 1100/1168\n",
      "Training Step: 3301  | total loss: \u001b[1m\u001b[32m0.01900\u001b[0m\u001b[0m | time: 0.292s\n",
      "| SGD | epoch: 056 | loss: 0.01900 - R2: 0.9994 -- iter: 1120/1168\n",
      "Training Step: 3302  | total loss: \u001b[1m\u001b[32m0.01926\u001b[0m\u001b[0m | time: 0.294s\n",
      "| SGD | epoch: 056 | loss: 0.01926 - R2: 0.9996 -- iter: 1140/1168\n",
      "Training Step: 3303  | total loss: \u001b[1m\u001b[32m0.03241\u001b[0m\u001b[0m | time: 0.295s\n",
      "| SGD | epoch: 056 | loss: 0.03241 - R2: 1.0009 -- iter: 1160/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3304  | total loss: \u001b[1m\u001b[32m0.02819\u001b[0m\u001b[0m | time: 1.301s\n",
      "| SGD | epoch: 056 | loss: 0.02819 - R2: 1.0008 | val_loss: 0.02285 - val_acc: 0.9984 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 3305  | total loss: \u001b[1m\u001b[32m0.02819\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 057 | loss: 0.02819 - R2: 1.0008 -- iter: 0020/1168\n",
      "Training Step: 3306  | total loss: \u001b[1m\u001b[32m0.02769\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 057 | loss: 0.02769 - R2: 0.9994 -- iter: 0040/1168\n",
      "Training Step: 3307  | total loss: \u001b[1m\u001b[32m0.02414\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 057 | loss: 0.02414 - R2: 0.9984 -- iter: 0060/1168\n",
      "Training Step: 3308  | total loss: \u001b[1m\u001b[32m0.02414\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 057 | loss: 0.02414 - R2: 0.9984 -- iter: 0080/1168\n",
      "Training Step: 3309  | total loss: \u001b[1m\u001b[32m0.02300\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 057 | loss: 0.02300 - R2: 0.9990 -- iter: 0100/1168\n",
      "Training Step: 3310  | total loss: \u001b[1m\u001b[32m0.02123\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 057 | loss: 0.02123 - R2: 1.0001 -- iter: 0120/1168\n",
      "Training Step: 3311  | total loss: \u001b[1m\u001b[32m0.01963\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 057 | loss: 0.01963 - R2: 1.0002 -- iter: 0140/1168\n",
      "Training Step: 3312  | total loss: \u001b[1m\u001b[32m0.01928\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 057 | loss: 0.01928 - R2: 1.0002 -- iter: 0160/1168\n",
      "Training Step: 3313  | total loss: \u001b[1m\u001b[32m0.01883\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 057 | loss: 0.01883 - R2: 0.9998 -- iter: 0180/1168\n",
      "Training Step: 3314  | total loss: \u001b[1m\u001b[32m0.01949\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 057 | loss: 0.01949 - R2: 1.0006 -- iter: 0200/1168\n",
      "Training Step: 3315  | total loss: \u001b[1m\u001b[32m0.02298\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 057 | loss: 0.02298 - R2: 1.0006 -- iter: 0220/1168\n",
      "Training Step: 3316  | total loss: \u001b[1m\u001b[32m0.02238\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 057 | loss: 0.02238 - R2: 1.0007 -- iter: 0240/1168\n",
      "Training Step: 3317  | total loss: \u001b[1m\u001b[32m0.02228\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 057 | loss: 0.02228 - R2: 1.0010 -- iter: 0260/1168\n",
      "Training Step: 3318  | total loss: \u001b[1m\u001b[32m0.02141\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 057 | loss: 0.02141 - R2: 1.0007 -- iter: 0280/1168\n",
      "Training Step: 3319  | total loss: \u001b[1m\u001b[32m0.01983\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 057 | loss: 0.01983 - R2: 1.0009 -- iter: 0300/1168\n",
      "Training Step: 3320  | total loss: \u001b[1m\u001b[32m0.01983\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 057 | loss: 0.01983 - R2: 1.0009 -- iter: 0320/1168\n",
      "Training Step: 3321  | total loss: \u001b[1m\u001b[32m0.01927\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 057 | loss: 0.01927 - R2: 1.0015 -- iter: 0340/1168\n",
      "Training Step: 3322  | total loss: \u001b[1m\u001b[32m0.01886\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 057 | loss: 0.01886 - R2: 1.0011 -- iter: 0360/1168\n",
      "Training Step: 3323  | total loss: \u001b[1m\u001b[32m0.01813\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 057 | loss: 0.01813 - R2: 1.0007 -- iter: 0380/1168\n",
      "Training Step: 3324  | total loss: \u001b[1m\u001b[32m0.02184\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 057 | loss: 0.02184 - R2: 1.0013 -- iter: 0400/1168\n",
      "Training Step: 3325  | total loss: \u001b[1m\u001b[32m0.02184\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 057 | loss: 0.02184 - R2: 1.0013 -- iter: 0420/1168\n",
      "Training Step: 3326  | total loss: \u001b[1m\u001b[32m0.02137\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 057 | loss: 0.02137 - R2: 1.0012 -- iter: 0440/1168\n",
      "Training Step: 3327  | total loss: \u001b[1m\u001b[32m0.02051\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 057 | loss: 0.02051 - R2: 1.0013 -- iter: 0460/1168\n",
      "Training Step: 3328  | total loss: \u001b[1m\u001b[32m0.01996\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 057 | loss: 0.01996 - R2: 1.0010 -- iter: 0480/1168\n",
      "Training Step: 3329  | total loss: \u001b[1m\u001b[32m0.01945\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 057 | loss: 0.01945 - R2: 1.0018 -- iter: 0500/1168\n",
      "Training Step: 3330  | total loss: \u001b[1m\u001b[32m0.01799\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 057 | loss: 0.01799 - R2: 1.0016 -- iter: 0520/1168\n",
      "Training Step: 3331  | total loss: \u001b[1m\u001b[32m0.01724\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 057 | loss: 0.01724 - R2: 1.0022 -- iter: 0540/1168\n",
      "Training Step: 3332  | total loss: \u001b[1m\u001b[32m0.01702\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 057 | loss: 0.01702 - R2: 1.0010 -- iter: 0560/1168\n",
      "Training Step: 3333  | total loss: \u001b[1m\u001b[32m0.01765\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 057 | loss: 0.01765 - R2: 1.0015 -- iter: 0580/1168\n",
      "Training Step: 3334  | total loss: \u001b[1m\u001b[32m0.01735\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 057 | loss: 0.01735 - R2: 1.0013 -- iter: 0600/1168\n",
      "Training Step: 3335  | total loss: \u001b[1m\u001b[32m0.01805\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 057 | loss: 0.01805 - R2: 1.0004 -- iter: 0620/1168\n",
      "Training Step: 3336  | total loss: \u001b[1m\u001b[32m0.01716\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 057 | loss: 0.01716 - R2: 1.0004 -- iter: 0640/1168\n",
      "Training Step: 3337  | total loss: \u001b[1m\u001b[32m0.01769\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 057 | loss: 0.01769 - R2: 0.9997 -- iter: 0660/1168\n",
      "Training Step: 3338  | total loss: \u001b[1m\u001b[32m0.02035\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 057 | loss: 0.02035 - R2: 1.0006 -- iter: 0680/1168\n",
      "Training Step: 3339  | total loss: \u001b[1m\u001b[32m0.01958\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 057 | loss: 0.01958 - R2: 1.0006 -- iter: 0700/1168\n",
      "Training Step: 3340  | total loss: \u001b[1m\u001b[32m0.01882\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 057 | loss: 0.01882 - R2: 1.0005 -- iter: 0720/1168\n",
      "Training Step: 3341  | total loss: \u001b[1m\u001b[32m0.01898\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 057 | loss: 0.01898 - R2: 1.0002 -- iter: 0740/1168\n",
      "Training Step: 3342  | total loss: \u001b[1m\u001b[32m0.01898\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 057 | loss: 0.01898 - R2: 1.0002 -- iter: 0760/1168\n",
      "Training Step: 3343  | total loss: \u001b[1m\u001b[32m0.01786\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 057 | loss: 0.01786 - R2: 1.0009 -- iter: 0780/1168\n",
      "Training Step: 3344  | total loss: \u001b[1m\u001b[32m0.02095\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 057 | loss: 0.02095 - R2: 1.0009 -- iter: 0800/1168\n",
      "Training Step: 3345  | total loss: \u001b[1m\u001b[32m0.01972\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 057 | loss: 0.01972 - R2: 1.0002 -- iter: 0820/1168\n",
      "Training Step: 3346  | total loss: \u001b[1m\u001b[32m0.01889\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 057 | loss: 0.01889 - R2: 1.0001 -- iter: 0840/1168\n",
      "Training Step: 3347  | total loss: \u001b[1m\u001b[32m0.01842\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 057 | loss: 0.01842 - R2: 1.0006 -- iter: 0860/1168\n",
      "Training Step: 3348  | total loss: \u001b[1m\u001b[32m0.01842\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 057 | loss: 0.01842 - R2: 1.0006 -- iter: 0880/1168\n",
      "Training Step: 3349  | total loss: \u001b[1m\u001b[32m0.01756\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 057 | loss: 0.01756 - R2: 1.0003 -- iter: 0900/1168\n",
      "Training Step: 3350  | total loss: \u001b[1m\u001b[32m0.01694\u001b[0m\u001b[0m | time: 0.176s\n",
      "| SGD | epoch: 057 | loss: 0.01694 - R2: 0.9996 -- iter: 0920/1168\n",
      "Training Step: 3351  | total loss: \u001b[1m\u001b[32m0.01657\u001b[0m\u001b[0m | time: 0.178s\n",
      "| SGD | epoch: 057 | loss: 0.01657 - R2: 1.0001 -- iter: 0940/1168\n",
      "Training Step: 3352  | total loss: \u001b[1m\u001b[32m0.01787\u001b[0m\u001b[0m | time: 0.181s\n",
      "| SGD | epoch: 057 | loss: 0.01787 - R2: 0.9994 -- iter: 0960/1168\n",
      "Training Step: 3353  | total loss: \u001b[1m\u001b[32m0.01700\u001b[0m\u001b[0m | time: 0.184s\n",
      "| SGD | epoch: 057 | loss: 0.01700 - R2: 0.9997 -- iter: 0980/1168\n",
      "Training Step: 3354  | total loss: \u001b[1m\u001b[32m0.01700\u001b[0m\u001b[0m | time: 0.186s\n",
      "| SGD | epoch: 057 | loss: 0.01700 - R2: 1.0000 -- iter: 1000/1168\n",
      "Training Step: 3355  | total loss: \u001b[1m\u001b[32m0.01824\u001b[0m\u001b[0m | time: 0.188s\n",
      "| SGD | epoch: 057 | loss: 0.01824 - R2: 1.0000 -- iter: 1020/1168\n",
      "Training Step: 3356  | total loss: \u001b[1m\u001b[32m0.01851\u001b[0m\u001b[0m | time: 0.190s\n",
      "| SGD | epoch: 057 | loss: 0.01851 - R2: 0.9997 -- iter: 1040/1168\n",
      "Training Step: 3357  | total loss: \u001b[1m\u001b[32m0.01763\u001b[0m\u001b[0m | time: 0.192s\n",
      "| SGD | epoch: 057 | loss: 0.01763 - R2: 0.9995 -- iter: 1060/1168\n",
      "Training Step: 3358  | total loss: \u001b[1m\u001b[32m0.01829\u001b[0m\u001b[0m | time: 0.194s\n",
      "| SGD | epoch: 057 | loss: 0.01829 - R2: 0.9991 -- iter: 1080/1168\n",
      "Training Step: 3359  | total loss: \u001b[1m\u001b[32m0.01907\u001b[0m\u001b[0m | time: 0.196s\n",
      "| SGD | epoch: 057 | loss: 0.01907 - R2: 1.0003 -- iter: 1100/1168\n",
      "Training Step: 3360  | total loss: \u001b[1m\u001b[32m0.01890\u001b[0m\u001b[0m | time: 0.199s\n",
      "| SGD | epoch: 057 | loss: 0.01890 - R2: 1.0003 -- iter: 1120/1168\n",
      "Training Step: 3361  | total loss: \u001b[1m\u001b[32m0.02127\u001b[0m\u001b[0m | time: 0.202s\n",
      "| SGD | epoch: 057 | loss: 0.02127 - R2: 1.0012 -- iter: 1140/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3362  | total loss: \u001b[1m\u001b[32m0.02127\u001b[0m\u001b[0m | time: 0.204s\n",
      "| SGD | epoch: 057 | loss: 0.02127 - R2: 1.0012 -- iter: 1160/1168\n",
      "Training Step: 3363  | total loss: \u001b[1m\u001b[32m0.02045\u001b[0m\u001b[0m | time: 1.210s\n",
      "| SGD | epoch: 057 | loss: 0.02045 - R2: 1.0007 | val_loss: 0.02305 - val_acc: 0.9980 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 3364  | total loss: \u001b[1m\u001b[32m0.01909\u001b[0m\u001b[0m | time: 0.011s\n",
      "| SGD | epoch: 058 | loss: 0.01909 - R2: 1.0005 -- iter: 0020/1168\n",
      "Training Step: 3365  | total loss: \u001b[1m\u001b[32m0.02044\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 058 | loss: 0.02044 - R2: 0.9991 -- iter: 0040/1168\n",
      "Training Step: 3366  | total loss: \u001b[1m\u001b[32m0.02001\u001b[0m\u001b[0m | time: 0.020s\n",
      "| SGD | epoch: 058 | loss: 0.02001 - R2: 0.9982 -- iter: 0060/1168\n",
      "Training Step: 3367  | total loss: \u001b[1m\u001b[32m0.02001\u001b[0m\u001b[0m | time: 0.023s\n",
      "| SGD | epoch: 058 | loss: 0.02001 - R2: 0.9982 -- iter: 0080/1168\n",
      "Training Step: 3368  | total loss: \u001b[1m\u001b[32m0.01912\u001b[0m\u001b[0m | time: 0.026s\n",
      "| SGD | epoch: 058 | loss: 0.01912 - R2: 0.9979 -- iter: 0100/1168\n",
      "Training Step: 3369  | total loss: \u001b[1m\u001b[32m0.01919\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 058 | loss: 0.01919 - R2: 0.9977 -- iter: 0120/1168\n",
      "Training Step: 3370  | total loss: \u001b[1m\u001b[32m0.01903\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 058 | loss: 0.01903 - R2: 0.9975 -- iter: 0140/1168\n",
      "Training Step: 3371  | total loss: \u001b[1m\u001b[32m0.01887\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 058 | loss: 0.01887 - R2: 0.9974 -- iter: 0160/1168\n",
      "Training Step: 3372  | total loss: \u001b[1m\u001b[32m0.01861\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 058 | loss: 0.01861 - R2: 0.9981 -- iter: 0180/1168\n",
      "Training Step: 3373  | total loss: \u001b[1m\u001b[32m0.01832\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 058 | loss: 0.01832 - R2: 0.9983 -- iter: 0200/1168\n",
      "Training Step: 3374  | total loss: \u001b[1m\u001b[32m0.01796\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 058 | loss: 0.01796 - R2: 0.9977 -- iter: 0220/1168\n",
      "Training Step: 3375  | total loss: \u001b[1m\u001b[32m0.01714\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 058 | loss: 0.01714 - R2: 0.9985 -- iter: 0240/1168\n",
      "Training Step: 3376  | total loss: \u001b[1m\u001b[32m0.01676\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 058 | loss: 0.01676 - R2: 0.9985 -- iter: 0260/1168\n",
      "Training Step: 3377  | total loss: \u001b[1m\u001b[32m0.01578\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 058 | loss: 0.01578 - R2: 0.9992 -- iter: 0280/1168\n",
      "Training Step: 3378  | total loss: \u001b[1m\u001b[32m0.01741\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 058 | loss: 0.01741 - R2: 0.9992 -- iter: 0300/1168\n",
      "Training Step: 3379  | total loss: \u001b[1m\u001b[32m0.03164\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 058 | loss: 0.03164 - R2: 1.0017 -- iter: 0320/1168\n",
      "Training Step: 3380  | total loss: \u001b[1m\u001b[32m0.03044\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 058 | loss: 0.03044 - R2: 1.0013 -- iter: 0340/1168\n",
      "Training Step: 3381  | total loss: \u001b[1m\u001b[32m0.02896\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 058 | loss: 0.02896 - R2: 1.0013 -- iter: 0360/1168\n",
      "Training Step: 3382  | total loss: \u001b[1m\u001b[32m0.02829\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 058 | loss: 0.02829 - R2: 1.0018 -- iter: 0380/1168\n",
      "Training Step: 3383  | total loss: \u001b[1m\u001b[32m0.02684\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 058 | loss: 0.02684 - R2: 1.0008 -- iter: 0400/1168\n",
      "Training Step: 3384  | total loss: \u001b[1m\u001b[32m0.02587\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 058 | loss: 0.02587 - R2: 1.0000 -- iter: 0420/1168\n",
      "Training Step: 3385  | total loss: \u001b[1m\u001b[32m0.02438\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 058 | loss: 0.02438 - R2: 1.0000 -- iter: 0440/1168\n",
      "Training Step: 3386  | total loss: \u001b[1m\u001b[32m0.02324\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 058 | loss: 0.02324 - R2: 1.0010 -- iter: 0460/1168\n",
      "Training Step: 3387  | total loss: \u001b[1m\u001b[32m0.02197\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 058 | loss: 0.02197 - R2: 1.0010 -- iter: 0480/1168\n",
      "Training Step: 3388  | total loss: \u001b[1m\u001b[32m0.02093\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 058 | loss: 0.02093 - R2: 1.0004 -- iter: 0500/1168\n",
      "Training Step: 3389  | total loss: \u001b[1m\u001b[32m0.01996\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 058 | loss: 0.01996 - R2: 1.0006 -- iter: 0520/1168\n",
      "Training Step: 3390  | total loss: \u001b[1m\u001b[32m0.02038\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 058 | loss: 0.02038 - R2: 1.0005 -- iter: 0540/1168\n",
      "Training Step: 3391  | total loss: \u001b[1m\u001b[32m0.01992\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 058 | loss: 0.01992 - R2: 1.0011 -- iter: 0560/1168\n",
      "Training Step: 3392  | total loss: \u001b[1m\u001b[32m0.01936\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 058 | loss: 0.01936 - R2: 1.0013 -- iter: 0580/1168\n",
      "Training Step: 3393  | total loss: \u001b[1m\u001b[32m0.01936\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 058 | loss: 0.01936 - R2: 1.0006 -- iter: 0600/1168\n",
      "Training Step: 3394  | total loss: \u001b[1m\u001b[32m0.01937\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 058 | loss: 0.01937 - R2: 1.0008 -- iter: 0620/1168\n",
      "Training Step: 3395  | total loss: \u001b[1m\u001b[32m0.01937\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 058 | loss: 0.01937 - R2: 1.0008 -- iter: 0640/1168\n",
      "Training Step: 3396  | total loss: \u001b[1m\u001b[32m0.01844\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 058 | loss: 0.01844 - R2: 1.0001 -- iter: 0660/1168\n",
      "Training Step: 3397  | total loss: \u001b[1m\u001b[32m0.01790\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 058 | loss: 0.01790 - R2: 0.9995 -- iter: 0680/1168\n",
      "Training Step: 3398  | total loss: \u001b[1m\u001b[32m0.02055\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 058 | loss: 0.02055 - R2: 1.0007 -- iter: 0700/1168\n",
      "Training Step: 3399  | total loss: \u001b[1m\u001b[32m0.02055\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 058 | loss: 0.02055 - R2: 1.0007 -- iter: 0720/1168\n",
      "Training Step: 3400  | total loss: \u001b[1m\u001b[32m0.02034\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 058 | loss: 0.02034 - R2: 0.9998 -- iter: 0740/1168\n",
      "Training Step: 3401  | total loss: \u001b[1m\u001b[32m0.01979\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 058 | loss: 0.01979 - R2: 0.9994 -- iter: 0760/1168\n",
      "Training Step: 3402  | total loss: \u001b[1m\u001b[32m0.01965\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 058 | loss: 0.01965 - R2: 0.9995 -- iter: 0780/1168\n",
      "Training Step: 3403  | total loss: \u001b[1m\u001b[32m0.01895\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 058 | loss: 0.01895 - R2: 0.9995 -- iter: 0800/1168\n",
      "Training Step: 3404  | total loss: \u001b[1m\u001b[32m0.01907\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 058 | loss: 0.01907 - R2: 1.0007 -- iter: 0820/1168\n",
      "Training Step: 3405  | total loss: \u001b[1m\u001b[32m0.01907\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 058 | loss: 0.01907 - R2: 1.0007 -- iter: 0840/1168\n",
      "Training Step: 3406  | total loss: \u001b[1m\u001b[32m0.01827\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 058 | loss: 0.01827 - R2: 1.0007 -- iter: 0860/1168\n",
      "Training Step: 3407  | total loss: \u001b[1m\u001b[32m0.01762\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 058 | loss: 0.01762 - R2: 1.0001 -- iter: 0880/1168\n",
      "Training Step: 3408  | total loss: \u001b[1m\u001b[32m0.01762\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 058 | loss: 0.01762 - R2: 0.9994 -- iter: 0900/1168\n",
      "Training Step: 3409  | total loss: \u001b[1m\u001b[32m0.01992\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 058 | loss: 0.01992 - R2: 0.9994 -- iter: 0920/1168\n",
      "Training Step: 3410  | total loss: \u001b[1m\u001b[32m0.02007\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 058 | loss: 0.02007 - R2: 0.9992 -- iter: 0940/1168\n",
      "Training Step: 3411  | total loss: \u001b[1m\u001b[32m0.01911\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 058 | loss: 0.01911 - R2: 0.9985 -- iter: 0960/1168\n",
      "Training Step: 3412  | total loss: \u001b[1m\u001b[32m0.01939\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 058 | loss: 0.01939 - R2: 0.9985 -- iter: 0980/1168\n",
      "Training Step: 3413  | total loss: \u001b[1m\u001b[32m0.02069\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 058 | loss: 0.02069 - R2: 0.9992 -- iter: 1000/1168\n",
      "Training Step: 3414  | total loss: \u001b[1m\u001b[32m0.01972\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 058 | loss: 0.01972 - R2: 0.9985 -- iter: 1020/1168\n",
      "Training Step: 3415  | total loss: \u001b[1m\u001b[32m0.01932\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 058 | loss: 0.01932 - R2: 0.9993 -- iter: 1040/1168\n",
      "Training Step: 3416  | total loss: \u001b[1m\u001b[32m0.01932\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 058 | loss: 0.01932 - R2: 0.9993 -- iter: 1060/1168\n",
      "Training Step: 3417  | total loss: \u001b[1m\u001b[32m0.01827\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 058 | loss: 0.01827 - R2: 0.9990 -- iter: 1080/1168\n",
      "Training Step: 3418  | total loss: \u001b[1m\u001b[32m0.01897\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 058 | loss: 0.01897 - R2: 0.9990 -- iter: 1100/1168\n",
      "Training Step: 3419  | total loss: \u001b[1m\u001b[32m0.01844\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 058 | loss: 0.01844 - R2: 0.9990 -- iter: 1120/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3420  | total loss: \u001b[1m\u001b[32m0.01793\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 058 | loss: 0.01793 - R2: 0.9995 -- iter: 1140/1168\n",
      "Training Step: 3421  | total loss: \u001b[1m\u001b[32m0.01793\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 058 | loss: 0.01793 - R2: 0.9995 -- iter: 1160/1168\n",
      "Training Step: 3422  | total loss: \u001b[1m\u001b[32m0.01795\u001b[0m\u001b[0m | time: 1.163s\n",
      "| SGD | epoch: 058 | loss: 0.01795 - R2: 0.9999 | val_loss: 0.02309 - val_acc: 0.9983 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 3423  | total loss: \u001b[1m\u001b[32m0.01782\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 059 | loss: 0.01782 - R2: 1.0004 -- iter: 0020/1168\n",
      "Training Step: 3424  | total loss: \u001b[1m\u001b[32m0.01838\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 059 | loss: 0.01838 - R2: 0.9997 -- iter: 0040/1168\n",
      "Training Step: 3425  | total loss: \u001b[1m\u001b[32m0.01730\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 059 | loss: 0.01730 - R2: 0.9995 -- iter: 0060/1168\n",
      "Training Step: 3426  | total loss: \u001b[1m\u001b[32m0.01730\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 059 | loss: 0.01730 - R2: 0.9995 -- iter: 0080/1168\n",
      "Training Step: 3427  | total loss: \u001b[1m\u001b[32m0.01822\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 059 | loss: 0.01822 - R2: 0.9996 -- iter: 0100/1168\n",
      "Training Step: 3428  | total loss: \u001b[1m\u001b[32m0.01807\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 059 | loss: 0.01807 - R2: 0.9996 -- iter: 0120/1168\n",
      "Training Step: 3429  | total loss: \u001b[1m\u001b[32m0.01807\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 059 | loss: 0.01807 - R2: 0.9996 -- iter: 0140/1168\n",
      "Training Step: 3430  | total loss: \u001b[1m\u001b[32m0.01742\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 059 | loss: 0.01742 - R2: 0.9976 -- iter: 0160/1168\n",
      "Training Step: 3431  | total loss: \u001b[1m\u001b[32m0.01742\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 059 | loss: 0.01742 - R2: 0.9976 -- iter: 0180/1168\n",
      "Training Step: 3432  | total loss: \u001b[1m\u001b[32m0.01685\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 059 | loss: 0.01685 - R2: 0.9982 -- iter: 0200/1168\n",
      "Training Step: 3433  | total loss: \u001b[1m\u001b[32m0.01669\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 059 | loss: 0.01669 - R2: 0.9983 -- iter: 0220/1168\n",
      "Training Step: 3434  | total loss: \u001b[1m\u001b[32m0.01628\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 059 | loss: 0.01628 - R2: 0.9983 -- iter: 0240/1168\n",
      "Training Step: 3435  | total loss: \u001b[1m\u001b[32m0.01617\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 059 | loss: 0.01617 - R2: 0.9984 -- iter: 0260/1168\n",
      "Training Step: 3436  | total loss: \u001b[1m\u001b[32m0.01535\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 059 | loss: 0.01535 - R2: 0.9990 -- iter: 0280/1168\n",
      "Training Step: 3437  | total loss: \u001b[1m\u001b[32m0.01489\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 059 | loss: 0.01489 - R2: 0.9990 -- iter: 0300/1168\n",
      "Training Step: 3438  | total loss: \u001b[1m\u001b[32m0.01442\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 059 | loss: 0.01442 - R2: 0.9988 -- iter: 0320/1168\n",
      "Training Step: 3439  | total loss: \u001b[1m\u001b[32m0.01406\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 059 | loss: 0.01406 - R2: 0.9986 -- iter: 0340/1168\n",
      "Training Step: 3440  | total loss: \u001b[1m\u001b[32m0.01434\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 059 | loss: 0.01434 - R2: 0.9981 -- iter: 0360/1168\n",
      "Training Step: 3441  | total loss: \u001b[1m\u001b[32m0.01521\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 059 | loss: 0.01521 - R2: 0.9993 -- iter: 0380/1168\n",
      "Training Step: 3442  | total loss: \u001b[1m\u001b[32m0.01724\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 059 | loss: 0.01724 - R2: 0.9999 -- iter: 0400/1168\n",
      "Training Step: 3443  | total loss: \u001b[1m\u001b[32m0.01724\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 059 | loss: 0.01724 - R2: 1.0000 -- iter: 0420/1168\n",
      "Training Step: 3444  | total loss: \u001b[1m\u001b[32m0.01681\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 059 | loss: 0.01681 - R2: 1.0000 -- iter: 0440/1168\n",
      "Training Step: 3445  | total loss: \u001b[1m\u001b[32m0.01734\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 059 | loss: 0.01734 - R2: 1.0004 -- iter: 0460/1168\n",
      "Training Step: 3446  | total loss: \u001b[1m\u001b[32m0.01652\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 059 | loss: 0.01652 - R2: 1.0000 -- iter: 0480/1168\n",
      "Training Step: 3447  | total loss: \u001b[1m\u001b[32m0.01652\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 059 | loss: 0.01652 - R2: 1.0000 -- iter: 0500/1168\n",
      "Training Step: 3448  | total loss: \u001b[1m\u001b[32m0.01566\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 059 | loss: 0.01566 - R2: 1.0016 -- iter: 0520/1168\n",
      "Training Step: 3449  | total loss: \u001b[1m\u001b[32m0.01684\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 059 | loss: 0.01684 - R2: 1.0012 -- iter: 0540/1168\n",
      "Training Step: 3450  | total loss: \u001b[1m\u001b[32m0.01755\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 059 | loss: 0.01755 - R2: 1.0012 -- iter: 0560/1168\n",
      "Training Step: 3451  | total loss: \u001b[1m\u001b[32m0.01755\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 059 | loss: 0.01755 - R2: 1.0009 -- iter: 0580/1168\n",
      "Training Step: 3452  | total loss: \u001b[1m\u001b[32m0.01888\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 059 | loss: 0.01888 - R2: 0.9995 -- iter: 0600/1168\n",
      "Training Step: 3453  | total loss: \u001b[1m\u001b[32m0.01888\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 059 | loss: 0.01888 - R2: 0.9995 -- iter: 0620/1168\n",
      "Training Step: 3454  | total loss: \u001b[1m\u001b[32m0.02906\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 059 | loss: 0.02906 - R2: 1.0007 -- iter: 0640/1168\n",
      "Training Step: 3455  | total loss: \u001b[1m\u001b[32m0.02906\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 059 | loss: 0.02906 - R2: 1.0007 -- iter: 0660/1168\n",
      "Training Step: 3456  | total loss: \u001b[1m\u001b[32m0.02726\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 059 | loss: 0.02726 - R2: 0.9997 -- iter: 0680/1168\n",
      "Training Step: 3457  | total loss: \u001b[1m\u001b[32m0.02510\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 059 | loss: 0.02510 - R2: 1.0002 -- iter: 0700/1168\n",
      "Training Step: 3458  | total loss: \u001b[1m\u001b[32m0.02510\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 059 | loss: 0.02510 - R2: 1.0002 -- iter: 0720/1168\n",
      "Training Step: 3459  | total loss: \u001b[1m\u001b[32m0.02213\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 059 | loss: 0.02213 - R2: 1.0011 -- iter: 0740/1168\n",
      "Training Step: 3460  | total loss: \u001b[1m\u001b[32m0.03283\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 059 | loss: 0.03283 - R2: 1.0022 -- iter: 0760/1168\n",
      "Training Step: 3461  | total loss: \u001b[1m\u001b[32m0.03067\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 059 | loss: 0.03067 - R2: 1.0023 -- iter: 0780/1168\n",
      "Training Step: 3462  | total loss: \u001b[1m\u001b[32m0.02938\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 059 | loss: 0.02938 - R2: 1.0022 -- iter: 0800/1168\n",
      "Training Step: 3463  | total loss: \u001b[1m\u001b[32m0.02704\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 059 | loss: 0.02704 - R2: 1.0017 -- iter: 0820/1168\n",
      "Training Step: 3464  | total loss: \u001b[1m\u001b[32m0.02927\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 059 | loss: 0.02927 - R2: 1.0022 -- iter: 0840/1168\n",
      "Training Step: 3465  | total loss: \u001b[1m\u001b[32m0.02763\u001b[0m\u001b[0m | time: 0.180s\n",
      "| SGD | epoch: 059 | loss: 0.02763 - R2: 1.0020 -- iter: 0860/1168\n",
      "Training Step: 3466  | total loss: \u001b[1m\u001b[32m0.02763\u001b[0m\u001b[0m | time: 0.184s\n",
      "| SGD | epoch: 059 | loss: 0.02763 - R2: 1.0020 -- iter: 0880/1168\n",
      "Training Step: 3467  | total loss: \u001b[1m\u001b[32m0.02694\u001b[0m\u001b[0m | time: 0.186s\n",
      "| SGD | epoch: 059 | loss: 0.02694 - R2: 1.0011 -- iter: 0900/1168\n",
      "Training Step: 3468  | total loss: \u001b[1m\u001b[32m0.02468\u001b[0m\u001b[0m | time: 0.190s\n",
      "| SGD | epoch: 059 | loss: 0.02468 - R2: 1.0005 -- iter: 0920/1168\n",
      "Training Step: 3469  | total loss: \u001b[1m\u001b[32m0.02468\u001b[0m\u001b[0m | time: 0.192s\n",
      "| SGD | epoch: 059 | loss: 0.02468 - R2: 1.0005 -- iter: 0940/1168\n",
      "Training Step: 3470  | total loss: \u001b[1m\u001b[32m0.02390\u001b[0m\u001b[0m | time: 0.195s\n",
      "| SGD | epoch: 059 | loss: 0.02390 - R2: 1.0004 -- iter: 0960/1168\n",
      "Training Step: 3471  | total loss: \u001b[1m\u001b[32m0.02288\u001b[0m\u001b[0m | time: 0.198s\n",
      "| SGD | epoch: 059 | loss: 0.02288 - R2: 1.0010 -- iter: 0980/1168\n",
      "Training Step: 3472  | total loss: \u001b[1m\u001b[32m0.02288\u001b[0m\u001b[0m | time: 0.201s\n",
      "| SGD | epoch: 059 | loss: 0.02288 - R2: 1.0006 -- iter: 1000/1168\n",
      "Training Step: 3473  | total loss: \u001b[1m\u001b[32m0.02283\u001b[0m\u001b[0m | time: 0.204s\n",
      "| SGD | epoch: 059 | loss: 0.02283 - R2: 0.9998 -- iter: 1020/1168\n",
      "Training Step: 3474  | total loss: \u001b[1m\u001b[32m0.02190\u001b[0m\u001b[0m | time: 0.206s\n",
      "| SGD | epoch: 059 | loss: 0.02190 - R2: 0.9993 -- iter: 1040/1168\n",
      "Training Step: 3475  | total loss: \u001b[1m\u001b[32m0.02134\u001b[0m\u001b[0m | time: 0.209s\n",
      "| SGD | epoch: 059 | loss: 0.02134 - R2: 0.9991 -- iter: 1060/1168\n",
      "Training Step: 3476  | total loss: \u001b[1m\u001b[32m0.02101\u001b[0m\u001b[0m | time: 0.212s\n",
      "| SGD | epoch: 059 | loss: 0.02101 - R2: 0.9990 -- iter: 1080/1168\n",
      "Training Step: 3477  | total loss: \u001b[1m\u001b[32m0.02015\u001b[0m\u001b[0m | time: 0.233s\n",
      "| SGD | epoch: 059 | loss: 0.02015 - R2: 0.9996 -- iter: 1100/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3478  | total loss: \u001b[1m\u001b[32m0.02015\u001b[0m\u001b[0m | time: 0.243s\n",
      "| SGD | epoch: 059 | loss: 0.02015 - R2: 0.9996 -- iter: 1120/1168\n",
      "Training Step: 3479  | total loss: \u001b[1m\u001b[32m0.02118\u001b[0m\u001b[0m | time: 0.247s\n",
      "| SGD | epoch: 059 | loss: 0.02118 - R2: 0.9999 -- iter: 1140/1168\n",
      "Training Step: 3480  | total loss: \u001b[1m\u001b[32m0.01974\u001b[0m\u001b[0m | time: 0.250s\n",
      "| SGD | epoch: 059 | loss: 0.01974 - R2: 0.9993 -- iter: 1160/1168\n",
      "Training Step: 3481  | total loss: \u001b[1m\u001b[32m0.01974\u001b[0m\u001b[0m | time: 1.256s\n",
      "| SGD | epoch: 059 | loss: 0.01974 - R2: 0.9991 | val_loss: 0.02318 - val_acc: 0.9983 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 3482  | total loss: \u001b[1m\u001b[32m0.01961\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 060 | loss: 0.01961 - R2: 0.9994 -- iter: 0020/1168\n",
      "Training Step: 3483  | total loss: \u001b[1m\u001b[32m0.01953\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 060 | loss: 0.01953 - R2: 0.9988 -- iter: 0040/1168\n",
      "Training Step: 3484  | total loss: \u001b[1m\u001b[32m0.01885\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 060 | loss: 0.01885 - R2: 0.9990 -- iter: 0060/1168\n",
      "Training Step: 3485  | total loss: \u001b[1m\u001b[32m0.01773\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 060 | loss: 0.01773 - R2: 0.9991 -- iter: 0080/1168\n",
      "Training Step: 3486  | total loss: \u001b[1m\u001b[32m0.01677\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 060 | loss: 0.01677 - R2: 0.9991 -- iter: 0100/1168\n",
      "Training Step: 3487  | total loss: \u001b[1m\u001b[32m0.01677\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 060 | loss: 0.01677 - R2: 0.9991 -- iter: 0120/1168\n",
      "Training Step: 3488  | total loss: \u001b[1m\u001b[32m0.01926\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 060 | loss: 0.01926 - R2: 0.9997 -- iter: 0140/1168\n",
      "Training Step: 3489  | total loss: \u001b[1m\u001b[32m0.01936\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 060 | loss: 0.01936 - R2: 0.9991 -- iter: 0160/1168\n",
      "Training Step: 3490  | total loss: \u001b[1m\u001b[32m0.01852\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 060 | loss: 0.01852 - R2: 1.0002 -- iter: 0180/1168\n",
      "Training Step: 3491  | total loss: \u001b[1m\u001b[32m0.01775\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 060 | loss: 0.01775 - R2: 1.0013 -- iter: 0200/1168\n",
      "Training Step: 3492  | total loss: \u001b[1m\u001b[32m0.01898\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 060 | loss: 0.01898 - R2: 1.0011 -- iter: 0220/1168\n",
      "Training Step: 3493  | total loss: \u001b[1m\u001b[32m0.01904\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 060 | loss: 0.01904 - R2: 1.0008 -- iter: 0240/1168\n",
      "Training Step: 3494  | total loss: \u001b[1m\u001b[32m0.01904\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 060 | loss: 0.01904 - R2: 1.0008 -- iter: 0260/1168\n",
      "Training Step: 3495  | total loss: \u001b[1m\u001b[32m0.01914\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 060 | loss: 0.01914 - R2: 1.0001 -- iter: 0280/1168\n",
      "Training Step: 3496  | total loss: \u001b[1m\u001b[32m0.01807\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 060 | loss: 0.01807 - R2: 1.0002 -- iter: 0300/1168\n",
      "Training Step: 3497  | total loss: \u001b[1m\u001b[32m0.01867\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 060 | loss: 0.01867 - R2: 0.9996 -- iter: 0320/1168\n",
      "Training Step: 3498  | total loss: \u001b[1m\u001b[32m0.01867\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 060 | loss: 0.01867 - R2: 0.9996 -- iter: 0340/1168\n",
      "Training Step: 3499  | total loss: \u001b[1m\u001b[32m0.01800\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 060 | loss: 0.01800 - R2: 0.9990 -- iter: 0360/1168\n",
      "Training Step: 3500  | total loss: \u001b[1m\u001b[32m0.01900\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 060 | loss: 0.01900 - R2: 0.9998 -- iter: 0380/1168\n",
      "Training Step: 3501  | total loss: \u001b[1m\u001b[32m0.01900\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 060 | loss: 0.01900 - R2: 0.9998 -- iter: 0400/1168\n",
      "Training Step: 3502  | total loss: \u001b[1m\u001b[32m0.01918\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 060 | loss: 0.01918 - R2: 0.9993 -- iter: 0420/1168\n",
      "Training Step: 3503  | total loss: \u001b[1m\u001b[32m0.01848\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 060 | loss: 0.01848 - R2: 0.9988 -- iter: 0440/1168\n",
      "Training Step: 3504  | total loss: \u001b[1m\u001b[32m0.01848\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 060 | loss: 0.01848 - R2: 0.9985 -- iter: 0460/1168\n",
      "Training Step: 3505  | total loss: \u001b[1m\u001b[32m0.01725\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 060 | loss: 0.01725 - R2: 0.9987 -- iter: 0480/1168\n",
      "Training Step: 3506  | total loss: \u001b[1m\u001b[32m0.01961\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 060 | loss: 0.01961 - R2: 0.9998 -- iter: 0500/1168\n",
      "Training Step: 3507  | total loss: \u001b[1m\u001b[32m0.01961\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 060 | loss: 0.01961 - R2: 0.9998 -- iter: 0520/1168\n",
      "Training Step: 3508  | total loss: \u001b[1m\u001b[32m0.01783\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 060 | loss: 0.01783 - R2: 0.9989 -- iter: 0540/1168\n",
      "Training Step: 3509  | total loss: \u001b[1m\u001b[32m0.01731\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 060 | loss: 0.01731 - R2: 0.9995 -- iter: 0560/1168\n",
      "Training Step: 3510  | total loss: \u001b[1m\u001b[32m0.01782\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 060 | loss: 0.01782 - R2: 1.0000 -- iter: 0580/1168\n",
      "Training Step: 3511  | total loss: \u001b[1m\u001b[32m0.01782\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 060 | loss: 0.01782 - R2: 1.0000 -- iter: 0600/1168\n",
      "Training Step: 3512  | total loss: \u001b[1m\u001b[32m0.01706\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 060 | loss: 0.01706 - R2: 1.0000 -- iter: 0620/1168\n",
      "Training Step: 3513  | total loss: \u001b[1m\u001b[32m0.01594\u001b[0m\u001b[0m | time: 0.165s\n",
      "| SGD | epoch: 060 | loss: 0.01594 - R2: 0.9996 -- iter: 0640/1168\n",
      "Training Step: 3514  | total loss: \u001b[1m\u001b[32m0.01547\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 060 | loss: 0.01547 - R2: 0.9999 -- iter: 0660/1168\n",
      "Training Step: 3515  | total loss: \u001b[1m\u001b[32m0.01547\u001b[0m\u001b[0m | time: 0.169s\n",
      "| SGD | epoch: 060 | loss: 0.01547 - R2: 0.9999 -- iter: 0680/1168\n",
      "Training Step: 3516  | total loss: \u001b[1m\u001b[32m0.01691\u001b[0m\u001b[0m | time: 0.172s\n",
      "| SGD | epoch: 060 | loss: 0.01691 - R2: 1.0009 -- iter: 0700/1168\n",
      "Training Step: 3517  | total loss: \u001b[1m\u001b[32m0.01691\u001b[0m\u001b[0m | time: 0.174s\n",
      "| SGD | epoch: 060 | loss: 0.01691 - R2: 1.0009 -- iter: 0720/1168\n",
      "Training Step: 3518  | total loss: \u001b[1m\u001b[32m0.01709\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 060 | loss: 0.01709 - R2: 1.0002 -- iter: 0740/1168\n",
      "Training Step: 3519  | total loss: \u001b[1m\u001b[32m0.01910\u001b[0m\u001b[0m | time: 0.186s\n",
      "| SGD | epoch: 060 | loss: 0.01910 - R2: 1.0004 -- iter: 0760/1168\n",
      "Training Step: 3520  | total loss: \u001b[1m\u001b[32m0.01910\u001b[0m\u001b[0m | time: 0.189s\n",
      "| SGD | epoch: 060 | loss: 0.01910 - R2: 1.0004 -- iter: 0780/1168\n",
      "Training Step: 3521  | total loss: \u001b[1m\u001b[32m0.02210\u001b[0m\u001b[0m | time: 0.192s\n",
      "| SGD | epoch: 060 | loss: 0.02210 - R2: 1.0007 -- iter: 0800/1168\n",
      "Training Step: 3522  | total loss: \u001b[1m\u001b[32m0.02137\u001b[0m\u001b[0m | time: 0.195s\n",
      "| SGD | epoch: 060 | loss: 0.02137 - R2: 1.0008 -- iter: 0820/1168\n",
      "Training Step: 3523  | total loss: \u001b[1m\u001b[32m0.02125\u001b[0m\u001b[0m | time: 0.201s\n",
      "| SGD | epoch: 060 | loss: 0.02125 - R2: 1.0002 -- iter: 0840/1168\n",
      "Training Step: 3524  | total loss: \u001b[1m\u001b[32m0.02149\u001b[0m\u001b[0m | time: 0.204s\n",
      "| SGD | epoch: 060 | loss: 0.02149 - R2: 1.0009 -- iter: 0860/1168\n",
      "Training Step: 3525  | total loss: \u001b[1m\u001b[32m0.02108\u001b[0m\u001b[0m | time: 0.207s\n",
      "| SGD | epoch: 060 | loss: 0.02108 - R2: 1.0004 -- iter: 0880/1168\n",
      "Training Step: 3526  | total loss: \u001b[1m\u001b[32m0.02108\u001b[0m\u001b[0m | time: 0.209s\n",
      "| SGD | epoch: 060 | loss: 0.02108 - R2: 1.0004 -- iter: 0900/1168\n",
      "Training Step: 3527  | total loss: \u001b[1m\u001b[32m0.02118\u001b[0m\u001b[0m | time: 0.211s\n",
      "| SGD | epoch: 060 | loss: 0.02118 - R2: 1.0000 -- iter: 0920/1168\n",
      "Training Step: 3528  | total loss: \u001b[1m\u001b[32m0.02136\u001b[0m\u001b[0m | time: 0.213s\n",
      "| SGD | epoch: 060 | loss: 0.02136 - R2: 1.0000 -- iter: 0940/1168\n",
      "Training Step: 3529  | total loss: \u001b[1m\u001b[32m0.02042\u001b[0m\u001b[0m | time: 0.215s\n",
      "| SGD | epoch: 060 | loss: 0.02042 - R2: 1.0000 -- iter: 0960/1168\n",
      "Training Step: 3530  | total loss: \u001b[1m\u001b[32m0.01937\u001b[0m\u001b[0m | time: 0.217s\n",
      "| SGD | epoch: 060 | loss: 0.01937 - R2: 1.0000 -- iter: 0980/1168\n",
      "Training Step: 3531  | total loss: \u001b[1m\u001b[32m0.02092\u001b[0m\u001b[0m | time: 0.224s\n",
      "| SGD | epoch: 060 | loss: 0.02092 - R2: 1.0015 -- iter: 1000/1168\n",
      "Training Step: 3532  | total loss: \u001b[1m\u001b[32m0.02092\u001b[0m\u001b[0m | time: 0.229s\n",
      "| SGD | epoch: 060 | loss: 0.02092 - R2: 1.0015 -- iter: 1020/1168\n",
      "Training Step: 3533  | total loss: \u001b[1m\u001b[32m0.02142\u001b[0m\u001b[0m | time: 0.234s\n",
      "| SGD | epoch: 060 | loss: 0.02142 - R2: 1.0019 -- iter: 1040/1168\n",
      "Training Step: 3534  | total loss: \u001b[1m\u001b[32m0.02073\u001b[0m\u001b[0m | time: 0.252s\n",
      "| SGD | epoch: 060 | loss: 0.02073 - R2: 1.0016 -- iter: 1060/1168\n",
      "Training Step: 3535  | total loss: \u001b[1m\u001b[32m0.02073\u001b[0m\u001b[0m | time: 0.259s\n",
      "| SGD | epoch: 060 | loss: 0.02073 - R2: 1.0016 -- iter: 1080/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3536  | total loss: \u001b[1m\u001b[32m0.02864\u001b[0m\u001b[0m | time: 0.267s\n",
      "| SGD | epoch: 060 | loss: 0.02864 - R2: 1.0028 -- iter: 1100/1168\n",
      "Training Step: 3537  | total loss: \u001b[1m\u001b[32m0.02864\u001b[0m\u001b[0m | time: 0.270s\n",
      "| SGD | epoch: 060 | loss: 0.02864 - R2: 1.0028 -- iter: 1120/1168\n",
      "Training Step: 3538  | total loss: \u001b[1m\u001b[32m0.02760\u001b[0m\u001b[0m | time: 0.273s\n",
      "| SGD | epoch: 060 | loss: 0.02760 - R2: 1.0022 -- iter: 1140/1168\n",
      "Training Step: 3539  | total loss: \u001b[1m\u001b[32m0.02555\u001b[0m\u001b[0m | time: 0.277s\n",
      "| SGD | epoch: 060 | loss: 0.02555 - R2: 1.0012 -- iter: 1160/1168\n",
      "Training Step: 3540  | total loss: \u001b[1m\u001b[32m0.02400\u001b[0m\u001b[0m | time: 1.289s\n",
      "| SGD | epoch: 060 | loss: 0.02400 - R2: 1.0009 | val_loss: 0.02328 - val_acc: 0.9979 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 3541  | total loss: \u001b[1m\u001b[32m0.02400\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 061 | loss: 0.02400 - R2: 1.0006 -- iter: 0020/1168\n",
      "Training Step: 3542  | total loss: \u001b[1m\u001b[32m0.02440\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 061 | loss: 0.02440 - R2: 1.0011 -- iter: 0040/1168\n",
      "Training Step: 3543  | total loss: \u001b[1m\u001b[32m0.02312\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 061 | loss: 0.02312 - R2: 1.0004 -- iter: 0060/1168\n",
      "Training Step: 3544  | total loss: \u001b[1m\u001b[32m0.02185\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 061 | loss: 0.02185 - R2: 1.0001 -- iter: 0080/1168\n",
      "Training Step: 3545  | total loss: \u001b[1m\u001b[32m0.02185\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 061 | loss: 0.02185 - R2: 1.0009 -- iter: 0100/1168\n",
      "Training Step: 3546  | total loss: \u001b[1m\u001b[32m0.02140\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 061 | loss: 0.02140 - R2: 1.0009 -- iter: 0120/1168\n",
      "Training Step: 3547  | total loss: \u001b[1m\u001b[32m0.02039\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 061 | loss: 0.02039 - R2: 1.0004 -- iter: 0140/1168\n",
      "Training Step: 3548  | total loss: \u001b[1m\u001b[32m0.01924\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 061 | loss: 0.01924 - R2: 0.9997 -- iter: 0160/1168\n",
      "Training Step: 3549  | total loss: \u001b[1m\u001b[32m0.01924\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 061 | loss: 0.01924 - R2: 0.9997 -- iter: 0180/1168\n",
      "Training Step: 3550  | total loss: \u001b[1m\u001b[32m0.01833\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 061 | loss: 0.01833 - R2: 1.0000 -- iter: 0200/1168\n",
      "Training Step: 3551  | total loss: \u001b[1m\u001b[32m0.01749\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 061 | loss: 0.01749 - R2: 1.0002 -- iter: 0220/1168\n",
      "Training Step: 3552  | total loss: \u001b[1m\u001b[32m0.01988\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 061 | loss: 0.01988 - R2: 0.9993 -- iter: 0240/1168\n",
      "Training Step: 3553  | total loss: \u001b[1m\u001b[32m0.01964\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 061 | loss: 0.01964 - R2: 0.9993 -- iter: 0260/1168\n",
      "Training Step: 3554  | total loss: \u001b[1m\u001b[32m0.01822\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 061 | loss: 0.01822 - R2: 0.9982 -- iter: 0280/1168\n",
      "Training Step: 3555  | total loss: \u001b[1m\u001b[32m0.01792\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 061 | loss: 0.01792 - R2: 0.9982 -- iter: 0300/1168\n",
      "Training Step: 3556  | total loss: \u001b[1m\u001b[32m0.01899\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 061 | loss: 0.01899 - R2: 0.9982 -- iter: 0320/1168\n",
      "Training Step: 3557  | total loss: \u001b[1m\u001b[32m0.01899\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 061 | loss: 0.01899 - R2: 0.9982 -- iter: 0340/1168\n",
      "Training Step: 3558  | total loss: \u001b[1m\u001b[32m0.01909\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 061 | loss: 0.01909 - R2: 0.9982 -- iter: 0360/1168\n",
      "Training Step: 3559  | total loss: \u001b[1m\u001b[32m0.01909\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 061 | loss: 0.01909 - R2: 0.9982 -- iter: 0380/1168\n",
      "Training Step: 3560  | total loss: \u001b[1m\u001b[32m0.01912\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 061 | loss: 0.01912 - R2: 0.9981 -- iter: 0400/1168\n",
      "Training Step: 3561  | total loss: \u001b[1m\u001b[32m0.01868\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 061 | loss: 0.01868 - R2: 0.9978 -- iter: 0420/1168\n",
      "Training Step: 3562  | total loss: \u001b[1m\u001b[32m0.01868\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 061 | loss: 0.01868 - R2: 0.9985 -- iter: 0440/1168\n",
      "Training Step: 3563  | total loss: \u001b[1m\u001b[32m0.01853\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 061 | loss: 0.01853 - R2: 0.9988 -- iter: 0460/1168\n",
      "Training Step: 3564  | total loss: \u001b[1m\u001b[32m0.01827\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 061 | loss: 0.01827 - R2: 0.9986 -- iter: 0480/1168\n",
      "Training Step: 3565  | total loss: \u001b[1m\u001b[32m0.01757\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 061 | loss: 0.01757 - R2: 0.9983 -- iter: 0500/1168\n",
      "Training Step: 3566  | total loss: \u001b[1m\u001b[32m0.01729\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 061 | loss: 0.01729 - R2: 0.9983 -- iter: 0520/1168\n",
      "Training Step: 3567  | total loss: \u001b[1m\u001b[32m0.01729\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 061 | loss: 0.01729 - R2: 0.9983 -- iter: 0540/1168\n",
      "Training Step: 3568  | total loss: \u001b[1m\u001b[32m0.01664\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 061 | loss: 0.01664 - R2: 0.9986 -- iter: 0560/1168\n",
      "Training Step: 3569  | total loss: \u001b[1m\u001b[32m0.01593\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 061 | loss: 0.01593 - R2: 0.9982 -- iter: 0580/1168\n",
      "Training Step: 3570  | total loss: \u001b[1m\u001b[32m0.01507\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 061 | loss: 0.01507 - R2: 0.9989 -- iter: 0600/1168\n",
      "Training Step: 3571  | total loss: \u001b[1m\u001b[32m0.01507\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 061 | loss: 0.01507 - R2: 0.9989 -- iter: 0620/1168\n",
      "Training Step: 3572  | total loss: \u001b[1m\u001b[32m0.01491\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 061 | loss: 0.01491 - R2: 0.9987 -- iter: 0640/1168\n",
      "Training Step: 3573  | total loss: \u001b[1m\u001b[32m0.01433\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 061 | loss: 0.01433 - R2: 0.9995 -- iter: 0660/1168\n",
      "Training Step: 3574  | total loss: \u001b[1m\u001b[32m0.01364\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 061 | loss: 0.01364 - R2: 0.9996 -- iter: 0680/1168\n",
      "Training Step: 3575  | total loss: \u001b[1m\u001b[32m0.01326\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 061 | loss: 0.01326 - R2: 0.9994 -- iter: 0700/1168\n",
      "Training Step: 3576  | total loss: \u001b[1m\u001b[32m0.01438\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 061 | loss: 0.01438 - R2: 0.9999 -- iter: 0720/1168\n",
      "Training Step: 3577  | total loss: \u001b[1m\u001b[32m0.06100\u001b[0m\u001b[0m | time: 0.169s\n",
      "| SGD | epoch: 061 | loss: 0.06100 - R2: 0.9974 -- iter: 0740/1168\n",
      "Training Step: 3578  | total loss: \u001b[1m\u001b[32m0.05765\u001b[0m\u001b[0m | time: 0.172s\n",
      "| SGD | epoch: 061 | loss: 0.05765 - R2: 0.9984 -- iter: 0760/1168\n",
      "Training Step: 3579  | total loss: \u001b[1m\u001b[32m0.05765\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 061 | loss: 0.05765 - R2: 0.9984 -- iter: 0780/1168\n",
      "Training Step: 3580  | total loss: \u001b[1m\u001b[32m0.05364\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 061 | loss: 0.05364 - R2: 0.9985 -- iter: 0800/1168\n",
      "Training Step: 3581  | total loss: \u001b[1m\u001b[32m0.04857\u001b[0m\u001b[0m | time: 0.180s\n",
      "| SGD | epoch: 061 | loss: 0.04857 - R2: 0.9975 -- iter: 0820/1168\n",
      "Training Step: 3582  | total loss: \u001b[1m\u001b[32m0.04857\u001b[0m\u001b[0m | time: 0.183s\n",
      "| SGD | epoch: 061 | loss: 0.04857 - R2: 0.9975 -- iter: 0840/1168\n",
      "Training Step: 3583  | total loss: \u001b[1m\u001b[32m0.04170\u001b[0m\u001b[0m | time: 0.185s\n",
      "| SGD | epoch: 061 | loss: 0.04170 - R2: 0.9975 -- iter: 0860/1168\n",
      "Training Step: 3584  | total loss: \u001b[1m\u001b[32m0.04170\u001b[0m\u001b[0m | time: 0.187s\n",
      "| SGD | epoch: 061 | loss: 0.04170 - R2: 0.9975 -- iter: 0880/1168\n",
      "Training Step: 3585  | total loss: \u001b[1m\u001b[32m0.03855\u001b[0m\u001b[0m | time: 0.189s\n",
      "| SGD | epoch: 061 | loss: 0.03855 - R2: 0.9979 -- iter: 0900/1168\n",
      "Training Step: 3586  | total loss: \u001b[1m\u001b[32m0.03596\u001b[0m\u001b[0m | time: 0.191s\n",
      "| SGD | epoch: 061 | loss: 0.03596 - R2: 0.9982 -- iter: 0920/1168\n",
      "Training Step: 3587  | total loss: \u001b[1m\u001b[32m0.03367\u001b[0m\u001b[0m | time: 0.193s\n",
      "| SGD | epoch: 061 | loss: 0.03367 - R2: 0.9983 -- iter: 0940/1168\n",
      "Training Step: 3588  | total loss: \u001b[1m\u001b[32m0.03983\u001b[0m\u001b[0m | time: 0.196s\n",
      "| SGD | epoch: 061 | loss: 0.03983 - R2: 1.0008 -- iter: 0960/1168\n",
      "Training Step: 3589  | total loss: \u001b[1m\u001b[32m0.03889\u001b[0m\u001b[0m | time: 0.199s\n",
      "| SGD | epoch: 061 | loss: 0.03889 - R2: 0.9995 -- iter: 0980/1168\n",
      "Training Step: 3590  | total loss: \u001b[1m\u001b[32m0.03889\u001b[0m\u001b[0m | time: 0.201s\n",
      "| SGD | epoch: 061 | loss: 0.03889 - R2: 0.9992 -- iter: 1000/1168\n",
      "Training Step: 3591  | total loss: \u001b[1m\u001b[32m0.03699\u001b[0m\u001b[0m | time: 0.202s\n",
      "| SGD | epoch: 061 | loss: 0.03699 - R2: 0.9992 -- iter: 1020/1168\n",
      "Training Step: 3592  | total loss: \u001b[1m\u001b[32m0.03581\u001b[0m\u001b[0m | time: 0.211s\n",
      "| SGD | epoch: 061 | loss: 0.03581 - R2: 0.9991 -- iter: 1040/1168\n",
      "Training Step: 3593  | total loss: \u001b[1m\u001b[32m0.03131\u001b[0m\u001b[0m | time: 0.220s\n",
      "| SGD | epoch: 061 | loss: 0.03131 - R2: 0.9998 -- iter: 1060/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3594  | total loss: \u001b[1m\u001b[32m0.02960\u001b[0m\u001b[0m | time: 0.223s\n",
      "| SGD | epoch: 061 | loss: 0.02960 - R2: 0.9998 -- iter: 1080/1168\n",
      "Training Step: 3595  | total loss: \u001b[1m\u001b[32m0.02759\u001b[0m\u001b[0m | time: 0.227s\n",
      "| SGD | epoch: 061 | loss: 0.02759 - R2: 0.9991 -- iter: 1100/1168\n",
      "Training Step: 3596  | total loss: \u001b[1m\u001b[32m0.02759\u001b[0m\u001b[0m | time: 0.229s\n",
      "| SGD | epoch: 061 | loss: 0.02759 - R2: 0.9987 -- iter: 1120/1168\n",
      "Training Step: 3597  | total loss: \u001b[1m\u001b[32m0.02602\u001b[0m\u001b[0m | time: 0.232s\n",
      "| SGD | epoch: 061 | loss: 0.02602 - R2: 0.9998 -- iter: 1140/1168\n",
      "Training Step: 3598  | total loss: \u001b[1m\u001b[32m0.02487\u001b[0m\u001b[0m | time: 0.236s\n",
      "| SGD | epoch: 061 | loss: 0.02487 - R2: 0.9996 -- iter: 1160/1168\n",
      "Training Step: 3599  | total loss: \u001b[1m\u001b[32m0.02367\u001b[0m\u001b[0m | time: 1.243s\n",
      "| SGD | epoch: 061 | loss: 0.02367 - R2: 0.9989 | val_loss: 0.02328 - val_acc: 0.9985 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 3600  | total loss: \u001b[1m\u001b[32m0.02367\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 062 | loss: 0.02367 - R2: 0.9989 -- iter: 0020/1168\n",
      "Training Step: 3601  | total loss: \u001b[1m\u001b[32m0.02258\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 062 | loss: 0.02258 - R2: 0.9997 -- iter: 0040/1168\n",
      "Training Step: 3602  | total loss: \u001b[1m\u001b[32m0.02210\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 062 | loss: 0.02210 - R2: 0.9996 -- iter: 0060/1168\n",
      "Training Step: 3603  | total loss: \u001b[1m\u001b[32m0.02124\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 062 | loss: 0.02124 - R2: 0.9998 -- iter: 0080/1168\n",
      "Training Step: 3604  | total loss: \u001b[1m\u001b[32m0.02007\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 062 | loss: 0.02007 - R2: 0.9999 -- iter: 0100/1168\n",
      "Training Step: 3605  | total loss: \u001b[1m\u001b[32m0.01909\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 062 | loss: 0.01909 - R2: 0.9999 -- iter: 0120/1168\n",
      "Training Step: 3606  | total loss: \u001b[1m\u001b[32m0.01939\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 062 | loss: 0.01939 - R2: 1.0007 -- iter: 0140/1168\n",
      "Training Step: 3607  | total loss: \u001b[1m\u001b[32m0.01876\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 062 | loss: 0.01876 - R2: 0.9998 -- iter: 0160/1168\n",
      "Training Step: 3608  | total loss: \u001b[1m\u001b[32m0.01996\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 062 | loss: 0.01996 - R2: 0.9998 -- iter: 0180/1168\n",
      "Training Step: 3609  | total loss: \u001b[1m\u001b[32m0.01996\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 062 | loss: 0.01996 - R2: 0.9998 -- iter: 0200/1168\n",
      "Training Step: 3610  | total loss: \u001b[1m\u001b[32m0.02075\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 062 | loss: 0.02075 - R2: 1.0003 -- iter: 0220/1168\n",
      "Training Step: 3611  | total loss: \u001b[1m\u001b[32m0.02103\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 062 | loss: 0.02103 - R2: 1.0002 -- iter: 0240/1168\n",
      "Training Step: 3612  | total loss: \u001b[1m\u001b[32m0.01995\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 062 | loss: 0.01995 - R2: 1.0006 -- iter: 0260/1168\n",
      "Training Step: 3613  | total loss: \u001b[1m\u001b[32m0.01896\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 062 | loss: 0.01896 - R2: 0.9999 -- iter: 0280/1168\n",
      "Training Step: 3614  | total loss: \u001b[1m\u001b[32m0.01816\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 062 | loss: 0.01816 - R2: 0.9996 -- iter: 0300/1168\n",
      "Training Step: 3615  | total loss: \u001b[1m\u001b[32m0.01726\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 062 | loss: 0.01726 - R2: 0.9997 -- iter: 0320/1168\n",
      "Training Step: 3616  | total loss: \u001b[1m\u001b[32m0.01697\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 062 | loss: 0.01697 - R2: 0.9996 -- iter: 0340/1168\n",
      "Training Step: 3617  | total loss: \u001b[1m\u001b[32m0.01721\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 062 | loss: 0.01721 - R2: 1.0002 -- iter: 0360/1168\n",
      "Training Step: 3618  | total loss: \u001b[1m\u001b[32m0.01721\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 062 | loss: 0.01721 - R2: 1.0013 -- iter: 0380/1168\n",
      "Training Step: 3619  | total loss: \u001b[1m\u001b[32m0.02061\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 062 | loss: 0.02061 - R2: 1.0011 -- iter: 0400/1168\n",
      "Training Step: 3620  | total loss: \u001b[1m\u001b[32m0.02030\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 062 | loss: 0.02030 - R2: 1.0006 -- iter: 0420/1168\n",
      "Training Step: 3621  | total loss: \u001b[1m\u001b[32m0.02030\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 062 | loss: 0.02030 - R2: 1.0006 -- iter: 0440/1168\n",
      "Training Step: 3622  | total loss: \u001b[1m\u001b[32m0.01906\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 062 | loss: 0.01906 - R2: 0.9996 -- iter: 0460/1168\n",
      "Training Step: 3623  | total loss: \u001b[1m\u001b[32m0.01906\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 062 | loss: 0.01906 - R2: 0.9996 -- iter: 0480/1168\n",
      "Training Step: 3624  | total loss: \u001b[1m\u001b[32m0.01855\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 062 | loss: 0.01855 - R2: 0.9997 -- iter: 0500/1168\n",
      "Training Step: 3625  | total loss: \u001b[1m\u001b[32m0.01898\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 062 | loss: 0.01898 - R2: 0.9998 -- iter: 0520/1168\n",
      "Training Step: 3626  | total loss: \u001b[1m\u001b[32m0.01840\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 062 | loss: 0.01840 - R2: 0.9993 -- iter: 0540/1168\n",
      "Training Step: 3627  | total loss: \u001b[1m\u001b[32m0.01801\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 062 | loss: 0.01801 - R2: 0.9984 -- iter: 0560/1168\n",
      "Training Step: 3628  | total loss: \u001b[1m\u001b[32m0.01793\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 062 | loss: 0.01793 - R2: 0.9989 -- iter: 0580/1168\n",
      "Training Step: 3629  | total loss: \u001b[1m\u001b[32m0.01793\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 062 | loss: 0.01793 - R2: 0.9995 -- iter: 0600/1168\n",
      "Training Step: 3630  | total loss: \u001b[1m\u001b[32m0.02055\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 062 | loss: 0.02055 - R2: 0.9995 -- iter: 0620/1168\n",
      "Training Step: 3631  | total loss: \u001b[1m\u001b[32m0.01783\u001b[0m\u001b[0m | time: 0.171s\n",
      "| SGD | epoch: 062 | loss: 0.01783 - R2: 0.9992 -- iter: 0640/1168\n",
      "Training Step: 3632  | total loss: \u001b[1m\u001b[32m0.01783\u001b[0m\u001b[0m | time: 0.173s\n",
      "| SGD | epoch: 062 | loss: 0.01783 - R2: 0.9992 -- iter: 0660/1168\n",
      "Training Step: 3633  | total loss: \u001b[1m\u001b[32m0.01729\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 062 | loss: 0.01729 - R2: 0.9990 -- iter: 0680/1168\n",
      "Training Step: 3634  | total loss: \u001b[1m\u001b[32m0.01833\u001b[0m\u001b[0m | time: 0.178s\n",
      "| SGD | epoch: 062 | loss: 0.01833 - R2: 0.9993 -- iter: 0700/1168\n",
      "Training Step: 3635  | total loss: \u001b[1m\u001b[32m0.01777\u001b[0m\u001b[0m | time: 0.188s\n",
      "| SGD | epoch: 062 | loss: 0.01777 - R2: 0.9996 -- iter: 0720/1168\n",
      "Training Step: 3636  | total loss: \u001b[1m\u001b[32m0.01777\u001b[0m\u001b[0m | time: 0.194s\n",
      "| SGD | epoch: 062 | loss: 0.01777 - R2: 0.9996 -- iter: 0740/1168\n",
      "Training Step: 3637  | total loss: \u001b[1m\u001b[32m0.01934\u001b[0m\u001b[0m | time: 0.201s\n",
      "| SGD | epoch: 062 | loss: 0.01934 - R2: 1.0008 -- iter: 0760/1168\n",
      "Training Step: 3638  | total loss: \u001b[1m\u001b[32m0.05340\u001b[0m\u001b[0m | time: 0.203s\n",
      "| SGD | epoch: 062 | loss: 0.05340 - R2: 0.9986 -- iter: 0780/1168\n",
      "Training Step: 3639  | total loss: \u001b[1m\u001b[32m0.04743\u001b[0m\u001b[0m | time: 0.206s\n",
      "| SGD | epoch: 062 | loss: 0.04743 - R2: 0.9993 -- iter: 0800/1168\n",
      "Training Step: 3640  | total loss: \u001b[1m\u001b[32m0.04743\u001b[0m\u001b[0m | time: 0.209s\n",
      "| SGD | epoch: 062 | loss: 0.04743 - R2: 0.9993 -- iter: 0820/1168\n",
      "Training Step: 3641  | total loss: \u001b[1m\u001b[32m0.04413\u001b[0m\u001b[0m | time: 0.211s\n",
      "| SGD | epoch: 062 | loss: 0.04413 - R2: 0.9990 -- iter: 0840/1168\n",
      "Training Step: 3642  | total loss: \u001b[1m\u001b[32m0.04132\u001b[0m\u001b[0m | time: 0.214s\n",
      "| SGD | epoch: 062 | loss: 0.04132 - R2: 1.0000 -- iter: 0860/1168\n",
      "Training Step: 3643  | total loss: \u001b[1m\u001b[32m0.03831\u001b[0m\u001b[0m | time: 0.216s\n",
      "| SGD | epoch: 062 | loss: 0.03831 - R2: 1.0001 -- iter: 0880/1168\n",
      "Training Step: 3644  | total loss: \u001b[1m\u001b[32m0.03543\u001b[0m\u001b[0m | time: 0.220s\n",
      "| SGD | epoch: 062 | loss: 0.03543 - R2: 1.0000 -- iter: 0900/1168\n",
      "Training Step: 3645  | total loss: \u001b[1m\u001b[32m0.03592\u001b[0m\u001b[0m | time: 0.224s\n",
      "| SGD | epoch: 062 | loss: 0.03592 - R2: 1.0012 -- iter: 0920/1168\n",
      "Training Step: 3646  | total loss: \u001b[1m\u001b[32m0.03592\u001b[0m\u001b[0m | time: 0.226s\n",
      "| SGD | epoch: 062 | loss: 0.03592 - R2: 1.0012 -- iter: 0940/1168\n",
      "Training Step: 3647  | total loss: \u001b[1m\u001b[32m0.03340\u001b[0m\u001b[0m | time: 0.229s\n",
      "| SGD | epoch: 062 | loss: 0.03340 - R2: 1.0005 -- iter: 0960/1168\n",
      "Training Step: 3648  | total loss: \u001b[1m\u001b[32m0.03068\u001b[0m\u001b[0m | time: 0.239s\n",
      "| SGD | epoch: 062 | loss: 0.03068 - R2: 1.0003 -- iter: 0980/1168\n",
      "Training Step: 3649  | total loss: \u001b[1m\u001b[32m0.02961\u001b[0m\u001b[0m | time: 0.251s\n",
      "| SGD | epoch: 062 | loss: 0.02961 - R2: 1.0000 -- iter: 1000/1168\n",
      "Training Step: 3650  | total loss: \u001b[1m\u001b[32m0.02568\u001b[0m\u001b[0m | time: 0.256s\n",
      "| SGD | epoch: 062 | loss: 0.02568 - R2: 0.9997 -- iter: 1020/1168\n",
      "Training Step: 3651  | total loss: \u001b[1m\u001b[32m0.02445\u001b[0m\u001b[0m | time: 0.261s\n",
      "| SGD | epoch: 062 | loss: 0.02445 - R2: 1.0004 -- iter: 1040/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3652  | total loss: \u001b[1m\u001b[32m0.02445\u001b[0m\u001b[0m | time: 0.264s\n",
      "| SGD | epoch: 062 | loss: 0.02445 - R2: 1.0004 -- iter: 1060/1168\n",
      "Training Step: 3653  | total loss: \u001b[1m\u001b[32m0.02404\u001b[0m\u001b[0m | time: 0.266s\n",
      "| SGD | epoch: 062 | loss: 0.02404 - R2: 1.0003 -- iter: 1080/1168\n",
      "Training Step: 3654  | total loss: \u001b[1m\u001b[32m0.02348\u001b[0m\u001b[0m | time: 0.276s\n",
      "| SGD | epoch: 062 | loss: 0.02348 - R2: 1.0009 -- iter: 1100/1168\n",
      "Training Step: 3655  | total loss: \u001b[1m\u001b[32m0.02311\u001b[0m\u001b[0m | time: 0.279s\n",
      "| SGD | epoch: 062 | loss: 0.02311 - R2: 1.0008 -- iter: 1120/1168\n",
      "Training Step: 3656  | total loss: \u001b[1m\u001b[32m0.02166\u001b[0m\u001b[0m | time: 0.283s\n",
      "| SGD | epoch: 062 | loss: 0.02166 - R2: 0.9999 -- iter: 1140/1168\n",
      "Training Step: 3657  | total loss: \u001b[1m\u001b[32m0.02166\u001b[0m\u001b[0m | time: 0.287s\n",
      "| SGD | epoch: 062 | loss: 0.02166 - R2: 0.9998 -- iter: 1160/1168\n",
      "Training Step: 3658  | total loss: \u001b[1m\u001b[32m0.02054\u001b[0m\u001b[0m | time: 1.293s\n",
      "| SGD | epoch: 062 | loss: 0.02054 - R2: 1.0003 | val_loss: 0.02334 - val_acc: 0.9983 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 3659  | total loss: \u001b[1m\u001b[32m0.02054\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 063 | loss: 0.02054 - R2: 1.0002 -- iter: 0020/1168\n",
      "Training Step: 3660  | total loss: \u001b[1m\u001b[32m0.01955\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 063 | loss: 0.01955 - R2: 1.0002 -- iter: 0040/1168\n",
      "Training Step: 3661  | total loss: \u001b[1m\u001b[32m0.02390\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 063 | loss: 0.02390 - R2: 1.0015 -- iter: 0060/1168\n",
      "Training Step: 3662  | total loss: \u001b[1m\u001b[32m0.02210\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 063 | loss: 0.02210 - R2: 1.0011 -- iter: 0080/1168\n",
      "Training Step: 3663  | total loss: \u001b[1m\u001b[32m0.02153\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 063 | loss: 0.02153 - R2: 1.0008 -- iter: 0100/1168\n",
      "Training Step: 3664  | total loss: \u001b[1m\u001b[32m0.02163\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 063 | loss: 0.02163 - R2: 0.9998 -- iter: 0120/1168\n",
      "Training Step: 3665  | total loss: \u001b[1m\u001b[32m0.02163\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 063 | loss: 0.02163 - R2: 0.9998 -- iter: 0140/1168\n",
      "Training Step: 3666  | total loss: \u001b[1m\u001b[32m0.02034\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 063 | loss: 0.02034 - R2: 0.9993 -- iter: 0160/1168\n",
      "Training Step: 3667  | total loss: \u001b[1m\u001b[32m0.01878\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 063 | loss: 0.01878 - R2: 0.9996 -- iter: 0180/1168\n",
      "Training Step: 3668  | total loss: \u001b[1m\u001b[32m0.01878\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 063 | loss: 0.01878 - R2: 0.9996 -- iter: 0200/1168\n",
      "Training Step: 3669  | total loss: \u001b[1m\u001b[32m0.01765\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 063 | loss: 0.01765 - R2: 0.9997 -- iter: 0220/1168\n",
      "Training Step: 3670  | total loss: \u001b[1m\u001b[32m0.01833\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 063 | loss: 0.01833 - R2: 0.9993 -- iter: 0240/1168\n",
      "Training Step: 3671  | total loss: \u001b[1m\u001b[32m0.01985\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 063 | loss: 0.01985 - R2: 0.9995 -- iter: 0260/1168\n",
      "Training Step: 3672  | total loss: \u001b[1m\u001b[32m0.01923\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 063 | loss: 0.01923 - R2: 0.9991 -- iter: 0280/1168\n",
      "Training Step: 3673  | total loss: \u001b[1m\u001b[32m0.01923\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 063 | loss: 0.01923 - R2: 0.9991 -- iter: 0300/1168\n",
      "Training Step: 3674  | total loss: \u001b[1m\u001b[32m0.01786\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 063 | loss: 0.01786 - R2: 0.9990 -- iter: 0320/1168\n",
      "Training Step: 3675  | total loss: \u001b[1m\u001b[32m0.01786\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 063 | loss: 0.01786 - R2: 0.9990 -- iter: 0340/1168\n",
      "Training Step: 3676  | total loss: \u001b[1m\u001b[32m0.01837\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 063 | loss: 0.01837 - R2: 0.9994 -- iter: 0360/1168\n",
      "Training Step: 3677  | total loss: \u001b[1m\u001b[32m0.01778\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 063 | loss: 0.01778 - R2: 0.9990 -- iter: 0380/1168\n",
      "Training Step: 3678  | total loss: \u001b[1m\u001b[32m0.01668\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 063 | loss: 0.01668 - R2: 0.9995 -- iter: 0400/1168\n",
      "Training Step: 3679  | total loss: \u001b[1m\u001b[32m0.01996\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 063 | loss: 0.01996 - R2: 1.0003 -- iter: 0420/1168\n",
      "Training Step: 3680  | total loss: \u001b[1m\u001b[32m0.01996\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 063 | loss: 0.01996 - R2: 1.0003 -- iter: 0440/1168\n",
      "Training Step: 3681  | total loss: \u001b[1m\u001b[32m0.01994\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 063 | loss: 0.01994 - R2: 0.9996 -- iter: 0460/1168\n",
      "Training Step: 3682  | total loss: \u001b[1m\u001b[32m0.01902\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 063 | loss: 0.01902 - R2: 0.9994 -- iter: 0480/1168\n",
      "Training Step: 3683  | total loss: \u001b[1m\u001b[32m0.01896\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 063 | loss: 0.01896 - R2: 0.9994 -- iter: 0500/1168\n",
      "Training Step: 3684  | total loss: \u001b[1m\u001b[32m0.01845\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 063 | loss: 0.01845 - R2: 1.0002 -- iter: 0520/1168\n",
      "Training Step: 3685  | total loss: \u001b[1m\u001b[32m0.01752\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 063 | loss: 0.01752 - R2: 1.0003 -- iter: 0540/1168\n",
      "Training Step: 3686  | total loss: \u001b[1m\u001b[32m0.01752\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 063 | loss: 0.01752 - R2: 1.0003 -- iter: 0560/1168\n",
      "Training Step: 3687  | total loss: \u001b[1m\u001b[32m0.02783\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 063 | loss: 0.02783 - R2: 1.0013 -- iter: 0580/1168\n",
      "Training Step: 3688  | total loss: \u001b[1m\u001b[32m0.02807\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 063 | loss: 0.02807 - R2: 1.0007 -- iter: 0600/1168\n",
      "Training Step: 3689  | total loss: \u001b[1m\u001b[32m0.02677\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 063 | loss: 0.02677 - R2: 1.0003 -- iter: 0620/1168\n",
      "Training Step: 3690  | total loss: \u001b[1m\u001b[32m0.02554\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 063 | loss: 0.02554 - R2: 1.0003 -- iter: 0640/1168\n",
      "Training Step: 3691  | total loss: \u001b[1m\u001b[32m0.02554\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 063 | loss: 0.02554 - R2: 0.9992 -- iter: 0660/1168\n",
      "Training Step: 3692  | total loss: \u001b[1m\u001b[32m0.02383\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 063 | loss: 0.02383 - R2: 0.9991 -- iter: 0680/1168\n",
      "Training Step: 3693  | total loss: \u001b[1m\u001b[32m0.02345\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 063 | loss: 0.02345 - R2: 0.9991 -- iter: 0700/1168\n",
      "Training Step: 3694  | total loss: \u001b[1m\u001b[32m0.02299\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 063 | loss: 0.02299 - R2: 1.0000 -- iter: 0720/1168\n",
      "Training Step: 3695  | total loss: \u001b[1m\u001b[32m0.02197\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 063 | loss: 0.02197 - R2: 1.0000 -- iter: 0740/1168\n",
      "Training Step: 3696  | total loss: \u001b[1m\u001b[32m0.02117\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 063 | loss: 0.02117 - R2: 0.9993 -- iter: 0760/1168\n",
      "Training Step: 3697  | total loss: \u001b[1m\u001b[32m0.02117\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 063 | loss: 0.02117 - R2: 0.9993 -- iter: 0780/1168\n",
      "Training Step: 3698  | total loss: \u001b[1m\u001b[32m0.02000\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 063 | loss: 0.02000 - R2: 0.9981 -- iter: 0800/1168\n",
      "Training Step: 3699  | total loss: \u001b[1m\u001b[32m0.02165\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 063 | loss: 0.02165 - R2: 0.9990 -- iter: 0820/1168\n",
      "Training Step: 3700  | total loss: \u001b[1m\u001b[32m0.02165\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 063 | loss: 0.02165 - R2: 0.9993 -- iter: 0840/1168\n",
      "Training Step: 3701  | total loss: \u001b[1m\u001b[32m0.02096\u001b[0m\u001b[0m | time: 0.173s\n",
      "| SGD | epoch: 063 | loss: 0.02096 - R2: 1.0000 -- iter: 0860/1168\n",
      "Training Step: 3702  | total loss: \u001b[1m\u001b[32m0.02007\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 063 | loss: 0.02007 - R2: 1.0002 -- iter: 0880/1168\n",
      "Training Step: 3703  | total loss: \u001b[1m\u001b[32m0.02007\u001b[0m\u001b[0m | time: 0.181s\n",
      "| SGD | epoch: 063 | loss: 0.02007 - R2: 1.0002 -- iter: 0900/1168\n",
      "Training Step: 3704  | total loss: \u001b[1m\u001b[32m0.01949\u001b[0m\u001b[0m | time: 0.184s\n",
      "| SGD | epoch: 063 | loss: 0.01949 - R2: 1.0012 -- iter: 0920/1168\n",
      "Training Step: 3705  | total loss: \u001b[1m\u001b[32m0.01949\u001b[0m\u001b[0m | time: 0.186s\n",
      "| SGD | epoch: 063 | loss: 0.01949 - R2: 1.0012 -- iter: 0940/1168\n",
      "Training Step: 3706  | total loss: \u001b[1m\u001b[32m0.01856\u001b[0m\u001b[0m | time: 0.188s\n",
      "| SGD | epoch: 063 | loss: 0.01856 - R2: 1.0010 -- iter: 0960/1168\n",
      "Training Step: 3707  | total loss: \u001b[1m\u001b[32m0.02307\u001b[0m\u001b[0m | time: 0.191s\n",
      "| SGD | epoch: 063 | loss: 0.02307 - R2: 1.0010 -- iter: 0980/1168\n",
      "Training Step: 3708  | total loss: \u001b[1m\u001b[32m0.02312\u001b[0m\u001b[0m | time: 0.192s\n",
      "| SGD | epoch: 063 | loss: 0.02312 - R2: 1.0001 -- iter: 1000/1168\n",
      "Training Step: 3709  | total loss: \u001b[1m\u001b[32m0.02312\u001b[0m\u001b[0m | time: 0.194s\n",
      "| SGD | epoch: 063 | loss: 0.02312 - R2: 1.0013 -- iter: 1020/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3710  | total loss: \u001b[1m\u001b[32m0.02395\u001b[0m\u001b[0m | time: 0.196s\n",
      "| SGD | epoch: 063 | loss: 0.02395 - R2: 1.0013 -- iter: 1040/1168\n",
      "Training Step: 3711  | total loss: \u001b[1m\u001b[32m0.02245\u001b[0m\u001b[0m | time: 0.200s\n",
      "| SGD | epoch: 063 | loss: 0.02245 - R2: 1.0014 -- iter: 1060/1168\n",
      "Training Step: 3712  | total loss: \u001b[1m\u001b[32m0.02142\u001b[0m\u001b[0m | time: 0.202s\n",
      "| SGD | epoch: 063 | loss: 0.02142 - R2: 1.0014 -- iter: 1080/1168\n",
      "Training Step: 3713  | total loss: \u001b[1m\u001b[32m0.02015\u001b[0m\u001b[0m | time: 0.203s\n",
      "| SGD | epoch: 063 | loss: 0.02015 - R2: 1.0009 -- iter: 1100/1168\n",
      "Training Step: 3714  | total loss: \u001b[1m\u001b[32m0.02146\u001b[0m\u001b[0m | time: 0.207s\n",
      "| SGD | epoch: 063 | loss: 0.02146 - R2: 1.0009 -- iter: 1120/1168\n",
      "Training Step: 3715  | total loss: \u001b[1m\u001b[32m0.02263\u001b[0m\u001b[0m | time: 0.221s\n",
      "| SGD | epoch: 063 | loss: 0.02263 - R2: 1.0002 -- iter: 1140/1168\n",
      "Training Step: 3716  | total loss: \u001b[1m\u001b[32m0.02263\u001b[0m\u001b[0m | time: 0.223s\n",
      "| SGD | epoch: 063 | loss: 0.02263 - R2: 1.0002 -- iter: 1160/1168\n",
      "Training Step: 3717  | total loss: \u001b[1m\u001b[32m0.02144\u001b[0m\u001b[0m | time: 1.230s\n",
      "| SGD | epoch: 063 | loss: 0.02144 - R2: 0.9991 | val_loss: 0.02339 - val_acc: 0.9984 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 3718  | total loss: \u001b[1m\u001b[32m0.02162\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 064 | loss: 0.02162 - R2: 0.9996 -- iter: 0020/1168\n",
      "Training Step: 3719  | total loss: \u001b[1m\u001b[32m0.02162\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 064 | loss: 0.02162 - R2: 0.9996 -- iter: 0040/1168\n",
      "Training Step: 3720  | total loss: \u001b[1m\u001b[32m0.02023\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 064 | loss: 0.02023 - R2: 1.0000 -- iter: 0060/1168\n",
      "Training Step: 3721  | total loss: \u001b[1m\u001b[32m0.02283\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 064 | loss: 0.02283 - R2: 1.0009 -- iter: 0080/1168\n",
      "Training Step: 3722  | total loss: \u001b[1m\u001b[32m0.02182\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 064 | loss: 0.02182 - R2: 1.0007 -- iter: 0100/1168\n",
      "Training Step: 3723  | total loss: \u001b[1m\u001b[32m0.02205\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 064 | loss: 0.02205 - R2: 1.0006 -- iter: 0120/1168\n",
      "Training Step: 3724  | total loss: \u001b[1m\u001b[32m0.02112\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 064 | loss: 0.02112 - R2: 1.0002 -- iter: 0140/1168\n",
      "Training Step: 3725  | total loss: \u001b[1m\u001b[32m0.01985\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 064 | loss: 0.01985 - R2: 1.0002 -- iter: 0160/1168\n",
      "Training Step: 3726  | total loss: \u001b[1m\u001b[32m0.01883\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 064 | loss: 0.01883 - R2: 0.9991 -- iter: 0180/1168\n",
      "Training Step: 3727  | total loss: \u001b[1m\u001b[32m0.01883\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 064 | loss: 0.01883 - R2: 0.9991 -- iter: 0200/1168\n",
      "Training Step: 3728  | total loss: \u001b[1m\u001b[32m0.02927\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 064 | loss: 0.02927 - R2: 1.0007 -- iter: 0220/1168\n",
      "Training Step: 3729  | total loss: \u001b[1m\u001b[32m0.02793\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 064 | loss: 0.02793 - R2: 1.0011 -- iter: 0240/1168\n",
      "Training Step: 3730  | total loss: \u001b[1m\u001b[32m0.02625\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 064 | loss: 0.02625 - R2: 1.0014 -- iter: 0260/1168\n",
      "Training Step: 3731  | total loss: \u001b[1m\u001b[32m0.02299\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 064 | loss: 0.02299 - R2: 1.0012 -- iter: 0280/1168\n",
      "Training Step: 3732  | total loss: \u001b[1m\u001b[32m0.02299\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 064 | loss: 0.02299 - R2: 1.0012 -- iter: 0300/1168\n",
      "Training Step: 3733  | total loss: \u001b[1m\u001b[32m0.02160\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 064 | loss: 0.02160 - R2: 1.0012 -- iter: 0320/1168\n",
      "Training Step: 3734  | total loss: \u001b[1m\u001b[32m0.02101\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 064 | loss: 0.02101 - R2: 1.0000 -- iter: 0340/1168\n",
      "Training Step: 3735  | total loss: \u001b[1m\u001b[32m0.02101\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 064 | loss: 0.02101 - R2: 1.0000 -- iter: 0360/1168\n",
      "Training Step: 3736  | total loss: \u001b[1m\u001b[32m0.02154\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 064 | loss: 0.02154 - R2: 0.9996 -- iter: 0380/1168\n",
      "Training Step: 3737  | total loss: \u001b[1m\u001b[32m0.01978\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 064 | loss: 0.01978 - R2: 0.9993 -- iter: 0400/1168\n",
      "Training Step: 3738  | total loss: \u001b[1m\u001b[32m0.01876\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 064 | loss: 0.01876 - R2: 0.9990 -- iter: 0420/1168\n",
      "Training Step: 3739  | total loss: \u001b[1m\u001b[32m0.02041\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 064 | loss: 0.02041 - R2: 0.9996 -- iter: 0440/1168\n",
      "Training Step: 3740  | total loss: \u001b[1m\u001b[32m0.02039\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 064 | loss: 0.02039 - R2: 0.9988 -- iter: 0460/1168\n",
      "Training Step: 3741  | total loss: \u001b[1m\u001b[32m0.01980\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 064 | loss: 0.01980 - R2: 0.9995 -- iter: 0480/1168\n",
      "Training Step: 3742  | total loss: \u001b[1m\u001b[32m0.01980\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 064 | loss: 0.01980 - R2: 0.9995 -- iter: 0500/1168\n",
      "Training Step: 3743  | total loss: \u001b[1m\u001b[32m0.01845\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 064 | loss: 0.01845 - R2: 0.9992 -- iter: 0520/1168\n",
      "Training Step: 3744  | total loss: \u001b[1m\u001b[32m0.01845\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 064 | loss: 0.01845 - R2: 0.9992 -- iter: 0540/1168\n",
      "Training Step: 3745  | total loss: \u001b[1m\u001b[32m0.01759\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 064 | loss: 0.01759 - R2: 0.9990 -- iter: 0560/1168\n",
      "Training Step: 3746  | total loss: \u001b[1m\u001b[32m0.01672\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 064 | loss: 0.01672 - R2: 0.9991 -- iter: 0580/1168\n",
      "Training Step: 3747  | total loss: \u001b[1m\u001b[32m0.01522\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 064 | loss: 0.01522 - R2: 0.9993 -- iter: 0600/1168\n",
      "Training Step: 3748  | total loss: \u001b[1m\u001b[32m0.01522\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 064 | loss: 0.01522 - R2: 0.9993 -- iter: 0620/1168\n",
      "Training Step: 3749  | total loss: \u001b[1m\u001b[32m0.01568\u001b[0m\u001b[0m | time: 0.174s\n",
      "| SGD | epoch: 064 | loss: 0.01568 - R2: 0.9988 -- iter: 0640/1168\n",
      "Training Step: 3750  | total loss: \u001b[1m\u001b[32m0.01568\u001b[0m\u001b[0m | time: 0.179s\n",
      "| SGD | epoch: 064 | loss: 0.01568 - R2: 0.9988 -- iter: 0660/1168\n",
      "Training Step: 3751  | total loss: \u001b[1m\u001b[32m0.01744\u001b[0m\u001b[0m | time: 0.184s\n",
      "| SGD | epoch: 064 | loss: 0.01744 - R2: 0.9988 -- iter: 0680/1168\n",
      "Training Step: 3752  | total loss: \u001b[1m\u001b[32m0.01744\u001b[0m\u001b[0m | time: 0.191s\n",
      "| SGD | epoch: 064 | loss: 0.01744 - R2: 0.9988 -- iter: 0700/1168\n",
      "Training Step: 3753  | total loss: \u001b[1m\u001b[32m0.01644\u001b[0m\u001b[0m | time: 0.198s\n",
      "| SGD | epoch: 064 | loss: 0.01644 - R2: 0.9987 -- iter: 0720/1168\n",
      "Training Step: 3754  | total loss: \u001b[1m\u001b[32m0.01580\u001b[0m\u001b[0m | time: 0.204s\n",
      "| SGD | epoch: 064 | loss: 0.01580 - R2: 0.9986 -- iter: 0740/1168\n",
      "Training Step: 3755  | total loss: \u001b[1m\u001b[32m0.01580\u001b[0m\u001b[0m | time: 0.208s\n",
      "| SGD | epoch: 064 | loss: 0.01580 - R2: 0.9989 -- iter: 0760/1168\n",
      "Training Step: 3756  | total loss: \u001b[1m\u001b[32m0.01533\u001b[0m\u001b[0m | time: 0.210s\n",
      "| SGD | epoch: 064 | loss: 0.01533 - R2: 0.9989 -- iter: 0780/1168\n",
      "Training Step: 3757  | total loss: \u001b[1m\u001b[32m0.01490\u001b[0m\u001b[0m | time: 0.211s\n",
      "| SGD | epoch: 064 | loss: 0.01490 - R2: 0.9991 -- iter: 0800/1168\n",
      "Training Step: 3758  | total loss: \u001b[1m\u001b[32m0.01441\u001b[0m\u001b[0m | time: 0.213s\n",
      "| SGD | epoch: 064 | loss: 0.01441 - R2: 0.9991 -- iter: 0820/1168\n",
      "Training Step: 3759  | total loss: \u001b[1m\u001b[32m0.01423\u001b[0m\u001b[0m | time: 0.215s\n",
      "| SGD | epoch: 064 | loss: 0.01423 - R2: 0.9989 -- iter: 0840/1168\n",
      "Training Step: 3760  | total loss: \u001b[1m\u001b[32m0.01530\u001b[0m\u001b[0m | time: 0.217s\n",
      "| SGD | epoch: 064 | loss: 0.01530 - R2: 0.9996 -- iter: 0860/1168\n",
      "Training Step: 3761  | total loss: \u001b[1m\u001b[32m0.01480\u001b[0m\u001b[0m | time: 0.219s\n",
      "| SGD | epoch: 064 | loss: 0.01480 - R2: 1.0009 -- iter: 0880/1168\n",
      "Training Step: 3762  | total loss: \u001b[1m\u001b[32m0.01666\u001b[0m\u001b[0m | time: 0.221s\n",
      "| SGD | epoch: 064 | loss: 0.01666 - R2: 1.0009 -- iter: 0900/1168\n",
      "Training Step: 3763  | total loss: \u001b[1m\u001b[32m0.01708\u001b[0m\u001b[0m | time: 0.223s\n",
      "| SGD | epoch: 064 | loss: 0.01708 - R2: 1.0033 -- iter: 0920/1168\n",
      "Training Step: 3764  | total loss: \u001b[1m\u001b[32m0.03028\u001b[0m\u001b[0m | time: 0.226s\n",
      "| SGD | epoch: 064 | loss: 0.03028 - R2: 1.0033 -- iter: 0940/1168\n",
      "Training Step: 3765  | total loss: \u001b[1m\u001b[32m0.02768\u001b[0m\u001b[0m | time: 0.229s\n",
      "| SGD | epoch: 064 | loss: 0.02768 - R2: 1.0031 -- iter: 0960/1168\n",
      "Training Step: 3766  | total loss: \u001b[1m\u001b[32m0.02768\u001b[0m\u001b[0m | time: 0.231s\n",
      "| SGD | epoch: 064 | loss: 0.02768 - R2: 1.0030 -- iter: 0980/1168\n",
      "Training Step: 3767  | total loss: \u001b[1m\u001b[32m0.02568\u001b[0m\u001b[0m | time: 0.233s\n",
      "| SGD | epoch: 064 | loss: 0.02568 - R2: 1.0030 -- iter: 1000/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3768  | total loss: \u001b[1m\u001b[32m0.02643\u001b[0m\u001b[0m | time: 0.234s\n",
      "| SGD | epoch: 064 | loss: 0.02643 - R2: 1.0026 -- iter: 1020/1168\n",
      "Training Step: 3769  | total loss: \u001b[1m\u001b[32m0.02586\u001b[0m\u001b[0m | time: 0.245s\n",
      "| SGD | epoch: 064 | loss: 0.02586 - R2: 1.0021 -- iter: 1040/1168\n",
      "Training Step: 3770  | total loss: \u001b[1m\u001b[32m0.02468\u001b[0m\u001b[0m | time: 0.249s\n",
      "| SGD | epoch: 064 | loss: 0.02468 - R2: 1.0013 -- iter: 1060/1168\n",
      "Training Step: 3771  | total loss: \u001b[1m\u001b[32m0.02612\u001b[0m\u001b[0m | time: 0.253s\n",
      "| SGD | epoch: 064 | loss: 0.02612 - R2: 1.0014 -- iter: 1080/1168\n",
      "Training Step: 3772  | total loss: \u001b[1m\u001b[32m0.02612\u001b[0m\u001b[0m | time: 0.257s\n",
      "| SGD | epoch: 064 | loss: 0.02612 - R2: 1.0014 -- iter: 1100/1168\n",
      "Training Step: 3773  | total loss: \u001b[1m\u001b[32m0.02547\u001b[0m\u001b[0m | time: 0.261s\n",
      "| SGD | epoch: 064 | loss: 0.02547 - R2: 1.0014 -- iter: 1120/1168\n",
      "Training Step: 3774  | total loss: \u001b[1m\u001b[32m0.02449\u001b[0m\u001b[0m | time: 0.265s\n",
      "| SGD | epoch: 064 | loss: 0.02449 - R2: 1.0014 -- iter: 1140/1168\n",
      "Training Step: 3775  | total loss: \u001b[1m\u001b[32m0.02449\u001b[0m\u001b[0m | time: 0.267s\n",
      "| SGD | epoch: 064 | loss: 0.02449 - R2: 1.0014 -- iter: 1160/1168\n",
      "Training Step: 3776  | total loss: \u001b[1m\u001b[32m0.02367\u001b[0m\u001b[0m | time: 1.272s\n",
      "| SGD | epoch: 064 | loss: 0.02367 - R2: 1.0009 | val_loss: 0.02348 - val_acc: 0.9981 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 3777  | total loss: \u001b[1m\u001b[32m0.02254\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 065 | loss: 0.02254 - R2: 1.0006 -- iter: 0020/1168\n",
      "Training Step: 3778  | total loss: \u001b[1m\u001b[32m0.02132\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 065 | loss: 0.02132 - R2: 1.0009 -- iter: 0040/1168\n",
      "Training Step: 3779  | total loss: \u001b[1m\u001b[32m0.02150\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 065 | loss: 0.02150 - R2: 1.0005 -- iter: 0060/1168\n",
      "Training Step: 3780  | total loss: \u001b[1m\u001b[32m0.02189\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 065 | loss: 0.02189 - R2: 1.0005 -- iter: 0080/1168\n",
      "Training Step: 3781  | total loss: \u001b[1m\u001b[32m0.02178\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 065 | loss: 0.02178 - R2: 1.0008 -- iter: 0100/1168\n",
      "Training Step: 3782  | total loss: \u001b[1m\u001b[32m0.02157\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 065 | loss: 0.02157 - R2: 1.0001 -- iter: 0120/1168\n",
      "Training Step: 3783  | total loss: \u001b[1m\u001b[32m0.02063\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 065 | loss: 0.02063 - R2: 1.0000 -- iter: 0140/1168\n",
      "Training Step: 3784  | total loss: \u001b[1m\u001b[32m0.01942\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 065 | loss: 0.01942 - R2: 1.0000 -- iter: 0160/1168\n",
      "Training Step: 3785  | total loss: \u001b[1m\u001b[32m0.01863\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 065 | loss: 0.01863 - R2: 0.9998 -- iter: 0180/1168\n",
      "Training Step: 3786  | total loss: \u001b[1m\u001b[32m0.01962\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 065 | loss: 0.01962 - R2: 0.9998 -- iter: 0200/1168\n",
      "Training Step: 3787  | total loss: \u001b[1m\u001b[32m0.02045\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 065 | loss: 0.02045 - R2: 0.9992 -- iter: 0220/1168\n",
      "Training Step: 3788  | total loss: \u001b[1m\u001b[32m0.01990\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 065 | loss: 0.01990 - R2: 0.9991 -- iter: 0240/1168\n",
      "Training Step: 3789  | total loss: \u001b[1m\u001b[32m0.01965\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 065 | loss: 0.01965 - R2: 0.9990 -- iter: 0260/1168\n",
      "Training Step: 3790  | total loss: \u001b[1m\u001b[32m0.01867\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 065 | loss: 0.01867 - R2: 0.9997 -- iter: 0280/1168\n",
      "Training Step: 3791  | total loss: \u001b[1m\u001b[32m0.01840\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 065 | loss: 0.01840 - R2: 1.0003 -- iter: 0300/1168\n",
      "Training Step: 3792  | total loss: \u001b[1m\u001b[32m0.01840\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 065 | loss: 0.01840 - R2: 1.0007 -- iter: 0320/1168\n",
      "Training Step: 3793  | total loss: \u001b[1m\u001b[32m0.01793\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 065 | loss: 0.01793 - R2: 0.9997 -- iter: 0340/1168\n",
      "Training Step: 3794  | total loss: \u001b[1m\u001b[32m0.01668\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 065 | loss: 0.01668 - R2: 0.9996 -- iter: 0360/1168\n",
      "Training Step: 3795  | total loss: \u001b[1m\u001b[32m0.02033\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 065 | loss: 0.02033 - R2: 1.0012 -- iter: 0380/1168\n",
      "Training Step: 3796  | total loss: \u001b[1m\u001b[32m0.01973\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 065 | loss: 0.01973 - R2: 1.0007 -- iter: 0400/1168\n",
      "Training Step: 3797  | total loss: \u001b[1m\u001b[32m0.01912\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 065 | loss: 0.01912 - R2: 1.0002 -- iter: 0420/1168\n",
      "Training Step: 3798  | total loss: \u001b[1m\u001b[32m0.01796\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 065 | loss: 0.01796 - R2: 0.9995 -- iter: 0440/1168\n",
      "Training Step: 3799  | total loss: \u001b[1m\u001b[32m0.01770\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 065 | loss: 0.01770 - R2: 0.9994 -- iter: 0460/1168\n",
      "Training Step: 3800  | total loss: \u001b[1m\u001b[32m0.01770\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 065 | loss: 0.01770 - R2: 0.9994 -- iter: 0480/1168\n",
      "Training Step: 3801  | total loss: \u001b[1m\u001b[32m0.01724\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 065 | loss: 0.01724 - R2: 0.9989 -- iter: 0500/1168\n",
      "Training Step: 3802  | total loss: \u001b[1m\u001b[32m0.01777\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 065 | loss: 0.01777 - R2: 0.9989 -- iter: 0520/1168\n",
      "Training Step: 3803  | total loss: \u001b[1m\u001b[32m0.01709\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 065 | loss: 0.01709 - R2: 0.9991 -- iter: 0540/1168\n",
      "Training Step: 3804  | total loss: \u001b[1m\u001b[32m0.01617\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 065 | loss: 0.01617 - R2: 0.9990 -- iter: 0560/1168\n",
      "Training Step: 3805  | total loss: \u001b[1m\u001b[32m0.01559\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 065 | loss: 0.01559 - R2: 1.0000 -- iter: 0580/1168\n",
      "Training Step: 3806  | total loss: \u001b[1m\u001b[32m0.01559\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 065 | loss: 0.01559 - R2: 1.0000 -- iter: 0600/1168\n",
      "Training Step: 3807  | total loss: \u001b[1m\u001b[32m0.01426\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 065 | loss: 0.01426 - R2: 0.9995 -- iter: 0620/1168\n",
      "Training Step: 3808  | total loss: \u001b[1m\u001b[32m0.01426\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 065 | loss: 0.01426 - R2: 0.9995 -- iter: 0640/1168\n",
      "Training Step: 3809  | total loss: \u001b[1m\u001b[32m0.01338\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 065 | loss: 0.01338 - R2: 0.9997 -- iter: 0660/1168\n",
      "Training Step: 3810  | total loss: \u001b[1m\u001b[32m0.01335\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 065 | loss: 0.01335 - R2: 0.9990 -- iter: 0680/1168\n",
      "Training Step: 3811  | total loss: \u001b[1m\u001b[32m0.01314\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 065 | loss: 0.01314 - R2: 0.9985 -- iter: 0700/1168\n",
      "Training Step: 3812  | total loss: \u001b[1m\u001b[32m0.01255\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 065 | loss: 0.01255 - R2: 0.9985 -- iter: 0720/1168\n",
      "Training Step: 3813  | total loss: \u001b[1m\u001b[32m0.01198\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 065 | loss: 0.01198 - R2: 0.9984 -- iter: 0740/1168\n",
      "Training Step: 3814  | total loss: \u001b[1m\u001b[32m0.01279\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 065 | loss: 0.01279 - R2: 0.9978 -- iter: 0760/1168\n",
      "Training Step: 3815  | total loss: \u001b[1m\u001b[32m0.01262\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 065 | loss: 0.01262 - R2: 0.9979 -- iter: 0780/1168\n",
      "Training Step: 3816  | total loss: \u001b[1m\u001b[32m0.01261\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 065 | loss: 0.01261 - R2: 0.9979 -- iter: 0800/1168\n",
      "Training Step: 3817  | total loss: \u001b[1m\u001b[32m0.01261\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 065 | loss: 0.01261 - R2: 0.9985 -- iter: 0820/1168\n",
      "Training Step: 3818  | total loss: \u001b[1m\u001b[32m0.01470\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 065 | loss: 0.01470 - R2: 0.9977 -- iter: 0840/1168\n",
      "Training Step: 3819  | total loss: \u001b[1m\u001b[32m0.01470\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 065 | loss: 0.01470 - R2: 0.9973 -- iter: 0860/1168\n",
      "Training Step: 3820  | total loss: \u001b[1m\u001b[32m0.01531\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 065 | loss: 0.01531 - R2: 0.9973 -- iter: 0880/1168\n",
      "Training Step: 3821  | total loss: \u001b[1m\u001b[32m0.01520\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 065 | loss: 0.01520 - R2: 0.9980 -- iter: 0900/1168\n",
      "Training Step: 3822  | total loss: \u001b[1m\u001b[32m0.01487\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 065 | loss: 0.01487 - R2: 0.9980 -- iter: 0920/1168\n",
      "Training Step: 3823  | total loss: \u001b[1m\u001b[32m0.01486\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 065 | loss: 0.01486 - R2: 0.9977 -- iter: 0940/1168\n",
      "Training Step: 3824  | total loss: \u001b[1m\u001b[32m0.01595\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 065 | loss: 0.01595 - R2: 0.9977 -- iter: 0960/1168\n",
      "Training Step: 3825  | total loss: \u001b[1m\u001b[32m0.01526\u001b[0m\u001b[0m | time: 0.173s\n",
      "| SGD | epoch: 065 | loss: 0.01526 - R2: 0.9974 -- iter: 0980/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3826  | total loss: \u001b[1m\u001b[32m0.01632\u001b[0m\u001b[0m | time: 0.178s\n",
      "| SGD | epoch: 065 | loss: 0.01632 - R2: 0.9973 -- iter: 1000/1168\n",
      "Training Step: 3827  | total loss: \u001b[1m\u001b[32m0.01523\u001b[0m\u001b[0m | time: 0.183s\n",
      "| SGD | epoch: 065 | loss: 0.01523 - R2: 0.9971 -- iter: 1020/1168\n",
      "Training Step: 3828  | total loss: \u001b[1m\u001b[32m0.01523\u001b[0m\u001b[0m | time: 0.187s\n",
      "| SGD | epoch: 065 | loss: 0.01523 - R2: 0.9971 -- iter: 1040/1168\n",
      "Training Step: 3829  | total loss: \u001b[1m\u001b[32m0.01487\u001b[0m\u001b[0m | time: 0.191s\n",
      "| SGD | epoch: 065 | loss: 0.01487 - R2: 0.9970 -- iter: 1060/1168\n",
      "Training Step: 3830  | total loss: \u001b[1m\u001b[32m0.01539\u001b[0m\u001b[0m | time: 0.197s\n",
      "| SGD | epoch: 065 | loss: 0.01539 - R2: 0.9972 -- iter: 1080/1168\n",
      "Training Step: 3831  | total loss: \u001b[1m\u001b[32m0.01457\u001b[0m\u001b[0m | time: 0.200s\n",
      "| SGD | epoch: 065 | loss: 0.01457 - R2: 0.9974 -- iter: 1100/1168\n",
      "Training Step: 3832  | total loss: \u001b[1m\u001b[32m0.01457\u001b[0m\u001b[0m | time: 0.205s\n",
      "| SGD | epoch: 065 | loss: 0.01457 - R2: 0.9974 -- iter: 1120/1168\n",
      "Training Step: 3833  | total loss: \u001b[1m\u001b[32m0.01503\u001b[0m\u001b[0m | time: 0.208s\n",
      "| SGD | epoch: 065 | loss: 0.01503 - R2: 0.9976 -- iter: 1140/1168\n",
      "Training Step: 3834  | total loss: \u001b[1m\u001b[32m0.01452\u001b[0m\u001b[0m | time: 0.211s\n",
      "| SGD | epoch: 065 | loss: 0.01452 - R2: 0.9983 -- iter: 1160/1168\n",
      "Training Step: 3835  | total loss: \u001b[1m\u001b[32m0.01440\u001b[0m\u001b[0m | time: 1.217s\n",
      "| SGD | epoch: 065 | loss: 0.01440 - R2: 0.9989 | val_loss: 0.02343 - val_acc: 0.9987 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 3836  | total loss: \u001b[1m\u001b[32m0.01440\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 066 | loss: 0.01440 - R2: 0.9989 -- iter: 0020/1168\n",
      "Training Step: 3837  | total loss: \u001b[1m\u001b[32m0.01544\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 066 | loss: 0.01544 - R2: 0.9991 -- iter: 0040/1168\n",
      "Training Step: 3838  | total loss: \u001b[1m\u001b[32m0.01586\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 066 | loss: 0.01586 - R2: 0.9985 -- iter: 0060/1168\n",
      "Training Step: 3839  | total loss: \u001b[1m\u001b[32m0.01589\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 066 | loss: 0.01589 - R2: 0.9989 -- iter: 0080/1168\n",
      "Training Step: 3840  | total loss: \u001b[1m\u001b[32m0.01589\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 066 | loss: 0.01589 - R2: 0.9989 -- iter: 0100/1168\n",
      "Training Step: 3841  | total loss: \u001b[1m\u001b[32m0.01505\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 066 | loss: 0.01505 - R2: 0.9993 -- iter: 0120/1168\n",
      "Training Step: 3842  | total loss: \u001b[1m\u001b[32m0.01615\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 066 | loss: 0.01615 - R2: 0.9991 -- iter: 0140/1168\n",
      "Training Step: 3843  | total loss: \u001b[1m\u001b[32m0.01629\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 066 | loss: 0.01629 - R2: 0.9996 -- iter: 0160/1168\n",
      "Training Step: 3844  | total loss: \u001b[1m\u001b[32m0.01581\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 066 | loss: 0.01581 - R2: 1.0003 -- iter: 0180/1168\n",
      "Training Step: 3845  | total loss: \u001b[1m\u001b[32m0.01581\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 066 | loss: 0.01581 - R2: 1.0003 -- iter: 0200/1168\n",
      "Training Step: 3846  | total loss: \u001b[1m\u001b[32m0.01715\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 066 | loss: 0.01715 - R2: 1.0009 -- iter: 0220/1168\n",
      "Training Step: 3847  | total loss: \u001b[1m\u001b[32m0.02710\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 066 | loss: 0.02710 - R2: 1.0015 -- iter: 0240/1168\n",
      "Training Step: 3848  | total loss: \u001b[1m\u001b[32m0.02623\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 066 | loss: 0.02623 - R2: 1.0025 -- iter: 0260/1168\n",
      "Training Step: 3849  | total loss: \u001b[1m\u001b[32m0.02419\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 066 | loss: 0.02419 - R2: 1.0024 -- iter: 0280/1168\n",
      "Training Step: 3850  | total loss: \u001b[1m\u001b[32m0.02234\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 066 | loss: 0.02234 - R2: 1.0023 -- iter: 0300/1168\n",
      "Training Step: 3851  | total loss: \u001b[1m\u001b[32m0.02234\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 066 | loss: 0.02234 - R2: 1.0023 -- iter: 0320/1168\n",
      "Training Step: 3852  | total loss: \u001b[1m\u001b[32m0.02473\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 066 | loss: 0.02473 - R2: 1.0017 -- iter: 0340/1168\n",
      "Training Step: 3853  | total loss: \u001b[1m\u001b[32m0.02381\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 066 | loss: 0.02381 - R2: 1.0014 -- iter: 0360/1168\n",
      "Training Step: 3854  | total loss: \u001b[1m\u001b[32m0.02370\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 066 | loss: 0.02370 - R2: 1.0014 -- iter: 0380/1168\n",
      "Training Step: 3855  | total loss: \u001b[1m\u001b[32m0.02370\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 066 | loss: 0.02370 - R2: 1.0010 -- iter: 0400/1168\n",
      "Training Step: 3856  | total loss: \u001b[1m\u001b[32m0.02220\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 066 | loss: 0.02220 - R2: 1.0010 -- iter: 0420/1168\n",
      "Training Step: 3857  | total loss: \u001b[1m\u001b[32m0.02064\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 066 | loss: 0.02064 - R2: 1.0013 -- iter: 0440/1168\n",
      "Training Step: 3858  | total loss: \u001b[1m\u001b[32m0.02031\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 066 | loss: 0.02031 - R2: 1.0013 -- iter: 0460/1168\n",
      "Training Step: 3859  | total loss: \u001b[1m\u001b[32m0.01919\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 066 | loss: 0.01919 - R2: 1.0009 -- iter: 0480/1168\n",
      "Training Step: 3860  | total loss: \u001b[1m\u001b[32m0.01957\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 066 | loss: 0.01957 - R2: 0.9999 -- iter: 0500/1168\n",
      "Training Step: 3861  | total loss: \u001b[1m\u001b[32m0.02295\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 066 | loss: 0.02295 - R2: 1.0017 -- iter: 0520/1168\n",
      "Training Step: 3862  | total loss: \u001b[1m\u001b[32m0.02436\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 066 | loss: 0.02436 - R2: 1.0017 -- iter: 0540/1168\n",
      "Training Step: 3863  | total loss: \u001b[1m\u001b[32m0.02402\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 066 | loss: 0.02402 - R2: 1.0021 -- iter: 0560/1168\n",
      "Training Step: 3864  | total loss: \u001b[1m\u001b[32m0.02402\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 066 | loss: 0.02402 - R2: 1.0021 -- iter: 0580/1168\n",
      "Training Step: 3865  | total loss: \u001b[1m\u001b[32m0.02247\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 066 | loss: 0.02247 - R2: 1.0011 -- iter: 0600/1168\n",
      "Training Step: 3866  | total loss: \u001b[1m\u001b[32m0.02050\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 066 | loss: 0.02050 - R2: 1.0008 -- iter: 0620/1168\n",
      "Training Step: 3867  | total loss: \u001b[1m\u001b[32m0.01915\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 066 | loss: 0.01915 - R2: 1.0003 -- iter: 0640/1168\n",
      "Training Step: 3868  | total loss: \u001b[1m\u001b[32m0.01961\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 066 | loss: 0.01961 - R2: 0.9991 -- iter: 0660/1168\n",
      "Training Step: 3869  | total loss: \u001b[1m\u001b[32m0.01961\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 066 | loss: 0.01961 - R2: 0.9991 -- iter: 0680/1168\n",
      "Training Step: 3870  | total loss: \u001b[1m\u001b[32m0.01896\u001b[0m\u001b[0m | time: 0.165s\n",
      "| SGD | epoch: 066 | loss: 0.01896 - R2: 0.9995 -- iter: 0700/1168\n",
      "Training Step: 3871  | total loss: \u001b[1m\u001b[32m0.01917\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 066 | loss: 0.01917 - R2: 0.9992 -- iter: 0720/1168\n",
      "Training Step: 3872  | total loss: \u001b[1m\u001b[32m0.01792\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 066 | loss: 0.01792 - R2: 0.9993 -- iter: 0740/1168\n",
      "Training Step: 3873  | total loss: \u001b[1m\u001b[32m0.01679\u001b[0m\u001b[0m | time: 0.172s\n",
      "| SGD | epoch: 066 | loss: 0.01679 - R2: 0.9993 -- iter: 0760/1168\n",
      "Training Step: 3874  | total loss: \u001b[1m\u001b[32m0.01844\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 066 | loss: 0.01844 - R2: 0.9993 -- iter: 0780/1168\n",
      "Training Step: 3875  | total loss: \u001b[1m\u001b[32m0.01826\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 066 | loss: 0.01826 - R2: 1.0005 -- iter: 0800/1168\n",
      "Training Step: 3876  | total loss: \u001b[1m\u001b[32m0.01813\u001b[0m\u001b[0m | time: 0.179s\n",
      "| SGD | epoch: 066 | loss: 0.01813 - R2: 1.0005 -- iter: 0820/1168\n",
      "Training Step: 3877  | total loss: \u001b[1m\u001b[32m0.01819\u001b[0m\u001b[0m | time: 0.182s\n",
      "| SGD | epoch: 066 | loss: 0.01819 - R2: 1.0012 -- iter: 0840/1168\n",
      "Training Step: 3878  | total loss: \u001b[1m\u001b[32m0.01919\u001b[0m\u001b[0m | time: 0.186s\n",
      "| SGD | epoch: 066 | loss: 0.01919 - R2: 1.0016 -- iter: 0860/1168\n",
      "Training Step: 3879  | total loss: \u001b[1m\u001b[32m0.01832\u001b[0m\u001b[0m | time: 0.188s\n",
      "| SGD | epoch: 066 | loss: 0.01832 - R2: 1.0008 -- iter: 0880/1168\n",
      "Training Step: 3880  | total loss: \u001b[1m\u001b[32m0.01795\u001b[0m\u001b[0m | time: 0.192s\n",
      "| SGD | epoch: 066 | loss: 0.01795 - R2: 1.0005 -- iter: 0900/1168\n",
      "Training Step: 3881  | total loss: \u001b[1m\u001b[32m0.01716\u001b[0m\u001b[0m | time: 0.194s\n",
      "| SGD | epoch: 066 | loss: 0.01716 - R2: 1.0005 -- iter: 0920/1168\n",
      "Training Step: 3882  | total loss: \u001b[1m\u001b[32m0.01648\u001b[0m\u001b[0m | time: 0.196s\n",
      "| SGD | epoch: 066 | loss: 0.01648 - R2: 1.0007 -- iter: 0940/1168\n",
      "Training Step: 3883  | total loss: \u001b[1m\u001b[32m0.01515\u001b[0m\u001b[0m | time: 0.199s\n",
      "| SGD | epoch: 066 | loss: 0.01515 - R2: 1.0007 -- iter: 0960/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3884  | total loss: \u001b[1m\u001b[32m0.01462\u001b[0m\u001b[0m | time: 0.201s\n",
      "| SGD | epoch: 066 | loss: 0.01462 - R2: 1.0005 -- iter: 0980/1168\n",
      "Training Step: 3885  | total loss: \u001b[1m\u001b[32m0.01491\u001b[0m\u001b[0m | time: 0.205s\n",
      "| SGD | epoch: 066 | loss: 0.01491 - R2: 0.9998 -- iter: 1000/1168\n",
      "Training Step: 3886  | total loss: \u001b[1m\u001b[32m0.01477\u001b[0m\u001b[0m | time: 0.209s\n",
      "| SGD | epoch: 066 | loss: 0.01477 - R2: 0.9996 -- iter: 1020/1168\n",
      "Training Step: 3887  | total loss: \u001b[1m\u001b[32m0.01477\u001b[0m\u001b[0m | time: 0.214s\n",
      "| SGD | epoch: 066 | loss: 0.01477 - R2: 0.9996 -- iter: 1040/1168\n",
      "Training Step: 3888  | total loss: \u001b[1m\u001b[32m0.01585\u001b[0m\u001b[0m | time: 0.216s\n",
      "| SGD | epoch: 066 | loss: 0.01585 - R2: 0.9990 -- iter: 1060/1168\n",
      "Training Step: 3889  | total loss: \u001b[1m\u001b[32m0.01530\u001b[0m\u001b[0m | time: 0.220s\n",
      "| SGD | epoch: 066 | loss: 0.01530 - R2: 0.9987 -- iter: 1080/1168\n",
      "Training Step: 3890  | total loss: \u001b[1m\u001b[32m0.01572\u001b[0m\u001b[0m | time: 0.224s\n",
      "| SGD | epoch: 066 | loss: 0.01572 - R2: 0.9993 -- iter: 1100/1168\n",
      "Training Step: 3891  | total loss: \u001b[1m\u001b[32m0.01572\u001b[0m\u001b[0m | time: 0.226s\n",
      "| SGD | epoch: 066 | loss: 0.01572 - R2: 0.9993 -- iter: 1120/1168\n",
      "Training Step: 3892  | total loss: \u001b[1m\u001b[32m0.01591\u001b[0m\u001b[0m | time: 0.228s\n",
      "| SGD | epoch: 066 | loss: 0.01591 - R2: 0.9996 -- iter: 1140/1168\n",
      "Training Step: 3893  | total loss: \u001b[1m\u001b[32m0.01548\u001b[0m\u001b[0m | time: 0.229s\n",
      "| SGD | epoch: 066 | loss: 0.01548 - R2: 0.9999 -- iter: 1160/1168\n",
      "Training Step: 3894  | total loss: \u001b[1m\u001b[32m0.01720\u001b[0m\u001b[0m | time: 1.236s\n",
      "| SGD | epoch: 066 | loss: 0.01720 - R2: 0.9992 | val_loss: 0.02348 - val_acc: 0.9985 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 3895  | total loss: \u001b[1m\u001b[32m0.01740\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 067 | loss: 0.01740 - R2: 0.9987 -- iter: 0020/1168\n",
      "Training Step: 3896  | total loss: \u001b[1m\u001b[32m0.01576\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 067 | loss: 0.01576 - R2: 0.9991 -- iter: 0040/1168\n",
      "Training Step: 3897  | total loss: \u001b[1m\u001b[32m0.01576\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 067 | loss: 0.01576 - R2: 0.9991 -- iter: 0060/1168\n",
      "Training Step: 3898  | total loss: \u001b[1m\u001b[32m0.01653\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 067 | loss: 0.01653 - R2: 0.9991 -- iter: 0080/1168\n",
      "Training Step: 3899  | total loss: \u001b[1m\u001b[32m0.01633\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 067 | loss: 0.01633 - R2: 0.9996 -- iter: 0100/1168\n",
      "Training Step: 3900  | total loss: \u001b[1m\u001b[32m0.01672\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 067 | loss: 0.01672 - R2: 0.9996 -- iter: 0120/1168\n",
      "Training Step: 3901  | total loss: \u001b[1m\u001b[32m0.01601\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 067 | loss: 0.01601 - R2: 0.9995 -- iter: 0140/1168\n",
      "Training Step: 3902  | total loss: \u001b[1m\u001b[32m0.01543\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 067 | loss: 0.01543 - R2: 0.9991 -- iter: 0160/1168\n",
      "Training Step: 3903  | total loss: \u001b[1m\u001b[32m0.01543\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 067 | loss: 0.01543 - R2: 0.9992 -- iter: 0180/1168\n",
      "Training Step: 3904  | total loss: \u001b[1m\u001b[32m0.01506\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 067 | loss: 0.01506 - R2: 0.9992 -- iter: 0200/1168\n",
      "Training Step: 3905  | total loss: \u001b[1m\u001b[32m0.01419\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 067 | loss: 0.01419 - R2: 0.9991 -- iter: 0220/1168\n",
      "Training Step: 3906  | total loss: \u001b[1m\u001b[32m0.01419\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 067 | loss: 0.01419 - R2: 0.9991 -- iter: 0240/1168\n",
      "Training Step: 3907  | total loss: \u001b[1m\u001b[32m0.01348\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 067 | loss: 0.01348 - R2: 0.9989 -- iter: 0260/1168\n",
      "Training Step: 3908  | total loss: \u001b[1m\u001b[32m0.01388\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 067 | loss: 0.01388 - R2: 0.9998 -- iter: 0280/1168\n",
      "Training Step: 3909  | total loss: \u001b[1m\u001b[32m0.01446\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 067 | loss: 0.01446 - R2: 0.9998 -- iter: 0300/1168\n",
      "Training Step: 3910  | total loss: \u001b[1m\u001b[32m0.02444\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 067 | loss: 0.02444 - R2: 1.0007 -- iter: 0320/1168\n",
      "Training Step: 3911  | total loss: \u001b[1m\u001b[32m0.02444\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 067 | loss: 0.02444 - R2: 1.0015 -- iter: 0340/1168\n",
      "Training Step: 3912  | total loss: \u001b[1m\u001b[32m0.02455\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 067 | loss: 0.02455 - R2: 1.0008 -- iter: 0360/1168\n",
      "Training Step: 3913  | total loss: \u001b[1m\u001b[32m0.02408\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 067 | loss: 0.02408 - R2: 1.0002 -- iter: 0380/1168\n",
      "Training Step: 3914  | total loss: \u001b[1m\u001b[32m0.02304\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 067 | loss: 0.02304 - R2: 0.9997 -- iter: 0400/1168\n",
      "Training Step: 3915  | total loss: \u001b[1m\u001b[32m0.02201\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 067 | loss: 0.02201 - R2: 0.9997 -- iter: 0420/1168\n",
      "Training Step: 3916  | total loss: \u001b[1m\u001b[32m0.02076\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 067 | loss: 0.02076 - R2: 1.0008 -- iter: 0440/1168\n",
      "Training Step: 3917  | total loss: \u001b[1m\u001b[32m0.02154\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 067 | loss: 0.02154 - R2: 1.0008 -- iter: 0460/1168\n",
      "Training Step: 3918  | total loss: \u001b[1m\u001b[32m0.02154\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 067 | loss: 0.02154 - R2: 1.0008 -- iter: 0480/1168\n",
      "Training Step: 3919  | total loss: \u001b[1m\u001b[32m0.02172\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 067 | loss: 0.02172 - R2: 0.9999 -- iter: 0500/1168\n",
      "Training Step: 3920  | total loss: \u001b[1m\u001b[32m0.02114\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 067 | loss: 0.02114 - R2: 0.9991 -- iter: 0520/1168\n",
      "Training Step: 3921  | total loss: \u001b[1m\u001b[32m0.02284\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 067 | loss: 0.02284 - R2: 0.9994 -- iter: 0540/1168\n",
      "Training Step: 3922  | total loss: \u001b[1m\u001b[32m0.02219\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 067 | loss: 0.02219 - R2: 0.9991 -- iter: 0560/1168\n",
      "Training Step: 3923  | total loss: \u001b[1m\u001b[32m0.02168\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 067 | loss: 0.02168 - R2: 1.0001 -- iter: 0580/1168\n",
      "Training Step: 3924  | total loss: \u001b[1m\u001b[32m0.02047\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 067 | loss: 0.02047 - R2: 1.0002 -- iter: 0600/1168\n",
      "Training Step: 3925  | total loss: \u001b[1m\u001b[32m0.01973\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 067 | loss: 0.01973 - R2: 1.0002 -- iter: 0620/1168\n",
      "Training Step: 3926  | total loss: \u001b[1m\u001b[32m0.01963\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 067 | loss: 0.01963 - R2: 1.0002 -- iter: 0640/1168\n",
      "Training Step: 3927  | total loss: \u001b[1m\u001b[32m0.01865\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 067 | loss: 0.01865 - R2: 1.0000 -- iter: 0660/1168\n",
      "Training Step: 3928  | total loss: \u001b[1m\u001b[32m0.01819\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 067 | loss: 0.01819 - R2: 0.9997 -- iter: 0680/1168\n",
      "Training Step: 3929  | total loss: \u001b[1m\u001b[32m0.01819\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 067 | loss: 0.01819 - R2: 0.9997 -- iter: 0700/1168\n",
      "Training Step: 3930  | total loss: \u001b[1m\u001b[32m0.02038\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 067 | loss: 0.02038 - R2: 1.0006 -- iter: 0720/1168\n",
      "Training Step: 3931  | total loss: \u001b[1m\u001b[32m0.02098\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 067 | loss: 0.02098 - R2: 1.0007 -- iter: 0740/1168\n",
      "Training Step: 3932  | total loss: \u001b[1m\u001b[32m0.02007\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 067 | loss: 0.02007 - R2: 1.0013 -- iter: 0760/1168\n",
      "Training Step: 3933  | total loss: \u001b[1m\u001b[32m0.02100\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 067 | loss: 0.02100 - R2: 1.0016 -- iter: 0780/1168\n",
      "Training Step: 3934  | total loss: \u001b[1m\u001b[32m0.02434\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 067 | loss: 0.02434 - R2: 1.0025 -- iter: 0800/1168\n",
      "Training Step: 3935  | total loss: \u001b[1m\u001b[32m0.02334\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 067 | loss: 0.02334 - R2: 1.0019 -- iter: 0820/1168\n",
      "Training Step: 3936  | total loss: \u001b[1m\u001b[32m0.02217\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 067 | loss: 0.02217 - R2: 1.0014 -- iter: 0840/1168\n",
      "Training Step: 3937  | total loss: \u001b[1m\u001b[32m0.02300\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 067 | loss: 0.02300 - R2: 1.0017 -- iter: 0860/1168\n",
      "Training Step: 3938  | total loss: \u001b[1m\u001b[32m0.03221\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 067 | loss: 0.03221 - R2: 1.0029 -- iter: 0880/1168\n",
      "Training Step: 3939  | total loss: \u001b[1m\u001b[32m0.03116\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 067 | loss: 0.03116 - R2: 1.0029 -- iter: 0900/1168\n",
      "Training Step: 3940  | total loss: \u001b[1m\u001b[32m0.02979\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 067 | loss: 0.02979 - R2: 1.0029 -- iter: 0920/1168\n",
      "Training Step: 3941  | total loss: \u001b[1m\u001b[32m0.02890\u001b[0m\u001b[0m | time: 0.173s\n",
      "| SGD | epoch: 067 | loss: 0.02890 - R2: 1.0027 -- iter: 0940/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3942  | total loss: \u001b[1m\u001b[32m0.02890\u001b[0m\u001b[0m | time: 0.176s\n",
      "| SGD | epoch: 067 | loss: 0.02890 - R2: 1.0027 -- iter: 0960/1168\n",
      "Training Step: 3943  | total loss: \u001b[1m\u001b[32m0.02630\u001b[0m\u001b[0m | time: 0.179s\n",
      "| SGD | epoch: 067 | loss: 0.02630 - R2: 1.0027 -- iter: 0980/1168\n",
      "Training Step: 3944  | total loss: \u001b[1m\u001b[32m0.02630\u001b[0m\u001b[0m | time: 0.182s\n",
      "| SGD | epoch: 067 | loss: 0.02630 - R2: 1.0020 -- iter: 1000/1168\n",
      "Training Step: 3945  | total loss: \u001b[1m\u001b[32m0.02389\u001b[0m\u001b[0m | time: 0.184s\n",
      "| SGD | epoch: 067 | loss: 0.02389 - R2: 1.0021 -- iter: 1020/1168\n",
      "Training Step: 3946  | total loss: \u001b[1m\u001b[32m0.02385\u001b[0m\u001b[0m | time: 0.186s\n",
      "| SGD | epoch: 067 | loss: 0.02385 - R2: 1.0026 -- iter: 1040/1168\n",
      "Training Step: 3947  | total loss: \u001b[1m\u001b[32m0.03296\u001b[0m\u001b[0m | time: 0.188s\n",
      "| SGD | epoch: 067 | loss: 0.03296 - R2: 1.0032 -- iter: 1060/1168\n",
      "Training Step: 3948  | total loss: \u001b[1m\u001b[32m0.03216\u001b[0m\u001b[0m | time: 0.191s\n",
      "| SGD | epoch: 067 | loss: 0.03216 - R2: 1.0026 -- iter: 1080/1168\n",
      "Training Step: 3949  | total loss: \u001b[1m\u001b[32m0.03027\u001b[0m\u001b[0m | time: 0.194s\n",
      "| SGD | epoch: 067 | loss: 0.03027 - R2: 1.0020 -- iter: 1100/1168\n",
      "Training Step: 3950  | total loss: \u001b[1m\u001b[32m0.03027\u001b[0m\u001b[0m | time: 0.197s\n",
      "| SGD | epoch: 067 | loss: 0.03027 - R2: 1.0020 -- iter: 1120/1168\n",
      "Training Step: 3951  | total loss: \u001b[1m\u001b[32m0.02881\u001b[0m\u001b[0m | time: 0.200s\n",
      "| SGD | epoch: 067 | loss: 0.02881 - R2: 1.0025 -- iter: 1140/1168\n",
      "Training Step: 3952  | total loss: \u001b[1m\u001b[32m0.02761\u001b[0m\u001b[0m | time: 0.213s\n",
      "| SGD | epoch: 067 | loss: 0.02761 - R2: 1.0025 -- iter: 1160/1168\n",
      "Training Step: 3953  | total loss: \u001b[1m\u001b[32m0.02633\u001b[0m\u001b[0m | time: 1.218s\n",
      "| SGD | epoch: 067 | loss: 0.02633 - R2: 1.0023 | val_loss: 0.02359 - val_acc: 0.9980 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 3954  | total loss: \u001b[1m\u001b[32m0.02496\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 068 | loss: 0.02496 - R2: 1.0013 -- iter: 0020/1168\n",
      "Training Step: 3955  | total loss: \u001b[1m\u001b[32m0.02496\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 068 | loss: 0.02496 - R2: 1.0013 -- iter: 0040/1168\n",
      "Training Step: 3956  | total loss: \u001b[1m\u001b[32m0.02328\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 068 | loss: 0.02328 - R2: 1.0012 -- iter: 0060/1168\n",
      "Training Step: 3957  | total loss: \u001b[1m\u001b[32m0.02198\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 068 | loss: 0.02198 - R2: 1.0022 -- iter: 0080/1168\n",
      "Training Step: 3958  | total loss: \u001b[1m\u001b[32m0.02198\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 068 | loss: 0.02198 - R2: 1.0022 -- iter: 0100/1168\n",
      "Training Step: 3959  | total loss: \u001b[1m\u001b[32m0.02382\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 068 | loss: 0.02382 - R2: 1.0025 -- iter: 0120/1168\n",
      "Training Step: 3960  | total loss: \u001b[1m\u001b[32m0.02351\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 068 | loss: 0.02351 - R2: 1.0013 -- iter: 0140/1168\n",
      "Training Step: 3961  | total loss: \u001b[1m\u001b[32m0.02138\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 068 | loss: 0.02138 - R2: 1.0010 -- iter: 0160/1168\n",
      "Training Step: 3962  | total loss: \u001b[1m\u001b[32m0.01985\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 068 | loss: 0.01985 - R2: 1.0011 -- iter: 0180/1168\n",
      "Training Step: 3963  | total loss: \u001b[1m\u001b[32m0.01985\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 068 | loss: 0.01985 - R2: 1.0006 -- iter: 0200/1168\n",
      "Training Step: 3964  | total loss: \u001b[1m\u001b[32m0.01957\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 068 | loss: 0.01957 - R2: 1.0002 -- iter: 0220/1168\n",
      "Training Step: 3965  | total loss: \u001b[1m\u001b[32m0.02189\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 068 | loss: 0.02189 - R2: 1.0006 -- iter: 0240/1168\n",
      "Training Step: 3966  | total loss: \u001b[1m\u001b[32m0.02481\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 068 | loss: 0.02481 - R2: 1.0005 -- iter: 0260/1168\n",
      "Training Step: 3967  | total loss: \u001b[1m\u001b[32m0.02368\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 068 | loss: 0.02368 - R2: 1.0002 -- iter: 0280/1168\n",
      "Training Step: 3968  | total loss: \u001b[1m\u001b[32m0.02221\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 068 | loss: 0.02221 - R2: 1.0000 -- iter: 0300/1168\n",
      "Training Step: 3969  | total loss: \u001b[1m\u001b[32m0.01962\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 068 | loss: 0.01962 - R2: 1.0000 -- iter: 0320/1168\n",
      "Training Step: 3970  | total loss: \u001b[1m\u001b[32m0.01962\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 068 | loss: 0.01962 - R2: 1.0000 -- iter: 0340/1168\n",
      "Training Step: 3971  | total loss: \u001b[1m\u001b[32m0.01878\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 068 | loss: 0.01878 - R2: 1.0004 -- iter: 0360/1168\n",
      "Training Step: 3972  | total loss: \u001b[1m\u001b[32m0.01847\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 068 | loss: 0.01847 - R2: 0.9994 -- iter: 0380/1168\n",
      "Training Step: 3973  | total loss: \u001b[1m\u001b[32m0.01847\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 068 | loss: 0.01847 - R2: 0.9994 -- iter: 0400/1168\n",
      "Training Step: 3974  | total loss: \u001b[1m\u001b[32m0.01751\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 068 | loss: 0.01751 - R2: 0.9995 -- iter: 0420/1168\n",
      "Training Step: 3975  | total loss: \u001b[1m\u001b[32m0.01751\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 068 | loss: 0.01751 - R2: 0.9990 -- iter: 0440/1168\n",
      "Training Step: 3976  | total loss: \u001b[1m\u001b[32m0.01751\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 068 | loss: 0.01751 - R2: 0.9990 -- iter: 0460/1168\n",
      "Training Step: 3977  | total loss: \u001b[1m\u001b[32m0.01690\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 068 | loss: 0.01690 - R2: 0.9989 -- iter: 0480/1168\n",
      "Training Step: 3978  | total loss: \u001b[1m\u001b[32m0.01690\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 068 | loss: 0.01690 - R2: 0.9989 -- iter: 0500/1168\n",
      "Training Step: 3979  | total loss: \u001b[1m\u001b[32m0.01688\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 068 | loss: 0.01688 - R2: 0.9989 -- iter: 0520/1168\n",
      "Training Step: 3980  | total loss: \u001b[1m\u001b[32m0.01627\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 068 | loss: 0.01627 - R2: 0.9983 -- iter: 0540/1168\n",
      "Training Step: 3981  | total loss: \u001b[1m\u001b[32m0.01627\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 068 | loss: 0.01627 - R2: 0.9983 -- iter: 0560/1168\n",
      "Training Step: 3982  | total loss: \u001b[1m\u001b[32m0.01629\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 068 | loss: 0.01629 - R2: 0.9982 -- iter: 0580/1168\n",
      "Training Step: 3983  | total loss: \u001b[1m\u001b[32m0.01579\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 068 | loss: 0.01579 - R2: 0.9975 -- iter: 0600/1168\n",
      "Training Step: 3984  | total loss: \u001b[1m\u001b[32m0.01511\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 068 | loss: 0.01511 - R2: 0.9975 -- iter: 0620/1168\n",
      "Training Step: 3985  | total loss: \u001b[1m\u001b[32m0.01496\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 068 | loss: 0.01496 - R2: 0.9971 -- iter: 0640/1168\n",
      "Training Step: 3986  | total loss: \u001b[1m\u001b[32m0.01561\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 068 | loss: 0.01561 - R2: 0.9973 -- iter: 0660/1168\n",
      "Training Step: 3987  | total loss: \u001b[1m\u001b[32m0.01561\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 068 | loss: 0.01561 - R2: 0.9977 -- iter: 0680/1168\n",
      "Training Step: 3988  | total loss: \u001b[1m\u001b[32m0.01584\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 068 | loss: 0.01584 - R2: 0.9977 -- iter: 0700/1168\n",
      "Training Step: 3989  | total loss: \u001b[1m\u001b[32m0.01687\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 068 | loss: 0.01687 - R2: 0.9985 -- iter: 0720/1168\n",
      "Training Step: 3990  | total loss: \u001b[1m\u001b[32m0.01630\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 068 | loss: 0.01630 - R2: 0.9988 -- iter: 0740/1168\n",
      "Training Step: 3991  | total loss: \u001b[1m\u001b[32m0.01630\u001b[0m\u001b[0m | time: 0.169s\n",
      "| SGD | epoch: 068 | loss: 0.01630 - R2: 0.9988 -- iter: 0760/1168\n",
      "Training Step: 3992  | total loss: \u001b[1m\u001b[32m0.01655\u001b[0m\u001b[0m | time: 0.171s\n",
      "| SGD | epoch: 068 | loss: 0.01655 - R2: 0.9989 -- iter: 0780/1168\n",
      "Training Step: 3993  | total loss: \u001b[1m\u001b[32m0.01567\u001b[0m\u001b[0m | time: 0.173s\n",
      "| SGD | epoch: 068 | loss: 0.01567 - R2: 0.9993 -- iter: 0800/1168\n",
      "Training Step: 3994  | total loss: \u001b[1m\u001b[32m0.01602\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 068 | loss: 0.01602 - R2: 0.9995 -- iter: 0820/1168\n",
      "Training Step: 3995  | total loss: \u001b[1m\u001b[32m0.01804\u001b[0m\u001b[0m | time: 0.180s\n",
      "| SGD | epoch: 068 | loss: 0.01804 - R2: 0.9985 -- iter: 0840/1168\n",
      "Training Step: 3996  | total loss: \u001b[1m\u001b[32m0.01945\u001b[0m\u001b[0m | time: 0.198s\n",
      "| SGD | epoch: 068 | loss: 0.01945 - R2: 0.9989 -- iter: 0860/1168\n",
      "Training Step: 3997  | total loss: \u001b[1m\u001b[32m0.01945\u001b[0m\u001b[0m | time: 0.200s\n",
      "| SGD | epoch: 068 | loss: 0.01945 - R2: 0.9989 -- iter: 0880/1168\n",
      "Training Step: 3998  | total loss: \u001b[1m\u001b[32m0.01925\u001b[0m\u001b[0m | time: 0.204s\n",
      "| SGD | epoch: 068 | loss: 0.01925 - R2: 0.9984 -- iter: 0900/1168\n",
      "Training Step: 3999  | total loss: \u001b[1m\u001b[32m0.01925\u001b[0m\u001b[0m | time: 0.206s\n",
      "| SGD | epoch: 068 | loss: 0.01925 - R2: 0.9984 -- iter: 0920/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4000  | total loss: \u001b[1m\u001b[32m0.01889\u001b[0m\u001b[0m | time: 0.208s\n",
      "| SGD | epoch: 068 | loss: 0.01889 - R2: 0.9992 -- iter: 0940/1168\n",
      "Training Step: 4001  | total loss: \u001b[1m\u001b[32m0.01933\u001b[0m\u001b[0m | time: 0.211s\n",
      "| SGD | epoch: 068 | loss: 0.01933 - R2: 0.9994 -- iter: 0960/1168\n",
      "Training Step: 4002  | total loss: \u001b[1m\u001b[32m0.01849\u001b[0m\u001b[0m | time: 0.214s\n",
      "| SGD | epoch: 068 | loss: 0.01849 - R2: 0.9984 -- iter: 0980/1168\n",
      "Training Step: 4003  | total loss: \u001b[1m\u001b[32m0.01909\u001b[0m\u001b[0m | time: 0.218s\n",
      "| SGD | epoch: 068 | loss: 0.01909 - R2: 0.9979 -- iter: 1000/1168\n",
      "Training Step: 4004  | total loss: \u001b[1m\u001b[32m0.01849\u001b[0m\u001b[0m | time: 0.223s\n",
      "| SGD | epoch: 068 | loss: 0.01849 - R2: 0.9981 -- iter: 1020/1168\n",
      "Training Step: 4005  | total loss: \u001b[1m\u001b[32m0.01849\u001b[0m\u001b[0m | time: 0.229s\n",
      "| SGD | epoch: 068 | loss: 0.01849 - R2: 0.9983 -- iter: 1040/1168\n",
      "Training Step: 4006  | total loss: \u001b[1m\u001b[32m0.01798\u001b[0m\u001b[0m | time: 0.233s\n",
      "| SGD | epoch: 068 | loss: 0.01798 - R2: 0.9988 -- iter: 1060/1168\n",
      "Training Step: 4007  | total loss: \u001b[1m\u001b[32m0.01711\u001b[0m\u001b[0m | time: 0.237s\n",
      "| SGD | epoch: 068 | loss: 0.01711 - R2: 0.9990 -- iter: 1080/1168\n",
      "Training Step: 4008  | total loss: \u001b[1m\u001b[32m0.01700\u001b[0m\u001b[0m | time: 0.240s\n",
      "| SGD | epoch: 068 | loss: 0.01700 - R2: 0.9993 -- iter: 1100/1168\n",
      "Training Step: 4009  | total loss: \u001b[1m\u001b[32m0.01687\u001b[0m\u001b[0m | time: 0.242s\n",
      "| SGD | epoch: 068 | loss: 0.01687 - R2: 0.9992 -- iter: 1120/1168\n",
      "Training Step: 4010  | total loss: \u001b[1m\u001b[32m0.01668\u001b[0m\u001b[0m | time: 0.252s\n",
      "| SGD | epoch: 068 | loss: 0.01668 - R2: 0.9986 -- iter: 1140/1168\n",
      "Training Step: 4011  | total loss: \u001b[1m\u001b[32m0.01668\u001b[0m\u001b[0m | time: 0.260s\n",
      "| SGD | epoch: 068 | loss: 0.01668 - R2: 0.9986 -- iter: 1160/1168\n",
      "Training Step: 4012  | total loss: \u001b[1m\u001b[32m0.01610\u001b[0m\u001b[0m | time: 1.270s\n",
      "| SGD | epoch: 068 | loss: 0.01610 - R2: 0.9986 | val_loss: 0.02355 - val_acc: 0.9984 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 4013  | total loss: \u001b[1m\u001b[32m0.01610\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 069 | loss: 0.01610 - R2: 0.9995 -- iter: 0020/1168\n",
      "Training Step: 4014  | total loss: \u001b[1m\u001b[32m0.01681\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 069 | loss: 0.01681 - R2: 0.9995 -- iter: 0040/1168\n",
      "Training Step: 4015  | total loss: \u001b[1m\u001b[32m0.01987\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 069 | loss: 0.01987 - R2: 1.0008 -- iter: 0060/1168\n",
      "Training Step: 4016  | total loss: \u001b[1m\u001b[32m0.01839\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 069 | loss: 0.01839 - R2: 1.0003 -- iter: 0080/1168\n",
      "Training Step: 4017  | total loss: \u001b[1m\u001b[32m0.01839\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 069 | loss: 0.01839 - R2: 1.0003 -- iter: 0100/1168\n",
      "Training Step: 4018  | total loss: \u001b[1m\u001b[32m0.01888\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 069 | loss: 0.01888 - R2: 0.9996 -- iter: 0120/1168\n",
      "Training Step: 4019  | total loss: \u001b[1m\u001b[32m0.01888\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 069 | loss: 0.01888 - R2: 0.9996 -- iter: 0140/1168\n",
      "Training Step: 4020  | total loss: \u001b[1m\u001b[32m0.01805\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 069 | loss: 0.01805 - R2: 0.9992 -- iter: 0160/1168\n",
      "Training Step: 4021  | total loss: \u001b[1m\u001b[32m0.01702\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 069 | loss: 0.01702 - R2: 0.9993 -- iter: 0180/1168\n",
      "Training Step: 4022  | total loss: \u001b[1m\u001b[32m0.01702\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 069 | loss: 0.01702 - R2: 0.9990 -- iter: 0200/1168\n",
      "Training Step: 4023  | total loss: \u001b[1m\u001b[32m0.01691\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 069 | loss: 0.01691 - R2: 0.9997 -- iter: 0220/1168\n",
      "Training Step: 4024  | total loss: \u001b[1m\u001b[32m0.01602\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 069 | loss: 0.01602 - R2: 1.0001 -- iter: 0240/1168\n",
      "Training Step: 4025  | total loss: \u001b[1m\u001b[32m0.01602\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 069 | loss: 0.01602 - R2: 1.0001 -- iter: 0260/1168\n",
      "Training Step: 4026  | total loss: \u001b[1m\u001b[32m0.01518\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 069 | loss: 0.01518 - R2: 1.0004 -- iter: 0280/1168\n",
      "Training Step: 4027  | total loss: \u001b[1m\u001b[32m0.01525\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 069 | loss: 0.01525 - R2: 1.0005 -- iter: 0300/1168\n",
      "Training Step: 4028  | total loss: \u001b[1m\u001b[32m0.01525\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 069 | loss: 0.01525 - R2: 1.0005 -- iter: 0320/1168\n",
      "Training Step: 4029  | total loss: \u001b[1m\u001b[32m0.01597\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 069 | loss: 0.01597 - R2: 1.0001 -- iter: 0340/1168\n",
      "Training Step: 4030  | total loss: \u001b[1m\u001b[32m0.01601\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 069 | loss: 0.01601 - R2: 1.0005 -- iter: 0360/1168\n",
      "Training Step: 4031  | total loss: \u001b[1m\u001b[32m0.01602\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 069 | loss: 0.01602 - R2: 1.0009 -- iter: 0380/1168\n",
      "Training Step: 4032  | total loss: \u001b[1m\u001b[32m0.01528\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 069 | loss: 0.01528 - R2: 1.0005 -- iter: 0400/1168\n",
      "Training Step: 4033  | total loss: \u001b[1m\u001b[32m0.01496\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 069 | loss: 0.01496 - R2: 0.9999 -- iter: 0420/1168\n",
      "Training Step: 4034  | total loss: \u001b[1m\u001b[32m0.01433\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 069 | loss: 0.01433 - R2: 0.9999 -- iter: 0440/1168\n",
      "Training Step: 4035  | total loss: \u001b[1m\u001b[32m0.01385\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 069 | loss: 0.01385 - R2: 0.9998 -- iter: 0460/1168\n",
      "Training Step: 4036  | total loss: \u001b[1m\u001b[32m0.01385\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 069 | loss: 0.01385 - R2: 0.9998 -- iter: 0480/1168\n",
      "Training Step: 4037  | total loss: \u001b[1m\u001b[32m0.02434\u001b[0m\u001b[0m | time: 0.165s\n",
      "| SGD | epoch: 069 | loss: 0.02434 - R2: 1.0013 -- iter: 0500/1168\n",
      "Training Step: 4038  | total loss: \u001b[1m\u001b[32m0.02507\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 069 | loss: 0.02507 - R2: 1.0008 -- iter: 0520/1168\n",
      "Training Step: 4039  | total loss: \u001b[1m\u001b[32m0.02239\u001b[0m\u001b[0m | time: 0.169s\n",
      "| SGD | epoch: 069 | loss: 0.02239 - R2: 1.0001 -- iter: 0540/1168\n",
      "Training Step: 4040  | total loss: \u001b[1m\u001b[32m0.02239\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 069 | loss: 0.02239 - R2: 1.0001 -- iter: 0560/1168\n",
      "Training Step: 4041  | total loss: \u001b[1m\u001b[32m0.02261\u001b[0m\u001b[0m | time: 0.173s\n",
      "| SGD | epoch: 069 | loss: 0.02261 - R2: 0.9994 -- iter: 0580/1168\n",
      "Training Step: 4042  | total loss: \u001b[1m\u001b[32m0.02261\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 069 | loss: 0.02261 - R2: 0.9994 -- iter: 0600/1168\n",
      "Training Step: 4043  | total loss: \u001b[1m\u001b[32m0.02362\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 069 | loss: 0.02362 - R2: 1.0000 -- iter: 0620/1168\n",
      "Training Step: 4044  | total loss: \u001b[1m\u001b[32m0.02210\u001b[0m\u001b[0m | time: 0.180s\n",
      "| SGD | epoch: 069 | loss: 0.02210 - R2: 0.9997 -- iter: 0640/1168\n",
      "Training Step: 4045  | total loss: \u001b[1m\u001b[32m0.02196\u001b[0m\u001b[0m | time: 0.182s\n",
      "| SGD | epoch: 069 | loss: 0.02196 - R2: 0.9998 -- iter: 0660/1168\n",
      "Training Step: 4046  | total loss: \u001b[1m\u001b[32m0.02095\u001b[0m\u001b[0m | time: 0.184s\n",
      "| SGD | epoch: 069 | loss: 0.02095 - R2: 0.9998 -- iter: 0680/1168\n",
      "Training Step: 4047  | total loss: \u001b[1m\u001b[32m0.02102\u001b[0m\u001b[0m | time: 0.186s\n",
      "| SGD | epoch: 069 | loss: 0.02102 - R2: 0.9999 -- iter: 0700/1168\n",
      "Training Step: 4048  | total loss: \u001b[1m\u001b[32m0.02304\u001b[0m\u001b[0m | time: 0.199s\n",
      "| SGD | epoch: 069 | loss: 0.02304 - R2: 0.9999 -- iter: 0720/1168\n",
      "Training Step: 4049  | total loss: \u001b[1m\u001b[32m0.02656\u001b[0m\u001b[0m | time: 0.209s\n",
      "| SGD | epoch: 069 | loss: 0.02656 - R2: 1.0012 -- iter: 0740/1168\n",
      "Training Step: 4050  | total loss: \u001b[1m\u001b[32m0.02483\u001b[0m\u001b[0m | time: 0.212s\n",
      "| SGD | epoch: 069 | loss: 0.02483 - R2: 1.0015 -- iter: 0760/1168\n",
      "Training Step: 4051  | total loss: \u001b[1m\u001b[32m0.02400\u001b[0m\u001b[0m | time: 0.223s\n",
      "| SGD | epoch: 069 | loss: 0.02400 - R2: 1.0019 -- iter: 0780/1168\n",
      "Training Step: 4052  | total loss: \u001b[1m\u001b[32m0.02456\u001b[0m\u001b[0m | time: 0.226s\n",
      "| SGD | epoch: 069 | loss: 0.02456 - R2: 1.0019 -- iter: 0800/1168\n",
      "Training Step: 4053  | total loss: \u001b[1m\u001b[32m0.02334\u001b[0m\u001b[0m | time: 0.229s\n",
      "| SGD | epoch: 069 | loss: 0.02334 - R2: 1.0014 -- iter: 0820/1168\n",
      "Training Step: 4054  | total loss: \u001b[1m\u001b[32m0.02238\u001b[0m\u001b[0m | time: 0.231s\n",
      "| SGD | epoch: 069 | loss: 0.02238 - R2: 1.0003 -- iter: 0840/1168\n",
      "Training Step: 4055  | total loss: \u001b[1m\u001b[32m0.02174\u001b[0m\u001b[0m | time: 0.235s\n",
      "| SGD | epoch: 069 | loss: 0.02174 - R2: 1.0005 -- iter: 0860/1168\n",
      "Training Step: 4056  | total loss: \u001b[1m\u001b[32m0.02174\u001b[0m\u001b[0m | time: 0.240s\n",
      "| SGD | epoch: 069 | loss: 0.02174 - R2: 1.0005 -- iter: 0880/1168\n",
      "Training Step: 4057  | total loss: \u001b[1m\u001b[32m0.02011\u001b[0m\u001b[0m | time: 0.242s\n",
      "| SGD | epoch: 069 | loss: 0.02011 - R2: 1.0000 -- iter: 0900/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4058  | total loss: \u001b[1m\u001b[32m0.01875\u001b[0m\u001b[0m | time: 0.245s\n",
      "| SGD | epoch: 069 | loss: 0.01875 - R2: 1.0001 -- iter: 0920/1168\n",
      "Training Step: 4059  | total loss: \u001b[1m\u001b[32m0.01848\u001b[0m\u001b[0m | time: 0.248s\n",
      "| SGD | epoch: 069 | loss: 0.01848 - R2: 1.0001 -- iter: 0940/1168\n",
      "Training Step: 4060  | total loss: \u001b[1m\u001b[32m0.01803\u001b[0m\u001b[0m | time: 0.250s\n",
      "| SGD | epoch: 069 | loss: 0.01803 - R2: 0.9993 -- iter: 0960/1168\n",
      "Training Step: 4061  | total loss: \u001b[1m\u001b[32m0.01749\u001b[0m\u001b[0m | time: 0.252s\n",
      "| SGD | epoch: 069 | loss: 0.01749 - R2: 1.0000 -- iter: 0980/1168\n",
      "Training Step: 4062  | total loss: \u001b[1m\u001b[32m0.01674\u001b[0m\u001b[0m | time: 0.254s\n",
      "| SGD | epoch: 069 | loss: 0.01674 - R2: 1.0000 -- iter: 1000/1168\n",
      "Training Step: 4063  | total loss: \u001b[1m\u001b[32m0.01681\u001b[0m\u001b[0m | time: 0.267s\n",
      "| SGD | epoch: 069 | loss: 0.01681 - R2: 1.0000 -- iter: 1020/1168\n",
      "Training Step: 4064  | total loss: \u001b[1m\u001b[32m0.01681\u001b[0m\u001b[0m | time: 0.270s\n",
      "| SGD | epoch: 069 | loss: 0.01681 - R2: 1.0000 -- iter: 1040/1168\n",
      "Training Step: 4065  | total loss: \u001b[1m\u001b[32m0.01827\u001b[0m\u001b[0m | time: 0.273s\n",
      "| SGD | epoch: 069 | loss: 0.01827 - R2: 0.9999 -- iter: 1060/1168\n",
      "Training Step: 4066  | total loss: \u001b[1m\u001b[32m0.01803\u001b[0m\u001b[0m | time: 0.276s\n",
      "| SGD | epoch: 069 | loss: 0.01803 - R2: 0.9995 -- iter: 1080/1168\n",
      "Training Step: 4067  | total loss: \u001b[1m\u001b[32m0.01948\u001b[0m\u001b[0m | time: 0.278s\n",
      "| SGD | epoch: 069 | loss: 0.01948 - R2: 1.0003 -- iter: 1100/1168\n",
      "Training Step: 4068  | total loss: \u001b[1m\u001b[32m0.01840\u001b[0m\u001b[0m | time: 0.280s\n",
      "| SGD | epoch: 069 | loss: 0.01840 - R2: 1.0008 -- iter: 1120/1168\n",
      "Training Step: 4069  | total loss: \u001b[1m\u001b[32m0.01840\u001b[0m\u001b[0m | time: 0.282s\n",
      "| SGD | epoch: 069 | loss: 0.01840 - R2: 1.0008 -- iter: 1140/1168\n",
      "Training Step: 4070  | total loss: \u001b[1m\u001b[32m0.01771\u001b[0m\u001b[0m | time: 0.284s\n",
      "| SGD | epoch: 069 | loss: 0.01771 - R2: 1.0004 -- iter: 1160/1168\n",
      "Training Step: 4071  | total loss: \u001b[1m\u001b[32m0.01749\u001b[0m\u001b[0m | time: 1.293s\n",
      "| SGD | epoch: 069 | loss: 0.01749 - R2: 1.0004 | val_loss: 0.02360 - val_acc: 0.9981 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 4072  | total loss: \u001b[1m\u001b[32m0.01688\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 070 | loss: 0.01688 - R2: 1.0011 -- iter: 0020/1168\n",
      "Training Step: 4073  | total loss: \u001b[1m\u001b[32m0.01678\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 070 | loss: 0.01678 - R2: 1.0004 -- iter: 0040/1168\n",
      "Training Step: 4074  | total loss: \u001b[1m\u001b[32m0.01587\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 070 | loss: 0.01587 - R2: 0.9999 -- iter: 0060/1168\n",
      "Training Step: 4075  | total loss: \u001b[1m\u001b[32m0.01454\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 070 | loss: 0.01454 - R2: 0.9994 -- iter: 0080/1168\n",
      "Training Step: 4076  | total loss: \u001b[1m\u001b[32m0.01454\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 070 | loss: 0.01454 - R2: 0.9994 -- iter: 0100/1168\n",
      "Training Step: 4077  | total loss: \u001b[1m\u001b[32m0.01520\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 070 | loss: 0.01520 - R2: 0.9992 -- iter: 0120/1168\n",
      "Training Step: 4078  | total loss: \u001b[1m\u001b[32m0.01537\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 070 | loss: 0.01537 - R2: 0.9988 -- iter: 0140/1168\n",
      "Training Step: 4079  | total loss: \u001b[1m\u001b[32m0.01564\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 070 | loss: 0.01564 - R2: 0.9992 -- iter: 0160/1168\n",
      "Training Step: 4080  | total loss: \u001b[1m\u001b[32m0.01660\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 070 | loss: 0.01660 - R2: 0.9993 -- iter: 0180/1168\n",
      "Training Step: 4081  | total loss: \u001b[1m\u001b[32m0.01619\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 070 | loss: 0.01619 - R2: 0.9998 -- iter: 0200/1168\n",
      "Training Step: 4082  | total loss: \u001b[1m\u001b[32m0.01576\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 070 | loss: 0.01576 - R2: 0.9988 -- iter: 0220/1168\n",
      "Training Step: 4083  | total loss: \u001b[1m\u001b[32m0.01647\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 070 | loss: 0.01647 - R2: 0.9992 -- iter: 0240/1168\n",
      "Training Step: 4084  | total loss: \u001b[1m\u001b[32m0.01647\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 070 | loss: 0.01647 - R2: 0.9992 -- iter: 0260/1168\n",
      "Training Step: 4085  | total loss: \u001b[1m\u001b[32m0.01592\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 070 | loss: 0.01592 - R2: 0.9997 -- iter: 0280/1168\n",
      "Training Step: 4086  | total loss: \u001b[1m\u001b[32m0.01589\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 070 | loss: 0.01589 - R2: 0.9991 -- iter: 0300/1168\n",
      "Training Step: 4087  | total loss: \u001b[1m\u001b[32m0.01535\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 070 | loss: 0.01535 - R2: 0.9991 -- iter: 0320/1168\n",
      "Training Step: 4088  | total loss: \u001b[1m\u001b[32m0.01535\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 070 | loss: 0.01535 - R2: 0.9987 -- iter: 0340/1168\n",
      "Training Step: 4089  | total loss: \u001b[1m\u001b[32m0.01485\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 070 | loss: 0.01485 - R2: 0.9991 -- iter: 0360/1168\n",
      "Training Step: 4090  | total loss: \u001b[1m\u001b[32m0.01470\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 070 | loss: 0.01470 - R2: 0.9996 -- iter: 0380/1168\n",
      "Training Step: 4091  | total loss: \u001b[1m\u001b[32m0.01456\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 070 | loss: 0.01456 - R2: 1.0000 -- iter: 0400/1168\n",
      "Training Step: 4092  | total loss: \u001b[1m\u001b[32m0.01614\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 070 | loss: 0.01614 - R2: 1.0002 -- iter: 0420/1168\n",
      "Training Step: 4093  | total loss: \u001b[1m\u001b[32m0.01614\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 070 | loss: 0.01614 - R2: 1.0002 -- iter: 0440/1168\n",
      "Training Step: 4094  | total loss: \u001b[1m\u001b[32m0.01534\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 070 | loss: 0.01534 - R2: 1.0000 -- iter: 0460/1168\n",
      "Training Step: 4095  | total loss: \u001b[1m\u001b[32m0.01549\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 070 | loss: 0.01549 - R2: 1.0004 -- iter: 0480/1168\n",
      "Training Step: 4096  | total loss: \u001b[1m\u001b[32m0.01628\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 070 | loss: 0.01628 - R2: 0.9994 -- iter: 0500/1168\n",
      "Training Step: 4097  | total loss: \u001b[1m\u001b[32m0.01940\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 070 | loss: 0.01940 - R2: 1.0002 -- iter: 0520/1168\n",
      "Training Step: 4098  | total loss: \u001b[1m\u001b[32m0.01840\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 070 | loss: 0.01840 - R2: 0.9999 -- iter: 0540/1168\n",
      "Training Step: 4099  | total loss: \u001b[1m\u001b[32m0.01960\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 070 | loss: 0.01960 - R2: 1.0003 -- iter: 0560/1168\n",
      "Training Step: 4100  | total loss: \u001b[1m\u001b[32m0.01980\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 070 | loss: 0.01980 - R2: 0.9992 -- iter: 0580/1168\n",
      "Training Step: 4101  | total loss: \u001b[1m\u001b[32m0.01908\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 070 | loss: 0.01908 - R2: 0.9997 -- iter: 0600/1168\n",
      "Training Step: 4102  | total loss: \u001b[1m\u001b[32m0.01896\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 070 | loss: 0.01896 - R2: 0.9993 -- iter: 0620/1168\n",
      "Training Step: 4103  | total loss: \u001b[1m\u001b[32m0.01896\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 070 | loss: 0.01896 - R2: 0.9993 -- iter: 0640/1168\n",
      "Training Step: 4104  | total loss: \u001b[1m\u001b[32m0.01851\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 070 | loss: 0.01851 - R2: 0.9987 -- iter: 0660/1168\n",
      "Training Step: 4105  | total loss: \u001b[1m\u001b[32m0.02115\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 070 | loss: 0.02115 - R2: 0.9994 -- iter: 0680/1168\n",
      "Training Step: 4106  | total loss: \u001b[1m\u001b[32m0.02071\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 070 | loss: 0.02071 - R2: 0.9994 -- iter: 0700/1168\n",
      "Training Step: 4107  | total loss: \u001b[1m\u001b[32m0.02071\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 070 | loss: 0.02071 - R2: 0.9994 -- iter: 0720/1168\n",
      "Training Step: 4108  | total loss: \u001b[1m\u001b[32m0.02026\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 070 | loss: 0.02026 - R2: 0.9996 -- iter: 0740/1168\n",
      "Training Step: 4109  | total loss: \u001b[1m\u001b[32m0.02026\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 070 | loss: 0.02026 - R2: 0.9996 -- iter: 0760/1168\n",
      "Training Step: 4110  | total loss: \u001b[1m\u001b[32m0.02287\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 070 | loss: 0.02287 - R2: 0.9997 -- iter: 0780/1168\n",
      "Training Step: 4111  | total loss: \u001b[1m\u001b[32m0.03176\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 070 | loss: 0.03176 - R2: 1.0009 -- iter: 0800/1168\n",
      "Training Step: 4112  | total loss: \u001b[1m\u001b[32m0.02929\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 070 | loss: 0.02929 - R2: 1.0009 -- iter: 0820/1168\n",
      "Training Step: 4113  | total loss: \u001b[1m\u001b[32m0.02929\u001b[0m\u001b[0m | time: 0.172s\n",
      "| SGD | epoch: 070 | loss: 0.02929 - R2: 1.0009 -- iter: 0840/1168\n",
      "Training Step: 4114  | total loss: \u001b[1m\u001b[32m0.02588\u001b[0m\u001b[0m | time: 0.176s\n",
      "| SGD | epoch: 070 | loss: 0.02588 - R2: 1.0001 -- iter: 0860/1168\n",
      "Training Step: 4115  | total loss: \u001b[1m\u001b[32m0.02564\u001b[0m\u001b[0m | time: 0.183s\n",
      "| SGD | epoch: 070 | loss: 0.02564 - R2: 1.0002 -- iter: 0880/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4116  | total loss: \u001b[1m\u001b[32m0.02446\u001b[0m\u001b[0m | time: 0.187s\n",
      "| SGD | epoch: 070 | loss: 0.02446 - R2: 1.0007 -- iter: 0900/1168\n",
      "Training Step: 4117  | total loss: \u001b[1m\u001b[32m0.02354\u001b[0m\u001b[0m | time: 0.192s\n",
      "| SGD | epoch: 070 | loss: 0.02354 - R2: 1.0009 -- iter: 0920/1168\n",
      "Training Step: 4118  | total loss: \u001b[1m\u001b[32m0.03442\u001b[0m\u001b[0m | time: 0.195s\n",
      "| SGD | epoch: 070 | loss: 0.03442 - R2: 1.0021 -- iter: 0940/1168\n",
      "Training Step: 4119  | total loss: \u001b[1m\u001b[32m0.03224\u001b[0m\u001b[0m | time: 0.198s\n",
      "| SGD | epoch: 070 | loss: 0.03224 - R2: 1.0026 -- iter: 0960/1168\n",
      "Training Step: 4120  | total loss: \u001b[1m\u001b[32m0.03222\u001b[0m\u001b[0m | time: 0.201s\n",
      "| SGD | epoch: 070 | loss: 0.03222 - R2: 1.0029 -- iter: 0980/1168\n",
      "Training Step: 4121  | total loss: \u001b[1m\u001b[32m0.03042\u001b[0m\u001b[0m | time: 0.205s\n",
      "| SGD | epoch: 070 | loss: 0.03042 - R2: 1.0028 -- iter: 1000/1168\n",
      "Training Step: 4122  | total loss: \u001b[1m\u001b[32m0.03102\u001b[0m\u001b[0m | time: 0.208s\n",
      "| SGD | epoch: 070 | loss: 0.03102 - R2: 1.0018 -- iter: 1020/1168\n",
      "Training Step: 4123  | total loss: \u001b[1m\u001b[32m0.03102\u001b[0m\u001b[0m | time: 0.211s\n",
      "| SGD | epoch: 070 | loss: 0.03102 - R2: 1.0018 -- iter: 1040/1168\n",
      "Training Step: 4124  | total loss: \u001b[1m\u001b[32m0.03011\u001b[0m\u001b[0m | time: 0.229s\n",
      "| SGD | epoch: 070 | loss: 0.03011 - R2: 1.0022 -- iter: 1060/1168\n",
      "Training Step: 4125  | total loss: \u001b[1m\u001b[32m0.02901\u001b[0m\u001b[0m | time: 0.232s\n",
      "| SGD | epoch: 070 | loss: 0.02901 - R2: 1.0011 -- iter: 1080/1168\n",
      "Training Step: 4126  | total loss: \u001b[1m\u001b[32m0.02662\u001b[0m\u001b[0m | time: 0.236s\n",
      "| SGD | epoch: 070 | loss: 0.02662 - R2: 1.0011 -- iter: 1100/1168\n",
      "Training Step: 4127  | total loss: \u001b[1m\u001b[32m0.02640\u001b[0m\u001b[0m | time: 0.239s\n",
      "| SGD | epoch: 070 | loss: 0.02640 - R2: 1.0005 -- iter: 1120/1168\n",
      "Training Step: 4128  | total loss: \u001b[1m\u001b[32m0.02640\u001b[0m\u001b[0m | time: 0.241s\n",
      "| SGD | epoch: 070 | loss: 0.02640 - R2: 1.0005 -- iter: 1140/1168\n",
      "Training Step: 4129  | total loss: \u001b[1m\u001b[32m0.02465\u001b[0m\u001b[0m | time: 0.243s\n",
      "| SGD | epoch: 070 | loss: 0.02465 - R2: 1.0003 -- iter: 1160/1168\n",
      "Training Step: 4130  | total loss: \u001b[1m\u001b[32m0.02505\u001b[0m\u001b[0m | time: 1.249s\n",
      "| SGD | epoch: 070 | loss: 0.02505 - R2: 0.9993 | val_loss: 0.02363 - val_acc: 0.9982 -- iter: 1168/1168\n",
      "--\n",
      "---------------------------------\n",
      "Run id: M9IWO9\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "INFO:tensorflow:Summary name StandardError/ (raw) is illegal; using StandardError/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 1168\n",
      "Validation samples: 292\n",
      "--\n",
      "Training Step: 1  | total loss: \u001b[1m\u001b[32m130.13251\u001b[0m\u001b[0m | time: 0.023s\n",
      "| SGD | epoch: 001 | loss: 130.13251 - R2: 0.0000 -- iter: 0020/1168\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m130.13251\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 001 | loss: 130.13251 - R2: 0.0000 -- iter: 0040/1168\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m142.02150\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 001 | loss: 142.02150 - R2: 0.0000 -- iter: 0060/1168\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m142.17647\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 001 | loss: 142.17647 - R2: 0.0001 -- iter: 0080/1168\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m142.35930\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 001 | loss: 142.35930 - R2: 0.0002 -- iter: 0100/1168\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m141.72215\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 001 | loss: 141.72215 - R2: 0.0003 -- iter: 0120/1168\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m139.68718\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 001 | loss: 139.68718 - R2: 0.0005 -- iter: 0140/1168\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m137.30559\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 001 | loss: 137.30559 - R2: 0.0007 -- iter: 0160/1168\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m139.03284\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 001 | loss: 139.03284 - R2: 0.0009 -- iter: 0180/1168\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m139.03284\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 001 | loss: 139.03284 - R2: 0.0009 -- iter: 0200/1168\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m135.23819\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 001 | loss: 135.23819 - R2: 0.0014 -- iter: 0220/1168\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m135.23819\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 001 | loss: 135.23819 - R2: 0.0014 -- iter: 0240/1168\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m133.39101\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 001 | loss: 133.39101 - R2: 0.0017 -- iter: 0260/1168\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m133.18747\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 001 | loss: 133.18747 - R2: 0.0020 -- iter: 0280/1168\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m134.52061\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 001 | loss: 134.52061 - R2: 0.0023 -- iter: 0300/1168\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m131.98553\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 001 | loss: 131.98553 - R2: 0.0027 -- iter: 0320/1168\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m131.51245\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 001 | loss: 131.51245 - R2: 0.0031 -- iter: 0340/1168\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m129.17596\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 001 | loss: 129.17596 - R2: 0.0035 -- iter: 0360/1168\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m127.63905\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 001 | loss: 127.63905 - R2: 0.0040 -- iter: 0380/1168\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m125.61279\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 001 | loss: 125.61279 - R2: 0.0050 -- iter: 0400/1168\n",
      "Training Step: 21  | total loss: \u001b[1m\u001b[32m125.61279\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 001 | loss: 125.61279 - R2: 0.0050 -- iter: 0420/1168\n",
      "Training Step: 22  | total loss: \u001b[1m\u001b[32m123.03754\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 001 | loss: 123.03754 - R2: 0.0061 -- iter: 0440/1168\n",
      "Training Step: 23  | total loss: \u001b[1m\u001b[32m123.03754\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 001 | loss: 123.03754 - R2: 0.0067 -- iter: 0460/1168\n",
      "Training Step: 24  | total loss: \u001b[1m\u001b[32m121.94880\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 001 | loss: 121.94880 - R2: 0.0067 -- iter: 0480/1168\n",
      "Training Step: 25  | total loss: \u001b[1m\u001b[32m121.43265\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 001 | loss: 121.43265 - R2: 0.0073 -- iter: 0500/1168\n",
      "Training Step: 26  | total loss: \u001b[1m\u001b[32m119.65354\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 001 | loss: 119.65354 - R2: 0.0080 -- iter: 0520/1168\n",
      "Training Step: 27  | total loss: \u001b[1m\u001b[32m118.42830\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 001 | loss: 118.42830 - R2: 0.0087 -- iter: 0540/1168\n",
      "Training Step: 28  | total loss: \u001b[1m\u001b[32m116.79378\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 001 | loss: 116.79378 - R2: 0.0095 -- iter: 0560/1168\n",
      "Training Step: 29  | total loss: \u001b[1m\u001b[32m116.13635\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 001 | loss: 116.13635 - R2: 0.0102 -- iter: 0580/1168\n",
      "Training Step: 30  | total loss: \u001b[1m\u001b[32m114.97269\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 001 | loss: 114.97269 - R2: 0.0110 -- iter: 0600/1168\n",
      "Training Step: 31  | total loss: \u001b[1m\u001b[32m114.67764\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 001 | loss: 114.67764 - R2: 0.0117 -- iter: 0620/1168\n",
      "Training Step: 32  | total loss: \u001b[1m\u001b[32m114.03934\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 001 | loss: 114.03934 - R2: 0.0133 -- iter: 0640/1168\n",
      "Training Step: 33  | total loss: \u001b[1m\u001b[32m113.14322\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 001 | loss: 113.14322 - R2: 0.0141 -- iter: 0660/1168\n",
      "Training Step: 34  | total loss: \u001b[1m\u001b[32m113.04433\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 001 | loss: 113.04433 - R2: 0.0141 -- iter: 0680/1168\n",
      "Training Step: 35  | total loss: \u001b[1m\u001b[32m111.08552\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 001 | loss: 111.08552 - R2: 0.0159 -- iter: 0700/1168\n",
      "Training Step: 36  | total loss: \u001b[1m\u001b[32m110.23236\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 001 | loss: 110.23236 - R2: 0.0168 -- iter: 0720/1168\n",
      "Training Step: 37  | total loss: \u001b[1m\u001b[32m109.10197\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 001 | loss: 109.10197 - R2: 0.0179 -- iter: 0740/1168\n",
      "Training Step: 38  | total loss: \u001b[1m\u001b[32m109.10197\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 001 | loss: 109.10197 - R2: 0.0179 -- iter: 0760/1168\n",
      "Training Step: 39  | total loss: \u001b[1m\u001b[32m107.21892\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 001 | loss: 107.21892 - R2: 0.0199 -- iter: 0780/1168\n",
      "Training Step: 40  | total loss: \u001b[1m\u001b[32m107.21892\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 001 | loss: 107.21892 - R2: 0.0199 -- iter: 0800/1168\n",
      "Training Step: 41  | total loss: \u001b[1m\u001b[32m105.55713\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 001 | loss: 105.55713 - R2: 0.0220 -- iter: 0820/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 42  | total loss: \u001b[1m\u001b[32m104.49445\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 001 | loss: 104.49445 - R2: 0.0232 -- iter: 0840/1168\n",
      "Training Step: 43  | total loss: \u001b[1m\u001b[32m104.49445\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 001 | loss: 104.49445 - R2: 0.0232 -- iter: 0860/1168\n",
      "Training Step: 44  | total loss: \u001b[1m\u001b[32m103.89248\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 001 | loss: 103.89248 - R2: 0.0242 -- iter: 0880/1168\n",
      "Training Step: 45  | total loss: \u001b[1m\u001b[32m102.36757\u001b[0m\u001b[0m | time: 0.165s\n",
      "| SGD | epoch: 001 | loss: 102.36757 - R2: 0.0268 -- iter: 0900/1168\n",
      "Training Step: 46  | total loss: \u001b[1m\u001b[32m100.31351\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 001 | loss: 100.31351 - R2: 0.0281 -- iter: 0920/1168\n",
      "Training Step: 47  | total loss: \u001b[1m\u001b[32m99.74792\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 001 | loss: 99.74792 - R2: 0.0293 -- iter: 0940/1168\n",
      "Training Step: 48  | total loss: \u001b[1m\u001b[32m98.42319\u001b[0m\u001b[0m | time: 0.173s\n",
      "| SGD | epoch: 001 | loss: 98.42319 - R2: 0.0307 -- iter: 0960/1168\n",
      "Training Step: 49  | total loss: \u001b[1m\u001b[32m98.06234\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 001 | loss: 98.06234 - R2: 0.0319 -- iter: 0980/1168\n",
      "Training Step: 50  | total loss: \u001b[1m\u001b[32m96.84718\u001b[0m\u001b[0m | time: 0.181s\n",
      "| SGD | epoch: 001 | loss: 96.84718 - R2: 0.0334 -- iter: 1000/1168\n",
      "Training Step: 51  | total loss: \u001b[1m\u001b[32m96.84718\u001b[0m\u001b[0m | time: 0.183s\n",
      "| SGD | epoch: 001 | loss: 96.84718 - R2: 0.0348 -- iter: 1020/1168\n",
      "Training Step: 52  | total loss: \u001b[1m\u001b[32m95.07135\u001b[0m\u001b[0m | time: 0.186s\n",
      "| SGD | epoch: 001 | loss: 95.07135 - R2: 0.0362 -- iter: 1040/1168\n",
      "Training Step: 53  | total loss: \u001b[1m\u001b[32m95.07135\u001b[0m\u001b[0m | time: 0.189s\n",
      "| SGD | epoch: 001 | loss: 95.07135 - R2: 0.0362 -- iter: 1060/1168\n",
      "Training Step: 54  | total loss: \u001b[1m\u001b[32m93.53363\u001b[0m\u001b[0m | time: 0.205s\n",
      "| SGD | epoch: 001 | loss: 93.53363 - R2: 0.0391 -- iter: 1080/1168\n",
      "Training Step: 55  | total loss: \u001b[1m\u001b[32m92.43119\u001b[0m\u001b[0m | time: 0.213s\n",
      "| SGD | epoch: 001 | loss: 92.43119 - R2: 0.0407 -- iter: 1100/1168\n",
      "Training Step: 56  | total loss: \u001b[1m\u001b[32m92.43119\u001b[0m\u001b[0m | time: 0.217s\n",
      "| SGD | epoch: 001 | loss: 92.43119 - R2: 0.0407 -- iter: 1120/1168\n",
      "Training Step: 57  | total loss: \u001b[1m\u001b[32m91.68016\u001b[0m\u001b[0m | time: 0.220s\n",
      "| SGD | epoch: 001 | loss: 91.68016 - R2: 0.0423 -- iter: 1140/1168\n",
      "Training Step: 58  | total loss: \u001b[1m\u001b[32m90.70534\u001b[0m\u001b[0m | time: 0.223s\n",
      "| SGD | epoch: 001 | loss: 90.70534 - R2: 0.0439 -- iter: 1160/1168\n",
      "Training Step: 59  | total loss: \u001b[1m\u001b[32m90.03834\u001b[0m\u001b[0m | time: 1.234s\n",
      "| SGD | epoch: 001 | loss: 90.03834 - R2: 0.0455 | val_loss: 81.05597 - val_acc: 0.0614 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 60  | total loss: \u001b[1m\u001b[32m89.38168\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 002 | loss: 89.38168 - R2: 0.0471 -- iter: 0020/1168\n",
      "Training Step: 61  | total loss: \u001b[1m\u001b[32m88.68731\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 002 | loss: 88.68731 - R2: 0.0487 -- iter: 0040/1168\n",
      "Training Step: 62  | total loss: \u001b[1m\u001b[32m87.75997\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 002 | loss: 87.75997 - R2: 0.0505 -- iter: 0060/1168\n",
      "Training Step: 63  | total loss: \u001b[1m\u001b[32m86.87654\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 002 | loss: 86.87654 - R2: 0.0545 -- iter: 0080/1168\n",
      "Training Step: 64  | total loss: \u001b[1m\u001b[32m85.65211\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 002 | loss: 85.65211 - R2: 0.0545 -- iter: 0100/1168\n",
      "Training Step: 65  | total loss: \u001b[1m\u001b[32m84.76970\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 002 | loss: 84.76970 - R2: 0.0564 -- iter: 0120/1168\n",
      "Training Step: 66  | total loss: \u001b[1m\u001b[32m83.05472\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 002 | loss: 83.05472 - R2: 0.0604 -- iter: 0140/1168\n",
      "Training Step: 67  | total loss: \u001b[1m\u001b[32m82.00149\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 002 | loss: 82.00149 - R2: 0.0626 -- iter: 0160/1168\n",
      "Training Step: 68  | total loss: \u001b[1m\u001b[32m82.00149\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 002 | loss: 82.00149 - R2: 0.0626 -- iter: 0180/1168\n",
      "Training Step: 69  | total loss: \u001b[1m\u001b[32m81.28300\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 002 | loss: 81.28300 - R2: 0.0646 -- iter: 0200/1168\n",
      "Training Step: 70  | total loss: \u001b[1m\u001b[32m80.43197\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 002 | loss: 80.43197 - R2: 0.0689 -- iter: 0220/1168\n",
      "Training Step: 71  | total loss: \u001b[1m\u001b[32m79.67591\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 002 | loss: 79.67591 - R2: 0.0689 -- iter: 0240/1168\n",
      "Training Step: 72  | total loss: \u001b[1m\u001b[32m78.78748\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 002 | loss: 78.78748 - R2: 0.0712 -- iter: 0260/1168\n",
      "Training Step: 73  | total loss: \u001b[1m\u001b[32m76.98891\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 002 | loss: 76.98891 - R2: 0.0761 -- iter: 0280/1168\n",
      "Training Step: 74  | total loss: \u001b[1m\u001b[32m76.98891\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 002 | loss: 76.98891 - R2: 0.0761 -- iter: 0300/1168\n",
      "Training Step: 75  | total loss: \u001b[1m\u001b[32m75.84647\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 002 | loss: 75.84647 - R2: 0.0788 -- iter: 0320/1168\n",
      "Training Step: 76  | total loss: \u001b[1m\u001b[32m73.90752\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 002 | loss: 73.90752 - R2: 0.0842 -- iter: 0340/1168\n",
      "Training Step: 77  | total loss: \u001b[1m\u001b[32m73.90752\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 002 | loss: 73.90752 - R2: 0.0842 -- iter: 0360/1168\n",
      "Training Step: 78  | total loss: \u001b[1m\u001b[32m72.96130\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 002 | loss: 72.96130 - R2: 0.0869 -- iter: 0380/1168\n",
      "Training Step: 79  | total loss: \u001b[1m\u001b[32m70.81264\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 002 | loss: 70.81264 - R2: 0.0930 -- iter: 0400/1168\n",
      "Training Step: 80  | total loss: \u001b[1m\u001b[32m69.92483\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 002 | loss: 69.92483 - R2: 0.0959 -- iter: 0420/1168\n",
      "Training Step: 81  | total loss: \u001b[1m\u001b[32m69.92483\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 002 | loss: 69.92483 - R2: 0.0959 -- iter: 0440/1168\n",
      "Training Step: 82  | total loss: \u001b[1m\u001b[32m69.04134\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 002 | loss: 69.04134 - R2: 0.0990 -- iter: 0460/1168\n",
      "Training Step: 83  | total loss: \u001b[1m\u001b[32m67.99534\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 002 | loss: 67.99534 - R2: 0.1024 -- iter: 0480/1168\n",
      "Training Step: 84  | total loss: \u001b[1m\u001b[32m67.18546\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 002 | loss: 67.18546 - R2: 0.1055 -- iter: 0500/1168\n",
      "Training Step: 85  | total loss: \u001b[1m\u001b[32m66.22013\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 002 | loss: 66.22013 - R2: 0.1089 -- iter: 0520/1168\n",
      "Training Step: 86  | total loss: \u001b[1m\u001b[32m64.98280\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 002 | loss: 64.98280 - R2: 0.1129 -- iter: 0540/1168\n",
      "Training Step: 87  | total loss: \u001b[1m\u001b[32m63.77363\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 002 | loss: 63.77363 - R2: 0.1170 -- iter: 0560/1168\n",
      "Training Step: 88  | total loss: \u001b[1m\u001b[32m62.69567\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 002 | loss: 62.69567 - R2: 0.1210 -- iter: 0580/1168\n",
      "Training Step: 89  | total loss: \u001b[1m\u001b[32m61.57773\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 002 | loss: 61.57773 - R2: 0.1251 -- iter: 0600/1168\n",
      "Training Step: 90  | total loss: \u001b[1m\u001b[32m59.65199\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 002 | loss: 59.65199 - R2: 0.1333 -- iter: 0620/1168\n",
      "Training Step: 91  | total loss: \u001b[1m\u001b[32m58.34669\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 002 | loss: 58.34669 - R2: 0.1383 -- iter: 0640/1168\n",
      "Training Step: 92  | total loss: \u001b[1m\u001b[32m58.34669\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 002 | loss: 58.34669 - R2: 0.1383 -- iter: 0660/1168\n",
      "Training Step: 93  | total loss: \u001b[1m\u001b[32m55.95200\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 002 | loss: 55.95200 - R2: 0.1483 -- iter: 0680/1168\n",
      "Training Step: 94  | total loss: \u001b[1m\u001b[32m55.95200\u001b[0m\u001b[0m | time: 0.172s\n",
      "| SGD | epoch: 002 | loss: 55.95200 - R2: 0.1483 -- iter: 0700/1168\n",
      "Training Step: 95  | total loss: \u001b[1m\u001b[32m53.85414\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 002 | loss: 53.85414 - R2: 0.1582 -- iter: 0720/1168\n",
      "Training Step: 96  | total loss: \u001b[1m\u001b[32m53.85414\u001b[0m\u001b[0m | time: 0.194s\n",
      "| SGD | epoch: 002 | loss: 53.85414 - R2: 0.1582 -- iter: 0740/1168\n",
      "Training Step: 97  | total loss: \u001b[1m\u001b[32m52.53225\u001b[0m\u001b[0m | time: 0.198s\n",
      "| SGD | epoch: 002 | loss: 52.53225 - R2: 0.1641 -- iter: 0760/1168\n",
      "Training Step: 98  | total loss: \u001b[1m\u001b[32m51.36216\u001b[0m\u001b[0m | time: 0.201s\n",
      "| SGD | epoch: 002 | loss: 51.36216 - R2: 0.1698 -- iter: 0780/1168\n",
      "Training Step: 99  | total loss: \u001b[1m\u001b[32m50.16770\u001b[0m\u001b[0m | time: 0.205s\n",
      "| SGD | epoch: 002 | loss: 50.16770 - R2: 0.1757 -- iter: 0800/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 100  | total loss: \u001b[1m\u001b[32m47.93410\u001b[0m\u001b[0m | time: 0.208s\n",
      "| SGD | epoch: 002 | loss: 47.93410 - R2: 0.1820 -- iter: 0820/1168\n",
      "Training Step: 101  | total loss: \u001b[1m\u001b[32m46.59803\u001b[0m\u001b[0m | time: 0.211s\n",
      "| SGD | epoch: 002 | loss: 46.59803 - R2: 0.1947 -- iter: 0840/1168\n",
      "Training Step: 102  | total loss: \u001b[1m\u001b[32m46.59803\u001b[0m\u001b[0m | time: 0.213s\n",
      "| SGD | epoch: 002 | loss: 46.59803 - R2: 0.1947 -- iter: 0860/1168\n",
      "Training Step: 103  | total loss: \u001b[1m\u001b[32m45.53341\u001b[0m\u001b[0m | time: 0.214s\n",
      "| SGD | epoch: 002 | loss: 45.53341 - R2: 0.2079 -- iter: 0880/1168\n",
      "Training Step: 104  | total loss: \u001b[1m\u001b[32m44.33103\u001b[0m\u001b[0m | time: 0.216s\n",
      "| SGD | epoch: 002 | loss: 44.33103 - R2: 0.2157 -- iter: 0900/1168\n",
      "Training Step: 105  | total loss: \u001b[1m\u001b[32m42.97727\u001b[0m\u001b[0m | time: 0.219s\n",
      "| SGD | epoch: 002 | loss: 42.97727 - R2: 0.2157 -- iter: 0920/1168\n",
      "Training Step: 106  | total loss: \u001b[1m\u001b[32m41.66172\u001b[0m\u001b[0m | time: 0.220s\n",
      "| SGD | epoch: 002 | loss: 41.66172 - R2: 0.2235 -- iter: 0940/1168\n",
      "Training Step: 107  | total loss: \u001b[1m\u001b[32m40.52057\u001b[0m\u001b[0m | time: 0.222s\n",
      "| SGD | epoch: 002 | loss: 40.52057 - R2: 0.2386 -- iter: 0960/1168\n",
      "Training Step: 108  | total loss: \u001b[1m\u001b[32m39.39075\u001b[0m\u001b[0m | time: 0.225s\n",
      "| SGD | epoch: 002 | loss: 39.39075 - R2: 0.2386 -- iter: 0980/1168\n",
      "Training Step: 109  | total loss: \u001b[1m\u001b[32m37.02854\u001b[0m\u001b[0m | time: 0.227s\n",
      "| SGD | epoch: 002 | loss: 37.02854 - R2: 0.2550 -- iter: 1000/1168\n",
      "Training Step: 110  | total loss: \u001b[1m\u001b[32m37.02854\u001b[0m\u001b[0m | time: 0.230s\n",
      "| SGD | epoch: 002 | loss: 37.02854 - R2: 0.2550 -- iter: 1020/1168\n",
      "Training Step: 111  | total loss: \u001b[1m\u001b[32m34.76914\u001b[0m\u001b[0m | time: 0.244s\n",
      "| SGD | epoch: 002 | loss: 34.76914 - R2: 0.2718 -- iter: 1040/1168\n",
      "Training Step: 112  | total loss: \u001b[1m\u001b[32m34.76914\u001b[0m\u001b[0m | time: 0.246s\n",
      "| SGD | epoch: 002 | loss: 34.76914 - R2: 0.2718 -- iter: 1060/1168\n",
      "Training Step: 113  | total loss: \u001b[1m\u001b[32m32.29697\u001b[0m\u001b[0m | time: 0.249s\n",
      "| SGD | epoch: 002 | loss: 32.29697 - R2: 0.2913 -- iter: 1080/1168\n",
      "Training Step: 114  | total loss: \u001b[1m\u001b[32m32.29697\u001b[0m\u001b[0m | time: 0.257s\n",
      "| SGD | epoch: 002 | loss: 32.29697 - R2: 0.2913 -- iter: 1100/1168\n",
      "Training Step: 115  | total loss: \u001b[1m\u001b[32m29.94437\u001b[0m\u001b[0m | time: 0.261s\n",
      "| SGD | epoch: 002 | loss: 29.94437 - R2: 0.3113 -- iter: 1120/1168\n",
      "Training Step: 116  | total loss: \u001b[1m\u001b[32m29.94437\u001b[0m\u001b[0m | time: 0.264s\n",
      "| SGD | epoch: 002 | loss: 29.94437 - R2: 0.3113 -- iter: 1140/1168\n",
      "Training Step: 117  | total loss: \u001b[1m\u001b[32m28.85498\u001b[0m\u001b[0m | time: 0.269s\n",
      "| SGD | epoch: 002 | loss: 28.85498 - R2: 0.3213 -- iter: 1160/1168\n",
      "Training Step: 118  | total loss: \u001b[1m\u001b[32m27.65665\u001b[0m\u001b[0m | time: 1.275s\n",
      "| SGD | epoch: 002 | loss: 27.65665 - R2: 0.3326 | val_loss: 14.62929 - val_acc: 0.4644 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 119  | total loss: \u001b[1m\u001b[32m25.21968\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 003 | loss: 25.21968 - R2: 0.3569 -- iter: 0020/1168\n",
      "Training Step: 120  | total loss: \u001b[1m\u001b[32m23.96667\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 003 | loss: 23.96667 - R2: 0.3703 -- iter: 0040/1168\n",
      "Training Step: 121  | total loss: \u001b[1m\u001b[32m22.84892\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 003 | loss: 22.84892 - R2: 0.3829 -- iter: 0060/1168\n",
      "Training Step: 122  | total loss: \u001b[1m\u001b[32m22.84892\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 003 | loss: 22.84892 - R2: 0.3829 -- iter: 0080/1168\n",
      "Training Step: 123  | total loss: \u001b[1m\u001b[32m21.84719\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 003 | loss: 21.84719 - R2: 0.3944 -- iter: 0100/1168\n",
      "Training Step: 124  | total loss: \u001b[1m\u001b[32m19.81167\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 003 | loss: 19.81167 - R2: 0.4194 -- iter: 0120/1168\n",
      "Training Step: 125  | total loss: \u001b[1m\u001b[32m19.81167\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 003 | loss: 19.81167 - R2: 0.4194 -- iter: 0140/1168\n",
      "Training Step: 126  | total loss: \u001b[1m\u001b[32m18.75305\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 003 | loss: 18.75305 - R2: 0.4337 -- iter: 0160/1168\n",
      "Training Step: 127  | total loss: \u001b[1m\u001b[32m17.76012\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 003 | loss: 17.76012 - R2: 0.4623 -- iter: 0180/1168\n",
      "Training Step: 128  | total loss: \u001b[1m\u001b[32m16.76723\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 003 | loss: 16.76723 - R2: 0.4623 -- iter: 0200/1168\n",
      "Training Step: 129  | total loss: \u001b[1m\u001b[32m15.77076\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 003 | loss: 15.77076 - R2: 0.4775 -- iter: 0220/1168\n",
      "Training Step: 130  | total loss: \u001b[1m\u001b[32m14.77360\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 003 | loss: 14.77360 - R2: 0.4939 -- iter: 0240/1168\n",
      "Training Step: 131  | total loss: \u001b[1m\u001b[32m13.85778\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 003 | loss: 13.85778 - R2: 0.5093 -- iter: 0260/1168\n",
      "Training Step: 132  | total loss: \u001b[1m\u001b[32m13.00858\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 003 | loss: 13.00858 - R2: 0.5243 -- iter: 0280/1168\n",
      "Training Step: 133  | total loss: \u001b[1m\u001b[32m12.14227\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 003 | loss: 12.14227 - R2: 0.5575 -- iter: 0300/1168\n",
      "Training Step: 134  | total loss: \u001b[1m\u001b[32m10.50696\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 003 | loss: 10.50696 - R2: 0.5739 -- iter: 0320/1168\n",
      "Training Step: 135  | total loss: \u001b[1m\u001b[32m10.50696\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 003 | loss: 10.50696 - R2: 0.5739 -- iter: 0340/1168\n",
      "Training Step: 136  | total loss: \u001b[1m\u001b[32m9.69385\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 003 | loss: 9.69385 - R2: 0.5930 -- iter: 0360/1168\n",
      "Training Step: 137  | total loss: \u001b[1m\u001b[32m8.95581\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 003 | loss: 8.95581 - R2: 0.6108 -- iter: 0380/1168\n",
      "Training Step: 138  | total loss: \u001b[1m\u001b[32m7.51729\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 003 | loss: 7.51729 - R2: 0.6505 -- iter: 0400/1168\n",
      "Training Step: 139  | total loss: \u001b[1m\u001b[32m7.51729\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 003 | loss: 7.51729 - R2: 0.6505 -- iter: 0420/1168\n",
      "Training Step: 140  | total loss: \u001b[1m\u001b[32m6.86871\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 003 | loss: 6.86871 - R2: 0.6913 -- iter: 0440/1168\n",
      "Training Step: 141  | total loss: \u001b[1m\u001b[32m6.25016\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 003 | loss: 6.25016 - R2: 0.6913 -- iter: 0460/1168\n",
      "Training Step: 142  | total loss: \u001b[1m\u001b[32m5.12226\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 003 | loss: 5.12226 - R2: 0.7369 -- iter: 0480/1168\n",
      "Training Step: 143  | total loss: \u001b[1m\u001b[32m5.12226\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 003 | loss: 5.12226 - R2: 0.7369 -- iter: 0500/1168\n",
      "Training Step: 144  | total loss: \u001b[1m\u001b[32m4.62445\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 003 | loss: 4.62445 - R2: 0.7578 -- iter: 0520/1168\n",
      "Training Step: 145  | total loss: \u001b[1m\u001b[32m4.17692\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 003 | loss: 4.17692 - R2: 0.7797 -- iter: 0540/1168\n",
      "Training Step: 146  | total loss: \u001b[1m\u001b[32m3.39902\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 003 | loss: 3.39902 - R2: 0.8182 -- iter: 0560/1168\n",
      "Training Step: 147  | total loss: \u001b[1m\u001b[32m3.06683\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 003 | loss: 3.06683 - R2: 0.8346 -- iter: 0580/1168\n",
      "Training Step: 148  | total loss: \u001b[1m\u001b[32m2.76274\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 003 | loss: 2.76274 - R2: 0.8507 -- iter: 0600/1168\n",
      "Training Step: 149  | total loss: \u001b[1m\u001b[32m2.76274\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 003 | loss: 2.76274 - R2: 0.8507 -- iter: 0620/1168\n",
      "Training Step: 150  | total loss: \u001b[1m\u001b[32m2.25915\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 003 | loss: 2.25915 - R2: 0.8777 -- iter: 0640/1168\n",
      "Training Step: 151  | total loss: \u001b[1m\u001b[32m2.04230\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 003 | loss: 2.04230 - R2: 0.8892 -- iter: 0660/1168\n",
      "Training Step: 152  | total loss: \u001b[1m\u001b[32m2.04230\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 003 | loss: 2.04230 - R2: 0.9020 -- iter: 0680/1168\n",
      "Training Step: 153  | total loss: \u001b[1m\u001b[32m1.84998\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 003 | loss: 1.84998 - R2: 0.9020 -- iter: 0700/1168\n",
      "Training Step: 154  | total loss: \u001b[1m\u001b[32m1.67295\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 003 | loss: 1.67295 - R2: 0.9125 -- iter: 0720/1168\n",
      "Training Step: 155  | total loss: \u001b[1m\u001b[32m1.36695\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 003 | loss: 1.36695 - R2: 0.9299 -- iter: 0740/1168\n",
      "Training Step: 156  | total loss: \u001b[1m\u001b[32m1.23142\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 003 | loss: 1.23142 - R2: 0.9360 -- iter: 0760/1168\n",
      "Training Step: 157  | total loss: \u001b[1m\u001b[32m1.11208\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 003 | loss: 1.11208 - R2: 0.9433 -- iter: 0780/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 158  | total loss: \u001b[1m\u001b[32m1.00316\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 003 | loss: 1.00316 - R2: 0.9488 -- iter: 0800/1168\n",
      "Training Step: 159  | total loss: \u001b[1m\u001b[32m0.90455\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 003 | loss: 0.90455 - R2: 0.9541 -- iter: 0820/1168\n",
      "Training Step: 160  | total loss: \u001b[1m\u001b[32m0.82302\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 003 | loss: 0.82302 - R2: 0.9571 -- iter: 0840/1168\n",
      "Training Step: 161  | total loss: \u001b[1m\u001b[32m0.74466\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 003 | loss: 0.74466 - R2: 0.9606 -- iter: 0860/1168\n",
      "Training Step: 162  | total loss: \u001b[1m\u001b[32m0.67400\u001b[0m\u001b[0m | time: 0.165s\n",
      "| SGD | epoch: 003 | loss: 0.67400 - R2: 0.9652 -- iter: 0880/1168\n",
      "Training Step: 163  | total loss: \u001b[1m\u001b[32m0.61121\u001b[0m\u001b[0m | time: 0.169s\n",
      "| SGD | epoch: 003 | loss: 0.61121 - R2: 0.9676 -- iter: 0900/1168\n",
      "Training Step: 164  | total loss: \u001b[1m\u001b[32m0.55526\u001b[0m\u001b[0m | time: 0.172s\n",
      "| SGD | epoch: 003 | loss: 0.55526 - R2: 0.9722 -- iter: 0920/1168\n",
      "Training Step: 165  | total loss: \u001b[1m\u001b[32m0.50506\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 003 | loss: 0.50506 - R2: 0.9755 -- iter: 0940/1168\n",
      "Training Step: 166  | total loss: \u001b[1m\u001b[32m0.45566\u001b[0m\u001b[0m | time: 0.180s\n",
      "| SGD | epoch: 003 | loss: 0.45566 - R2: 0.9778 -- iter: 0960/1168\n",
      "Training Step: 167  | total loss: \u001b[1m\u001b[32m0.45566\u001b[0m\u001b[0m | time: 0.183s\n",
      "| SGD | epoch: 003 | loss: 0.45566 - R2: 0.9778 -- iter: 0980/1168\n",
      "Training Step: 168  | total loss: \u001b[1m\u001b[32m0.37342\u001b[0m\u001b[0m | time: 0.187s\n",
      "| SGD | epoch: 003 | loss: 0.37342 - R2: 0.9809 -- iter: 1000/1168\n",
      "Training Step: 169  | total loss: \u001b[1m\u001b[32m0.37342\u001b[0m\u001b[0m | time: 0.190s\n",
      "| SGD | epoch: 003 | loss: 0.37342 - R2: 0.9809 -- iter: 1020/1168\n",
      "Training Step: 170  | total loss: \u001b[1m\u001b[32m0.31279\u001b[0m\u001b[0m | time: 0.194s\n",
      "| SGD | epoch: 003 | loss: 0.31279 - R2: 0.9856 -- iter: 1040/1168\n",
      "Training Step: 171  | total loss: \u001b[1m\u001b[32m0.28543\u001b[0m\u001b[0m | time: 0.197s\n",
      "| SGD | epoch: 003 | loss: 0.28543 - R2: 0.9879 -- iter: 1060/1168\n",
      "Training Step: 172  | total loss: \u001b[1m\u001b[32m0.26138\u001b[0m\u001b[0m | time: 0.200s\n",
      "| SGD | epoch: 003 | loss: 0.26138 - R2: 0.9899 -- iter: 1080/1168\n",
      "Training Step: 173  | total loss: \u001b[1m\u001b[32m0.26138\u001b[0m\u001b[0m | time: 0.203s\n",
      "| SGD | epoch: 003 | loss: 0.26138 - R2: 0.9905 -- iter: 1100/1168\n",
      "Training Step: 174  | total loss: \u001b[1m\u001b[32m0.21533\u001b[0m\u001b[0m | time: 0.206s\n",
      "| SGD | epoch: 003 | loss: 0.21533 - R2: 0.9907 -- iter: 1120/1168\n",
      "Training Step: 175  | total loss: \u001b[1m\u001b[32m0.21533\u001b[0m\u001b[0m | time: 0.208s\n",
      "| SGD | epoch: 003 | loss: 0.21533 - R2: 0.9907 -- iter: 1140/1168\n",
      "Training Step: 176  | total loss: \u001b[1m\u001b[32m0.17924\u001b[0m\u001b[0m | time: 0.211s\n",
      "| SGD | epoch: 003 | loss: 0.17924 - R2: 0.9915 -- iter: 1160/1168\n",
      "Training Step: 177  | total loss: \u001b[1m\u001b[32m0.17924\u001b[0m\u001b[0m | time: 1.215s\n",
      "| SGD | epoch: 003 | loss: 0.17924 - R2: 0.9915 | val_loss: 0.04031 - val_acc: 0.9996 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 178  | total loss: \u001b[1m\u001b[32m0.16373\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 004 | loss: 0.16373 - R2: 0.9927 -- iter: 0020/1168\n",
      "Training Step: 179  | total loss: \u001b[1m\u001b[32m0.15874\u001b[0m\u001b[0m | time: 0.016s\n",
      "| SGD | epoch: 004 | loss: 0.15874 - R2: 0.9927 -- iter: 0040/1168\n",
      "Training Step: 180  | total loss: \u001b[1m\u001b[32m0.14633\u001b[0m\u001b[0m | time: 0.018s\n",
      "| SGD | epoch: 004 | loss: 0.14633 - R2: 0.9927 -- iter: 0060/1168\n",
      "Training Step: 181  | total loss: \u001b[1m\u001b[32m0.13273\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 004 | loss: 0.13273 - R2: 0.9928 -- iter: 0080/1168\n",
      "Training Step: 182  | total loss: \u001b[1m\u001b[32m0.12050\u001b[0m\u001b[0m | time: 0.023s\n",
      "| SGD | epoch: 004 | loss: 0.12050 - R2: 0.9943 -- iter: 0100/1168\n",
      "Training Step: 183  | total loss: \u001b[1m\u001b[32m0.11152\u001b[0m\u001b[0m | time: 0.026s\n",
      "| SGD | epoch: 004 | loss: 0.11152 - R2: 0.9946 -- iter: 0120/1168\n",
      "Training Step: 184  | total loss: \u001b[1m\u001b[32m0.11029\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 004 | loss: 0.11029 - R2: 0.9960 -- iter: 0140/1168\n",
      "Training Step: 185  | total loss: \u001b[1m\u001b[32m0.10146\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 004 | loss: 0.10146 - R2: 0.9959 -- iter: 0160/1168\n",
      "Training Step: 186  | total loss: \u001b[1m\u001b[32m0.09331\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 004 | loss: 0.09331 - R2: 0.9967 -- iter: 0180/1168\n",
      "Training Step: 187  | total loss: \u001b[1m\u001b[32m0.08526\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 004 | loss: 0.08526 - R2: 0.9967 -- iter: 0200/1168\n",
      "Training Step: 188  | total loss: \u001b[1m\u001b[32m0.07796\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 004 | loss: 0.07796 - R2: 0.9969 -- iter: 0220/1168\n",
      "Training Step: 189  | total loss: \u001b[1m\u001b[32m0.07478\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 004 | loss: 0.07478 - R2: 0.9974 -- iter: 0240/1168\n",
      "Training Step: 190  | total loss: \u001b[1m\u001b[32m0.06840\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 004 | loss: 0.06840 - R2: 0.9983 -- iter: 0260/1168\n",
      "Training Step: 191  | total loss: \u001b[1m\u001b[32m0.06278\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 004 | loss: 0.06278 - R2: 0.9982 -- iter: 0280/1168\n",
      "Training Step: 192  | total loss: \u001b[1m\u001b[32m0.05819\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 004 | loss: 0.05819 - R2: 0.9976 -- iter: 0300/1168\n",
      "Training Step: 193  | total loss: \u001b[1m\u001b[32m0.05455\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 004 | loss: 0.05455 - R2: 0.9979 -- iter: 0320/1168\n",
      "Training Step: 194  | total loss: \u001b[1m\u001b[32m0.04889\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 004 | loss: 0.04889 - R2: 0.9984 -- iter: 0340/1168\n",
      "Training Step: 195  | total loss: \u001b[1m\u001b[32m0.04625\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 004 | loss: 0.04625 - R2: 0.9987 -- iter: 0360/1168\n",
      "Training Step: 196  | total loss: \u001b[1m\u001b[32m0.04370\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 004 | loss: 0.04370 - R2: 0.9988 -- iter: 0380/1168\n",
      "Training Step: 197  | total loss: \u001b[1m\u001b[32m0.04112\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 004 | loss: 0.04112 - R2: 0.9981 -- iter: 0400/1168\n",
      "Training Step: 198  | total loss: \u001b[1m\u001b[32m0.04112\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 004 | loss: 0.04112 - R2: 0.9981 -- iter: 0420/1168\n",
      "Training Step: 199  | total loss: \u001b[1m\u001b[32m0.03855\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 004 | loss: 0.03855 - R2: 0.9982 -- iter: 0440/1168\n",
      "Training Step: 200  | total loss: \u001b[1m\u001b[32m0.03432\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 004 | loss: 0.03432 - R2: 0.9998 -- iter: 0460/1168\n",
      "Training Step: 201  | total loss: \u001b[1m\u001b[32m0.03230\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 004 | loss: 0.03230 - R2: 0.9997 -- iter: 0480/1168\n",
      "Training Step: 202  | total loss: \u001b[1m\u001b[32m0.03054\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 004 | loss: 0.03054 - R2: 1.0006 -- iter: 0500/1168\n",
      "Training Step: 203  | total loss: \u001b[1m\u001b[32m0.03054\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 004 | loss: 0.03054 - R2: 1.0006 -- iter: 0520/1168\n",
      "Training Step: 204  | total loss: \u001b[1m\u001b[32m0.02783\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 004 | loss: 0.02783 - R2: 1.0005 -- iter: 0540/1168\n",
      "Training Step: 205  | total loss: \u001b[1m\u001b[32m0.02479\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 004 | loss: 0.02479 - R2: 1.0002 -- iter: 0560/1168\n",
      "Training Step: 206  | total loss: \u001b[1m\u001b[32m0.02450\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 004 | loss: 0.02450 - R2: 0.9993 -- iter: 0580/1168\n",
      "Training Step: 207  | total loss: \u001b[1m\u001b[32m0.02450\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 004 | loss: 0.02450 - R2: 0.9993 -- iter: 0600/1168\n",
      "Training Step: 208  | total loss: \u001b[1m\u001b[32m0.02411\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 004 | loss: 0.02411 - R2: 0.9994 -- iter: 0620/1168\n",
      "Training Step: 209  | total loss: \u001b[1m\u001b[32m0.02567\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 004 | loss: 0.02567 - R2: 0.9999 -- iter: 0640/1168\n",
      "Training Step: 210  | total loss: \u001b[1m\u001b[32m0.02576\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 004 | loss: 0.02576 - R2: 0.9998 -- iter: 0660/1168\n",
      "Training Step: 211  | total loss: \u001b[1m\u001b[32m0.02356\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 004 | loss: 0.02356 - R2: 0.9994 -- iter: 0680/1168\n",
      "Training Step: 212  | total loss: \u001b[1m\u001b[32m0.02356\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 004 | loss: 0.02356 - R2: 0.9994 -- iter: 0700/1168\n",
      "Training Step: 213  | total loss: \u001b[1m\u001b[32m0.02228\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 004 | loss: 0.02228 - R2: 0.9993 -- iter: 0720/1168\n",
      "Training Step: 214  | total loss: \u001b[1m\u001b[32m0.02067\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 004 | loss: 0.02067 - R2: 0.9998 -- iter: 0740/1168\n",
      "Training Step: 215  | total loss: \u001b[1m\u001b[32m0.01944\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 004 | loss: 0.01944 - R2: 0.9993 -- iter: 0760/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 216  | total loss: \u001b[1m\u001b[32m0.01856\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 004 | loss: 0.01856 - R2: 0.9990 -- iter: 0780/1168\n",
      "Training Step: 217  | total loss: \u001b[1m\u001b[32m0.01856\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 004 | loss: 0.01856 - R2: 0.9990 -- iter: 0800/1168\n",
      "Training Step: 218  | total loss: \u001b[1m\u001b[32m0.02089\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 004 | loss: 0.02089 - R2: 0.9993 -- iter: 0820/1168\n",
      "Training Step: 219  | total loss: \u001b[1m\u001b[32m0.02096\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 004 | loss: 0.02096 - R2: 0.9997 -- iter: 0840/1168\n",
      "Training Step: 220  | total loss: \u001b[1m\u001b[32m0.02022\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 004 | loss: 0.02022 - R2: 0.9994 -- iter: 0860/1168\n",
      "Training Step: 221  | total loss: \u001b[1m\u001b[32m0.01922\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 004 | loss: 0.01922 - R2: 0.9994 -- iter: 0880/1168\n",
      "Training Step: 222  | total loss: \u001b[1m\u001b[32m0.01878\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 004 | loss: 0.01878 - R2: 1.0003 -- iter: 0900/1168\n",
      "Training Step: 223  | total loss: \u001b[1m\u001b[32m0.01878\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 004 | loss: 0.01878 - R2: 1.0003 -- iter: 0920/1168\n",
      "Training Step: 224  | total loss: \u001b[1m\u001b[32m0.01879\u001b[0m\u001b[0m | time: 0.165s\n",
      "| SGD | epoch: 004 | loss: 0.01879 - R2: 0.9997 -- iter: 0940/1168\n",
      "Training Step: 225  | total loss: \u001b[1m\u001b[32m0.02410\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 004 | loss: 0.02410 - R2: 1.0014 -- iter: 0960/1168\n",
      "Training Step: 226  | total loss: \u001b[1m\u001b[32m0.02254\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 004 | loss: 0.02254 - R2: 1.0014 -- iter: 0980/1168\n",
      "Training Step: 227  | total loss: \u001b[1m\u001b[32m0.02254\u001b[0m\u001b[0m | time: 0.172s\n",
      "| SGD | epoch: 004 | loss: 0.02254 - R2: 1.0008 -- iter: 1000/1168\n",
      "Training Step: 228  | total loss: \u001b[1m\u001b[32m0.02032\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 004 | loss: 0.02032 - R2: 1.0003 -- iter: 1020/1168\n",
      "Training Step: 229  | total loss: \u001b[1m\u001b[32m0.02032\u001b[0m\u001b[0m | time: 0.176s\n",
      "| SGD | epoch: 004 | loss: 0.02032 - R2: 1.0003 -- iter: 1040/1168\n",
      "Training Step: 230  | total loss: \u001b[1m\u001b[32m0.02359\u001b[0m\u001b[0m | time: 0.179s\n",
      "| SGD | epoch: 004 | loss: 0.02359 - R2: 1.0010 -- iter: 1060/1168\n",
      "Training Step: 231  | total loss: \u001b[1m\u001b[32m0.02254\u001b[0m\u001b[0m | time: 0.184s\n",
      "| SGD | epoch: 004 | loss: 0.02254 - R2: 1.0006 -- iter: 1080/1168\n",
      "Training Step: 232  | total loss: \u001b[1m\u001b[32m0.02140\u001b[0m\u001b[0m | time: 0.186s\n",
      "| SGD | epoch: 004 | loss: 0.02140 - R2: 1.0006 -- iter: 1100/1168\n",
      "Training Step: 233  | total loss: \u001b[1m\u001b[32m0.02255\u001b[0m\u001b[0m | time: 0.188s\n",
      "| SGD | epoch: 004 | loss: 0.02255 - R2: 0.9998 -- iter: 1120/1168\n",
      "Training Step: 234  | total loss: \u001b[1m\u001b[32m0.02291\u001b[0m\u001b[0m | time: 0.192s\n",
      "| SGD | epoch: 004 | loss: 0.02291 - R2: 1.0006 -- iter: 1140/1168\n",
      "Training Step: 235  | total loss: \u001b[1m\u001b[32m0.02291\u001b[0m\u001b[0m | time: 0.195s\n",
      "| SGD | epoch: 004 | loss: 0.02291 - R2: 1.0006 -- iter: 1160/1168\n",
      "Training Step: 236  | total loss: \u001b[1m\u001b[32m0.02275\u001b[0m\u001b[0m | time: 1.212s\n",
      "| SGD | epoch: 004 | loss: 0.02275 - R2: 0.9999 | val_loss: 0.02581 - val_acc: 1.0024 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 237  | total loss: \u001b[1m\u001b[32m0.02183\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 005 | loss: 0.02183 - R2: 0.9995 -- iter: 0020/1168\n",
      "Training Step: 238  | total loss: \u001b[1m\u001b[32m0.02107\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 005 | loss: 0.02107 - R2: 0.9998 -- iter: 0040/1168\n",
      "Training Step: 239  | total loss: \u001b[1m\u001b[32m0.01983\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 005 | loss: 0.01983 - R2: 0.9999 -- iter: 0060/1168\n",
      "Training Step: 240  | total loss: \u001b[1m\u001b[32m0.01922\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 005 | loss: 0.01922 - R2: 1.0003 -- iter: 0080/1168\n",
      "Training Step: 241  | total loss: \u001b[1m\u001b[32m0.01821\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 005 | loss: 0.01821 - R2: 1.0005 -- iter: 0100/1168\n",
      "Training Step: 242  | total loss: \u001b[1m\u001b[32m0.01940\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 005 | loss: 0.01940 - R2: 0.9995 -- iter: 0120/1168\n",
      "Training Step: 243  | total loss: \u001b[1m\u001b[32m0.01940\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 005 | loss: 0.01940 - R2: 0.9995 -- iter: 0140/1168\n",
      "Training Step: 244  | total loss: \u001b[1m\u001b[32m0.01825\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 005 | loss: 0.01825 - R2: 0.9991 -- iter: 0160/1168\n",
      "Training Step: 245  | total loss: \u001b[1m\u001b[32m0.01912\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 005 | loss: 0.01912 - R2: 0.9993 -- iter: 0180/1168\n",
      "Training Step: 246  | total loss: \u001b[1m\u001b[32m0.03666\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 005 | loss: 0.03666 - R2: 1.0003 -- iter: 0200/1168\n",
      "Training Step: 247  | total loss: \u001b[1m\u001b[32m0.03546\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 005 | loss: 0.03546 - R2: 1.0000 -- iter: 0220/1168\n",
      "Training Step: 248  | total loss: \u001b[1m\u001b[32m0.03371\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 005 | loss: 0.03371 - R2: 0.9990 -- iter: 0240/1168\n",
      "Training Step: 249  | total loss: \u001b[1m\u001b[32m0.03371\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 005 | loss: 0.03371 - R2: 0.9990 -- iter: 0260/1168\n",
      "Training Step: 250  | total loss: \u001b[1m\u001b[32m0.03150\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 005 | loss: 0.03150 - R2: 0.9985 -- iter: 0280/1168\n",
      "Training Step: 251  | total loss: \u001b[1m\u001b[32m0.03192\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 005 | loss: 0.03192 - R2: 0.9983 -- iter: 0300/1168\n",
      "Training Step: 252  | total loss: \u001b[1m\u001b[32m0.02977\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 005 | loss: 0.02977 - R2: 0.9991 -- iter: 0320/1168\n",
      "Training Step: 253  | total loss: \u001b[1m\u001b[32m0.02844\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 005 | loss: 0.02844 - R2: 0.9987 -- iter: 0340/1168\n",
      "Training Step: 254  | total loss: \u001b[1m\u001b[32m0.02678\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 005 | loss: 0.02678 - R2: 0.9999 -- iter: 0360/1168\n",
      "Training Step: 255  | total loss: \u001b[1m\u001b[32m0.02678\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 005 | loss: 0.02678 - R2: 0.9999 -- iter: 0380/1168\n",
      "Training Step: 256  | total loss: \u001b[1m\u001b[32m0.02536\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 005 | loss: 0.02536 - R2: 1.0000 -- iter: 0400/1168\n",
      "Training Step: 257  | total loss: \u001b[1m\u001b[32m0.02374\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 005 | loss: 0.02374 - R2: 1.0000 -- iter: 0420/1168\n",
      "Training Step: 258  | total loss: \u001b[1m\u001b[32m0.02374\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 005 | loss: 0.02374 - R2: 1.0000 -- iter: 0440/1168\n",
      "Training Step: 259  | total loss: \u001b[1m\u001b[32m0.02376\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 005 | loss: 0.02376 - R2: 1.0010 -- iter: 0460/1168\n",
      "Training Step: 260  | total loss: \u001b[1m\u001b[32m0.02297\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 005 | loss: 0.02297 - R2: 1.0011 -- iter: 0480/1168\n",
      "Training Step: 261  | total loss: \u001b[1m\u001b[32m0.02269\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 005 | loss: 0.02269 - R2: 1.0007 -- iter: 0500/1168\n",
      "Training Step: 262  | total loss: \u001b[1m\u001b[32m0.02282\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 005 | loss: 0.02282 - R2: 1.0002 -- iter: 0520/1168\n",
      "Training Step: 263  | total loss: \u001b[1m\u001b[32m0.02200\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 005 | loss: 0.02200 - R2: 0.9999 -- iter: 0540/1168\n",
      "Training Step: 264  | total loss: \u001b[1m\u001b[32m0.02148\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 005 | loss: 0.02148 - R2: 0.9994 -- iter: 0560/1168\n",
      "Training Step: 265  | total loss: \u001b[1m\u001b[32m0.02066\u001b[0m\u001b[0m | time: 0.178s\n",
      "| SGD | epoch: 005 | loss: 0.02066 - R2: 0.9993 -- iter: 0580/1168\n",
      "Training Step: 266  | total loss: \u001b[1m\u001b[32m0.01989\u001b[0m\u001b[0m | time: 0.182s\n",
      "| SGD | epoch: 005 | loss: 0.01989 - R2: 0.9992 -- iter: 0600/1168\n",
      "Training Step: 267  | total loss: \u001b[1m\u001b[32m0.02016\u001b[0m\u001b[0m | time: 0.188s\n",
      "| SGD | epoch: 005 | loss: 0.02016 - R2: 0.9993 -- iter: 0620/1168\n",
      "Training Step: 268  | total loss: \u001b[1m\u001b[32m0.03280\u001b[0m\u001b[0m | time: 0.193s\n",
      "| SGD | epoch: 005 | loss: 0.03280 - R2: 1.0016 -- iter: 0640/1168\n",
      "Training Step: 269  | total loss: \u001b[1m\u001b[32m0.03072\u001b[0m\u001b[0m | time: 0.203s\n",
      "| SGD | epoch: 005 | loss: 0.03072 - R2: 1.0009 -- iter: 0660/1168\n",
      "Training Step: 270  | total loss: \u001b[1m\u001b[32m0.02885\u001b[0m\u001b[0m | time: 0.206s\n",
      "| SGD | epoch: 005 | loss: 0.02885 - R2: 1.0004 -- iter: 0680/1168\n",
      "Training Step: 271  | total loss: \u001b[1m\u001b[32m0.02964\u001b[0m\u001b[0m | time: 0.210s\n",
      "| SGD | epoch: 005 | loss: 0.02964 - R2: 1.0003 -- iter: 0700/1168\n",
      "Training Step: 272  | total loss: \u001b[1m\u001b[32m0.02964\u001b[0m\u001b[0m | time: 0.236s\n",
      "| SGD | epoch: 005 | loss: 0.02964 - R2: 1.0003 -- iter: 0720/1168\n",
      "Training Step: 273  | total loss: \u001b[1m\u001b[32m0.02925\u001b[0m\u001b[0m | time: 0.238s\n",
      "| SGD | epoch: 005 | loss: 0.02925 - R2: 0.9992 -- iter: 0740/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 274  | total loss: \u001b[1m\u001b[32m0.03065\u001b[0m\u001b[0m | time: 0.241s\n",
      "| SGD | epoch: 005 | loss: 0.03065 - R2: 0.9990 -- iter: 0760/1168\n",
      "Training Step: 275  | total loss: \u001b[1m\u001b[32m0.02683\u001b[0m\u001b[0m | time: 0.244s\n",
      "| SGD | epoch: 005 | loss: 0.02683 - R2: 0.9995 -- iter: 0780/1168\n",
      "Training Step: 276  | total loss: \u001b[1m\u001b[32m0.02595\u001b[0m\u001b[0m | time: 0.248s\n",
      "| SGD | epoch: 005 | loss: 0.02595 - R2: 0.9996 -- iter: 0800/1168\n",
      "Training Step: 277  | total loss: \u001b[1m\u001b[32m0.02595\u001b[0m\u001b[0m | time: 0.249s\n",
      "| SGD | epoch: 005 | loss: 0.02595 - R2: 0.9996 -- iter: 0820/1168\n",
      "Training Step: 278  | total loss: \u001b[1m\u001b[32m0.02505\u001b[0m\u001b[0m | time: 0.252s\n",
      "| SGD | epoch: 005 | loss: 0.02505 - R2: 1.0001 -- iter: 0840/1168\n",
      "Training Step: 279  | total loss: \u001b[1m\u001b[32m0.02416\u001b[0m\u001b[0m | time: 0.255s\n",
      "| SGD | epoch: 005 | loss: 0.02416 - R2: 1.0016 -- iter: 0860/1168\n",
      "Training Step: 280  | total loss: \u001b[1m\u001b[32m0.02469\u001b[0m\u001b[0m | time: 0.258s\n",
      "| SGD | epoch: 005 | loss: 0.02469 - R2: 1.0007 -- iter: 0880/1168\n",
      "Training Step: 281  | total loss: \u001b[1m\u001b[32m0.02489\u001b[0m\u001b[0m | time: 0.261s\n",
      "| SGD | epoch: 005 | loss: 0.02489 - R2: 0.9991 -- iter: 0900/1168\n",
      "Training Step: 282  | total loss: \u001b[1m\u001b[32m0.02410\u001b[0m\u001b[0m | time: 0.265s\n",
      "| SGD | epoch: 005 | loss: 0.02410 - R2: 0.9992 -- iter: 0920/1168\n",
      "Training Step: 283  | total loss: \u001b[1m\u001b[32m0.02379\u001b[0m\u001b[0m | time: 0.269s\n",
      "| SGD | epoch: 005 | loss: 0.02379 - R2: 0.9999 -- iter: 0940/1168\n",
      "Training Step: 284  | total loss: \u001b[1m\u001b[32m0.02271\u001b[0m\u001b[0m | time: 0.272s\n",
      "| SGD | epoch: 005 | loss: 0.02271 - R2: 1.0002 -- iter: 0960/1168\n",
      "Training Step: 285  | total loss: \u001b[1m\u001b[32m0.02171\u001b[0m\u001b[0m | time: 0.275s\n",
      "| SGD | epoch: 005 | loss: 0.02171 - R2: 0.9999 -- iter: 0980/1168\n",
      "Training Step: 286  | total loss: \u001b[1m\u001b[32m0.02171\u001b[0m\u001b[0m | time: 0.279s\n",
      "| SGD | epoch: 005 | loss: 0.02171 - R2: 0.9999 -- iter: 1000/1168\n",
      "Training Step: 287  | total loss: \u001b[1m\u001b[32m0.02151\u001b[0m\u001b[0m | time: 0.304s\n",
      "| SGD | epoch: 005 | loss: 0.02151 - R2: 0.9995 -- iter: 1020/1168\n",
      "Training Step: 288  | total loss: \u001b[1m\u001b[32m0.02154\u001b[0m\u001b[0m | time: 0.314s\n",
      "| SGD | epoch: 005 | loss: 0.02154 - R2: 0.9993 -- iter: 1040/1168\n",
      "Training Step: 289  | total loss: \u001b[1m\u001b[32m0.02299\u001b[0m\u001b[0m | time: 0.322s\n",
      "| SGD | epoch: 005 | loss: 0.02299 - R2: 1.0005 -- iter: 1060/1168\n",
      "Training Step: 290  | total loss: \u001b[1m\u001b[32m0.02213\u001b[0m\u001b[0m | time: 0.324s\n",
      "| SGD | epoch: 005 | loss: 0.02213 - R2: 1.0004 -- iter: 1080/1168\n",
      "Training Step: 291  | total loss: \u001b[1m\u001b[32m0.02727\u001b[0m\u001b[0m | time: 0.326s\n",
      "| SGD | epoch: 005 | loss: 0.02727 - R2: 0.9995 -- iter: 1100/1168\n",
      "Training Step: 292  | total loss: \u001b[1m\u001b[32m0.02490\u001b[0m\u001b[0m | time: 0.329s\n",
      "| SGD | epoch: 005 | loss: 0.02490 - R2: 0.9988 -- iter: 1120/1168\n",
      "Training Step: 293  | total loss: \u001b[1m\u001b[32m0.02392\u001b[0m\u001b[0m | time: 0.332s\n",
      "| SGD | epoch: 005 | loss: 0.02392 - R2: 1.0002 -- iter: 1140/1168\n",
      "Training Step: 294  | total loss: \u001b[1m\u001b[32m0.02392\u001b[0m\u001b[0m | time: 0.335s\n",
      "| SGD | epoch: 005 | loss: 0.02392 - R2: 1.0002 -- iter: 1160/1168\n",
      "Training Step: 295  | total loss: \u001b[1m\u001b[32m0.02255\u001b[0m\u001b[0m | time: 1.343s\n",
      "| SGD | epoch: 005 | loss: 0.02255 - R2: 0.9996 | val_loss: 0.02679 - val_acc: 1.0005 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 296  | total loss: \u001b[1m\u001b[32m0.02255\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 006 | loss: 0.02255 - R2: 0.9996 -- iter: 0020/1168\n",
      "Training Step: 297  | total loss: \u001b[1m\u001b[32m0.02233\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 006 | loss: 0.02233 - R2: 1.0000 -- iter: 0040/1168\n",
      "Training Step: 298  | total loss: \u001b[1m\u001b[32m0.02233\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 006 | loss: 0.02233 - R2: 1.0000 -- iter: 0060/1168\n",
      "Training Step: 299  | total loss: \u001b[1m\u001b[32m0.02162\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 006 | loss: 0.02162 - R2: 0.9997 -- iter: 0080/1168\n",
      "Training Step: 300  | total loss: \u001b[1m\u001b[32m0.02181\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 006 | loss: 0.02181 - R2: 1.0003 -- iter: 0100/1168\n",
      "Training Step: 301  | total loss: \u001b[1m\u001b[32m0.02086\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 006 | loss: 0.02086 - R2: 1.0004 -- iter: 0120/1168\n",
      "Training Step: 302  | total loss: \u001b[1m\u001b[32m0.02024\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 006 | loss: 0.02024 - R2: 0.9998 -- iter: 0140/1168\n",
      "Training Step: 303  | total loss: \u001b[1m\u001b[32m0.01930\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 006 | loss: 0.01930 - R2: 1.0000 -- iter: 0160/1168\n",
      "Training Step: 304  | total loss: \u001b[1m\u001b[32m0.01827\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 006 | loss: 0.01827 - R2: 1.0000 -- iter: 0180/1168\n",
      "Training Step: 305  | total loss: \u001b[1m\u001b[32m0.02010\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 006 | loss: 0.02010 - R2: 0.9992 -- iter: 0200/1168\n",
      "Training Step: 306  | total loss: \u001b[1m\u001b[32m0.01974\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 006 | loss: 0.01974 - R2: 0.9999 -- iter: 0220/1168\n",
      "Training Step: 307  | total loss: \u001b[1m\u001b[32m0.02370\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 006 | loss: 0.02370 - R2: 1.0009 -- iter: 0240/1168\n",
      "Training Step: 308  | total loss: \u001b[1m\u001b[32m0.02242\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 006 | loss: 0.02242 - R2: 1.0006 -- iter: 0260/1168\n",
      "Training Step: 309  | total loss: \u001b[1m\u001b[32m0.02114\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 006 | loss: 0.02114 - R2: 1.0007 -- iter: 0280/1168\n",
      "Training Step: 310  | total loss: \u001b[1m\u001b[32m0.02042\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 006 | loss: 0.02042 - R2: 1.0001 -- iter: 0300/1168\n",
      "Training Step: 311  | total loss: \u001b[1m\u001b[32m0.02051\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 006 | loss: 0.02051 - R2: 1.0006 -- iter: 0320/1168\n",
      "Training Step: 312  | total loss: \u001b[1m\u001b[32m0.01977\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 006 | loss: 0.01977 - R2: 0.9999 -- iter: 0340/1168\n",
      "Training Step: 313  | total loss: \u001b[1m\u001b[32m0.01977\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 006 | loss: 0.01977 - R2: 0.9999 -- iter: 0360/1168\n",
      "Training Step: 314  | total loss: \u001b[1m\u001b[32m0.01922\u001b[0m\u001b[0m | time: 0.171s\n",
      "| SGD | epoch: 006 | loss: 0.01922 - R2: 0.9995 -- iter: 0380/1168\n",
      "Training Step: 315  | total loss: \u001b[1m\u001b[32m0.01908\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 006 | loss: 0.01908 - R2: 0.9990 -- iter: 0400/1168\n",
      "Training Step: 316  | total loss: \u001b[1m\u001b[32m0.01846\u001b[0m\u001b[0m | time: 0.180s\n",
      "| SGD | epoch: 006 | loss: 0.01846 - R2: 0.9984 -- iter: 0420/1168\n",
      "Training Step: 317  | total loss: \u001b[1m\u001b[32m0.01784\u001b[0m\u001b[0m | time: 0.185s\n",
      "| SGD | epoch: 006 | loss: 0.01784 - R2: 0.9988 -- iter: 0440/1168\n",
      "Training Step: 318  | total loss: \u001b[1m\u001b[32m0.01876\u001b[0m\u001b[0m | time: 0.189s\n",
      "| SGD | epoch: 006 | loss: 0.01876 - R2: 0.9986 -- iter: 0460/1168\n",
      "Training Step: 319  | total loss: \u001b[1m\u001b[32m0.01876\u001b[0m\u001b[0m | time: 0.191s\n",
      "| SGD | epoch: 006 | loss: 0.01876 - R2: 0.9986 -- iter: 0480/1168\n",
      "Training Step: 320  | total loss: \u001b[1m\u001b[32m0.01788\u001b[0m\u001b[0m | time: 0.194s\n",
      "| SGD | epoch: 006 | loss: 0.01788 - R2: 0.9985 -- iter: 0500/1168\n",
      "Training Step: 321  | total loss: \u001b[1m\u001b[32m0.01672\u001b[0m\u001b[0m | time: 0.197s\n",
      "| SGD | epoch: 006 | loss: 0.01672 - R2: 0.9991 -- iter: 0520/1168\n",
      "Training Step: 322  | total loss: \u001b[1m\u001b[32m0.01738\u001b[0m\u001b[0m | time: 0.200s\n",
      "| SGD | epoch: 006 | loss: 0.01738 - R2: 0.9995 -- iter: 0540/1168\n",
      "Training Step: 323  | total loss: \u001b[1m\u001b[32m0.01738\u001b[0m\u001b[0m | time: 0.203s\n",
      "| SGD | epoch: 006 | loss: 0.01738 - R2: 0.9995 -- iter: 0560/1168\n",
      "Training Step: 324  | total loss: \u001b[1m\u001b[32m0.01627\u001b[0m\u001b[0m | time: 0.208s\n",
      "| SGD | epoch: 006 | loss: 0.01627 - R2: 0.9998 -- iter: 0580/1168\n",
      "Training Step: 325  | total loss: \u001b[1m\u001b[32m0.01813\u001b[0m\u001b[0m | time: 0.219s\n",
      "| SGD | epoch: 006 | loss: 0.01813 - R2: 0.9980 -- iter: 0600/1168\n",
      "Training Step: 326  | total loss: \u001b[1m\u001b[32m0.01735\u001b[0m\u001b[0m | time: 0.224s\n",
      "| SGD | epoch: 006 | loss: 0.01735 - R2: 0.9981 -- iter: 0620/1168\n",
      "Training Step: 327  | total loss: \u001b[1m\u001b[32m0.01706\u001b[0m\u001b[0m | time: 0.228s\n",
      "| SGD | epoch: 006 | loss: 0.01706 - R2: 0.9988 -- iter: 0640/1168\n",
      "Training Step: 328  | total loss: \u001b[1m\u001b[32m0.01706\u001b[0m\u001b[0m | time: 0.231s\n",
      "| SGD | epoch: 006 | loss: 0.01706 - R2: 0.9991 -- iter: 0660/1168\n",
      "Training Step: 329  | total loss: \u001b[1m\u001b[32m0.01766\u001b[0m\u001b[0m | time: 0.233s\n",
      "| SGD | epoch: 006 | loss: 0.01766 - R2: 0.9991 -- iter: 0680/1168\n",
      "Training Step: 330  | total loss: \u001b[1m\u001b[32m0.01786\u001b[0m\u001b[0m | time: 0.236s\n",
      "| SGD | epoch: 006 | loss: 0.01786 - R2: 1.0002 -- iter: 0700/1168\n",
      "Training Step: 331  | total loss: \u001b[1m\u001b[32m0.01683\u001b[0m\u001b[0m | time: 0.240s\n",
      "| SGD | epoch: 006 | loss: 0.01683 - R2: 1.0002 -- iter: 0720/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 332  | total loss: \u001b[1m\u001b[32m0.01677\u001b[0m\u001b[0m | time: 0.245s\n",
      "| SGD | epoch: 006 | loss: 0.01677 - R2: 0.9996 -- iter: 0740/1168\n",
      "Training Step: 333  | total loss: \u001b[1m\u001b[32m0.01593\u001b[0m\u001b[0m | time: 0.249s\n",
      "| SGD | epoch: 006 | loss: 0.01593 - R2: 0.9996 -- iter: 0760/1168\n",
      "Training Step: 334  | total loss: \u001b[1m\u001b[32m0.01586\u001b[0m\u001b[0m | time: 0.254s\n",
      "| SGD | epoch: 006 | loss: 0.01586 - R2: 0.9992 -- iter: 0780/1168\n",
      "Training Step: 335  | total loss: \u001b[1m\u001b[32m0.01638\u001b[0m\u001b[0m | time: 0.258s\n",
      "| SGD | epoch: 006 | loss: 0.01638 - R2: 1.0002 -- iter: 0800/1168\n",
      "Training Step: 336  | total loss: \u001b[1m\u001b[32m0.01609\u001b[0m\u001b[0m | time: 0.262s\n",
      "| SGD | epoch: 006 | loss: 0.01609 - R2: 1.0007 -- iter: 0820/1168\n",
      "Training Step: 337  | total loss: \u001b[1m\u001b[32m0.01534\u001b[0m\u001b[0m | time: 0.266s\n",
      "| SGD | epoch: 006 | loss: 0.01534 - R2: 1.0006 -- iter: 0840/1168\n",
      "Training Step: 338  | total loss: \u001b[1m\u001b[32m0.01534\u001b[0m\u001b[0m | time: 0.270s\n",
      "| SGD | epoch: 006 | loss: 0.01534 - R2: 1.0006 -- iter: 0860/1168\n",
      "Training Step: 339  | total loss: \u001b[1m\u001b[32m0.01489\u001b[0m\u001b[0m | time: 0.278s\n",
      "| SGD | epoch: 006 | loss: 0.01489 - R2: 0.9997 -- iter: 0880/1168\n",
      "Training Step: 340  | total loss: \u001b[1m\u001b[32m0.01471\u001b[0m\u001b[0m | time: 0.297s\n",
      "| SGD | epoch: 006 | loss: 0.01471 - R2: 0.9997 -- iter: 0900/1168\n",
      "Training Step: 341  | total loss: \u001b[1m\u001b[32m0.01506\u001b[0m\u001b[0m | time: 0.301s\n",
      "| SGD | epoch: 006 | loss: 0.01506 - R2: 0.9993 -- iter: 0920/1168\n",
      "Training Step: 342  | total loss: \u001b[1m\u001b[32m0.01546\u001b[0m\u001b[0m | time: 0.303s\n",
      "| SGD | epoch: 006 | loss: 0.01546 - R2: 1.0004 -- iter: 0940/1168\n",
      "Training Step: 343  | total loss: \u001b[1m\u001b[32m0.01641\u001b[0m\u001b[0m | time: 0.306s\n",
      "| SGD | epoch: 006 | loss: 0.01641 - R2: 0.9996 -- iter: 0960/1168\n",
      "Training Step: 344  | total loss: \u001b[1m\u001b[32m0.01629\u001b[0m\u001b[0m | time: 0.310s\n",
      "| SGD | epoch: 006 | loss: 0.01629 - R2: 0.9999 -- iter: 0980/1168\n",
      "Training Step: 345  | total loss: \u001b[1m\u001b[32m0.01629\u001b[0m\u001b[0m | time: 0.314s\n",
      "| SGD | epoch: 006 | loss: 0.01629 - R2: 0.9999 -- iter: 1000/1168\n",
      "Training Step: 346  | total loss: \u001b[1m\u001b[32m0.01561\u001b[0m\u001b[0m | time: 0.320s\n",
      "| SGD | epoch: 006 | loss: 0.01561 - R2: 1.0009 -- iter: 1020/1168\n",
      "Training Step: 347  | total loss: \u001b[1m\u001b[32m0.01499\u001b[0m\u001b[0m | time: 0.324s\n",
      "| SGD | epoch: 006 | loss: 0.01499 - R2: 1.0008 -- iter: 1040/1168\n",
      "Training Step: 348  | total loss: \u001b[1m\u001b[32m0.01499\u001b[0m\u001b[0m | time: 0.327s\n",
      "| SGD | epoch: 006 | loss: 0.01499 - R2: 1.0008 -- iter: 1060/1168\n",
      "Training Step: 349  | total loss: \u001b[1m\u001b[32m0.01512\u001b[0m\u001b[0m | time: 0.330s\n",
      "| SGD | epoch: 006 | loss: 0.01512 - R2: 1.0002 -- iter: 1080/1168\n",
      "Training Step: 350  | total loss: \u001b[1m\u001b[32m0.01521\u001b[0m\u001b[0m | time: 0.334s\n",
      "| SGD | epoch: 006 | loss: 0.01521 - R2: 0.9999 -- iter: 1100/1168\n",
      "Training Step: 351  | total loss: \u001b[1m\u001b[32m0.01652\u001b[0m\u001b[0m | time: 0.337s\n",
      "| SGD | epoch: 006 | loss: 0.01652 - R2: 0.9998 -- iter: 1120/1168\n",
      "Training Step: 352  | total loss: \u001b[1m\u001b[32m0.01559\u001b[0m\u001b[0m | time: 0.344s\n",
      "| SGD | epoch: 006 | loss: 0.01559 - R2: 0.9999 -- iter: 1140/1168\n",
      "Training Step: 353  | total loss: \u001b[1m\u001b[32m0.01482\u001b[0m\u001b[0m | time: 0.347s\n",
      "| SGD | epoch: 006 | loss: 0.01482 - R2: 0.9997 -- iter: 1160/1168\n",
      "Training Step: 354  | total loss: \u001b[1m\u001b[32m0.01482\u001b[0m\u001b[0m | time: 1.353s\n",
      "| SGD | epoch: 006 | loss: 0.01482 - R2: 0.9997 | val_loss: 0.02451 - val_acc: 1.0033 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 355  | total loss: \u001b[1m\u001b[32m0.01557\u001b[0m\u001b[0m | time: 0.007s\n",
      "| SGD | epoch: 007 | loss: 0.01557 - R2: 1.0000 -- iter: 0020/1168\n",
      "Training Step: 356  | total loss: \u001b[1m\u001b[32m0.01525\u001b[0m\u001b[0m | time: 0.010s\n",
      "| SGD | epoch: 007 | loss: 0.01525 - R2: 0.9992 -- iter: 0040/1168\n",
      "Training Step: 357  | total loss: \u001b[1m\u001b[32m0.01653\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 007 | loss: 0.01653 - R2: 1.0005 -- iter: 0060/1168\n",
      "Training Step: 358  | total loss: \u001b[1m\u001b[32m0.01805\u001b[0m\u001b[0m | time: 0.018s\n",
      "| SGD | epoch: 007 | loss: 0.01805 - R2: 1.0013 -- iter: 0080/1168\n",
      "Training Step: 359  | total loss: \u001b[1m\u001b[32m0.01805\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 007 | loss: 0.01805 - R2: 1.0013 -- iter: 0100/1168\n",
      "Training Step: 360  | total loss: \u001b[1m\u001b[32m0.03550\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 007 | loss: 0.03550 - R2: 1.0047 -- iter: 0120/1168\n",
      "Training Step: 361  | total loss: \u001b[1m\u001b[32m0.04283\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 007 | loss: 0.04283 - R2: 1.0047 -- iter: 0140/1168\n",
      "Training Step: 362  | total loss: \u001b[1m\u001b[32m0.04068\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 007 | loss: 0.04068 - R2: 1.0008 -- iter: 0160/1168\n",
      "Training Step: 363  | total loss: \u001b[1m\u001b[32m0.04068\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 007 | loss: 0.04068 - R2: 1.0008 -- iter: 0180/1168\n",
      "Training Step: 364  | total loss: \u001b[1m\u001b[32m0.03821\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 007 | loss: 0.03821 - R2: 0.9996 -- iter: 0200/1168\n",
      "Training Step: 365  | total loss: \u001b[1m\u001b[32m0.05057\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 007 | loss: 0.05057 - R2: 1.0000 -- iter: 0220/1168\n",
      "Training Step: 366  | total loss: \u001b[1m\u001b[32m0.04566\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 007 | loss: 0.04566 - R2: 0.9992 -- iter: 0240/1168\n",
      "Training Step: 367  | total loss: \u001b[1m\u001b[32m0.04566\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 007 | loss: 0.04566 - R2: 0.9992 -- iter: 0260/1168\n",
      "Training Step: 368  | total loss: \u001b[1m\u001b[32m0.03987\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 007 | loss: 0.03987 - R2: 0.9984 -- iter: 0280/1168\n",
      "Training Step: 369  | total loss: \u001b[1m\u001b[32m0.03987\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 007 | loss: 0.03987 - R2: 0.9984 -- iter: 0300/1168\n",
      "Training Step: 370  | total loss: \u001b[1m\u001b[32m0.03696\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 007 | loss: 0.03696 - R2: 0.9991 -- iter: 0320/1168\n",
      "Training Step: 371  | total loss: \u001b[1m\u001b[32m0.03349\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 007 | loss: 0.03349 - R2: 0.9989 -- iter: 0340/1168\n",
      "Training Step: 372  | total loss: \u001b[1m\u001b[32m0.03205\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 007 | loss: 0.03205 - R2: 0.9982 -- iter: 0360/1168\n",
      "Training Step: 373  | total loss: \u001b[1m\u001b[32m0.03170\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 007 | loss: 0.03170 - R2: 0.9982 -- iter: 0380/1168\n",
      "Training Step: 374  | total loss: \u001b[1m\u001b[32m0.03170\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 007 | loss: 0.03170 - R2: 0.9982 -- iter: 0400/1168\n",
      "Training Step: 375  | total loss: \u001b[1m\u001b[32m0.02948\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 007 | loss: 0.02948 - R2: 0.9977 -- iter: 0420/1168\n",
      "Training Step: 376  | total loss: \u001b[1m\u001b[32m0.02768\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 007 | loss: 0.02768 - R2: 0.9977 -- iter: 0440/1168\n",
      "Training Step: 377  | total loss: \u001b[1m\u001b[32m0.02768\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 007 | loss: 0.02768 - R2: 0.9982 -- iter: 0460/1168\n",
      "Training Step: 378  | total loss: \u001b[1m\u001b[32m0.02770\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 007 | loss: 0.02770 - R2: 0.9994 -- iter: 0480/1168\n",
      "Training Step: 379  | total loss: \u001b[1m\u001b[32m0.02782\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 007 | loss: 0.02782 - R2: 0.9994 -- iter: 0500/1168\n",
      "Training Step: 380  | total loss: \u001b[1m\u001b[32m0.02727\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 007 | loss: 0.02727 - R2: 0.9994 -- iter: 0520/1168\n",
      "Training Step: 381  | total loss: \u001b[1m\u001b[32m0.02680\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 007 | loss: 0.02680 - R2: 0.9994 -- iter: 0540/1168\n",
      "Training Step: 382  | total loss: \u001b[1m\u001b[32m0.02882\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 007 | loss: 0.02882 - R2: 1.0008 -- iter: 0560/1168\n",
      "Training Step: 383  | total loss: \u001b[1m\u001b[32m0.02854\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 007 | loss: 0.02854 - R2: 1.0019 -- iter: 0580/1168\n",
      "Training Step: 384  | total loss: \u001b[1m\u001b[32m0.02576\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 007 | loss: 0.02576 - R2: 1.0016 -- iter: 0600/1168\n",
      "Training Step: 385  | total loss: \u001b[1m\u001b[32m0.02576\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 007 | loss: 0.02576 - R2: 1.0016 -- iter: 0620/1168\n",
      "Training Step: 386  | total loss: \u001b[1m\u001b[32m0.02512\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 007 | loss: 0.02512 - R2: 1.0001 -- iter: 0640/1168\n",
      "Training Step: 387  | total loss: \u001b[1m\u001b[32m0.02512\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 007 | loss: 0.02512 - R2: 1.0001 -- iter: 0660/1168\n",
      "Training Step: 388  | total loss: \u001b[1m\u001b[32m0.02520\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 007 | loss: 0.02520 - R2: 0.9991 -- iter: 0680/1168\n",
      "Training Step: 389  | total loss: \u001b[1m\u001b[32m0.02520\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 007 | loss: 0.02520 - R2: 0.9985 -- iter: 0700/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 390  | total loss: \u001b[1m\u001b[32m0.02396\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 007 | loss: 0.02396 - R2: 0.9985 -- iter: 0720/1168\n",
      "Training Step: 391  | total loss: \u001b[1m\u001b[32m0.02299\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 007 | loss: 0.02299 - R2: 0.9989 -- iter: 0740/1168\n",
      "Training Step: 392  | total loss: \u001b[1m\u001b[32m0.02159\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 007 | loss: 0.02159 - R2: 0.9991 -- iter: 0760/1168\n",
      "Training Step: 393  | total loss: \u001b[1m\u001b[32m0.03084\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 007 | loss: 0.03084 - R2: 1.0002 -- iter: 0780/1168\n",
      "Training Step: 394  | total loss: \u001b[1m\u001b[32m0.03171\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 007 | loss: 0.03171 - R2: 0.9987 -- iter: 0800/1168\n",
      "Training Step: 395  | total loss: \u001b[1m\u001b[32m0.03013\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 007 | loss: 0.03013 - R2: 0.9985 -- iter: 0820/1168\n",
      "Training Step: 396  | total loss: \u001b[1m\u001b[32m0.03017\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 007 | loss: 0.03017 - R2: 0.9994 -- iter: 0840/1168\n",
      "Training Step: 397  | total loss: \u001b[1m\u001b[32m0.03017\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 007 | loss: 0.03017 - R2: 0.9994 -- iter: 0860/1168\n",
      "Training Step: 398  | total loss: \u001b[1m\u001b[32m0.02876\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 007 | loss: 0.02876 - R2: 0.9994 -- iter: 0880/1168\n",
      "Training Step: 399  | total loss: \u001b[1m\u001b[32m0.02528\u001b[0m\u001b[0m | time: 0.173s\n",
      "| SGD | epoch: 007 | loss: 0.02528 - R2: 1.0004 -- iter: 0900/1168\n",
      "Training Step: 400  | total loss: \u001b[1m\u001b[32m0.02379\u001b[0m\u001b[0m | time: 0.179s\n",
      "| SGD | epoch: 007 | loss: 0.02379 - R2: 1.0005 -- iter: 0920/1168\n",
      "Training Step: 401  | total loss: \u001b[1m\u001b[32m0.02379\u001b[0m\u001b[0m | time: 0.189s\n",
      "| SGD | epoch: 007 | loss: 0.02379 - R2: 1.0005 -- iter: 0940/1168\n",
      "Training Step: 402  | total loss: \u001b[1m\u001b[32m0.02532\u001b[0m\u001b[0m | time: 0.196s\n",
      "| SGD | epoch: 007 | loss: 0.02532 - R2: 1.0005 -- iter: 0960/1168\n",
      "Training Step: 403  | total loss: \u001b[1m\u001b[32m0.02239\u001b[0m\u001b[0m | time: 0.201s\n",
      "| SGD | epoch: 007 | loss: 0.02239 - R2: 0.9995 -- iter: 0980/1168\n",
      "Training Step: 404  | total loss: \u001b[1m\u001b[32m0.02149\u001b[0m\u001b[0m | time: 0.204s\n",
      "| SGD | epoch: 007 | loss: 0.02149 - R2: 0.9998 -- iter: 1000/1168\n",
      "Training Step: 405  | total loss: \u001b[1m\u001b[32m0.02057\u001b[0m\u001b[0m | time: 0.208s\n",
      "| SGD | epoch: 007 | loss: 0.02057 - R2: 1.0004 -- iter: 1020/1168\n",
      "Training Step: 406  | total loss: \u001b[1m\u001b[32m0.02019\u001b[0m\u001b[0m | time: 0.213s\n",
      "| SGD | epoch: 007 | loss: 0.02019 - R2: 1.0003 -- iter: 1040/1168\n",
      "Training Step: 407  | total loss: \u001b[1m\u001b[32m0.01977\u001b[0m\u001b[0m | time: 0.216s\n",
      "| SGD | epoch: 007 | loss: 0.01977 - R2: 1.0002 -- iter: 1060/1168\n",
      "Training Step: 408  | total loss: \u001b[1m\u001b[32m0.01991\u001b[0m\u001b[0m | time: 0.218s\n",
      "| SGD | epoch: 007 | loss: 0.01991 - R2: 1.0000 -- iter: 1080/1168\n",
      "Training Step: 409  | total loss: \u001b[1m\u001b[32m0.01885\u001b[0m\u001b[0m | time: 0.222s\n",
      "| SGD | epoch: 007 | loss: 0.01885 - R2: 1.0000 -- iter: 1100/1168\n",
      "Training Step: 410  | total loss: \u001b[1m\u001b[32m0.01885\u001b[0m\u001b[0m | time: 0.230s\n",
      "| SGD | epoch: 007 | loss: 0.01885 - R2: 1.0000 -- iter: 1120/1168\n",
      "Training Step: 411  | total loss: \u001b[1m\u001b[32m0.01770\u001b[0m\u001b[0m | time: 0.233s\n",
      "| SGD | epoch: 007 | loss: 0.01770 - R2: 0.9993 -- iter: 1140/1168\n",
      "Training Step: 412  | total loss: \u001b[1m\u001b[32m0.01839\u001b[0m\u001b[0m | time: 0.236s\n",
      "| SGD | epoch: 007 | loss: 0.01839 - R2: 0.9996 -- iter: 1160/1168\n",
      "Training Step: 413  | total loss: \u001b[1m\u001b[32m0.01765\u001b[0m\u001b[0m | time: 1.246s\n",
      "| SGD | epoch: 007 | loss: 0.01765 - R2: 1.0002 | val_loss: 0.02848 - val_acc: 1.0000 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 414  | total loss: \u001b[1m\u001b[32m0.01675\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 008 | loss: 0.01675 - R2: 0.9999 -- iter: 0020/1168\n",
      "Training Step: 415  | total loss: \u001b[1m\u001b[32m0.01654\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 008 | loss: 0.01654 - R2: 0.9997 -- iter: 0040/1168\n",
      "Training Step: 416  | total loss: \u001b[1m\u001b[32m0.01620\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 008 | loss: 0.01620 - R2: 1.0000 -- iter: 0060/1168\n",
      "Training Step: 417  | total loss: \u001b[1m\u001b[32m0.01620\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 008 | loss: 0.01620 - R2: 1.0000 -- iter: 0080/1168\n",
      "Training Step: 418  | total loss: \u001b[1m\u001b[32m0.01930\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 008 | loss: 0.01930 - R2: 0.9989 -- iter: 0100/1168\n",
      "Training Step: 419  | total loss: \u001b[1m\u001b[32m0.01811\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 008 | loss: 0.01811 - R2: 1.0002 -- iter: 0120/1168\n",
      "Training Step: 420  | total loss: \u001b[1m\u001b[32m0.01664\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 008 | loss: 0.01664 - R2: 1.0008 -- iter: 0140/1168\n",
      "Training Step: 421  | total loss: \u001b[1m\u001b[32m0.01735\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 008 | loss: 0.01735 - R2: 1.0005 -- iter: 0160/1168\n",
      "Training Step: 422  | total loss: \u001b[1m\u001b[32m0.01735\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 008 | loss: 0.01735 - R2: 1.0005 -- iter: 0180/1168\n",
      "Training Step: 423  | total loss: \u001b[1m\u001b[32m0.01722\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 008 | loss: 0.01722 - R2: 0.9994 -- iter: 0200/1168\n",
      "Training Step: 424  | total loss: \u001b[1m\u001b[32m0.01695\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 008 | loss: 0.01695 - R2: 0.9995 -- iter: 0220/1168\n",
      "Training Step: 425  | total loss: \u001b[1m\u001b[32m0.01800\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 008 | loss: 0.01800 - R2: 1.0004 -- iter: 0240/1168\n",
      "Training Step: 426  | total loss: \u001b[1m\u001b[32m0.02142\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 008 | loss: 0.02142 - R2: 1.0008 -- iter: 0260/1168\n",
      "Training Step: 427  | total loss: \u001b[1m\u001b[32m0.02109\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 008 | loss: 0.02109 - R2: 1.0003 -- iter: 0280/1168\n",
      "Training Step: 428  | total loss: \u001b[1m\u001b[32m0.02171\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 008 | loss: 0.02171 - R2: 0.9995 -- iter: 0300/1168\n",
      "Training Step: 429  | total loss: \u001b[1m\u001b[32m0.02492\u001b[0m\u001b[0m | time: 0.165s\n",
      "| SGD | epoch: 008 | loss: 0.02492 - R2: 1.0009 -- iter: 0320/1168\n",
      "Training Step: 430  | total loss: \u001b[1m\u001b[32m0.02341\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 008 | loss: 0.02341 - R2: 1.0009 -- iter: 0340/1168\n",
      "Training Step: 431  | total loss: \u001b[1m\u001b[32m0.02315\u001b[0m\u001b[0m | time: 0.173s\n",
      "| SGD | epoch: 008 | loss: 0.02315 - R2: 1.0001 -- iter: 0360/1168\n",
      "Training Step: 432  | total loss: \u001b[1m\u001b[32m0.02348\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 008 | loss: 0.02348 - R2: 0.9997 -- iter: 0380/1168\n",
      "Training Step: 433  | total loss: \u001b[1m\u001b[32m0.02348\u001b[0m\u001b[0m | time: 0.181s\n",
      "| SGD | epoch: 008 | loss: 0.02348 - R2: 0.9997 -- iter: 0400/1168\n",
      "Training Step: 434  | total loss: \u001b[1m\u001b[32m0.02262\u001b[0m\u001b[0m | time: 0.183s\n",
      "| SGD | epoch: 008 | loss: 0.02262 - R2: 0.9998 -- iter: 0420/1168\n",
      "Training Step: 435  | total loss: \u001b[1m\u001b[32m0.02199\u001b[0m\u001b[0m | time: 0.185s\n",
      "| SGD | epoch: 008 | loss: 0.02199 - R2: 1.0000 -- iter: 0440/1168\n",
      "Training Step: 436  | total loss: \u001b[1m\u001b[32m0.02132\u001b[0m\u001b[0m | time: 0.188s\n",
      "| SGD | epoch: 008 | loss: 0.02132 - R2: 0.9997 -- iter: 0460/1168\n",
      "Training Step: 437  | total loss: \u001b[1m\u001b[32m0.02132\u001b[0m\u001b[0m | time: 0.190s\n",
      "| SGD | epoch: 008 | loss: 0.02132 - R2: 1.0006 -- iter: 0480/1168\n",
      "Training Step: 438  | total loss: \u001b[1m\u001b[32m0.02761\u001b[0m\u001b[0m | time: 0.193s\n",
      "| SGD | epoch: 008 | loss: 0.02761 - R2: 0.9990 -- iter: 0500/1168\n",
      "Training Step: 439  | total loss: \u001b[1m\u001b[32m0.02709\u001b[0m\u001b[0m | time: 0.196s\n",
      "| SGD | epoch: 008 | loss: 0.02709 - R2: 0.9991 -- iter: 0520/1168\n",
      "Training Step: 440  | total loss: \u001b[1m\u001b[32m0.02709\u001b[0m\u001b[0m | time: 0.198s\n",
      "| SGD | epoch: 008 | loss: 0.02709 - R2: 0.9993 -- iter: 0540/1168\n",
      "Training Step: 441  | total loss: \u001b[1m\u001b[32m0.02617\u001b[0m\u001b[0m | time: 0.200s\n",
      "| SGD | epoch: 008 | loss: 0.02617 - R2: 0.9997 -- iter: 0560/1168\n",
      "Training Step: 442  | total loss: \u001b[1m\u001b[32m0.02617\u001b[0m\u001b[0m | time: 0.205s\n",
      "| SGD | epoch: 008 | loss: 0.02617 - R2: 0.9997 -- iter: 0580/1168\n",
      "Training Step: 443  | total loss: \u001b[1m\u001b[32m0.02336\u001b[0m\u001b[0m | time: 0.208s\n",
      "| SGD | epoch: 008 | loss: 0.02336 - R2: 1.0002 -- iter: 0600/1168\n",
      "Training Step: 444  | total loss: \u001b[1m\u001b[32m0.02336\u001b[0m\u001b[0m | time: 0.211s\n",
      "| SGD | epoch: 008 | loss: 0.02336 - R2: 1.0000 -- iter: 0620/1168\n",
      "Training Step: 445  | total loss: \u001b[1m\u001b[32m0.02164\u001b[0m\u001b[0m | time: 0.213s\n",
      "| SGD | epoch: 008 | loss: 0.02164 - R2: 1.0000 -- iter: 0640/1168\n",
      "Training Step: 446  | total loss: \u001b[1m\u001b[32m0.02094\u001b[0m\u001b[0m | time: 0.215s\n",
      "| SGD | epoch: 008 | loss: 0.02094 - R2: 0.9995 -- iter: 0660/1168\n",
      "Training Step: 447  | total loss: \u001b[1m\u001b[32m0.02213\u001b[0m\u001b[0m | time: 0.217s\n",
      "| SGD | epoch: 008 | loss: 0.02213 - R2: 1.0000 -- iter: 0680/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 448  | total loss: \u001b[1m\u001b[32m0.04262\u001b[0m\u001b[0m | time: 0.219s\n",
      "| SGD | epoch: 008 | loss: 0.04262 - R2: 0.9969 -- iter: 0700/1168\n",
      "Training Step: 449  | total loss: \u001b[1m\u001b[32m0.04065\u001b[0m\u001b[0m | time: 0.220s\n",
      "| SGD | epoch: 008 | loss: 0.04065 - R2: 0.9982 -- iter: 0720/1168\n",
      "Training Step: 450  | total loss: \u001b[1m\u001b[32m0.03745\u001b[0m\u001b[0m | time: 0.227s\n",
      "| SGD | epoch: 008 | loss: 0.03745 - R2: 0.9983 -- iter: 0740/1168\n",
      "Training Step: 451  | total loss: \u001b[1m\u001b[32m0.03657\u001b[0m\u001b[0m | time: 0.231s\n",
      "| SGD | epoch: 008 | loss: 0.03657 - R2: 0.9978 -- iter: 0760/1168\n",
      "Training Step: 452  | total loss: \u001b[1m\u001b[32m0.03418\u001b[0m\u001b[0m | time: 0.234s\n",
      "| SGD | epoch: 008 | loss: 0.03418 - R2: 0.9985 -- iter: 0780/1168\n",
      "Training Step: 453  | total loss: \u001b[1m\u001b[32m0.03222\u001b[0m\u001b[0m | time: 0.253s\n",
      "| SGD | epoch: 008 | loss: 0.03222 - R2: 0.9994 -- iter: 0800/1168\n",
      "Training Step: 454  | total loss: \u001b[1m\u001b[32m0.03069\u001b[0m\u001b[0m | time: 0.256s\n",
      "| SGD | epoch: 008 | loss: 0.03069 - R2: 0.9997 -- iter: 0820/1168\n",
      "Training Step: 455  | total loss: \u001b[1m\u001b[32m0.02874\u001b[0m\u001b[0m | time: 0.261s\n",
      "| SGD | epoch: 008 | loss: 0.02874 - R2: 0.9994 -- iter: 0840/1168\n",
      "Training Step: 456  | total loss: \u001b[1m\u001b[32m0.02743\u001b[0m\u001b[0m | time: 0.263s\n",
      "| SGD | epoch: 008 | loss: 0.02743 - R2: 1.0001 -- iter: 0860/1168\n",
      "Training Step: 457  | total loss: \u001b[1m\u001b[32m0.02405\u001b[0m\u001b[0m | time: 0.265s\n",
      "| SGD | epoch: 008 | loss: 0.02405 - R2: 1.0002 -- iter: 0880/1168\n",
      "Training Step: 458  | total loss: \u001b[1m\u001b[32m0.02405\u001b[0m\u001b[0m | time: 0.267s\n",
      "| SGD | epoch: 008 | loss: 0.02405 - R2: 0.9994 -- iter: 0900/1168\n",
      "Training Step: 459  | total loss: \u001b[1m\u001b[32m0.02309\u001b[0m\u001b[0m | time: 0.268s\n",
      "| SGD | epoch: 008 | loss: 0.02309 - R2: 0.9999 -- iter: 0920/1168\n",
      "Training Step: 460  | total loss: \u001b[1m\u001b[32m0.02309\u001b[0m\u001b[0m | time: 0.270s\n",
      "| SGD | epoch: 008 | loss: 0.02309 - R2: 0.9999 -- iter: 0940/1168\n",
      "Training Step: 461  | total loss: \u001b[1m\u001b[32m0.02193\u001b[0m\u001b[0m | time: 0.272s\n",
      "| SGD | epoch: 008 | loss: 0.02193 - R2: 1.0004 -- iter: 0960/1168\n",
      "Training Step: 462  | total loss: \u001b[1m\u001b[32m0.02003\u001b[0m\u001b[0m | time: 0.275s\n",
      "| SGD | epoch: 008 | loss: 0.02003 - R2: 1.0005 -- iter: 0980/1168\n",
      "Training Step: 463  | total loss: \u001b[1m\u001b[32m0.02003\u001b[0m\u001b[0m | time: 0.277s\n",
      "| SGD | epoch: 008 | loss: 0.02003 - R2: 1.0005 -- iter: 1000/1168\n",
      "Training Step: 464  | total loss: \u001b[1m\u001b[32m0.02016\u001b[0m\u001b[0m | time: 0.279s\n",
      "| SGD | epoch: 008 | loss: 0.02016 - R2: 0.9997 -- iter: 1020/1168\n",
      "Training Step: 465  | total loss: \u001b[1m\u001b[32m0.01945\u001b[0m\u001b[0m | time: 0.281s\n",
      "| SGD | epoch: 008 | loss: 0.01945 - R2: 0.9997 -- iter: 1040/1168\n",
      "Training Step: 466  | total loss: \u001b[1m\u001b[32m0.02580\u001b[0m\u001b[0m | time: 0.284s\n",
      "| SGD | epoch: 008 | loss: 0.02580 - R2: 1.0006 -- iter: 1060/1168\n",
      "Training Step: 467  | total loss: \u001b[1m\u001b[32m0.02410\u001b[0m\u001b[0m | time: 0.303s\n",
      "| SGD | epoch: 008 | loss: 0.02410 - R2: 1.0003 -- iter: 1080/1168\n",
      "Training Step: 468  | total loss: \u001b[1m\u001b[32m0.02320\u001b[0m\u001b[0m | time: 0.305s\n",
      "| SGD | epoch: 008 | loss: 0.02320 - R2: 0.9991 -- iter: 1100/1168\n",
      "Training Step: 469  | total loss: \u001b[1m\u001b[32m0.02214\u001b[0m\u001b[0m | time: 0.308s\n",
      "| SGD | epoch: 008 | loss: 0.02214 - R2: 1.0004 -- iter: 1120/1168\n",
      "Training Step: 470  | total loss: \u001b[1m\u001b[32m0.02214\u001b[0m\u001b[0m | time: 0.311s\n",
      "| SGD | epoch: 008 | loss: 0.02214 - R2: 1.0004 -- iter: 1140/1168\n",
      "Training Step: 471  | total loss: \u001b[1m\u001b[32m0.02108\u001b[0m\u001b[0m | time: 0.313s\n",
      "| SGD | epoch: 008 | loss: 0.02108 - R2: 0.9998 -- iter: 1160/1168\n",
      "Training Step: 472  | total loss: \u001b[1m\u001b[32m0.02096\u001b[0m\u001b[0m | time: 1.319s\n",
      "| SGD | epoch: 008 | loss: 0.02096 - R2: 1.0002 | val_loss: 0.02443 - val_acc: 1.0026 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 473  | total loss: \u001b[1m\u001b[32m0.02096\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 009 | loss: 0.02096 - R2: 1.0002 -- iter: 0020/1168\n",
      "Training Step: 474  | total loss: \u001b[1m\u001b[32m0.02014\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 009 | loss: 0.02014 - R2: 0.9998 -- iter: 0040/1168\n",
      "Training Step: 475  | total loss: \u001b[1m\u001b[32m0.02019\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 009 | loss: 0.02019 - R2: 0.9992 -- iter: 0060/1168\n",
      "Training Step: 476  | total loss: \u001b[1m\u001b[32m0.02019\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 009 | loss: 0.02019 - R2: 0.9992 -- iter: 0080/1168\n",
      "Training Step: 477  | total loss: \u001b[1m\u001b[32m0.02010\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 009 | loss: 0.02010 - R2: 1.0006 -- iter: 0100/1168\n",
      "Training Step: 478  | total loss: \u001b[1m\u001b[32m0.02172\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 009 | loss: 0.02172 - R2: 1.0005 -- iter: 0120/1168\n",
      "Training Step: 479  | total loss: \u001b[1m\u001b[32m0.01889\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 009 | loss: 0.01889 - R2: 1.0010 -- iter: 0140/1168\n",
      "Training Step: 480  | total loss: \u001b[1m\u001b[32m0.01752\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 009 | loss: 0.01752 - R2: 1.0011 -- iter: 0160/1168\n",
      "Training Step: 481  | total loss: \u001b[1m\u001b[32m0.01752\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 009 | loss: 0.01752 - R2: 1.0011 -- iter: 0180/1168\n",
      "Training Step: 482  | total loss: \u001b[1m\u001b[32m0.01782\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 009 | loss: 0.01782 - R2: 1.0012 -- iter: 0200/1168\n",
      "Training Step: 483  | total loss: \u001b[1m\u001b[32m0.01782\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 009 | loss: 0.01782 - R2: 1.0012 -- iter: 0220/1168\n",
      "Training Step: 484  | total loss: \u001b[1m\u001b[32m0.01855\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 009 | loss: 0.01855 - R2: 1.0003 -- iter: 0240/1168\n",
      "Training Step: 485  | total loss: \u001b[1m\u001b[32m0.02035\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 009 | loss: 0.02035 - R2: 1.0008 -- iter: 0260/1168\n",
      "Training Step: 486  | total loss: \u001b[1m\u001b[32m0.01928\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 009 | loss: 0.01928 - R2: 1.0013 -- iter: 0280/1168\n",
      "Training Step: 487  | total loss: \u001b[1m\u001b[32m0.01853\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 009 | loss: 0.01853 - R2: 1.0007 -- iter: 0300/1168\n",
      "Training Step: 488  | total loss: \u001b[1m\u001b[32m0.02078\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 009 | loss: 0.02078 - R2: 0.9996 -- iter: 0320/1168\n",
      "Training Step: 489  | total loss: \u001b[1m\u001b[32m0.01985\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 009 | loss: 0.01985 - R2: 1.0000 -- iter: 0340/1168\n",
      "Training Step: 490  | total loss: \u001b[1m\u001b[32m0.01985\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 009 | loss: 0.01985 - R2: 1.0000 -- iter: 0360/1168\n",
      "Training Step: 491  | total loss: \u001b[1m\u001b[32m0.01874\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 009 | loss: 0.01874 - R2: 0.9999 -- iter: 0380/1168\n",
      "Training Step: 492  | total loss: \u001b[1m\u001b[32m0.01788\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 009 | loss: 0.01788 - R2: 0.9999 -- iter: 0400/1168\n",
      "Training Step: 493  | total loss: \u001b[1m\u001b[32m0.01854\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 009 | loss: 0.01854 - R2: 1.0004 -- iter: 0420/1168\n",
      "Training Step: 494  | total loss: \u001b[1m\u001b[32m0.01815\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 009 | loss: 0.01815 - R2: 1.0011 -- iter: 0440/1168\n",
      "Training Step: 495  | total loss: \u001b[1m\u001b[32m0.01760\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 009 | loss: 0.01760 - R2: 1.0004 -- iter: 0460/1168\n",
      "Training Step: 496  | total loss: \u001b[1m\u001b[32m0.01735\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 009 | loss: 0.01735 - R2: 1.0009 -- iter: 0480/1168\n",
      "Training Step: 497  | total loss: \u001b[1m\u001b[32m0.02006\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 009 | loss: 0.02006 - R2: 1.0008 -- iter: 0500/1168\n",
      "Training Step: 498  | total loss: \u001b[1m\u001b[32m0.01958\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 009 | loss: 0.01958 - R2: 0.9995 -- iter: 0520/1168\n",
      "Training Step: 499  | total loss: \u001b[1m\u001b[32m0.01958\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 009 | loss: 0.01958 - R2: 0.9989 -- iter: 0540/1168\n",
      "Training Step: 500  | total loss: \u001b[1m\u001b[32m0.01999\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 009 | loss: 0.01999 - R2: 0.9989 -- iter: 0560/1168\n",
      "Training Step: 501  | total loss: \u001b[1m\u001b[32m0.03621\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 009 | loss: 0.03621 - R2: 1.0022 -- iter: 0580/1168\n",
      "Training Step: 502  | total loss: \u001b[1m\u001b[32m0.03621\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 009 | loss: 0.03621 - R2: 1.0022 -- iter: 0600/1168\n",
      "Training Step: 503  | total loss: \u001b[1m\u001b[32m0.03640\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 009 | loss: 0.03640 - R2: 1.0003 -- iter: 0620/1168\n",
      "Training Step: 504  | total loss: \u001b[1m\u001b[32m0.03529\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 009 | loss: 0.03529 - R2: 0.9989 -- iter: 0640/1168\n",
      "Training Step: 505  | total loss: \u001b[1m\u001b[32m0.03341\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 009 | loss: 0.03341 - R2: 0.9991 -- iter: 0660/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 506  | total loss: \u001b[1m\u001b[32m0.03153\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 009 | loss: 0.03153 - R2: 0.9991 -- iter: 0680/1168\n",
      "Training Step: 507  | total loss: \u001b[1m\u001b[32m0.02967\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 009 | loss: 0.02967 - R2: 0.9998 -- iter: 0700/1168\n",
      "Training Step: 508  | total loss: \u001b[1m\u001b[32m0.02901\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 009 | loss: 0.02901 - R2: 0.9988 -- iter: 0720/1168\n",
      "Training Step: 509  | total loss: \u001b[1m\u001b[32m0.02801\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 009 | loss: 0.02801 - R2: 0.9991 -- iter: 0740/1168\n",
      "Training Step: 510  | total loss: \u001b[1m\u001b[32m0.03401\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 009 | loss: 0.03401 - R2: 0.9997 -- iter: 0760/1168\n",
      "Training Step: 511  | total loss: \u001b[1m\u001b[32m0.03401\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 009 | loss: 0.03401 - R2: 0.9997 -- iter: 0780/1168\n",
      "Training Step: 512  | total loss: \u001b[1m\u001b[32m0.02988\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 009 | loss: 0.02988 - R2: 0.9996 -- iter: 0800/1168\n",
      "Training Step: 513  | total loss: \u001b[1m\u001b[32m0.02988\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 009 | loss: 0.02988 - R2: 0.9996 -- iter: 0820/1168\n",
      "Training Step: 514  | total loss: \u001b[1m\u001b[32m0.02871\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 009 | loss: 0.02871 - R2: 1.0002 -- iter: 0840/1168\n",
      "Training Step: 515  | total loss: \u001b[1m\u001b[32m0.03029\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 009 | loss: 0.03029 - R2: 1.0002 -- iter: 0860/1168\n",
      "Training Step: 516  | total loss: \u001b[1m\u001b[32m0.02853\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 009 | loss: 0.02853 - R2: 1.0002 -- iter: 0880/1168\n",
      "Training Step: 517  | total loss: \u001b[1m\u001b[32m0.02779\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 009 | loss: 0.02779 - R2: 0.9997 -- iter: 0900/1168\n",
      "Training Step: 518  | total loss: \u001b[1m\u001b[32m0.02779\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 009 | loss: 0.02779 - R2: 0.9997 -- iter: 0920/1168\n",
      "Training Step: 519  | total loss: \u001b[1m\u001b[32m0.02753\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 009 | loss: 0.02753 - R2: 0.9997 -- iter: 0940/1168\n",
      "Training Step: 520  | total loss: \u001b[1m\u001b[32m0.02594\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 009 | loss: 0.02594 - R2: 0.9997 -- iter: 0960/1168\n",
      "Training Step: 521  | total loss: \u001b[1m\u001b[32m0.02451\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 009 | loss: 0.02451 - R2: 1.0000 -- iter: 0980/1168\n",
      "Training Step: 522  | total loss: \u001b[1m\u001b[32m0.02299\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 009 | loss: 0.02299 - R2: 1.0001 -- iter: 1000/1168\n",
      "Training Step: 523  | total loss: \u001b[1m\u001b[32m0.02177\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 009 | loss: 0.02177 - R2: 0.9998 -- iter: 1020/1168\n",
      "Training Step: 524  | total loss: \u001b[1m\u001b[32m0.02062\u001b[0m\u001b[0m | time: 0.178s\n",
      "| SGD | epoch: 009 | loss: 0.02062 - R2: 0.9996 -- iter: 1040/1168\n",
      "Training Step: 525  | total loss: \u001b[1m\u001b[32m0.02062\u001b[0m\u001b[0m | time: 0.181s\n",
      "| SGD | epoch: 009 | loss: 0.02062 - R2: 0.9996 -- iter: 1060/1168\n",
      "Training Step: 526  | total loss: \u001b[1m\u001b[32m0.02018\u001b[0m\u001b[0m | time: 0.183s\n",
      "| SGD | epoch: 009 | loss: 0.02018 - R2: 0.9994 -- iter: 1080/1168\n",
      "Training Step: 527  | total loss: \u001b[1m\u001b[32m0.01905\u001b[0m\u001b[0m | time: 0.185s\n",
      "| SGD | epoch: 009 | loss: 0.01905 - R2: 0.9994 -- iter: 1100/1168\n",
      "Training Step: 528  | total loss: \u001b[1m\u001b[32m0.02227\u001b[0m\u001b[0m | time: 0.187s\n",
      "| SGD | epoch: 009 | loss: 0.02227 - R2: 0.9995 -- iter: 1120/1168\n",
      "Training Step: 529  | total loss: \u001b[1m\u001b[32m0.02056\u001b[0m\u001b[0m | time: 0.189s\n",
      "| SGD | epoch: 009 | loss: 0.02056 - R2: 0.9995 -- iter: 1140/1168\n",
      "Training Step: 530  | total loss: \u001b[1m\u001b[32m0.01947\u001b[0m\u001b[0m | time: 0.191s\n",
      "| SGD | epoch: 009 | loss: 0.01947 - R2: 0.9993 -- iter: 1160/1168\n",
      "Training Step: 531  | total loss: \u001b[1m\u001b[32m0.01916\u001b[0m\u001b[0m | time: 1.197s\n",
      "| SGD | epoch: 009 | loss: 0.01916 - R2: 0.9990 | val_loss: 0.02479 - val_acc: 1.0033 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 532  | total loss: \u001b[1m\u001b[32m0.01984\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 010 | loss: 0.01984 - R2: 1.0002 -- iter: 0020/1168\n",
      "Training Step: 533  | total loss: \u001b[1m\u001b[32m0.01984\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 010 | loss: 0.01984 - R2: 1.0002 -- iter: 0040/1168\n",
      "Training Step: 534  | total loss: \u001b[1m\u001b[32m0.01937\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 010 | loss: 0.01937 - R2: 1.0011 -- iter: 0060/1168\n",
      "Training Step: 535  | total loss: \u001b[1m\u001b[32m0.01937\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 010 | loss: 0.01937 - R2: 1.0011 -- iter: 0080/1168\n",
      "Training Step: 536  | total loss: \u001b[1m\u001b[32m0.01953\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 010 | loss: 0.01953 - R2: 1.0009 -- iter: 0100/1168\n",
      "Training Step: 537  | total loss: \u001b[1m\u001b[32m0.02387\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 010 | loss: 0.02387 - R2: 0.9999 -- iter: 0120/1168\n",
      "Training Step: 538  | total loss: \u001b[1m\u001b[32m0.02247\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 010 | loss: 0.02247 - R2: 1.0002 -- iter: 0140/1168\n",
      "Training Step: 539  | total loss: \u001b[1m\u001b[32m0.02247\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 010 | loss: 0.02247 - R2: 1.0002 -- iter: 0160/1168\n",
      "Training Step: 540  | total loss: \u001b[1m\u001b[32m0.02281\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 010 | loss: 0.02281 - R2: 1.0000 -- iter: 0180/1168\n",
      "Training Step: 541  | total loss: \u001b[1m\u001b[32m0.02106\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 010 | loss: 0.02106 - R2: 1.0001 -- iter: 0200/1168\n",
      "Training Step: 542  | total loss: \u001b[1m\u001b[32m0.02106\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 010 | loss: 0.02106 - R2: 1.0001 -- iter: 0220/1168\n",
      "Training Step: 543  | total loss: \u001b[1m\u001b[32m0.02158\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 010 | loss: 0.02158 - R2: 0.9996 -- iter: 0240/1168\n",
      "Training Step: 544  | total loss: \u001b[1m\u001b[32m0.02206\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 010 | loss: 0.02206 - R2: 0.9998 -- iter: 0260/1168\n",
      "Training Step: 545  | total loss: \u001b[1m\u001b[32m0.02093\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 010 | loss: 0.02093 - R2: 0.9994 -- iter: 0280/1168\n",
      "Training Step: 546  | total loss: \u001b[1m\u001b[32m0.02093\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 010 | loss: 0.02093 - R2: 0.9994 -- iter: 0300/1168\n",
      "Training Step: 547  | total loss: \u001b[1m\u001b[32m0.02047\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 010 | loss: 0.02047 - R2: 1.0002 -- iter: 0320/1168\n",
      "Training Step: 548  | total loss: \u001b[1m\u001b[32m0.02047\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 010 | loss: 0.02047 - R2: 1.0002 -- iter: 0340/1168\n",
      "Training Step: 549  | total loss: \u001b[1m\u001b[32m0.01990\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 010 | loss: 0.01990 - R2: 0.9994 -- iter: 0360/1168\n",
      "Training Step: 550  | total loss: \u001b[1m\u001b[32m0.01881\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 010 | loss: 0.01881 - R2: 0.9992 -- iter: 0380/1168\n",
      "Training Step: 551  | total loss: \u001b[1m\u001b[32m0.01779\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 010 | loss: 0.01779 - R2: 0.9988 -- iter: 0400/1168\n",
      "Training Step: 552  | total loss: \u001b[1m\u001b[32m0.01779\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 010 | loss: 0.01779 - R2: 0.9988 -- iter: 0420/1168\n",
      "Training Step: 553  | total loss: \u001b[1m\u001b[32m0.01771\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 010 | loss: 0.01771 - R2: 0.9990 -- iter: 0440/1168\n",
      "Training Step: 554  | total loss: \u001b[1m\u001b[32m0.01721\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 010 | loss: 0.01721 - R2: 1.0004 -- iter: 0460/1168\n",
      "Training Step: 555  | total loss: \u001b[1m\u001b[32m0.01721\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 010 | loss: 0.01721 - R2: 1.0004 -- iter: 0480/1168\n",
      "Training Step: 556  | total loss: \u001b[1m\u001b[32m0.01710\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 010 | loss: 0.01710 - R2: 1.0005 -- iter: 0500/1168\n",
      "Training Step: 557  | total loss: \u001b[1m\u001b[32m0.01694\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 010 | loss: 0.01694 - R2: 1.0006 -- iter: 0520/1168\n",
      "Training Step: 558  | total loss: \u001b[1m\u001b[32m0.01635\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 010 | loss: 0.01635 - R2: 0.9998 -- iter: 0540/1168\n",
      "Training Step: 559  | total loss: \u001b[1m\u001b[32m0.01636\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 010 | loss: 0.01636 - R2: 1.0003 -- iter: 0560/1168\n",
      "Training Step: 560  | total loss: \u001b[1m\u001b[32m0.01649\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 010 | loss: 0.01649 - R2: 1.0003 -- iter: 0580/1168\n",
      "Training Step: 561  | total loss: \u001b[1m\u001b[32m0.01546\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 010 | loss: 0.01546 - R2: 0.9997 -- iter: 0600/1168\n",
      "Training Step: 562  | total loss: \u001b[1m\u001b[32m0.01546\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 010 | loss: 0.01546 - R2: 0.9997 -- iter: 0620/1168\n",
      "Training Step: 563  | total loss: \u001b[1m\u001b[32m0.01619\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 010 | loss: 0.01619 - R2: 0.9999 -- iter: 0640/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 564  | total loss: \u001b[1m\u001b[32m0.01579\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 010 | loss: 0.01579 - R2: 1.0002 -- iter: 0660/1168\n",
      "Training Step: 565  | total loss: \u001b[1m\u001b[32m0.01579\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 010 | loss: 0.01579 - R2: 1.0002 -- iter: 0680/1168\n",
      "Training Step: 566  | total loss: \u001b[1m\u001b[32m0.03007\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 010 | loss: 0.03007 - R2: 1.0011 -- iter: 0700/1168\n",
      "Training Step: 567  | total loss: \u001b[1m\u001b[32m0.03007\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 010 | loss: 0.03007 - R2: 1.0011 -- iter: 0720/1168\n",
      "Training Step: 568  | total loss: \u001b[1m\u001b[32m0.02827\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 010 | loss: 0.02827 - R2: 0.9996 -- iter: 0740/1168\n",
      "Training Step: 569  | total loss: \u001b[1m\u001b[32m0.02827\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 010 | loss: 0.02827 - R2: 0.9996 -- iter: 0760/1168\n",
      "Training Step: 570  | total loss: \u001b[1m\u001b[32m0.02656\u001b[0m\u001b[0m | time: 0.165s\n",
      "| SGD | epoch: 010 | loss: 0.02656 - R2: 0.9997 -- iter: 0780/1168\n",
      "Training Step: 571  | total loss: \u001b[1m\u001b[32m0.02524\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 010 | loss: 0.02524 - R2: 1.0006 -- iter: 0800/1168\n",
      "Training Step: 572  | total loss: \u001b[1m\u001b[32m0.02457\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 010 | loss: 0.02457 - R2: 1.0003 -- iter: 0820/1168\n",
      "Training Step: 573  | total loss: \u001b[1m\u001b[32m0.02417\u001b[0m\u001b[0m | time: 0.202s\n",
      "| SGD | epoch: 010 | loss: 0.02417 - R2: 1.0000 -- iter: 0840/1168\n",
      "Training Step: 574  | total loss: \u001b[1m\u001b[32m0.02458\u001b[0m\u001b[0m | time: 0.205s\n",
      "| SGD | epoch: 010 | loss: 0.02458 - R2: 0.9995 -- iter: 0860/1168\n",
      "Training Step: 575  | total loss: \u001b[1m\u001b[32m0.02458\u001b[0m\u001b[0m | time: 0.208s\n",
      "| SGD | epoch: 010 | loss: 0.02458 - R2: 0.9995 -- iter: 0880/1168\n",
      "Training Step: 576  | total loss: \u001b[1m\u001b[32m0.02404\u001b[0m\u001b[0m | time: 0.211s\n",
      "| SGD | epoch: 010 | loss: 0.02404 - R2: 0.9999 -- iter: 0900/1168\n",
      "Training Step: 577  | total loss: \u001b[1m\u001b[32m0.02375\u001b[0m\u001b[0m | time: 0.213s\n",
      "| SGD | epoch: 010 | loss: 0.02375 - R2: 0.9999 -- iter: 0920/1168\n",
      "Training Step: 578  | total loss: \u001b[1m\u001b[32m0.02894\u001b[0m\u001b[0m | time: 0.214s\n",
      "| SGD | epoch: 010 | loss: 0.02894 - R2: 1.0014 -- iter: 0940/1168\n",
      "Training Step: 579  | total loss: \u001b[1m\u001b[32m0.02716\u001b[0m\u001b[0m | time: 0.217s\n",
      "| SGD | epoch: 010 | loss: 0.02716 - R2: 1.0016 -- iter: 0960/1168\n",
      "Training Step: 580  | total loss: \u001b[1m\u001b[32m0.02785\u001b[0m\u001b[0m | time: 0.218s\n",
      "| SGD | epoch: 010 | loss: 0.02785 - R2: 0.9992 -- iter: 0980/1168\n",
      "Training Step: 581  | total loss: \u001b[1m\u001b[32m0.02555\u001b[0m\u001b[0m | time: 0.223s\n",
      "| SGD | epoch: 010 | loss: 0.02555 - R2: 0.9989 -- iter: 1000/1168\n",
      "Training Step: 582  | total loss: \u001b[1m\u001b[32m0.02411\u001b[0m\u001b[0m | time: 0.225s\n",
      "| SGD | epoch: 010 | loss: 0.02411 - R2: 0.9991 -- iter: 1020/1168\n",
      "Training Step: 583  | total loss: \u001b[1m\u001b[32m0.02498\u001b[0m\u001b[0m | time: 0.227s\n",
      "| SGD | epoch: 010 | loss: 0.02498 - R2: 0.9997 -- iter: 1040/1168\n",
      "Training Step: 584  | total loss: \u001b[1m\u001b[32m0.02333\u001b[0m\u001b[0m | time: 0.228s\n",
      "| SGD | epoch: 010 | loss: 0.02333 - R2: 1.0002 -- iter: 1060/1168\n",
      "Training Step: 585  | total loss: \u001b[1m\u001b[32m0.02165\u001b[0m\u001b[0m | time: 0.230s\n",
      "| SGD | epoch: 010 | loss: 0.02165 - R2: 0.9991 -- iter: 1080/1168\n",
      "Training Step: 586  | total loss: \u001b[1m\u001b[32m0.02165\u001b[0m\u001b[0m | time: 0.232s\n",
      "| SGD | epoch: 010 | loss: 0.02165 - R2: 0.9991 -- iter: 1100/1168\n",
      "Training Step: 587  | total loss: \u001b[1m\u001b[32m0.02082\u001b[0m\u001b[0m | time: 0.236s\n",
      "| SGD | epoch: 010 | loss: 0.02082 - R2: 0.9983 -- iter: 1120/1168\n",
      "Training Step: 588  | total loss: \u001b[1m\u001b[32m0.02082\u001b[0m\u001b[0m | time: 0.237s\n",
      "| SGD | epoch: 010 | loss: 0.02082 - R2: 0.9983 -- iter: 1140/1168\n",
      "Training Step: 589  | total loss: \u001b[1m\u001b[32m0.02539\u001b[0m\u001b[0m | time: 0.239s\n",
      "| SGD | epoch: 010 | loss: 0.02539 - R2: 0.9976 -- iter: 1160/1168\n",
      "Training Step: 590  | total loss: \u001b[1m\u001b[32m0.02353\u001b[0m\u001b[0m | time: 1.254s\n",
      "| SGD | epoch: 010 | loss: 0.02353 - R2: 0.9987 | val_loss: 0.02629 - val_acc: 1.0062 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 591  | total loss: \u001b[1m\u001b[32m0.02126\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 011 | loss: 0.02126 - R2: 0.9989 -- iter: 0020/1168\n",
      "Training Step: 592  | total loss: \u001b[1m\u001b[32m0.02126\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 011 | loss: 0.02126 - R2: 0.9989 -- iter: 0040/1168\n",
      "Training Step: 593  | total loss: \u001b[1m\u001b[32m0.02428\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 011 | loss: 0.02428 - R2: 0.9998 -- iter: 0060/1168\n",
      "Training Step: 594  | total loss: \u001b[1m\u001b[32m0.02355\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 011 | loss: 0.02355 - R2: 1.0009 -- iter: 0080/1168\n",
      "Training Step: 595  | total loss: \u001b[1m\u001b[32m0.02281\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 011 | loss: 0.02281 - R2: 1.0008 -- iter: 0100/1168\n",
      "Training Step: 596  | total loss: \u001b[1m\u001b[32m0.02281\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 011 | loss: 0.02281 - R2: 1.0008 -- iter: 0120/1168\n",
      "Training Step: 597  | total loss: \u001b[1m\u001b[32m0.02082\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 011 | loss: 0.02082 - R2: 1.0005 -- iter: 0140/1168\n",
      "Training Step: 598  | total loss: \u001b[1m\u001b[32m0.02093\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 011 | loss: 0.02093 - R2: 1.0004 -- iter: 0160/1168\n",
      "Training Step: 599  | total loss: \u001b[1m\u001b[32m0.03135\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 011 | loss: 0.03135 - R2: 1.0034 -- iter: 0180/1168\n",
      "Training Step: 600  | total loss: \u001b[1m\u001b[32m0.03135\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 011 | loss: 0.03135 - R2: 1.0034 -- iter: 0200/1168\n",
      "Training Step: 601  | total loss: \u001b[1m\u001b[32m0.03833\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 011 | loss: 0.03833 - R2: 1.0052 -- iter: 0220/1168\n",
      "Training Step: 602  | total loss: \u001b[1m\u001b[32m0.03715\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 011 | loss: 0.03715 - R2: 1.0027 -- iter: 0240/1168\n",
      "Training Step: 603  | total loss: \u001b[1m\u001b[32m0.03299\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 011 | loss: 0.03299 - R2: 1.0010 -- iter: 0260/1168\n",
      "Training Step: 604  | total loss: \u001b[1m\u001b[32m0.03110\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 011 | loss: 0.03110 - R2: 1.0004 -- iter: 0280/1168\n",
      "Training Step: 605  | total loss: \u001b[1m\u001b[32m0.03083\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 011 | loss: 0.03083 - R2: 0.9993 -- iter: 0300/1168\n",
      "Training Step: 606  | total loss: \u001b[1m\u001b[32m0.02868\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 011 | loss: 0.02868 - R2: 0.9995 -- iter: 0320/1168\n",
      "Training Step: 607  | total loss: \u001b[1m\u001b[32m0.02868\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 011 | loss: 0.02868 - R2: 0.9995 -- iter: 0340/1168\n",
      "Training Step: 608  | total loss: \u001b[1m\u001b[32m0.02612\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 011 | loss: 0.02612 - R2: 0.9988 -- iter: 0360/1168\n",
      "Training Step: 609  | total loss: \u001b[1m\u001b[32m0.02631\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 011 | loss: 0.02631 - R2: 0.9980 -- iter: 0380/1168\n",
      "Training Step: 610  | total loss: \u001b[1m\u001b[32m0.02547\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 011 | loss: 0.02547 - R2: 0.9979 -- iter: 0400/1168\n",
      "Training Step: 611  | total loss: \u001b[1m\u001b[32m0.02547\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 011 | loss: 0.02547 - R2: 0.9979 -- iter: 0420/1168\n",
      "Training Step: 612  | total loss: \u001b[1m\u001b[32m0.02451\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 011 | loss: 0.02451 - R2: 0.9989 -- iter: 0440/1168\n",
      "Training Step: 613  | total loss: \u001b[1m\u001b[32m0.02225\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 011 | loss: 0.02225 - R2: 1.0003 -- iter: 0460/1168\n",
      "Training Step: 614  | total loss: \u001b[1m\u001b[32m0.02352\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 011 | loss: 0.02352 - R2: 1.0017 -- iter: 0480/1168\n",
      "Training Step: 615  | total loss: \u001b[1m\u001b[32m0.02432\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 011 | loss: 0.02432 - R2: 1.0005 -- iter: 0500/1168\n",
      "Training Step: 616  | total loss: \u001b[1m\u001b[32m0.02390\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 011 | loss: 0.02390 - R2: 1.0001 -- iter: 0520/1168\n",
      "Training Step: 617  | total loss: \u001b[1m\u001b[32m0.02311\u001b[0m\u001b[0m | time: 0.172s\n",
      "| SGD | epoch: 011 | loss: 0.02311 - R2: 1.0004 -- iter: 0540/1168\n",
      "Training Step: 618  | total loss: \u001b[1m\u001b[32m0.02172\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 011 | loss: 0.02172 - R2: 1.0002 -- iter: 0560/1168\n",
      "Training Step: 619  | total loss: \u001b[1m\u001b[32m0.02172\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 011 | loss: 0.02172 - R2: 1.0002 -- iter: 0580/1168\n",
      "Training Step: 620  | total loss: \u001b[1m\u001b[32m0.02144\u001b[0m\u001b[0m | time: 0.179s\n",
      "| SGD | epoch: 011 | loss: 0.02144 - R2: 1.0002 -- iter: 0600/1168\n",
      "Training Step: 621  | total loss: \u001b[1m\u001b[32m0.02536\u001b[0m\u001b[0m | time: 0.182s\n",
      "| SGD | epoch: 011 | loss: 0.02536 - R2: 0.9997 -- iter: 0620/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 622  | total loss: \u001b[1m\u001b[32m0.02536\u001b[0m\u001b[0m | time: 0.185s\n",
      "| SGD | epoch: 011 | loss: 0.02536 - R2: 0.9997 -- iter: 0640/1168\n",
      "Training Step: 623  | total loss: \u001b[1m\u001b[32m0.02470\u001b[0m\u001b[0m | time: 0.187s\n",
      "| SGD | epoch: 011 | loss: 0.02470 - R2: 0.9991 -- iter: 0660/1168\n",
      "Training Step: 624  | total loss: \u001b[1m\u001b[32m0.02470\u001b[0m\u001b[0m | time: 0.190s\n",
      "| SGD | epoch: 011 | loss: 0.02470 - R2: 0.9991 -- iter: 0680/1168\n",
      "Training Step: 625  | total loss: \u001b[1m\u001b[32m0.02437\u001b[0m\u001b[0m | time: 0.193s\n",
      "| SGD | epoch: 011 | loss: 0.02437 - R2: 0.9994 -- iter: 0700/1168\n",
      "Training Step: 626  | total loss: \u001b[1m\u001b[32m0.02444\u001b[0m\u001b[0m | time: 0.195s\n",
      "| SGD | epoch: 011 | loss: 0.02444 - R2: 0.9990 -- iter: 0720/1168\n",
      "Training Step: 627  | total loss: \u001b[1m\u001b[32m0.02343\u001b[0m\u001b[0m | time: 0.196s\n",
      "| SGD | epoch: 011 | loss: 0.02343 - R2: 0.9990 -- iter: 0740/1168\n",
      "Training Step: 628  | total loss: \u001b[1m\u001b[32m0.02715\u001b[0m\u001b[0m | time: 0.199s\n",
      "| SGD | epoch: 011 | loss: 0.02715 - R2: 1.0001 -- iter: 0760/1168\n",
      "Training Step: 629  | total loss: \u001b[1m\u001b[32m0.02579\u001b[0m\u001b[0m | time: 0.202s\n",
      "| SGD | epoch: 011 | loss: 0.02579 - R2: 1.0006 -- iter: 0780/1168\n",
      "Training Step: 630  | total loss: \u001b[1m\u001b[32m0.02442\u001b[0m\u001b[0m | time: 0.216s\n",
      "| SGD | epoch: 011 | loss: 0.02442 - R2: 1.0002 -- iter: 0800/1168\n",
      "Training Step: 631  | total loss: \u001b[1m\u001b[32m0.02311\u001b[0m\u001b[0m | time: 0.220s\n",
      "| SGD | epoch: 011 | loss: 0.02311 - R2: 1.0000 -- iter: 0820/1168\n",
      "Training Step: 632  | total loss: \u001b[1m\u001b[32m0.02311\u001b[0m\u001b[0m | time: 0.224s\n",
      "| SGD | epoch: 011 | loss: 0.02311 - R2: 1.0000 -- iter: 0840/1168\n",
      "Training Step: 633  | total loss: \u001b[1m\u001b[32m0.02194\u001b[0m\u001b[0m | time: 0.226s\n",
      "| SGD | epoch: 011 | loss: 0.02194 - R2: 0.9996 -- iter: 0860/1168\n",
      "Training Step: 634  | total loss: \u001b[1m\u001b[32m0.02084\u001b[0m\u001b[0m | time: 0.228s\n",
      "| SGD | epoch: 011 | loss: 0.02084 - R2: 0.9996 -- iter: 0880/1168\n",
      "Training Step: 635  | total loss: \u001b[1m\u001b[32m0.02084\u001b[0m\u001b[0m | time: 0.231s\n",
      "| SGD | epoch: 011 | loss: 0.02084 - R2: 0.9996 -- iter: 0900/1168\n",
      "Training Step: 636  | total loss: \u001b[1m\u001b[32m0.02134\u001b[0m\u001b[0m | time: 0.236s\n",
      "| SGD | epoch: 011 | loss: 0.02134 - R2: 0.9997 -- iter: 0920/1168\n",
      "Training Step: 637  | total loss: \u001b[1m\u001b[32m0.02134\u001b[0m\u001b[0m | time: 0.240s\n",
      "| SGD | epoch: 011 | loss: 0.02134 - R2: 0.9997 -- iter: 0940/1168\n",
      "Training Step: 638  | total loss: \u001b[1m\u001b[32m0.01944\u001b[0m\u001b[0m | time: 0.243s\n",
      "| SGD | epoch: 011 | loss: 0.01944 - R2: 0.9994 -- iter: 0960/1168\n",
      "Training Step: 639  | total loss: \u001b[1m\u001b[32m0.01944\u001b[0m\u001b[0m | time: 0.245s\n",
      "| SGD | epoch: 011 | loss: 0.01944 - R2: 0.9994 -- iter: 0980/1168\n",
      "Training Step: 640  | total loss: \u001b[1m\u001b[32m0.01824\u001b[0m\u001b[0m | time: 0.249s\n",
      "| SGD | epoch: 011 | loss: 0.01824 - R2: 0.9991 -- iter: 1000/1168\n",
      "Training Step: 641  | total loss: \u001b[1m\u001b[32m0.01704\u001b[0m\u001b[0m | time: 0.253s\n",
      "| SGD | epoch: 011 | loss: 0.01704 - R2: 0.9989 -- iter: 1020/1168\n",
      "Training Step: 642  | total loss: \u001b[1m\u001b[32m0.01649\u001b[0m\u001b[0m | time: 0.256s\n",
      "| SGD | epoch: 011 | loss: 0.01649 - R2: 0.9989 -- iter: 1040/1168\n",
      "Training Step: 643  | total loss: \u001b[1m\u001b[32m0.01591\u001b[0m\u001b[0m | time: 0.257s\n",
      "| SGD | epoch: 011 | loss: 0.01591 - R2: 0.9995 -- iter: 1060/1168\n",
      "Training Step: 644  | total loss: \u001b[1m\u001b[32m0.01616\u001b[0m\u001b[0m | time: 0.260s\n",
      "| SGD | epoch: 011 | loss: 0.01616 - R2: 0.9998 -- iter: 1080/1168\n",
      "Training Step: 645  | total loss: \u001b[1m\u001b[32m0.01636\u001b[0m\u001b[0m | time: 0.262s\n",
      "| SGD | epoch: 011 | loss: 0.01636 - R2: 0.9993 -- iter: 1100/1168\n",
      "Training Step: 646  | total loss: \u001b[1m\u001b[32m0.01715\u001b[0m\u001b[0m | time: 0.266s\n",
      "| SGD | epoch: 011 | loss: 0.01715 - R2: 0.9996 -- iter: 1120/1168\n",
      "Training Step: 647  | total loss: \u001b[1m\u001b[32m0.01631\u001b[0m\u001b[0m | time: 0.270s\n",
      "| SGD | epoch: 011 | loss: 0.01631 - R2: 1.0001 -- iter: 1140/1168\n",
      "Training Step: 648  | total loss: \u001b[1m\u001b[32m0.01631\u001b[0m\u001b[0m | time: 0.272s\n",
      "| SGD | epoch: 011 | loss: 0.01631 - R2: 1.0001 -- iter: 1160/1168\n",
      "Training Step: 649  | total loss: \u001b[1m\u001b[32m0.01679\u001b[0m\u001b[0m | time: 1.293s\n",
      "| SGD | epoch: 011 | loss: 0.01679 - R2: 1.0000 | val_loss: 0.02361 - val_acc: 1.0006 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 650  | total loss: \u001b[1m\u001b[32m0.01644\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 012 | loss: 0.01644 - R2: 1.0004 -- iter: 0020/1168\n",
      "Training Step: 651  | total loss: \u001b[1m\u001b[32m0.01646\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 012 | loss: 0.01646 - R2: 1.0005 -- iter: 0040/1168\n",
      "Training Step: 652  | total loss: \u001b[1m\u001b[32m0.01646\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 012 | loss: 0.01646 - R2: 1.0005 -- iter: 0060/1168\n",
      "Training Step: 653  | total loss: \u001b[1m\u001b[32m0.01553\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 012 | loss: 0.01553 - R2: 0.9998 -- iter: 0080/1168\n",
      "Training Step: 654  | total loss: \u001b[1m\u001b[32m0.01560\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 012 | loss: 0.01560 - R2: 1.0001 -- iter: 0100/1168\n",
      "Training Step: 655  | total loss: \u001b[1m\u001b[32m0.01616\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 012 | loss: 0.01616 - R2: 0.9999 -- iter: 0120/1168\n",
      "Training Step: 656  | total loss: \u001b[1m\u001b[32m0.01616\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 012 | loss: 0.01616 - R2: 0.9999 -- iter: 0140/1168\n",
      "Training Step: 657  | total loss: \u001b[1m\u001b[32m0.01685\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 012 | loss: 0.01685 - R2: 0.9997 -- iter: 0160/1168\n",
      "Training Step: 658  | total loss: \u001b[1m\u001b[32m0.01680\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 012 | loss: 0.01680 - R2: 0.9997 -- iter: 0180/1168\n",
      "Training Step: 659  | total loss: \u001b[1m\u001b[32m0.01755\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 012 | loss: 0.01755 - R2: 0.9990 -- iter: 0200/1168\n",
      "Training Step: 660  | total loss: \u001b[1m\u001b[32m0.02241\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 012 | loss: 0.02241 - R2: 0.9993 -- iter: 0220/1168\n",
      "Training Step: 661  | total loss: \u001b[1m\u001b[32m0.02241\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 012 | loss: 0.02241 - R2: 0.9993 -- iter: 0240/1168\n",
      "Training Step: 662  | total loss: \u001b[1m\u001b[32m0.02115\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 012 | loss: 0.02115 - R2: 0.9998 -- iter: 0260/1168\n",
      "Training Step: 663  | total loss: \u001b[1m\u001b[32m0.02061\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 012 | loss: 0.02061 - R2: 0.9998 -- iter: 0280/1168\n",
      "Training Step: 664  | total loss: \u001b[1m\u001b[32m0.01936\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 012 | loss: 0.01936 - R2: 1.0012 -- iter: 0300/1168\n",
      "Training Step: 665  | total loss: \u001b[1m\u001b[32m0.01936\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 012 | loss: 0.01936 - R2: 1.0012 -- iter: 0320/1168\n",
      "Training Step: 666  | total loss: \u001b[1m\u001b[32m0.02018\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 012 | loss: 0.02018 - R2: 1.0006 -- iter: 0340/1168\n",
      "Training Step: 667  | total loss: \u001b[1m\u001b[32m0.01982\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 012 | loss: 0.01982 - R2: 1.0004 -- iter: 0360/1168\n",
      "Training Step: 668  | total loss: \u001b[1m\u001b[32m0.02185\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 012 | loss: 0.02185 - R2: 0.9998 -- iter: 0380/1168\n",
      "Training Step: 669  | total loss: \u001b[1m\u001b[32m0.02157\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 012 | loss: 0.02157 - R2: 0.9993 -- iter: 0400/1168\n",
      "Training Step: 670  | total loss: \u001b[1m\u001b[32m0.02054\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 012 | loss: 0.02054 - R2: 0.9995 -- iter: 0420/1168\n",
      "Training Step: 671  | total loss: \u001b[1m\u001b[32m0.02023\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 012 | loss: 0.02023 - R2: 1.0003 -- iter: 0440/1168\n",
      "Training Step: 672  | total loss: \u001b[1m\u001b[32m0.01999\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 012 | loss: 0.01999 - R2: 1.0004 -- iter: 0460/1168\n",
      "Training Step: 673  | total loss: \u001b[1m\u001b[32m0.01971\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 012 | loss: 0.01971 - R2: 1.0004 -- iter: 0480/1168\n",
      "Training Step: 674  | total loss: \u001b[1m\u001b[32m0.01897\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 012 | loss: 0.01897 - R2: 1.0003 -- iter: 0500/1168\n",
      "Training Step: 675  | total loss: \u001b[1m\u001b[32m0.02025\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 012 | loss: 0.02025 - R2: 1.0003 -- iter: 0520/1168\n",
      "Training Step: 676  | total loss: \u001b[1m\u001b[32m0.01927\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 012 | loss: 0.01927 - R2: 0.9989 -- iter: 0540/1168\n",
      "Training Step: 677  | total loss: \u001b[1m\u001b[32m0.01861\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 012 | loss: 0.01861 - R2: 0.9990 -- iter: 0560/1168\n",
      "Training Step: 678  | total loss: \u001b[1m\u001b[32m0.01861\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 012 | loss: 0.01861 - R2: 0.9990 -- iter: 0580/1168\n",
      "Training Step: 679  | total loss: \u001b[1m\u001b[32m0.01812\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 012 | loss: 0.01812 - R2: 0.9991 -- iter: 0600/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 680  | total loss: \u001b[1m\u001b[32m0.01812\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 012 | loss: 0.01812 - R2: 0.9988 -- iter: 0620/1168\n",
      "Training Step: 681  | total loss: \u001b[1m\u001b[32m0.01765\u001b[0m\u001b[0m | time: 0.165s\n",
      "| SGD | epoch: 012 | loss: 0.01765 - R2: 0.9993 -- iter: 0640/1168\n",
      "Training Step: 682  | total loss: \u001b[1m\u001b[32m0.01756\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 012 | loss: 0.01756 - R2: 1.0006 -- iter: 0660/1168\n",
      "Training Step: 683  | total loss: \u001b[1m\u001b[32m0.03397\u001b[0m\u001b[0m | time: 0.169s\n",
      "| SGD | epoch: 012 | loss: 0.03397 - R2: 1.0008 -- iter: 0680/1168\n",
      "Training Step: 684  | total loss: \u001b[1m\u001b[32m0.03586\u001b[0m\u001b[0m | time: 0.171s\n",
      "| SGD | epoch: 012 | loss: 0.03586 - R2: 1.0008 -- iter: 0700/1168\n",
      "Training Step: 685  | total loss: \u001b[1m\u001b[32m0.03349\u001b[0m\u001b[0m | time: 0.173s\n",
      "| SGD | epoch: 012 | loss: 0.03349 - R2: 1.0001 -- iter: 0720/1168\n",
      "Training Step: 686  | total loss: \u001b[1m\u001b[32m0.03309\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 012 | loss: 0.03309 - R2: 1.0012 -- iter: 0740/1168\n",
      "Training Step: 687  | total loss: \u001b[1m\u001b[32m0.03032\u001b[0m\u001b[0m | time: 0.179s\n",
      "| SGD | epoch: 012 | loss: 0.03032 - R2: 1.0005 -- iter: 0760/1168\n",
      "Training Step: 688  | total loss: \u001b[1m\u001b[32m0.03032\u001b[0m\u001b[0m | time: 0.191s\n",
      "| SGD | epoch: 012 | loss: 0.03032 - R2: 1.0005 -- iter: 0780/1168\n",
      "Training Step: 689  | total loss: \u001b[1m\u001b[32m0.02609\u001b[0m\u001b[0m | time: 0.194s\n",
      "| SGD | epoch: 012 | loss: 0.02609 - R2: 1.0004 -- iter: 0800/1168\n",
      "Training Step: 690  | total loss: \u001b[1m\u001b[32m0.02569\u001b[0m\u001b[0m | time: 0.197s\n",
      "| SGD | epoch: 012 | loss: 0.02569 - R2: 0.9991 -- iter: 0820/1168\n",
      "Training Step: 691  | total loss: \u001b[1m\u001b[32m0.02432\u001b[0m\u001b[0m | time: 0.200s\n",
      "| SGD | epoch: 012 | loss: 0.02432 - R2: 0.9991 -- iter: 0840/1168\n",
      "Training Step: 692  | total loss: \u001b[1m\u001b[32m0.02432\u001b[0m\u001b[0m | time: 0.202s\n",
      "| SGD | epoch: 012 | loss: 0.02432 - R2: 0.9991 -- iter: 0860/1168\n",
      "Training Step: 693  | total loss: \u001b[1m\u001b[32m0.02410\u001b[0m\u001b[0m | time: 0.205s\n",
      "| SGD | epoch: 012 | loss: 0.02410 - R2: 0.9992 -- iter: 0880/1168\n",
      "Training Step: 694  | total loss: \u001b[1m\u001b[32m0.02410\u001b[0m\u001b[0m | time: 0.208s\n",
      "| SGD | epoch: 012 | loss: 0.02410 - R2: 0.9992 -- iter: 0900/1168\n",
      "Training Step: 695  | total loss: \u001b[1m\u001b[32m0.02409\u001b[0m\u001b[0m | time: 0.212s\n",
      "| SGD | epoch: 012 | loss: 0.02409 - R2: 0.9989 -- iter: 0920/1168\n",
      "Training Step: 696  | total loss: \u001b[1m\u001b[32m0.02097\u001b[0m\u001b[0m | time: 0.215s\n",
      "| SGD | epoch: 012 | loss: 0.02097 - R2: 0.9993 -- iter: 0940/1168\n",
      "Training Step: 697  | total loss: \u001b[1m\u001b[32m0.02219\u001b[0m\u001b[0m | time: 0.219s\n",
      "| SGD | epoch: 012 | loss: 0.02219 - R2: 0.9993 -- iter: 0960/1168\n",
      "Training Step: 698  | total loss: \u001b[1m\u001b[32m0.02092\u001b[0m\u001b[0m | time: 0.221s\n",
      "| SGD | epoch: 012 | loss: 0.02092 - R2: 0.9991 -- iter: 0980/1168\n",
      "Training Step: 699  | total loss: \u001b[1m\u001b[32m0.02076\u001b[0m\u001b[0m | time: 0.226s\n",
      "| SGD | epoch: 012 | loss: 0.02076 - R2: 0.9997 -- iter: 1000/1168\n",
      "Training Step: 700  | total loss: \u001b[1m\u001b[32m0.02076\u001b[0m\u001b[0m | time: 0.229s\n",
      "| SGD | epoch: 012 | loss: 0.02076 - R2: 1.0003 -- iter: 1020/1168\n",
      "Training Step: 701  | total loss: \u001b[1m\u001b[32m0.02184\u001b[0m\u001b[0m | time: 0.232s\n",
      "| SGD | epoch: 012 | loss: 0.02184 - R2: 0.9998 -- iter: 1040/1168\n",
      "Training Step: 702  | total loss: \u001b[1m\u001b[32m0.02148\u001b[0m\u001b[0m | time: 0.234s\n",
      "| SGD | epoch: 012 | loss: 0.02148 - R2: 1.0004 -- iter: 1060/1168\n",
      "Training Step: 703  | total loss: \u001b[1m\u001b[32m0.02079\u001b[0m\u001b[0m | time: 0.237s\n",
      "| SGD | epoch: 012 | loss: 0.02079 - R2: 0.9998 -- iter: 1080/1168\n",
      "Training Step: 704  | total loss: \u001b[1m\u001b[32m0.02079\u001b[0m\u001b[0m | time: 0.239s\n",
      "| SGD | epoch: 012 | loss: 0.02079 - R2: 1.0003 -- iter: 1100/1168\n",
      "Training Step: 705  | total loss: \u001b[1m\u001b[32m0.02038\u001b[0m\u001b[0m | time: 0.241s\n",
      "| SGD | epoch: 012 | loss: 0.02038 - R2: 1.0003 -- iter: 1120/1168\n",
      "Training Step: 706  | total loss: \u001b[1m\u001b[32m0.01923\u001b[0m\u001b[0m | time: 0.244s\n",
      "| SGD | epoch: 012 | loss: 0.01923 - R2: 1.0002 -- iter: 1140/1168\n",
      "Training Step: 707  | total loss: \u001b[1m\u001b[32m0.01838\u001b[0m\u001b[0m | time: 0.246s\n",
      "| SGD | epoch: 012 | loss: 0.01838 - R2: 0.9999 -- iter: 1160/1168\n",
      "Training Step: 708  | total loss: \u001b[1m\u001b[32m0.01828\u001b[0m\u001b[0m | time: 1.253s\n",
      "| SGD | epoch: 012 | loss: 0.01828 - R2: 0.9999 | val_loss: 0.02480 - val_acc: 0.9997 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 709  | total loss: \u001b[1m\u001b[32m0.02401\u001b[0m\u001b[0m | time: 0.016s\n",
      "| SGD | epoch: 013 | loss: 0.02401 - R2: 1.0014 -- iter: 0020/1168\n",
      "Training Step: 710  | total loss: \u001b[1m\u001b[32m0.02401\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 013 | loss: 0.02401 - R2: 1.0014 -- iter: 0040/1168\n",
      "Training Step: 711  | total loss: \u001b[1m\u001b[32m0.02208\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 013 | loss: 0.02208 - R2: 1.0005 -- iter: 0060/1168\n",
      "Training Step: 712  | total loss: \u001b[1m\u001b[32m0.02054\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 013 | loss: 0.02054 - R2: 1.0002 -- iter: 0080/1168\n",
      "Training Step: 713  | total loss: \u001b[1m\u001b[32m0.02006\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 013 | loss: 0.02006 - R2: 1.0007 -- iter: 0100/1168\n",
      "Training Step: 714  | total loss: \u001b[1m\u001b[32m0.02006\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 013 | loss: 0.02006 - R2: 1.0007 -- iter: 0120/1168\n",
      "Training Step: 715  | total loss: \u001b[1m\u001b[32m0.01970\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 013 | loss: 0.01970 - R2: 0.9998 -- iter: 0140/1168\n",
      "Training Step: 716  | total loss: \u001b[1m\u001b[32m0.01877\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 013 | loss: 0.01877 - R2: 0.9994 -- iter: 0160/1168\n",
      "Training Step: 717  | total loss: \u001b[1m\u001b[32m0.01792\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 013 | loss: 0.01792 - R2: 0.9996 -- iter: 0180/1168\n",
      "Training Step: 718  | total loss: \u001b[1m\u001b[32m0.01792\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 013 | loss: 0.01792 - R2: 0.9996 -- iter: 0200/1168\n",
      "Training Step: 719  | total loss: \u001b[1m\u001b[32m0.01706\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 013 | loss: 0.01706 - R2: 0.9990 -- iter: 0220/1168\n",
      "Training Step: 720  | total loss: \u001b[1m\u001b[32m0.02137\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 013 | loss: 0.02137 - R2: 1.0002 -- iter: 0240/1168\n",
      "Training Step: 721  | total loss: \u001b[1m\u001b[32m0.02221\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 013 | loss: 0.02221 - R2: 0.9989 -- iter: 0260/1168\n",
      "Training Step: 722  | total loss: \u001b[1m\u001b[32m0.02462\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 013 | loss: 0.02462 - R2: 0.9989 -- iter: 0280/1168\n",
      "Training Step: 723  | total loss: \u001b[1m\u001b[32m0.02304\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 013 | loss: 0.02304 - R2: 0.9989 -- iter: 0300/1168\n",
      "Training Step: 724  | total loss: \u001b[1m\u001b[32m0.02556\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 013 | loss: 0.02556 - R2: 0.9995 -- iter: 0320/1168\n",
      "Training Step: 725  | total loss: \u001b[1m\u001b[32m0.02486\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 013 | loss: 0.02486 - R2: 1.0005 -- iter: 0340/1168\n",
      "Training Step: 726  | total loss: \u001b[1m\u001b[32m0.02486\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 013 | loss: 0.02486 - R2: 1.0010 -- iter: 0360/1168\n",
      "Training Step: 727  | total loss: \u001b[1m\u001b[32m0.02403\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 013 | loss: 0.02403 - R2: 1.0010 -- iter: 0380/1168\n",
      "Training Step: 728  | total loss: \u001b[1m\u001b[32m0.02989\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 013 | loss: 0.02989 - R2: 1.0006 -- iter: 0400/1168\n",
      "Training Step: 729  | total loss: \u001b[1m\u001b[32m0.02964\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 013 | loss: 0.02964 - R2: 1.0006 -- iter: 0420/1168\n",
      "Training Step: 730  | total loss: \u001b[1m\u001b[32m0.02764\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 013 | loss: 0.02764 - R2: 1.0001 -- iter: 0440/1168\n",
      "Training Step: 731  | total loss: \u001b[1m\u001b[32m0.02764\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 013 | loss: 0.02764 - R2: 1.0001 -- iter: 0460/1168\n",
      "Training Step: 732  | total loss: \u001b[1m\u001b[32m0.02773\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 013 | loss: 0.02773 - R2: 1.0000 -- iter: 0480/1168\n",
      "Training Step: 733  | total loss: \u001b[1m\u001b[32m0.03057\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 013 | loss: 0.03057 - R2: 1.0004 -- iter: 0500/1168\n",
      "Training Step: 734  | total loss: \u001b[1m\u001b[32m0.02910\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 013 | loss: 0.02910 - R2: 0.9993 -- iter: 0520/1168\n",
      "Training Step: 735  | total loss: \u001b[1m\u001b[32m0.02756\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 013 | loss: 0.02756 - R2: 0.9995 -- iter: 0540/1168\n",
      "Training Step: 736  | total loss: \u001b[1m\u001b[32m0.02641\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 013 | loss: 0.02641 - R2: 0.9993 -- iter: 0560/1168\n",
      "Training Step: 737  | total loss: \u001b[1m\u001b[32m0.02567\u001b[0m\u001b[0m | time: 0.169s\n",
      "| SGD | epoch: 013 | loss: 0.02567 - R2: 0.9999 -- iter: 0580/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 738  | total loss: \u001b[1m\u001b[32m0.02422\u001b[0m\u001b[0m | time: 0.173s\n",
      "| SGD | epoch: 013 | loss: 0.02422 - R2: 0.9997 -- iter: 0600/1168\n",
      "Training Step: 739  | total loss: \u001b[1m\u001b[32m0.02422\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 013 | loss: 0.02422 - R2: 0.9997 -- iter: 0620/1168\n",
      "Training Step: 740  | total loss: \u001b[1m\u001b[32m0.02330\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 013 | loss: 0.02330 - R2: 0.9996 -- iter: 0640/1168\n",
      "Training Step: 741  | total loss: \u001b[1m\u001b[32m0.02090\u001b[0m\u001b[0m | time: 0.180s\n",
      "| SGD | epoch: 013 | loss: 0.02090 - R2: 1.0001 -- iter: 0660/1168\n",
      "Training Step: 742  | total loss: \u001b[1m\u001b[32m0.02001\u001b[0m\u001b[0m | time: 0.184s\n",
      "| SGD | epoch: 013 | loss: 0.02001 - R2: 1.0003 -- iter: 0680/1168\n",
      "Training Step: 743  | total loss: \u001b[1m\u001b[32m0.02001\u001b[0m\u001b[0m | time: 0.185s\n",
      "| SGD | epoch: 013 | loss: 0.02001 - R2: 1.0003 -- iter: 0700/1168\n",
      "Training Step: 744  | total loss: \u001b[1m\u001b[32m0.02062\u001b[0m\u001b[0m | time: 0.189s\n",
      "| SGD | epoch: 013 | loss: 0.02062 - R2: 0.9997 -- iter: 0720/1168\n",
      "Training Step: 745  | total loss: \u001b[1m\u001b[32m0.02180\u001b[0m\u001b[0m | time: 0.192s\n",
      "| SGD | epoch: 013 | loss: 0.02180 - R2: 1.0000 -- iter: 0740/1168\n",
      "Training Step: 746  | total loss: \u001b[1m\u001b[32m0.02221\u001b[0m\u001b[0m | time: 0.195s\n",
      "| SGD | epoch: 013 | loss: 0.02221 - R2: 1.0002 -- iter: 0760/1168\n",
      "Training Step: 747  | total loss: \u001b[1m\u001b[32m0.02131\u001b[0m\u001b[0m | time: 0.198s\n",
      "| SGD | epoch: 013 | loss: 0.02131 - R2: 1.0002 -- iter: 0780/1168\n",
      "Training Step: 748  | total loss: \u001b[1m\u001b[32m0.02131\u001b[0m\u001b[0m | time: 0.200s\n",
      "| SGD | epoch: 013 | loss: 0.02131 - R2: 1.0002 -- iter: 0800/1168\n",
      "Training Step: 749  | total loss: \u001b[1m\u001b[32m0.02050\u001b[0m\u001b[0m | time: 0.203s\n",
      "| SGD | epoch: 013 | loss: 0.02050 - R2: 1.0002 -- iter: 0820/1168\n",
      "Training Step: 750  | total loss: \u001b[1m\u001b[32m0.02352\u001b[0m\u001b[0m | time: 0.206s\n",
      "| SGD | epoch: 013 | loss: 0.02352 - R2: 1.0002 -- iter: 0840/1168\n",
      "Training Step: 751  | total loss: \u001b[1m\u001b[32m0.02259\u001b[0m\u001b[0m | time: 0.208s\n",
      "| SGD | epoch: 013 | loss: 0.02259 - R2: 1.0008 -- iter: 0860/1168\n",
      "Training Step: 752  | total loss: \u001b[1m\u001b[32m0.02311\u001b[0m\u001b[0m | time: 0.210s\n",
      "| SGD | epoch: 013 | loss: 0.02311 - R2: 1.0008 -- iter: 0880/1168\n",
      "Training Step: 753  | total loss: \u001b[1m\u001b[32m0.02196\u001b[0m\u001b[0m | time: 0.212s\n",
      "| SGD | epoch: 013 | loss: 0.02196 - R2: 0.9993 -- iter: 0900/1168\n",
      "Training Step: 754  | total loss: \u001b[1m\u001b[32m0.02226\u001b[0m\u001b[0m | time: 0.214s\n",
      "| SGD | epoch: 013 | loss: 0.02226 - R2: 0.9993 -- iter: 0920/1168\n",
      "Training Step: 755  | total loss: \u001b[1m\u001b[32m0.02226\u001b[0m\u001b[0m | time: 0.216s\n",
      "| SGD | epoch: 013 | loss: 0.02226 - R2: 0.9992 -- iter: 0940/1168\n",
      "Training Step: 756  | total loss: \u001b[1m\u001b[32m0.02125\u001b[0m\u001b[0m | time: 0.220s\n",
      "| SGD | epoch: 013 | loss: 0.02125 - R2: 0.9992 -- iter: 0960/1168\n",
      "Training Step: 757  | total loss: \u001b[1m\u001b[32m0.02094\u001b[0m\u001b[0m | time: 0.224s\n",
      "| SGD | epoch: 013 | loss: 0.02094 - R2: 0.9995 -- iter: 0980/1168\n",
      "Training Step: 758  | total loss: \u001b[1m\u001b[32m0.02094\u001b[0m\u001b[0m | time: 0.227s\n",
      "| SGD | epoch: 013 | loss: 0.02094 - R2: 1.0000 -- iter: 1000/1168\n",
      "Training Step: 759  | total loss: \u001b[1m\u001b[32m0.02040\u001b[0m\u001b[0m | time: 0.231s\n",
      "| SGD | epoch: 013 | loss: 0.02040 - R2: 0.9997 -- iter: 1020/1168\n",
      "Training Step: 760  | total loss: \u001b[1m\u001b[32m0.01971\u001b[0m\u001b[0m | time: 0.235s\n",
      "| SGD | epoch: 013 | loss: 0.01971 - R2: 0.9996 -- iter: 1040/1168\n",
      "Training Step: 761  | total loss: \u001b[1m\u001b[32m0.01971\u001b[0m\u001b[0m | time: 0.237s\n",
      "| SGD | epoch: 013 | loss: 0.01971 - R2: 0.9996 -- iter: 1060/1168\n",
      "Training Step: 762  | total loss: \u001b[1m\u001b[32m0.01879\u001b[0m\u001b[0m | time: 0.240s\n",
      "| SGD | epoch: 013 | loss: 0.01879 - R2: 1.0002 -- iter: 1080/1168\n",
      "Training Step: 763  | total loss: \u001b[1m\u001b[32m0.01797\u001b[0m\u001b[0m | time: 0.243s\n",
      "| SGD | epoch: 013 | loss: 0.01797 - R2: 1.0004 -- iter: 1100/1168\n",
      "Training Step: 764  | total loss: \u001b[1m\u001b[32m0.01797\u001b[0m\u001b[0m | time: 0.245s\n",
      "| SGD | epoch: 013 | loss: 0.01797 - R2: 1.0004 -- iter: 1120/1168\n",
      "Training Step: 765  | total loss: \u001b[1m\u001b[32m0.01698\u001b[0m\u001b[0m | time: 0.248s\n",
      "| SGD | epoch: 013 | loss: 0.01698 - R2: 0.9999 -- iter: 1140/1168\n",
      "Training Step: 766  | total loss: \u001b[1m\u001b[32m0.01713\u001b[0m\u001b[0m | time: 0.251s\n",
      "| SGD | epoch: 013 | loss: 0.01713 - R2: 1.0005 -- iter: 1160/1168\n",
      "Training Step: 767  | total loss: \u001b[1m\u001b[32m0.01726\u001b[0m\u001b[0m | time: 1.256s\n",
      "| SGD | epoch: 013 | loss: 0.01726 - R2: 0.9998 | val_loss: 0.02375 - val_acc: 1.0010 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 768  | total loss: \u001b[1m\u001b[32m0.01726\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 014 | loss: 0.01726 - R2: 0.9998 -- iter: 0020/1168\n",
      "Training Step: 769  | total loss: \u001b[1m\u001b[32m0.01633\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 014 | loss: 0.01633 - R2: 1.0001 -- iter: 0040/1168\n",
      "Training Step: 770  | total loss: \u001b[1m\u001b[32m0.01587\u001b[0m\u001b[0m | time: 0.026s\n",
      "| SGD | epoch: 014 | loss: 0.01587 - R2: 0.9997 -- iter: 0060/1168\n",
      "Training Step: 771  | total loss: \u001b[1m\u001b[32m0.01587\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 014 | loss: 0.01587 - R2: 0.9993 -- iter: 0080/1168\n",
      "Training Step: 772  | total loss: \u001b[1m\u001b[32m0.01544\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 014 | loss: 0.01544 - R2: 0.9993 -- iter: 0100/1168\n",
      "Training Step: 773  | total loss: \u001b[1m\u001b[32m0.01845\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 014 | loss: 0.01845 - R2: 0.9998 -- iter: 0120/1168\n",
      "Training Step: 774  | total loss: \u001b[1m\u001b[32m0.01891\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 014 | loss: 0.01891 - R2: 0.9995 -- iter: 0140/1168\n",
      "Training Step: 775  | total loss: \u001b[1m\u001b[32m0.01932\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 014 | loss: 0.01932 - R2: 0.9996 -- iter: 0160/1168\n",
      "Training Step: 776  | total loss: \u001b[1m\u001b[32m0.02592\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 014 | loss: 0.02592 - R2: 1.0000 -- iter: 0180/1168\n",
      "Training Step: 777  | total loss: \u001b[1m\u001b[32m0.02496\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 014 | loss: 0.02496 - R2: 1.0004 -- iter: 0200/1168\n",
      "Training Step: 778  | total loss: \u001b[1m\u001b[32m0.02496\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 014 | loss: 0.02496 - R2: 1.0004 -- iter: 0220/1168\n",
      "Training Step: 779  | total loss: \u001b[1m\u001b[32m0.02365\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 014 | loss: 0.02365 - R2: 0.9999 -- iter: 0240/1168\n",
      "Training Step: 780  | total loss: \u001b[1m\u001b[32m0.02212\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 014 | loss: 0.02212 - R2: 1.0007 -- iter: 0260/1168\n",
      "Training Step: 781  | total loss: \u001b[1m\u001b[32m0.02049\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 014 | loss: 0.02049 - R2: 1.0012 -- iter: 0280/1168\n",
      "Training Step: 782  | total loss: \u001b[1m\u001b[32m0.01917\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 014 | loss: 0.01917 - R2: 1.0005 -- iter: 0300/1168\n",
      "Training Step: 783  | total loss: \u001b[1m\u001b[32m0.01952\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 014 | loss: 0.01952 - R2: 0.9991 -- iter: 0320/1168\n",
      "Training Step: 784  | total loss: \u001b[1m\u001b[32m0.01952\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 014 | loss: 0.01952 - R2: 0.9991 -- iter: 0340/1168\n",
      "Training Step: 785  | total loss: \u001b[1m\u001b[32m0.02099\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 014 | loss: 0.02099 - R2: 0.9993 -- iter: 0360/1168\n",
      "Training Step: 786  | total loss: \u001b[1m\u001b[32m0.01917\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 014 | loss: 0.01917 - R2: 0.9991 -- iter: 0380/1168\n",
      "Training Step: 787  | total loss: \u001b[1m\u001b[32m0.02692\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 014 | loss: 0.02692 - R2: 1.0007 -- iter: 0400/1168\n",
      "Training Step: 788  | total loss: \u001b[1m\u001b[32m0.02571\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 014 | loss: 0.02571 - R2: 1.0000 -- iter: 0420/1168\n",
      "Training Step: 789  | total loss: \u001b[1m\u001b[32m0.02571\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 014 | loss: 0.02571 - R2: 1.0000 -- iter: 0440/1168\n",
      "Training Step: 790  | total loss: \u001b[1m\u001b[32m0.02580\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 014 | loss: 0.02580 - R2: 1.0000 -- iter: 0460/1168\n",
      "Training Step: 791  | total loss: \u001b[1m\u001b[32m0.02659\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 014 | loss: 0.02659 - R2: 0.9995 -- iter: 0480/1168\n",
      "Training Step: 792  | total loss: \u001b[1m\u001b[32m0.02624\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 014 | loss: 0.02624 - R2: 1.0004 -- iter: 0500/1168\n",
      "Training Step: 793  | total loss: \u001b[1m\u001b[32m0.02624\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 014 | loss: 0.02624 - R2: 0.9998 -- iter: 0520/1168\n",
      "Training Step: 794  | total loss: \u001b[1m\u001b[32m0.02517\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 014 | loss: 0.02517 - R2: 0.9998 -- iter: 0540/1168\n",
      "Training Step: 795  | total loss: \u001b[1m\u001b[32m0.02711\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 014 | loss: 0.02711 - R2: 0.9991 -- iter: 0560/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 796  | total loss: \u001b[1m\u001b[32m0.02716\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 014 | loss: 0.02716 - R2: 0.9996 -- iter: 0580/1168\n",
      "Training Step: 797  | total loss: \u001b[1m\u001b[32m0.02678\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 014 | loss: 0.02678 - R2: 0.9992 -- iter: 0600/1168\n",
      "Training Step: 798  | total loss: \u001b[1m\u001b[32m0.02571\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 014 | loss: 0.02571 - R2: 0.9992 -- iter: 0620/1168\n",
      "Training Step: 799  | total loss: \u001b[1m\u001b[32m0.02237\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 014 | loss: 0.02237 - R2: 0.9995 -- iter: 0640/1168\n",
      "Training Step: 800  | total loss: \u001b[1m\u001b[32m0.02237\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 014 | loss: 0.02237 - R2: 0.9995 -- iter: 0660/1168\n",
      "Training Step: 801  | total loss: \u001b[1m\u001b[32m0.02216\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 014 | loss: 0.02216 - R2: 0.9989 -- iter: 0680/1168\n",
      "Training Step: 802  | total loss: \u001b[1m\u001b[32m0.02167\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 014 | loss: 0.02167 - R2: 0.9989 -- iter: 0700/1168\n",
      "Training Step: 803  | total loss: \u001b[1m\u001b[32m0.02130\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 014 | loss: 0.02130 - R2: 0.9982 -- iter: 0720/1168\n",
      "Training Step: 804  | total loss: \u001b[1m\u001b[32m0.02002\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 014 | loss: 0.02002 - R2: 0.9984 -- iter: 0740/1168\n",
      "Training Step: 805  | total loss: \u001b[1m\u001b[32m0.01938\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 014 | loss: 0.01938 - R2: 0.9991 -- iter: 0760/1168\n",
      "Training Step: 806  | total loss: \u001b[1m\u001b[32m0.02056\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 014 | loss: 0.02056 - R2: 0.9998 -- iter: 0780/1168\n",
      "Training Step: 807  | total loss: \u001b[1m\u001b[32m0.01922\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 014 | loss: 0.01922 - R2: 0.9995 -- iter: 0800/1168\n",
      "Training Step: 808  | total loss: \u001b[1m\u001b[32m0.01889\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 014 | loss: 0.01889 - R2: 0.9994 -- iter: 0820/1168\n",
      "Training Step: 809  | total loss: \u001b[1m\u001b[32m0.01866\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 014 | loss: 0.01866 - R2: 0.9987 -- iter: 0840/1168\n",
      "Training Step: 810  | total loss: \u001b[1m\u001b[32m0.01772\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 014 | loss: 0.01772 - R2: 1.0000 -- iter: 0860/1168\n",
      "Training Step: 811  | total loss: \u001b[1m\u001b[32m0.01750\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 014 | loss: 0.01750 - R2: 0.9994 -- iter: 0880/1168\n",
      "Training Step: 812  | total loss: \u001b[1m\u001b[32m0.01676\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 014 | loss: 0.01676 - R2: 0.9994 -- iter: 0900/1168\n",
      "Training Step: 813  | total loss: \u001b[1m\u001b[32m0.01591\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 014 | loss: 0.01591 - R2: 0.9998 -- iter: 0920/1168\n",
      "Training Step: 814  | total loss: \u001b[1m\u001b[32m0.02398\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 014 | loss: 0.02398 - R2: 1.0009 -- iter: 0940/1168\n",
      "Training Step: 815  | total loss: \u001b[1m\u001b[32m0.02801\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 014 | loss: 0.02801 - R2: 1.0009 -- iter: 0960/1168\n",
      "Training Step: 816  | total loss: \u001b[1m\u001b[32m0.02766\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 014 | loss: 0.02766 - R2: 1.0017 -- iter: 0980/1168\n",
      "Training Step: 817  | total loss: \u001b[1m\u001b[32m0.02643\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 014 | loss: 0.02643 - R2: 1.0008 -- iter: 1000/1168\n",
      "Training Step: 818  | total loss: \u001b[1m\u001b[32m0.02643\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 014 | loss: 0.02643 - R2: 1.0008 -- iter: 1020/1168\n",
      "Training Step: 819  | total loss: \u001b[1m\u001b[32m0.02498\u001b[0m\u001b[0m | time: 0.171s\n",
      "| SGD | epoch: 014 | loss: 0.02498 - R2: 0.9997 -- iter: 1040/1168\n",
      "Training Step: 820  | total loss: \u001b[1m\u001b[32m0.02365\u001b[0m\u001b[0m | time: 0.173s\n",
      "| SGD | epoch: 014 | loss: 0.02365 - R2: 0.9997 -- iter: 1060/1168\n",
      "Training Step: 821  | total loss: \u001b[1m\u001b[32m0.02365\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 014 | loss: 0.02365 - R2: 1.0001 -- iter: 1080/1168\n",
      "Training Step: 822  | total loss: \u001b[1m\u001b[32m0.02462\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 014 | loss: 0.02462 - R2: 0.9988 -- iter: 1100/1168\n",
      "Training Step: 823  | total loss: \u001b[1m\u001b[32m0.02400\u001b[0m\u001b[0m | time: 0.179s\n",
      "| SGD | epoch: 014 | loss: 0.02400 - R2: 0.9988 -- iter: 1120/1168\n",
      "Training Step: 824  | total loss: \u001b[1m\u001b[32m0.02362\u001b[0m\u001b[0m | time: 0.182s\n",
      "| SGD | epoch: 014 | loss: 0.02362 - R2: 0.9992 -- iter: 1140/1168\n",
      "Training Step: 825  | total loss: \u001b[1m\u001b[32m0.02297\u001b[0m\u001b[0m | time: 0.185s\n",
      "| SGD | epoch: 014 | loss: 0.02297 - R2: 0.9997 -- iter: 1160/1168\n",
      "Training Step: 826  | total loss: \u001b[1m\u001b[32m0.02218\u001b[0m\u001b[0m | time: 1.190s\n",
      "| SGD | epoch: 014 | loss: 0.02218 - R2: 0.9995 | val_loss: 0.02546 - val_acc: 1.0008 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 827  | total loss: \u001b[1m\u001b[32m0.02219\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 015 | loss: 0.02219 - R2: 1.0003 -- iter: 0020/1168\n",
      "Training Step: 828  | total loss: \u001b[1m\u001b[32m0.02182\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 015 | loss: 0.02182 - R2: 1.0008 -- iter: 0040/1168\n",
      "Training Step: 829  | total loss: \u001b[1m\u001b[32m0.02110\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 015 | loss: 0.02110 - R2: 1.0004 -- iter: 0060/1168\n",
      "Training Step: 830  | total loss: \u001b[1m\u001b[32m0.02060\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 015 | loss: 0.02060 - R2: 1.0002 -- iter: 0080/1168\n",
      "Training Step: 831  | total loss: \u001b[1m\u001b[32m0.01986\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 015 | loss: 0.01986 - R2: 0.9998 -- iter: 0100/1168\n",
      "Training Step: 832  | total loss: \u001b[1m\u001b[32m0.02000\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 015 | loss: 0.02000 - R2: 1.0005 -- iter: 0120/1168\n",
      "Training Step: 833  | total loss: \u001b[1m\u001b[32m0.01921\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 015 | loss: 0.01921 - R2: 1.0002 -- iter: 0140/1168\n",
      "Training Step: 834  | total loss: \u001b[1m\u001b[32m0.02395\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 015 | loss: 0.02395 - R2: 0.9990 -- iter: 0160/1168\n",
      "Training Step: 835  | total loss: \u001b[1m\u001b[32m0.02376\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 015 | loss: 0.02376 - R2: 0.9989 -- iter: 0180/1168\n",
      "Training Step: 836  | total loss: \u001b[1m\u001b[32m0.02376\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 015 | loss: 0.02376 - R2: 0.9989 -- iter: 0200/1168\n",
      "Training Step: 837  | total loss: \u001b[1m\u001b[32m0.02426\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 015 | loss: 0.02426 - R2: 0.9990 -- iter: 0220/1168\n",
      "Training Step: 838  | total loss: \u001b[1m\u001b[32m0.02324\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 015 | loss: 0.02324 - R2: 0.9997 -- iter: 0240/1168\n",
      "Training Step: 839  | total loss: \u001b[1m\u001b[32m0.02244\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 015 | loss: 0.02244 - R2: 0.9993 -- iter: 0260/1168\n",
      "Training Step: 840  | total loss: \u001b[1m\u001b[32m0.02244\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 015 | loss: 0.02244 - R2: 0.9993 -- iter: 0280/1168\n",
      "Training Step: 841  | total loss: \u001b[1m\u001b[32m0.02119\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 015 | loss: 0.02119 - R2: 0.9991 -- iter: 0300/1168\n",
      "Training Step: 842  | total loss: \u001b[1m\u001b[32m0.02072\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 015 | loss: 0.02072 - R2: 0.9990 -- iter: 0320/1168\n",
      "Training Step: 843  | total loss: \u001b[1m\u001b[32m0.01999\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 015 | loss: 0.01999 - R2: 0.9997 -- iter: 0340/1168\n",
      "Training Step: 844  | total loss: \u001b[1m\u001b[32m0.02064\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 015 | loss: 0.02064 - R2: 0.9994 -- iter: 0360/1168\n",
      "Training Step: 845  | total loss: \u001b[1m\u001b[32m0.02150\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 015 | loss: 0.02150 - R2: 1.0004 -- iter: 0380/1168\n",
      "Training Step: 846  | total loss: \u001b[1m\u001b[32m0.02217\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 015 | loss: 0.02217 - R2: 0.9998 -- iter: 0400/1168\n",
      "Training Step: 847  | total loss: \u001b[1m\u001b[32m0.02106\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 015 | loss: 0.02106 - R2: 0.9999 -- iter: 0420/1168\n",
      "Training Step: 848  | total loss: \u001b[1m\u001b[32m0.02065\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 015 | loss: 0.02065 - R2: 1.0003 -- iter: 0440/1168\n",
      "Training Step: 849  | total loss: \u001b[1m\u001b[32m0.02035\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 015 | loss: 0.02035 - R2: 1.0007 -- iter: 0460/1168\n",
      "Training Step: 850  | total loss: \u001b[1m\u001b[32m0.01952\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 015 | loss: 0.01952 - R2: 1.0006 -- iter: 0480/1168\n",
      "Training Step: 851  | total loss: \u001b[1m\u001b[32m0.01803\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 015 | loss: 0.01803 - R2: 1.0005 -- iter: 0500/1168\n",
      "Training Step: 852  | total loss: \u001b[1m\u001b[32m0.01803\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 015 | loss: 0.01803 - R2: 1.0005 -- iter: 0520/1168\n",
      "Training Step: 853  | total loss: \u001b[1m\u001b[32m0.01811\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 015 | loss: 0.01811 - R2: 1.0002 -- iter: 0540/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 854  | total loss: \u001b[1m\u001b[32m0.01717\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 015 | loss: 0.01717 - R2: 1.0004 -- iter: 0560/1168\n",
      "Training Step: 855  | total loss: \u001b[1m\u001b[32m0.01717\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 015 | loss: 0.01717 - R2: 1.0004 -- iter: 0580/1168\n",
      "Training Step: 856  | total loss: \u001b[1m\u001b[32m0.01710\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 015 | loss: 0.01710 - R2: 1.0004 -- iter: 0600/1168\n",
      "Training Step: 857  | total loss: \u001b[1m\u001b[32m0.02043\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 015 | loss: 0.02043 - R2: 0.9993 -- iter: 0620/1168\n",
      "Training Step: 858  | total loss: \u001b[1m\u001b[32m0.02043\u001b[0m\u001b[0m | time: 0.172s\n",
      "| SGD | epoch: 015 | loss: 0.02043 - R2: 0.9993 -- iter: 0640/1168\n",
      "Training Step: 859  | total loss: \u001b[1m\u001b[32m0.01898\u001b[0m\u001b[0m | time: 0.174s\n",
      "| SGD | epoch: 015 | loss: 0.01898 - R2: 1.0000 -- iter: 0660/1168\n",
      "Training Step: 860  | total loss: \u001b[1m\u001b[32m0.01829\u001b[0m\u001b[0m | time: 0.176s\n",
      "| SGD | epoch: 015 | loss: 0.01829 - R2: 1.0004 -- iter: 0680/1168\n",
      "Training Step: 861  | total loss: \u001b[1m\u001b[32m0.01813\u001b[0m\u001b[0m | time: 0.178s\n",
      "| SGD | epoch: 015 | loss: 0.01813 - R2: 1.0006 -- iter: 0700/1168\n",
      "Training Step: 862  | total loss: \u001b[1m\u001b[32m0.01745\u001b[0m\u001b[0m | time: 0.181s\n",
      "| SGD | epoch: 015 | loss: 0.01745 - R2: 1.0005 -- iter: 0720/1168\n",
      "Training Step: 863  | total loss: \u001b[1m\u001b[32m0.01745\u001b[0m\u001b[0m | time: 0.182s\n",
      "| SGD | epoch: 015 | loss: 0.01745 - R2: 1.0003 -- iter: 0740/1168\n",
      "Training Step: 864  | total loss: \u001b[1m\u001b[32m0.01850\u001b[0m\u001b[0m | time: 0.184s\n",
      "| SGD | epoch: 015 | loss: 0.01850 - R2: 0.9988 -- iter: 0760/1168\n",
      "Training Step: 865  | total loss: \u001b[1m\u001b[32m0.01826\u001b[0m\u001b[0m | time: 0.186s\n",
      "| SGD | epoch: 015 | loss: 0.01826 - R2: 0.9988 -- iter: 0780/1168\n",
      "Training Step: 866  | total loss: \u001b[1m\u001b[32m0.01703\u001b[0m\u001b[0m | time: 0.190s\n",
      "| SGD | epoch: 015 | loss: 0.01703 - R2: 0.9988 -- iter: 0800/1168\n",
      "Training Step: 867  | total loss: \u001b[1m\u001b[32m0.01703\u001b[0m\u001b[0m | time: 0.192s\n",
      "| SGD | epoch: 015 | loss: 0.01703 - R2: 0.9988 -- iter: 0820/1168\n",
      "Training Step: 868  | total loss: \u001b[1m\u001b[32m0.01811\u001b[0m\u001b[0m | time: 0.194s\n",
      "| SGD | epoch: 015 | loss: 0.01811 - R2: 0.9989 -- iter: 0840/1168\n",
      "Training Step: 869  | total loss: \u001b[1m\u001b[32m0.01811\u001b[0m\u001b[0m | time: 0.196s\n",
      "| SGD | epoch: 015 | loss: 0.01811 - R2: 0.9989 -- iter: 0860/1168\n",
      "Training Step: 870  | total loss: \u001b[1m\u001b[32m0.01748\u001b[0m\u001b[0m | time: 0.199s\n",
      "| SGD | epoch: 015 | loss: 0.01748 - R2: 0.9991 -- iter: 0880/1168\n",
      "Training Step: 871  | total loss: \u001b[1m\u001b[32m0.01836\u001b[0m\u001b[0m | time: 0.205s\n",
      "| SGD | epoch: 015 | loss: 0.01836 - R2: 1.0000 -- iter: 0900/1168\n",
      "Training Step: 872  | total loss: \u001b[1m\u001b[32m0.01876\u001b[0m\u001b[0m | time: 0.208s\n",
      "| SGD | epoch: 015 | loss: 0.01876 - R2: 0.9997 -- iter: 0920/1168\n",
      "Training Step: 873  | total loss: \u001b[1m\u001b[32m0.01792\u001b[0m\u001b[0m | time: 0.210s\n",
      "| SGD | epoch: 015 | loss: 0.01792 - R2: 0.9996 -- iter: 0940/1168\n",
      "Training Step: 874  | total loss: \u001b[1m\u001b[32m0.01711\u001b[0m\u001b[0m | time: 0.212s\n",
      "| SGD | epoch: 015 | loss: 0.01711 - R2: 0.9996 -- iter: 0960/1168\n",
      "Training Step: 875  | total loss: \u001b[1m\u001b[32m0.01813\u001b[0m\u001b[0m | time: 0.217s\n",
      "| SGD | epoch: 015 | loss: 0.01813 - R2: 1.0000 -- iter: 0980/1168\n",
      "Training Step: 876  | total loss: \u001b[1m\u001b[32m0.02496\u001b[0m\u001b[0m | time: 0.219s\n",
      "| SGD | epoch: 015 | loss: 0.02496 - R2: 1.0014 -- iter: 1000/1168\n",
      "Training Step: 877  | total loss: \u001b[1m\u001b[32m0.02381\u001b[0m\u001b[0m | time: 0.225s\n",
      "| SGD | epoch: 015 | loss: 0.02381 - R2: 1.0009 -- iter: 1020/1168\n",
      "Training Step: 878  | total loss: \u001b[1m\u001b[32m0.02381\u001b[0m\u001b[0m | time: 0.228s\n",
      "| SGD | epoch: 015 | loss: 0.02381 - R2: 1.0009 -- iter: 1040/1168\n",
      "Training Step: 879  | total loss: \u001b[1m\u001b[32m0.02256\u001b[0m\u001b[0m | time: 0.229s\n",
      "| SGD | epoch: 015 | loss: 0.02256 - R2: 1.0004 -- iter: 1060/1168\n",
      "Training Step: 880  | total loss: \u001b[1m\u001b[32m0.02152\u001b[0m\u001b[0m | time: 0.231s\n",
      "| SGD | epoch: 015 | loss: 0.02152 - R2: 1.0002 -- iter: 1080/1168\n",
      "Training Step: 881  | total loss: \u001b[1m\u001b[32m0.02043\u001b[0m\u001b[0m | time: 0.233s\n",
      "| SGD | epoch: 015 | loss: 0.02043 - R2: 0.9999 -- iter: 1100/1168\n",
      "Training Step: 882  | total loss: \u001b[1m\u001b[32m0.03084\u001b[0m\u001b[0m | time: 0.235s\n",
      "| SGD | epoch: 015 | loss: 0.03084 - R2: 1.0010 -- iter: 1120/1168\n",
      "Training Step: 883  | total loss: \u001b[1m\u001b[32m0.02952\u001b[0m\u001b[0m | time: 0.237s\n",
      "| SGD | epoch: 015 | loss: 0.02952 - R2: 1.0007 -- iter: 1140/1168\n",
      "Training Step: 884  | total loss: \u001b[1m\u001b[32m0.02923\u001b[0m\u001b[0m | time: 0.239s\n",
      "| SGD | epoch: 015 | loss: 0.02923 - R2: 1.0007 -- iter: 1160/1168\n",
      "Training Step: 885  | total loss: \u001b[1m\u001b[32m0.02730\u001b[0m\u001b[0m | time: 1.245s\n",
      "| SGD | epoch: 015 | loss: 0.02730 - R2: 1.0006 | val_loss: 0.02500 - val_acc: 0.9997 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 886  | total loss: \u001b[1m\u001b[32m0.02730\u001b[0m\u001b[0m | time: 0.016s\n",
      "| SGD | epoch: 016 | loss: 0.02730 - R2: 1.0006 -- iter: 0020/1168\n",
      "Training Step: 887  | total loss: \u001b[1m\u001b[32m0.02582\u001b[0m\u001b[0m | time: 0.019s\n",
      "| SGD | epoch: 016 | loss: 0.02582 - R2: 0.9998 -- iter: 0040/1168\n",
      "Training Step: 888  | total loss: \u001b[1m\u001b[32m0.02509\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 016 | loss: 0.02509 - R2: 0.9991 -- iter: 0060/1168\n",
      "Training Step: 889  | total loss: \u001b[1m\u001b[32m0.02567\u001b[0m\u001b[0m | time: 0.023s\n",
      "| SGD | epoch: 016 | loss: 0.02567 - R2: 0.9983 -- iter: 0080/1168\n",
      "Training Step: 890  | total loss: \u001b[1m\u001b[32m0.02432\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 016 | loss: 0.02432 - R2: 0.9990 -- iter: 0100/1168\n",
      "Training Step: 891  | total loss: \u001b[1m\u001b[32m0.02506\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 016 | loss: 0.02506 - R2: 0.9984 -- iter: 0120/1168\n",
      "Training Step: 892  | total loss: \u001b[1m\u001b[32m0.02181\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 016 | loss: 0.02181 - R2: 0.9992 -- iter: 0140/1168\n",
      "Training Step: 893  | total loss: \u001b[1m\u001b[32m0.02181\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 016 | loss: 0.02181 - R2: 0.9994 -- iter: 0160/1168\n",
      "Training Step: 894  | total loss: \u001b[1m\u001b[32m0.02126\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 016 | loss: 0.02126 - R2: 0.9987 -- iter: 0180/1168\n",
      "Training Step: 895  | total loss: \u001b[1m\u001b[32m0.01988\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 016 | loss: 0.01988 - R2: 0.9987 -- iter: 0200/1168\n",
      "Training Step: 896  | total loss: \u001b[1m\u001b[32m0.01945\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 016 | loss: 0.01945 - R2: 0.9992 -- iter: 0220/1168\n",
      "Training Step: 897  | total loss: \u001b[1m\u001b[32m0.01957\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 016 | loss: 0.01957 - R2: 0.9989 -- iter: 0240/1168\n",
      "Training Step: 898  | total loss: \u001b[1m\u001b[32m0.01875\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 016 | loss: 0.01875 - R2: 0.9989 -- iter: 0260/1168\n",
      "Training Step: 899  | total loss: \u001b[1m\u001b[32m0.01760\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 016 | loss: 0.01760 - R2: 0.9996 -- iter: 0280/1168\n",
      "Training Step: 900  | total loss: \u001b[1m\u001b[32m0.01760\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 016 | loss: 0.01760 - R2: 0.9996 -- iter: 0300/1168\n",
      "Training Step: 901  | total loss: \u001b[1m\u001b[32m0.01944\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 016 | loss: 0.01944 - R2: 1.0005 -- iter: 0320/1168\n",
      "Training Step: 902  | total loss: \u001b[1m\u001b[32m0.01944\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 016 | loss: 0.01944 - R2: 1.0007 -- iter: 0340/1168\n",
      "Training Step: 903  | total loss: \u001b[1m\u001b[32m0.01833\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 016 | loss: 0.01833 - R2: 1.0013 -- iter: 0360/1168\n",
      "Training Step: 904  | total loss: \u001b[1m\u001b[32m0.01833\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 016 | loss: 0.01833 - R2: 1.0013 -- iter: 0380/1168\n",
      "Training Step: 905  | total loss: \u001b[1m\u001b[32m0.01815\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 016 | loss: 0.01815 - R2: 1.0008 -- iter: 0400/1168\n",
      "Training Step: 906  | total loss: \u001b[1m\u001b[32m0.01819\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 016 | loss: 0.01819 - R2: 1.0004 -- iter: 0420/1168\n",
      "Training Step: 907  | total loss: \u001b[1m\u001b[32m0.01819\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 016 | loss: 0.01819 - R2: 1.0004 -- iter: 0440/1168\n",
      "Training Step: 908  | total loss: \u001b[1m\u001b[32m0.01886\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 016 | loss: 0.01886 - R2: 0.9996 -- iter: 0460/1168\n",
      "Training Step: 909  | total loss: \u001b[1m\u001b[32m0.01801\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 016 | loss: 0.01801 - R2: 0.9992 -- iter: 0480/1168\n",
      "Training Step: 910  | total loss: \u001b[1m\u001b[32m0.01830\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 016 | loss: 0.01830 - R2: 0.9997 -- iter: 0500/1168\n",
      "Training Step: 911  | total loss: \u001b[1m\u001b[32m0.01862\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 016 | loss: 0.01862 - R2: 1.0006 -- iter: 0520/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 912  | total loss: \u001b[1m\u001b[32m0.01855\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 016 | loss: 0.01855 - R2: 1.0005 -- iter: 0540/1168\n",
      "Training Step: 913  | total loss: \u001b[1m\u001b[32m0.01643\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 016 | loss: 0.01643 - R2: 1.0001 -- iter: 0560/1168\n",
      "Training Step: 914  | total loss: \u001b[1m\u001b[32m0.01797\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 016 | loss: 0.01797 - R2: 1.0004 -- iter: 0580/1168\n",
      "Training Step: 915  | total loss: \u001b[1m\u001b[32m0.01797\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 016 | loss: 0.01797 - R2: 1.0004 -- iter: 0600/1168\n",
      "Training Step: 916  | total loss: \u001b[1m\u001b[32m0.01731\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 016 | loss: 0.01731 - R2: 1.0006 -- iter: 0620/1168\n",
      "Training Step: 917  | total loss: \u001b[1m\u001b[32m0.01694\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 016 | loss: 0.01694 - R2: 1.0001 -- iter: 0640/1168\n",
      "Training Step: 918  | total loss: \u001b[1m\u001b[32m0.01694\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 016 | loss: 0.01694 - R2: 1.0001 -- iter: 0660/1168\n",
      "Training Step: 919  | total loss: \u001b[1m\u001b[32m0.02008\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 016 | loss: 0.02008 - R2: 0.9999 -- iter: 0680/1168\n",
      "Training Step: 920  | total loss: \u001b[1m\u001b[32m0.01912\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 016 | loss: 0.01912 - R2: 0.9994 -- iter: 0700/1168\n",
      "Training Step: 921  | total loss: \u001b[1m\u001b[32m0.01814\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 016 | loss: 0.01814 - R2: 0.9993 -- iter: 0720/1168\n",
      "Training Step: 922  | total loss: \u001b[1m\u001b[32m0.01814\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 016 | loss: 0.01814 - R2: 0.9989 -- iter: 0740/1168\n",
      "Training Step: 923  | total loss: \u001b[1m\u001b[32m0.01714\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 016 | loss: 0.01714 - R2: 0.9989 -- iter: 0760/1168\n",
      "Training Step: 924  | total loss: \u001b[1m\u001b[32m0.01801\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 016 | loss: 0.01801 - R2: 0.9992 -- iter: 0780/1168\n",
      "Training Step: 925  | total loss: \u001b[1m\u001b[32m0.01810\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 016 | loss: 0.01810 - R2: 0.9992 -- iter: 0800/1168\n",
      "Training Step: 926  | total loss: \u001b[1m\u001b[32m0.01771\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 016 | loss: 0.01771 - R2: 1.0001 -- iter: 0820/1168\n",
      "Training Step: 927  | total loss: \u001b[1m\u001b[32m0.01779\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 016 | loss: 0.01779 - R2: 1.0000 -- iter: 0840/1168\n",
      "Training Step: 928  | total loss: \u001b[1m\u001b[32m0.01822\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 016 | loss: 0.01822 - R2: 1.0004 -- iter: 0860/1168\n",
      "Training Step: 929  | total loss: \u001b[1m\u001b[32m0.01784\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 016 | loss: 0.01784 - R2: 1.0001 -- iter: 0880/1168\n",
      "Training Step: 930  | total loss: \u001b[1m\u001b[32m0.01679\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 016 | loss: 0.01679 - R2: 1.0007 -- iter: 0900/1168\n",
      "Training Step: 931  | total loss: \u001b[1m\u001b[32m0.01679\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 016 | loss: 0.01679 - R2: 1.0007 -- iter: 0920/1168\n",
      "Training Step: 932  | total loss: \u001b[1m\u001b[32m0.01703\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 016 | loss: 0.01703 - R2: 1.0013 -- iter: 0940/1168\n",
      "Training Step: 933  | total loss: \u001b[1m\u001b[32m0.01755\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 016 | loss: 0.01755 - R2: 1.0012 -- iter: 0960/1168\n",
      "Training Step: 934  | total loss: \u001b[1m\u001b[32m0.01719\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 016 | loss: 0.01719 - R2: 1.0000 -- iter: 0980/1168\n",
      "Training Step: 935  | total loss: \u001b[1m\u001b[32m0.01670\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 016 | loss: 0.01670 - R2: 0.9998 -- iter: 1000/1168\n",
      "Training Step: 936  | total loss: \u001b[1m\u001b[32m0.01670\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 016 | loss: 0.01670 - R2: 0.9998 -- iter: 1020/1168\n",
      "Training Step: 937  | total loss: \u001b[1m\u001b[32m0.01646\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 016 | loss: 0.01646 - R2: 0.9991 -- iter: 1040/1168\n",
      "Training Step: 938  | total loss: \u001b[1m\u001b[32m0.01545\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 016 | loss: 0.01545 - R2: 0.9990 -- iter: 1060/1168\n",
      "Training Step: 939  | total loss: \u001b[1m\u001b[32m0.01495\u001b[0m\u001b[0m | time: 0.171s\n",
      "| SGD | epoch: 016 | loss: 0.01495 - R2: 0.9998 -- iter: 1080/1168\n",
      "Training Step: 940  | total loss: \u001b[1m\u001b[32m0.01564\u001b[0m\u001b[0m | time: 0.172s\n",
      "| SGD | epoch: 016 | loss: 0.01564 - R2: 1.0002 -- iter: 1100/1168\n",
      "Training Step: 941  | total loss: \u001b[1m\u001b[32m0.02407\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 016 | loss: 0.02407 - R2: 1.0015 -- iter: 1120/1168\n",
      "Training Step: 942  | total loss: \u001b[1m\u001b[32m0.02288\u001b[0m\u001b[0m | time: 0.178s\n",
      "| SGD | epoch: 016 | loss: 0.02288 - R2: 1.0011 -- iter: 1140/1168\n",
      "Training Step: 943  | total loss: \u001b[1m\u001b[32m0.02288\u001b[0m\u001b[0m | time: 0.179s\n",
      "| SGD | epoch: 016 | loss: 0.02288 - R2: 1.0011 -- iter: 1160/1168\n",
      "Training Step: 944  | total loss: \u001b[1m\u001b[32m0.02208\u001b[0m\u001b[0m | time: 1.185s\n",
      "| SGD | epoch: 016 | loss: 0.02208 - R2: 1.0014 | val_loss: 0.02349 - val_acc: 0.9987 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 945  | total loss: \u001b[1m\u001b[32m0.02012\u001b[0m\u001b[0m | time: 0.005s\n",
      "| SGD | epoch: 017 | loss: 0.02012 - R2: 1.0015 -- iter: 0020/1168\n",
      "Training Step: 946  | total loss: \u001b[1m\u001b[32m0.01963\u001b[0m\u001b[0m | time: 0.010s\n",
      "| SGD | epoch: 017 | loss: 0.01963 - R2: 1.0005 -- iter: 0040/1168\n",
      "Training Step: 947  | total loss: \u001b[1m\u001b[32m0.01869\u001b[0m\u001b[0m | time: 0.013s\n",
      "| SGD | epoch: 017 | loss: 0.01869 - R2: 0.9999 -- iter: 0060/1168\n",
      "Training Step: 948  | total loss: \u001b[1m\u001b[32m0.01775\u001b[0m\u001b[0m | time: 0.016s\n",
      "| SGD | epoch: 017 | loss: 0.01775 - R2: 0.9994 -- iter: 0080/1168\n",
      "Training Step: 949  | total loss: \u001b[1m\u001b[32m0.01775\u001b[0m\u001b[0m | time: 0.018s\n",
      "| SGD | epoch: 017 | loss: 0.01775 - R2: 0.9994 -- iter: 0100/1168\n",
      "Training Step: 950  | total loss: \u001b[1m\u001b[32m0.01742\u001b[0m\u001b[0m | time: 0.020s\n",
      "| SGD | epoch: 017 | loss: 0.01742 - R2: 1.0002 -- iter: 0120/1168\n",
      "Training Step: 951  | total loss: \u001b[1m\u001b[32m0.01750\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 017 | loss: 0.01750 - R2: 0.9998 -- iter: 0140/1168\n",
      "Training Step: 952  | total loss: \u001b[1m\u001b[32m0.01750\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 017 | loss: 0.01750 - R2: 0.9992 -- iter: 0160/1168\n",
      "Training Step: 953  | total loss: \u001b[1m\u001b[32m0.01710\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 017 | loss: 0.01710 - R2: 0.9991 -- iter: 0180/1168\n",
      "Training Step: 954  | total loss: \u001b[1m\u001b[32m0.02166\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 017 | loss: 0.02166 - R2: 0.9997 -- iter: 0200/1168\n",
      "Training Step: 955  | total loss: \u001b[1m\u001b[32m0.02166\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 017 | loss: 0.02166 - R2: 0.9999 -- iter: 0220/1168\n",
      "Training Step: 956  | total loss: \u001b[1m\u001b[32m0.02034\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 017 | loss: 0.02034 - R2: 1.0000 -- iter: 0240/1168\n",
      "Training Step: 957  | total loss: \u001b[1m\u001b[32m0.01880\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 017 | loss: 0.01880 - R2: 0.9996 -- iter: 0260/1168\n",
      "Training Step: 958  | total loss: \u001b[1m\u001b[32m0.01813\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 017 | loss: 0.01813 - R2: 0.9996 -- iter: 0280/1168\n",
      "Training Step: 959  | total loss: \u001b[1m\u001b[32m0.01845\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 017 | loss: 0.01845 - R2: 0.9988 -- iter: 0300/1168\n",
      "Training Step: 960  | total loss: \u001b[1m\u001b[32m0.01693\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 017 | loss: 0.01693 - R2: 0.9988 -- iter: 0320/1168\n",
      "Training Step: 961  | total loss: \u001b[1m\u001b[32m0.01550\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 017 | loss: 0.01550 - R2: 0.9989 -- iter: 0340/1168\n",
      "Training Step: 962  | total loss: \u001b[1m\u001b[32m0.01471\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 017 | loss: 0.01471 - R2: 0.9989 -- iter: 0360/1168\n",
      "Training Step: 963  | total loss: \u001b[1m\u001b[32m0.01512\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 017 | loss: 0.01512 - R2: 0.9984 -- iter: 0380/1168\n",
      "Training Step: 964  | total loss: \u001b[1m\u001b[32m0.01556\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 017 | loss: 0.01556 - R2: 0.9984 -- iter: 0400/1168\n",
      "Training Step: 965  | total loss: \u001b[1m\u001b[32m0.03061\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 017 | loss: 0.03061 - R2: 1.0009 -- iter: 0420/1168\n",
      "Training Step: 966  | total loss: \u001b[1m\u001b[32m0.03052\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 017 | loss: 0.03052 - R2: 1.0001 -- iter: 0440/1168\n",
      "Training Step: 967  | total loss: \u001b[1m\u001b[32m0.02899\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 017 | loss: 0.02899 - R2: 1.0004 -- iter: 0460/1168\n",
      "Training Step: 968  | total loss: \u001b[1m\u001b[32m0.02821\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 017 | loss: 0.02821 - R2: 1.0004 -- iter: 0480/1168\n",
      "Training Step: 969  | total loss: \u001b[1m\u001b[32m0.02825\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 017 | loss: 0.02825 - R2: 0.9998 -- iter: 0500/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 970  | total loss: \u001b[1m\u001b[32m0.02903\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 017 | loss: 0.02903 - R2: 0.9995 -- iter: 0520/1168\n",
      "Training Step: 971  | total loss: \u001b[1m\u001b[32m0.02903\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 017 | loss: 0.02903 - R2: 0.9995 -- iter: 0540/1168\n",
      "Training Step: 972  | total loss: \u001b[1m\u001b[32m0.02696\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 017 | loss: 0.02696 - R2: 1.0000 -- iter: 0560/1168\n",
      "Training Step: 973  | total loss: \u001b[1m\u001b[32m0.02713\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 017 | loss: 0.02713 - R2: 0.9987 -- iter: 0580/1168\n",
      "Training Step: 974  | total loss: \u001b[1m\u001b[32m0.02596\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 017 | loss: 0.02596 - R2: 0.9995 -- iter: 0600/1168\n",
      "Training Step: 975  | total loss: \u001b[1m\u001b[32m0.02495\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 017 | loss: 0.02495 - R2: 0.9990 -- iter: 0620/1168\n",
      "Training Step: 976  | total loss: \u001b[1m\u001b[32m0.02356\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 017 | loss: 0.02356 - R2: 1.0005 -- iter: 0640/1168\n",
      "Training Step: 977  | total loss: \u001b[1m\u001b[32m0.02265\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 017 | loss: 0.02265 - R2: 1.0006 -- iter: 0660/1168\n",
      "Training Step: 978  | total loss: \u001b[1m\u001b[32m0.02268\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 017 | loss: 0.02268 - R2: 1.0006 -- iter: 0680/1168\n",
      "Training Step: 979  | total loss: \u001b[1m\u001b[32m0.02180\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 017 | loss: 0.02180 - R2: 1.0000 -- iter: 0700/1168\n",
      "Training Step: 980  | total loss: \u001b[1m\u001b[32m0.02131\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 017 | loss: 0.02131 - R2: 1.0007 -- iter: 0720/1168\n",
      "Training Step: 981  | total loss: \u001b[1m\u001b[32m0.02069\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 017 | loss: 0.02069 - R2: 1.0004 -- iter: 0740/1168\n",
      "Training Step: 982  | total loss: \u001b[1m\u001b[32m0.01951\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 017 | loss: 0.01951 - R2: 1.0004 -- iter: 0760/1168\n",
      "Training Step: 983  | total loss: \u001b[1m\u001b[32m0.02020\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 017 | loss: 0.02020 - R2: 0.9998 -- iter: 0780/1168\n",
      "Training Step: 984  | total loss: \u001b[1m\u001b[32m0.01921\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 017 | loss: 0.01921 - R2: 0.9998 -- iter: 0800/1168\n",
      "Training Step: 985  | total loss: \u001b[1m\u001b[32m0.01841\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 017 | loss: 0.01841 - R2: 1.0000 -- iter: 0820/1168\n",
      "Training Step: 986  | total loss: \u001b[1m\u001b[32m0.01765\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 017 | loss: 0.01765 - R2: 0.9999 -- iter: 0840/1168\n",
      "Training Step: 987  | total loss: \u001b[1m\u001b[32m0.01720\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 017 | loss: 0.01720 - R2: 0.9995 -- iter: 0860/1168\n",
      "Training Step: 988  | total loss: \u001b[1m\u001b[32m0.01947\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 017 | loss: 0.01947 - R2: 0.9997 -- iter: 0880/1168\n",
      "Training Step: 989  | total loss: \u001b[1m\u001b[32m0.01850\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 017 | loss: 0.01850 - R2: 0.9989 -- iter: 0900/1168\n",
      "Training Step: 990  | total loss: \u001b[1m\u001b[32m0.01759\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 017 | loss: 0.01759 - R2: 0.9986 -- iter: 0920/1168\n",
      "Training Step: 991  | total loss: \u001b[1m\u001b[32m0.01704\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 017 | loss: 0.01704 - R2: 0.9990 -- iter: 0940/1168\n",
      "Training Step: 992  | total loss: \u001b[1m\u001b[32m0.01704\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 017 | loss: 0.01704 - R2: 0.9990 -- iter: 0960/1168\n",
      "Training Step: 993  | total loss: \u001b[1m\u001b[32m0.01629\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 017 | loss: 0.01629 - R2: 0.9995 -- iter: 0980/1168\n",
      "Training Step: 994  | total loss: \u001b[1m\u001b[32m0.01632\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 017 | loss: 0.01632 - R2: 0.9998 -- iter: 1000/1168\n",
      "Training Step: 995  | total loss: \u001b[1m\u001b[32m0.01583\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 017 | loss: 0.01583 - R2: 0.9998 -- iter: 1020/1168\n",
      "Training Step: 996  | total loss: \u001b[1m\u001b[32m0.01652\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 017 | loss: 0.01652 - R2: 0.9999 -- iter: 1040/1168\n",
      "Training Step: 997  | total loss: \u001b[1m\u001b[32m0.02310\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 017 | loss: 0.02310 - R2: 1.0008 -- iter: 1060/1168\n",
      "Training Step: 998  | total loss: \u001b[1m\u001b[32m0.02181\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 017 | loss: 0.02181 - R2: 1.0007 -- iter: 1080/1168\n",
      "Training Step: 999  | total loss: \u001b[1m\u001b[32m0.02087\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 017 | loss: 0.02087 - R2: 1.0005 -- iter: 1100/1168\n",
      "Training Step: 1000  | total loss: \u001b[1m\u001b[32m0.02087\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 017 | loss: 0.02087 - R2: 1.0016 -- iter: 1120/1168\n",
      "Training Step: 1001  | total loss: \u001b[1m\u001b[32m0.02096\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 017 | loss: 0.02096 - R2: 1.0010 -- iter: 1140/1168\n",
      "Training Step: 1002  | total loss: \u001b[1m\u001b[32m0.02191\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 017 | loss: 0.02191 - R2: 1.0013 -- iter: 1160/1168\n",
      "Training Step: 1003  | total loss: \u001b[1m\u001b[32m0.02091\u001b[0m\u001b[0m | time: 1.148s\n",
      "| SGD | epoch: 017 | loss: 0.02091 - R2: 1.0010 | val_loss: 0.02375 - val_acc: 0.9996 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 1004  | total loss: \u001b[1m\u001b[32m0.01982\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 018 | loss: 0.01982 - R2: 1.0010 -- iter: 0020/1168\n",
      "Training Step: 1005  | total loss: \u001b[1m\u001b[32m0.01958\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 018 | loss: 0.01958 - R2: 1.0009 -- iter: 0040/1168\n",
      "Training Step: 1006  | total loss: \u001b[1m\u001b[32m0.01838\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 018 | loss: 0.01838 - R2: 1.0009 -- iter: 0060/1168\n",
      "Training Step: 1007  | total loss: \u001b[1m\u001b[32m0.01757\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 018 | loss: 0.01757 - R2: 1.0006 -- iter: 0080/1168\n",
      "Training Step: 1008  | total loss: \u001b[1m\u001b[32m0.01648\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 018 | loss: 0.01648 - R2: 1.0006 -- iter: 0100/1168\n",
      "Training Step: 1009  | total loss: \u001b[1m\u001b[32m0.02681\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 018 | loss: 0.02681 - R2: 1.0012 -- iter: 0120/1168\n",
      "Training Step: 1010  | total loss: \u001b[1m\u001b[32m0.02539\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 018 | loss: 0.02539 - R2: 1.0008 -- iter: 0140/1168\n",
      "Training Step: 1011  | total loss: \u001b[1m\u001b[32m0.02752\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 018 | loss: 0.02752 - R2: 0.9999 -- iter: 0160/1168\n",
      "Training Step: 1012  | total loss: \u001b[1m\u001b[32m0.02669\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 018 | loss: 0.02669 - R2: 1.0001 -- iter: 0180/1168\n",
      "Training Step: 1013  | total loss: \u001b[1m\u001b[32m0.02626\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 018 | loss: 0.02626 - R2: 1.0004 -- iter: 0200/1168\n",
      "Training Step: 1014  | total loss: \u001b[1m\u001b[32m0.02626\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 018 | loss: 0.02626 - R2: 1.0004 -- iter: 0220/1168\n",
      "Training Step: 1015  | total loss: \u001b[1m\u001b[32m0.02592\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 018 | loss: 0.02592 - R2: 0.9990 -- iter: 0240/1168\n",
      "Training Step: 1016  | total loss: \u001b[1m\u001b[32m0.02485\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 018 | loss: 0.02485 - R2: 0.9976 -- iter: 0260/1168\n",
      "Training Step: 1017  | total loss: \u001b[1m\u001b[32m0.02351\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 018 | loss: 0.02351 - R2: 0.9976 -- iter: 0280/1168\n",
      "Training Step: 1018  | total loss: \u001b[1m\u001b[32m0.02250\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 018 | loss: 0.02250 - R2: 0.9979 -- iter: 0300/1168\n",
      "Training Step: 1019  | total loss: \u001b[1m\u001b[32m0.02015\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 018 | loss: 0.02015 - R2: 0.9985 -- iter: 0320/1168\n",
      "Training Step: 1020  | total loss: \u001b[1m\u001b[32m0.02015\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 018 | loss: 0.02015 - R2: 0.9985 -- iter: 0340/1168\n",
      "Training Step: 1021  | total loss: \u001b[1m\u001b[32m0.01916\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 018 | loss: 0.01916 - R2: 0.9988 -- iter: 0360/1168\n",
      "Training Step: 1022  | total loss: \u001b[1m\u001b[32m0.01792\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 018 | loss: 0.01792 - R2: 0.9997 -- iter: 0380/1168\n",
      "Training Step: 1023  | total loss: \u001b[1m\u001b[32m0.01975\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 018 | loss: 0.01975 - R2: 0.9995 -- iter: 0400/1168\n",
      "Training Step: 1024  | total loss: \u001b[1m\u001b[32m0.01975\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 018 | loss: 0.01975 - R2: 0.9993 -- iter: 0420/1168\n",
      "Training Step: 1025  | total loss: \u001b[1m\u001b[32m0.01896\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 018 | loss: 0.01896 - R2: 0.9993 -- iter: 0440/1168\n",
      "Training Step: 1026  | total loss: \u001b[1m\u001b[32m0.01931\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 018 | loss: 0.01931 - R2: 0.9999 -- iter: 0460/1168\n",
      "Training Step: 1027  | total loss: \u001b[1m\u001b[32m0.01807\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 018 | loss: 0.01807 - R2: 0.9993 -- iter: 0480/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1028  | total loss: \u001b[1m\u001b[32m0.01807\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 018 | loss: 0.01807 - R2: 0.9993 -- iter: 0500/1168\n",
      "Training Step: 1029  | total loss: \u001b[1m\u001b[32m0.01802\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 018 | loss: 0.01802 - R2: 1.0002 -- iter: 0520/1168\n",
      "Training Step: 1030  | total loss: \u001b[1m\u001b[32m0.01768\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 018 | loss: 0.01768 - R2: 1.0002 -- iter: 0540/1168\n",
      "Training Step: 1031  | total loss: \u001b[1m\u001b[32m0.01761\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 018 | loss: 0.01761 - R2: 0.9999 -- iter: 0560/1168\n",
      "Training Step: 1032  | total loss: \u001b[1m\u001b[32m0.01694\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 018 | loss: 0.01694 - R2: 1.0009 -- iter: 0580/1168\n",
      "Training Step: 1033  | total loss: \u001b[1m\u001b[32m0.01694\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 018 | loss: 0.01694 - R2: 1.0006 -- iter: 0600/1168\n",
      "Training Step: 1034  | total loss: \u001b[1m\u001b[32m0.01597\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 018 | loss: 0.01597 - R2: 0.9999 -- iter: 0620/1168\n",
      "Training Step: 1035  | total loss: \u001b[1m\u001b[32m0.01741\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 018 | loss: 0.01741 - R2: 0.9997 -- iter: 0640/1168\n",
      "Training Step: 1036  | total loss: \u001b[1m\u001b[32m0.01916\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 018 | loss: 0.01916 - R2: 0.9998 -- iter: 0660/1168\n",
      "Training Step: 1037  | total loss: \u001b[1m\u001b[32m0.01916\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 018 | loss: 0.01916 - R2: 1.0002 -- iter: 0680/1168\n",
      "Training Step: 1038  | total loss: \u001b[1m\u001b[32m0.02178\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 018 | loss: 0.02178 - R2: 1.0002 -- iter: 0700/1168\n",
      "Training Step: 1039  | total loss: \u001b[1m\u001b[32m0.02178\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 018 | loss: 0.02178 - R2: 1.0007 -- iter: 0720/1168\n",
      "Training Step: 1040  | total loss: \u001b[1m\u001b[32m0.02259\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 018 | loss: 0.02259 - R2: 1.0007 -- iter: 0740/1168\n",
      "Training Step: 1041  | total loss: \u001b[1m\u001b[32m0.02171\u001b[0m\u001b[0m | time: 0.165s\n",
      "| SGD | epoch: 018 | loss: 0.02171 - R2: 1.0007 -- iter: 0760/1168\n",
      "Training Step: 1042  | total loss: \u001b[1m\u001b[32m0.02133\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 018 | loss: 0.02133 - R2: 1.0000 -- iter: 0780/1168\n",
      "Training Step: 1043  | total loss: \u001b[1m\u001b[32m0.02342\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 018 | loss: 0.02342 - R2: 0.9997 -- iter: 0800/1168\n",
      "Training Step: 1044  | total loss: \u001b[1m\u001b[32m0.02342\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 018 | loss: 0.02342 - R2: 0.9997 -- iter: 0820/1168\n",
      "Training Step: 1045  | total loss: \u001b[1m\u001b[32m0.02105\u001b[0m\u001b[0m | time: 0.183s\n",
      "| SGD | epoch: 018 | loss: 0.02105 - R2: 1.0003 -- iter: 0840/1168\n",
      "Training Step: 1046  | total loss: \u001b[1m\u001b[32m0.02105\u001b[0m\u001b[0m | time: 0.186s\n",
      "| SGD | epoch: 018 | loss: 0.02105 - R2: 1.0003 -- iter: 0860/1168\n",
      "Training Step: 1047  | total loss: \u001b[1m\u001b[32m0.02035\u001b[0m\u001b[0m | time: 0.188s\n",
      "| SGD | epoch: 018 | loss: 0.02035 - R2: 0.9999 -- iter: 0880/1168\n",
      "Training Step: 1048  | total loss: \u001b[1m\u001b[32m0.02028\u001b[0m\u001b[0m | time: 0.190s\n",
      "| SGD | epoch: 018 | loss: 0.02028 - R2: 0.9996 -- iter: 0900/1168\n",
      "Training Step: 1049  | total loss: \u001b[1m\u001b[32m0.02000\u001b[0m\u001b[0m | time: 0.197s\n",
      "| SGD | epoch: 018 | loss: 0.02000 - R2: 0.9993 -- iter: 0920/1168\n",
      "Training Step: 1050  | total loss: \u001b[1m\u001b[32m0.02052\u001b[0m\u001b[0m | time: 0.200s\n",
      "| SGD | epoch: 018 | loss: 0.02052 - R2: 1.0002 -- iter: 0940/1168\n",
      "Training Step: 1051  | total loss: \u001b[1m\u001b[32m0.02052\u001b[0m\u001b[0m | time: 0.201s\n",
      "| SGD | epoch: 018 | loss: 0.02052 - R2: 1.0002 -- iter: 0960/1168\n",
      "Training Step: 1052  | total loss: \u001b[1m\u001b[32m0.01977\u001b[0m\u001b[0m | time: 0.204s\n",
      "| SGD | epoch: 018 | loss: 0.01977 - R2: 1.0003 -- iter: 0980/1168\n",
      "Training Step: 1053  | total loss: \u001b[1m\u001b[32m0.01978\u001b[0m\u001b[0m | time: 0.206s\n",
      "| SGD | epoch: 018 | loss: 0.01978 - R2: 1.0003 -- iter: 1000/1168\n",
      "Training Step: 1054  | total loss: \u001b[1m\u001b[32m0.01894\u001b[0m\u001b[0m | time: 0.208s\n",
      "| SGD | epoch: 018 | loss: 0.01894 - R2: 1.0005 -- iter: 1020/1168\n",
      "Training Step: 1055  | total loss: \u001b[1m\u001b[32m0.01800\u001b[0m\u001b[0m | time: 0.210s\n",
      "| SGD | epoch: 018 | loss: 0.01800 - R2: 1.0004 -- iter: 1040/1168\n",
      "Training Step: 1056  | total loss: \u001b[1m\u001b[32m0.01784\u001b[0m\u001b[0m | time: 0.212s\n",
      "| SGD | epoch: 018 | loss: 0.01784 - R2: 1.0006 -- iter: 1060/1168\n",
      "Training Step: 1057  | total loss: \u001b[1m\u001b[32m0.01734\u001b[0m\u001b[0m | time: 0.214s\n",
      "| SGD | epoch: 018 | loss: 0.01734 - R2: 1.0013 -- iter: 1080/1168\n",
      "Training Step: 1058  | total loss: \u001b[1m\u001b[32m0.01651\u001b[0m\u001b[0m | time: 0.216s\n",
      "| SGD | epoch: 018 | loss: 0.01651 - R2: 1.0013 -- iter: 1100/1168\n",
      "Training Step: 1059  | total loss: \u001b[1m\u001b[32m0.01678\u001b[0m\u001b[0m | time: 0.217s\n",
      "| SGD | epoch: 018 | loss: 0.01678 - R2: 1.0009 -- iter: 1120/1168\n",
      "Training Step: 1060  | total loss: \u001b[1m\u001b[32m0.01696\u001b[0m\u001b[0m | time: 0.219s\n",
      "| SGD | epoch: 018 | loss: 0.01696 - R2: 1.0009 -- iter: 1140/1168\n",
      "Training Step: 1061  | total loss: \u001b[1m\u001b[32m0.01652\u001b[0m\u001b[0m | time: 0.220s\n",
      "| SGD | epoch: 018 | loss: 0.01652 - R2: 1.0005 -- iter: 1160/1168\n",
      "Training Step: 1062  | total loss: \u001b[1m\u001b[32m0.01586\u001b[0m\u001b[0m | time: 1.225s\n",
      "| SGD | epoch: 018 | loss: 0.01586 - R2: 1.0005 | val_loss: 0.02390 - val_acc: 1.0005 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 1063  | total loss: \u001b[1m\u001b[32m0.01627\u001b[0m\u001b[0m | time: 0.008s\n",
      "| SGD | epoch: 019 | loss: 0.01627 - R2: 0.9999 -- iter: 0020/1168\n",
      "Training Step: 1064  | total loss: \u001b[1m\u001b[32m0.01701\u001b[0m\u001b[0m | time: 0.017s\n",
      "| SGD | epoch: 019 | loss: 0.01701 - R2: 1.0005 -- iter: 0040/1168\n",
      "Training Step: 1065  | total loss: \u001b[1m\u001b[32m0.01625\u001b[0m\u001b[0m | time: 0.019s\n",
      "| SGD | epoch: 019 | loss: 0.01625 - R2: 1.0005 -- iter: 0060/1168\n",
      "Training Step: 1066  | total loss: \u001b[1m\u001b[32m0.01702\u001b[0m\u001b[0m | time: 0.023s\n",
      "| SGD | epoch: 019 | loss: 0.01702 - R2: 1.0001 -- iter: 0080/1168\n",
      "Training Step: 1067  | total loss: \u001b[1m\u001b[32m0.01778\u001b[0m\u001b[0m | time: 0.026s\n",
      "| SGD | epoch: 019 | loss: 0.01778 - R2: 0.9986 -- iter: 0100/1168\n",
      "Training Step: 1068  | total loss: \u001b[1m\u001b[32m0.02161\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 019 | loss: 0.02161 - R2: 0.9994 -- iter: 0120/1168\n",
      "Training Step: 1069  | total loss: \u001b[1m\u001b[32m0.02161\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 019 | loss: 0.02161 - R2: 0.9994 -- iter: 0140/1168\n",
      "Training Step: 1070  | total loss: \u001b[1m\u001b[32m0.02100\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 019 | loss: 0.02100 - R2: 0.9991 -- iter: 0160/1168\n",
      "Training Step: 1071  | total loss: \u001b[1m\u001b[32m0.02140\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 019 | loss: 0.02140 - R2: 0.9989 -- iter: 0180/1168\n",
      "Training Step: 1072  | total loss: \u001b[1m\u001b[32m0.02144\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 019 | loss: 0.02144 - R2: 0.9997 -- iter: 0200/1168\n",
      "Training Step: 1073  | total loss: \u001b[1m\u001b[32m0.02147\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 019 | loss: 0.02147 - R2: 0.9993 -- iter: 0220/1168\n",
      "Training Step: 1074  | total loss: \u001b[1m\u001b[32m0.02028\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 019 | loss: 0.02028 - R2: 0.9991 -- iter: 0240/1168\n",
      "Training Step: 1075  | total loss: \u001b[1m\u001b[32m0.01936\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 019 | loss: 0.01936 - R2: 0.9990 -- iter: 0260/1168\n",
      "Training Step: 1076  | total loss: \u001b[1m\u001b[32m0.01838\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 019 | loss: 0.01838 - R2: 0.9990 -- iter: 0280/1168\n",
      "Training Step: 1077  | total loss: \u001b[1m\u001b[32m0.01845\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 019 | loss: 0.01845 - R2: 0.9985 -- iter: 0300/1168\n",
      "Training Step: 1078  | total loss: \u001b[1m\u001b[32m0.01668\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 019 | loss: 0.01668 - R2: 0.9978 -- iter: 0320/1168\n",
      "Training Step: 1079  | total loss: \u001b[1m\u001b[32m0.01585\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 019 | loss: 0.01585 - R2: 0.9979 -- iter: 0340/1168\n",
      "Training Step: 1080  | total loss: \u001b[1m\u001b[32m0.01496\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 019 | loss: 0.01496 - R2: 0.9981 -- iter: 0360/1168\n",
      "Training Step: 1081  | total loss: \u001b[1m\u001b[32m0.01494\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 019 | loss: 0.01494 - R2: 0.9988 -- iter: 0380/1168\n",
      "Training Step: 1082  | total loss: \u001b[1m\u001b[32m0.01494\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 019 | loss: 0.01494 - R2: 0.9988 -- iter: 0400/1168\n",
      "Training Step: 1083  | total loss: \u001b[1m\u001b[32m0.01563\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 019 | loss: 0.01563 - R2: 0.9991 -- iter: 0420/1168\n",
      "Training Step: 1084  | total loss: \u001b[1m\u001b[32m0.02255\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 019 | loss: 0.02255 - R2: 1.0005 -- iter: 0440/1168\n",
      "Training Step: 1085  | total loss: \u001b[1m\u001b[32m0.02255\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 019 | loss: 0.02255 - R2: 1.0005 -- iter: 0460/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1086  | total loss: \u001b[1m\u001b[32m0.02140\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 019 | loss: 0.02140 - R2: 1.0014 -- iter: 0480/1168\n",
      "Training Step: 1087  | total loss: \u001b[1m\u001b[32m0.02269\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 019 | loss: 0.02269 - R2: 1.0011 -- iter: 0500/1168\n",
      "Training Step: 1088  | total loss: \u001b[1m\u001b[32m0.02269\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 019 | loss: 0.02269 - R2: 1.0011 -- iter: 0520/1168\n",
      "Training Step: 1089  | total loss: \u001b[1m\u001b[32m0.02284\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 019 | loss: 0.02284 - R2: 1.0007 -- iter: 0540/1168\n",
      "Training Step: 1090  | total loss: \u001b[1m\u001b[32m0.03281\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 019 | loss: 0.03281 - R2: 1.0017 -- iter: 0560/1168\n",
      "Training Step: 1091  | total loss: \u001b[1m\u001b[32m0.03061\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 019 | loss: 0.03061 - R2: 1.0019 -- iter: 0580/1168\n",
      "Training Step: 1092  | total loss: \u001b[1m\u001b[32m0.03020\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 019 | loss: 0.03020 - R2: 1.0010 -- iter: 0600/1168\n",
      "Training Step: 1093  | total loss: \u001b[1m\u001b[32m0.03115\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 019 | loss: 0.03115 - R2: 0.9998 -- iter: 0620/1168\n",
      "Training Step: 1094  | total loss: \u001b[1m\u001b[32m0.02639\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 019 | loss: 0.02639 - R2: 0.9999 -- iter: 0640/1168\n",
      "Training Step: 1095  | total loss: \u001b[1m\u001b[32m0.02639\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 019 | loss: 0.02639 - R2: 0.9999 -- iter: 0660/1168\n",
      "Training Step: 1096  | total loss: \u001b[1m\u001b[32m0.02594\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 019 | loss: 0.02594 - R2: 0.9989 -- iter: 0680/1168\n",
      "Training Step: 1097  | total loss: \u001b[1m\u001b[32m0.02532\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 019 | loss: 0.02532 - R2: 0.9989 -- iter: 0700/1168\n",
      "Training Step: 1098  | total loss: \u001b[1m\u001b[32m0.02373\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 019 | loss: 0.02373 - R2: 0.9991 -- iter: 0720/1168\n",
      "Training Step: 1099  | total loss: \u001b[1m\u001b[32m0.02444\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 019 | loss: 0.02444 - R2: 0.9987 -- iter: 0740/1168\n",
      "Training Step: 1100  | total loss: \u001b[1m\u001b[32m0.02277\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 019 | loss: 0.02277 - R2: 1.0000 -- iter: 0760/1168\n",
      "Training Step: 1101  | total loss: \u001b[1m\u001b[32m0.02277\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 019 | loss: 0.02277 - R2: 1.0000 -- iter: 0780/1168\n",
      "Training Step: 1102  | total loss: \u001b[1m\u001b[32m0.02160\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 019 | loss: 0.02160 - R2: 0.9998 -- iter: 0800/1168\n",
      "Training Step: 1103  | total loss: \u001b[1m\u001b[32m0.02160\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 019 | loss: 0.02160 - R2: 0.9998 -- iter: 0820/1168\n",
      "Training Step: 1104  | total loss: \u001b[1m\u001b[32m0.02037\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 019 | loss: 0.02037 - R2: 1.0001 -- iter: 0840/1168\n",
      "Training Step: 1105  | total loss: \u001b[1m\u001b[32m0.01935\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 019 | loss: 0.01935 - R2: 1.0000 -- iter: 0860/1168\n",
      "Training Step: 1106  | total loss: \u001b[1m\u001b[32m0.02036\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 019 | loss: 0.02036 - R2: 1.0000 -- iter: 0880/1168\n",
      "Training Step: 1107  | total loss: \u001b[1m\u001b[32m0.01924\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 019 | loss: 0.01924 - R2: 1.0001 -- iter: 0900/1168\n",
      "Training Step: 1108  | total loss: \u001b[1m\u001b[32m0.01831\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 019 | loss: 0.01831 - R2: 1.0002 -- iter: 0920/1168\n",
      "Training Step: 1109  | total loss: \u001b[1m\u001b[32m0.01831\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 019 | loss: 0.01831 - R2: 1.0002 -- iter: 0940/1168\n",
      "Training Step: 1110  | total loss: \u001b[1m\u001b[32m0.01759\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 019 | loss: 0.01759 - R2: 0.9994 -- iter: 0960/1168\n",
      "Training Step: 1111  | total loss: \u001b[1m\u001b[32m0.01722\u001b[0m\u001b[0m | time: 0.173s\n",
      "| SGD | epoch: 019 | loss: 0.01722 - R2: 0.9993 -- iter: 0980/1168\n",
      "Training Step: 1112  | total loss: \u001b[1m\u001b[32m0.01722\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 019 | loss: 0.01722 - R2: 0.9993 -- iter: 1000/1168\n",
      "Training Step: 1113  | total loss: \u001b[1m\u001b[32m0.01955\u001b[0m\u001b[0m | time: 0.183s\n",
      "| SGD | epoch: 019 | loss: 0.01955 - R2: 1.0004 -- iter: 1020/1168\n",
      "Training Step: 1114  | total loss: \u001b[1m\u001b[32m0.01822\u001b[0m\u001b[0m | time: 0.197s\n",
      "| SGD | epoch: 019 | loss: 0.01822 - R2: 1.0006 -- iter: 1040/1168\n",
      "Training Step: 1115  | total loss: \u001b[1m\u001b[32m0.01761\u001b[0m\u001b[0m | time: 0.202s\n",
      "| SGD | epoch: 019 | loss: 0.01761 - R2: 1.0000 -- iter: 1060/1168\n",
      "Training Step: 1116  | total loss: \u001b[1m\u001b[32m0.01744\u001b[0m\u001b[0m | time: 0.206s\n",
      "| SGD | epoch: 019 | loss: 0.01744 - R2: 0.9999 -- iter: 1080/1168\n",
      "Training Step: 1117  | total loss: \u001b[1m\u001b[32m0.01744\u001b[0m\u001b[0m | time: 0.210s\n",
      "| SGD | epoch: 019 | loss: 0.01744 - R2: 0.9999 -- iter: 1100/1168\n",
      "Training Step: 1118  | total loss: \u001b[1m\u001b[32m0.01795\u001b[0m\u001b[0m | time: 0.213s\n",
      "| SGD | epoch: 019 | loss: 0.01795 - R2: 0.9999 -- iter: 1120/1168\n",
      "Training Step: 1119  | total loss: \u001b[1m\u001b[32m0.01795\u001b[0m\u001b[0m | time: 0.217s\n",
      "| SGD | epoch: 019 | loss: 0.01795 - R2: 0.9999 -- iter: 1140/1168\n",
      "Training Step: 1120  | total loss: \u001b[1m\u001b[32m0.01715\u001b[0m\u001b[0m | time: 0.220s\n",
      "| SGD | epoch: 019 | loss: 0.01715 - R2: 1.0003 -- iter: 1160/1168\n",
      "Training Step: 1121  | total loss: \u001b[1m\u001b[32m0.01715\u001b[0m\u001b[0m | time: 1.228s\n",
      "| SGD | epoch: 019 | loss: 0.01715 - R2: 1.0003 | val_loss: 0.02428 - val_acc: 1.0017 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 1122  | total loss: \u001b[1m\u001b[32m0.01669\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 020 | loss: 0.01669 - R2: 0.9998 -- iter: 0020/1168\n",
      "Training Step: 1123  | total loss: \u001b[1m\u001b[32m0.01788\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 020 | loss: 0.01788 - R2: 1.0005 -- iter: 0040/1168\n",
      "Training Step: 1124  | total loss: \u001b[1m\u001b[32m0.01816\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 020 | loss: 0.01816 - R2: 1.0005 -- iter: 0060/1168\n",
      "Training Step: 1125  | total loss: \u001b[1m\u001b[32m0.01819\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 020 | loss: 0.01819 - R2: 1.0000 -- iter: 0080/1168\n",
      "Training Step: 1126  | total loss: \u001b[1m\u001b[32m0.02464\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 020 | loss: 0.02464 - R2: 1.0012 -- iter: 0100/1168\n",
      "Training Step: 1127  | total loss: \u001b[1m\u001b[32m0.02360\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 020 | loss: 0.02360 - R2: 1.0002 -- iter: 0120/1168\n",
      "Training Step: 1128  | total loss: \u001b[1m\u001b[32m0.02266\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 020 | loss: 0.02266 - R2: 0.9992 -- iter: 0140/1168\n",
      "Training Step: 1129  | total loss: \u001b[1m\u001b[32m0.02266\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 020 | loss: 0.02266 - R2: 0.9992 -- iter: 0160/1168\n",
      "Training Step: 1130  | total loss: \u001b[1m\u001b[32m0.02129\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 020 | loss: 0.02129 - R2: 0.9990 -- iter: 0180/1168\n",
      "Training Step: 1131  | total loss: \u001b[1m\u001b[32m0.02129\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 020 | loss: 0.02129 - R2: 0.9990 -- iter: 0200/1168\n",
      "Training Step: 1132  | total loss: \u001b[1m\u001b[32m0.02050\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 020 | loss: 0.02050 - R2: 0.9991 -- iter: 0220/1168\n",
      "Training Step: 1133  | total loss: \u001b[1m\u001b[32m0.01917\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 020 | loss: 0.01917 - R2: 0.9997 -- iter: 0240/1168\n",
      "Training Step: 1134  | total loss: \u001b[1m\u001b[32m0.01874\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 020 | loss: 0.01874 - R2: 0.9991 -- iter: 0260/1168\n",
      "Training Step: 1135  | total loss: \u001b[1m\u001b[32m0.01874\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 020 | loss: 0.01874 - R2: 0.9991 -- iter: 0280/1168\n",
      "Training Step: 1136  | total loss: \u001b[1m\u001b[32m0.01812\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 020 | loss: 0.01812 - R2: 0.9988 -- iter: 0300/1168\n",
      "Training Step: 1137  | total loss: \u001b[1m\u001b[32m0.01643\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 020 | loss: 0.01643 - R2: 0.9988 -- iter: 0320/1168\n",
      "Training Step: 1138  | total loss: \u001b[1m\u001b[32m0.01643\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 020 | loss: 0.01643 - R2: 1.0015 -- iter: 0340/1168\n",
      "Training Step: 1139  | total loss: \u001b[1m\u001b[32m0.02732\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 020 | loss: 0.02732 - R2: 1.0015 -- iter: 0360/1168\n",
      "Training Step: 1140  | total loss: \u001b[1m\u001b[32m0.02747\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 020 | loss: 0.02747 - R2: 1.0019 -- iter: 0380/1168\n",
      "Training Step: 1141  | total loss: \u001b[1m\u001b[32m0.02659\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 020 | loss: 0.02659 - R2: 1.0019 -- iter: 0400/1168\n",
      "Training Step: 1142  | total loss: \u001b[1m\u001b[32m0.02488\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 020 | loss: 0.02488 - R2: 1.0011 -- iter: 0420/1168\n",
      "Training Step: 1143  | total loss: \u001b[1m\u001b[32m0.02395\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 020 | loss: 0.02395 - R2: 1.0008 -- iter: 0440/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1144  | total loss: \u001b[1m\u001b[32m0.02457\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 020 | loss: 0.02457 - R2: 1.0008 -- iter: 0460/1168\n",
      "Training Step: 1145  | total loss: \u001b[1m\u001b[32m0.02348\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 020 | loss: 0.02348 - R2: 0.9997 -- iter: 0480/1168\n",
      "Training Step: 1146  | total loss: \u001b[1m\u001b[32m0.02160\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 020 | loss: 0.02160 - R2: 0.9995 -- iter: 0500/1168\n",
      "Training Step: 1147  | total loss: \u001b[1m\u001b[32m0.02117\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 020 | loss: 0.02117 - R2: 1.0003 -- iter: 0520/1168\n",
      "Training Step: 1148  | total loss: \u001b[1m\u001b[32m0.02163\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 020 | loss: 0.02163 - R2: 1.0006 -- iter: 0540/1168\n",
      "Training Step: 1149  | total loss: \u001b[1m\u001b[32m0.02077\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 020 | loss: 0.02077 - R2: 1.0006 -- iter: 0560/1168\n",
      "Training Step: 1150  | total loss: \u001b[1m\u001b[32m0.02037\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 020 | loss: 0.02037 - R2: 1.0001 -- iter: 0580/1168\n",
      "Training Step: 1151  | total loss: \u001b[1m\u001b[32m0.01974\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 020 | loss: 0.01974 - R2: 1.0008 -- iter: 0600/1168\n",
      "Training Step: 1152  | total loss: \u001b[1m\u001b[32m0.01974\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 020 | loss: 0.01974 - R2: 1.0008 -- iter: 0620/1168\n",
      "Training Step: 1153  | total loss: \u001b[1m\u001b[32m0.01941\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 020 | loss: 0.01941 - R2: 1.0008 -- iter: 0640/1168\n",
      "Training Step: 1154  | total loss: \u001b[1m\u001b[32m0.01778\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 020 | loss: 0.01778 - R2: 1.0006 -- iter: 0660/1168\n",
      "Training Step: 1155  | total loss: \u001b[1m\u001b[32m0.02031\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 020 | loss: 0.02031 - R2: 0.9992 -- iter: 0680/1168\n",
      "Training Step: 1156  | total loss: \u001b[1m\u001b[32m0.02031\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 020 | loss: 0.02031 - R2: 0.9990 -- iter: 0700/1168\n",
      "Training Step: 1157  | total loss: \u001b[1m\u001b[32m0.02015\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 020 | loss: 0.02015 - R2: 0.9990 -- iter: 0720/1168\n",
      "Training Step: 1158  | total loss: \u001b[1m\u001b[32m0.01855\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 020 | loss: 0.01855 - R2: 1.0001 -- iter: 0740/1168\n",
      "Training Step: 1159  | total loss: \u001b[1m\u001b[32m0.01788\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 020 | loss: 0.01788 - R2: 1.0006 -- iter: 0760/1168\n",
      "Training Step: 1160  | total loss: \u001b[1m\u001b[32m0.01788\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 020 | loss: 0.01788 - R2: 1.0006 -- iter: 0780/1168\n",
      "Training Step: 1161  | total loss: \u001b[1m\u001b[32m0.01690\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 020 | loss: 0.01690 - R2: 1.0004 -- iter: 0800/1168\n",
      "Training Step: 1162  | total loss: \u001b[1m\u001b[32m0.01937\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 020 | loss: 0.01937 - R2: 0.9995 -- iter: 0820/1168\n",
      "Training Step: 1163  | total loss: \u001b[1m\u001b[32m0.01937\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 020 | loss: 0.01937 - R2: 0.9995 -- iter: 0840/1168\n",
      "Training Step: 1164  | total loss: \u001b[1m\u001b[32m0.02245\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 020 | loss: 0.02245 - R2: 1.0005 -- iter: 0860/1168\n",
      "Training Step: 1165  | total loss: \u001b[1m\u001b[32m0.02245\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 020 | loss: 0.02245 - R2: 1.0005 -- iter: 0880/1168\n",
      "Training Step: 1166  | total loss: \u001b[1m\u001b[32m0.02458\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 020 | loss: 0.02458 - R2: 0.9995 -- iter: 0900/1168\n",
      "Training Step: 1167  | total loss: \u001b[1m\u001b[32m0.02458\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 020 | loss: 0.02458 - R2: 0.9995 -- iter: 0920/1168\n",
      "Training Step: 1168  | total loss: \u001b[1m\u001b[32m0.05504\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 020 | loss: 0.05504 - R2: 1.0015 -- iter: 0940/1168\n",
      "Training Step: 1169  | total loss: \u001b[1m\u001b[32m0.04806\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 020 | loss: 0.04806 - R2: 1.0016 -- iter: 0960/1168\n",
      "Training Step: 1170  | total loss: \u001b[1m\u001b[32m0.04806\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 020 | loss: 0.04806 - R2: 1.0015 -- iter: 0980/1168\n",
      "Training Step: 1171  | total loss: \u001b[1m\u001b[32m0.04857\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 020 | loss: 0.04857 - R2: 1.0015 -- iter: 1000/1168\n",
      "Training Step: 1172  | total loss: \u001b[1m\u001b[32m0.04564\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 020 | loss: 0.04564 - R2: 1.0012 -- iter: 1020/1168\n",
      "Training Step: 1173  | total loss: \u001b[1m\u001b[32m0.04297\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 020 | loss: 0.04297 - R2: 1.0005 -- iter: 1040/1168\n",
      "Training Step: 1174  | total loss: \u001b[1m\u001b[32m0.03990\u001b[0m\u001b[0m | time: 0.172s\n",
      "| SGD | epoch: 020 | loss: 0.03990 - R2: 1.0009 -- iter: 1060/1168\n",
      "Training Step: 1175  | total loss: \u001b[1m\u001b[32m0.03777\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 020 | loss: 0.03777 - R2: 0.9994 -- iter: 1080/1168\n",
      "Training Step: 1176  | total loss: \u001b[1m\u001b[32m0.03603\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 020 | loss: 0.03603 - R2: 0.9994 -- iter: 1100/1168\n",
      "Training Step: 1177  | total loss: \u001b[1m\u001b[32m0.03517\u001b[0m\u001b[0m | time: 0.179s\n",
      "| SGD | epoch: 020 | loss: 0.03517 - R2: 0.9993 -- iter: 1120/1168\n",
      "Training Step: 1178  | total loss: \u001b[1m\u001b[32m0.03334\u001b[0m\u001b[0m | time: 0.180s\n",
      "| SGD | epoch: 020 | loss: 0.03334 - R2: 0.9997 -- iter: 1140/1168\n",
      "Training Step: 1179  | total loss: \u001b[1m\u001b[32m0.03060\u001b[0m\u001b[0m | time: 0.182s\n",
      "| SGD | epoch: 020 | loss: 0.03060 - R2: 0.9996 -- iter: 1160/1168\n",
      "Training Step: 1180  | total loss: \u001b[1m\u001b[32m0.03139\u001b[0m\u001b[0m | time: 1.187s\n",
      "| SGD | epoch: 020 | loss: 0.03139 - R2: 0.9991 | val_loss: 0.02419 - val_acc: 1.0006 -- iter: 1168/1168\n",
      "--\n",
      "---------------------------------\n",
      "Run id: WBJVCB\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "INFO:tensorflow:Summary name StandardError/ (raw) is illegal; using StandardError/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 1168\n",
      "Validation samples: 292\n",
      "--\n",
      "Training Step: 1  | time: 0.029s\n",
      "| SGD | epoch: 001 | loss: 0.00000 - R2: 0.0000 -- iter: 0050/1168\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m130.47955\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 001 | loss: 130.47955 - R2: 0.0000 -- iter: 0100/1168\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m141.75780\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 001 | loss: 141.75780 - R2: 0.0001 -- iter: 0150/1168\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m140.95346\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 001 | loss: 140.95346 - R2: 0.0001 -- iter: 0200/1168\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m140.13831\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 001 | loss: 140.13831 - R2: 0.0002 -- iter: 0250/1168\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m140.13831\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 001 | loss: 140.13831 - R2: 0.0002 -- iter: 0300/1168\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m138.47232\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 001 | loss: 138.47232 - R2: 0.0005 -- iter: 0350/1168\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m138.08218\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 001 | loss: 138.08218 - R2: 0.0007 -- iter: 0400/1168\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m138.08218\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 001 | loss: 138.08218 - R2: 0.0009 -- iter: 0450/1168\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m135.03726\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 001 | loss: 135.03726 - R2: 0.0011 -- iter: 0500/1168\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m134.04544\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 001 | loss: 134.04544 - R2: 0.0014 -- iter: 0550/1168\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m132.98273\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 001 | loss: 132.98273 - R2: 0.0017 -- iter: 0600/1168\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m132.85269\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 001 | loss: 132.85269 - R2: 0.0020 -- iter: 0650/1168\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m131.74986\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 001 | loss: 131.74986 - R2: 0.0023 -- iter: 0700/1168\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m130.70528\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 001 | loss: 130.70528 - R2: 0.0027 -- iter: 0750/1168\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m128.33902\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 001 | loss: 128.33902 - R2: 0.0032 -- iter: 0800/1168\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m127.57419\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 001 | loss: 127.57419 - R2: 0.0036 -- iter: 0850/1168\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m126.99025\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 001 | loss: 126.99025 - R2: 0.0040 -- iter: 0900/1168\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m126.99025\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 001 | loss: 126.99025 - R2: 0.0040 -- iter: 0950/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m126.52018\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 001 | loss: 126.52018 - R2: 0.0050 -- iter: 1000/1168\n",
      "Training Step: 21  | total loss: \u001b[1m\u001b[32m125.32157\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 001 | loss: 125.32157 - R2: 0.0050 -- iter: 1050/1168\n",
      "Training Step: 22  | total loss: \u001b[1m\u001b[32m122.21944\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 001 | loss: 122.21944 - R2: 0.0056 -- iter: 1100/1168\n",
      "Training Step: 23  | total loss: \u001b[1m\u001b[32m121.22050\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 001 | loss: 121.22050 - R2: 0.0068 -- iter: 1150/1168\n",
      "Training Step: 24  | total loss: \u001b[1m\u001b[32m119.43989\u001b[0m\u001b[0m | time: 1.164s\n",
      "| SGD | epoch: 001 | loss: 119.43989 - R2: 0.0075 | val_loss: 118.12394 - val_acc: 0.0097 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 25  | total loss: \u001b[1m\u001b[32m119.43989\u001b[0m\u001b[0m | time: 0.005s\n",
      "| SGD | epoch: 002 | loss: 119.43989 - R2: 0.0082 -- iter: 0050/1168\n",
      "Training Step: 26  | total loss: \u001b[1m\u001b[32m117.90457\u001b[0m\u001b[0m | time: 0.010s\n",
      "| SGD | epoch: 002 | loss: 117.90457 - R2: 0.0082 -- iter: 0100/1168\n",
      "Training Step: 27  | total loss: \u001b[1m\u001b[32m117.56171\u001b[0m\u001b[0m | time: 0.014s\n",
      "| SGD | epoch: 002 | loss: 117.56171 - R2: 0.0088 -- iter: 0150/1168\n",
      "Training Step: 28  | total loss: \u001b[1m\u001b[32m116.81805\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 002 | loss: 116.81805 - R2: 0.0102 -- iter: 0200/1168\n",
      "Training Step: 29  | total loss: \u001b[1m\u001b[32m115.71105\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 002 | loss: 115.71105 - R2: 0.0109 -- iter: 0250/1168\n",
      "Training Step: 30  | total loss: \u001b[1m\u001b[32m115.71105\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 002 | loss: 115.71105 - R2: 0.0116 -- iter: 0300/1168\n",
      "Training Step: 31  | total loss: \u001b[1m\u001b[32m115.18224\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 002 | loss: 115.18224 - R2: 0.0116 -- iter: 0350/1168\n",
      "Training Step: 32  | total loss: \u001b[1m\u001b[32m113.07323\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 002 | loss: 113.07323 - R2: 0.0133 -- iter: 0400/1168\n",
      "Training Step: 33  | total loss: \u001b[1m\u001b[32m112.52209\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 002 | loss: 112.52209 - R2: 0.0141 -- iter: 0450/1168\n",
      "Training Step: 34  | total loss: \u001b[1m\u001b[32m112.52209\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 002 | loss: 112.52209 - R2: 0.0141 -- iter: 0500/1168\n",
      "Training Step: 35  | total loss: \u001b[1m\u001b[32m110.55417\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 002 | loss: 110.55417 - R2: 0.0160 -- iter: 0550/1168\n",
      "Training Step: 36  | total loss: \u001b[1m\u001b[32m110.55417\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 002 | loss: 110.55417 - R2: 0.0160 -- iter: 0600/1168\n",
      "Training Step: 37  | total loss: \u001b[1m\u001b[32m109.38945\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 002 | loss: 109.38945 - R2: 0.0170 -- iter: 0650/1168\n",
      "Training Step: 38  | total loss: \u001b[1m\u001b[32m107.60021\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 002 | loss: 107.60021 - R2: 0.0190 -- iter: 0700/1168\n",
      "Training Step: 39  | total loss: \u001b[1m\u001b[32m107.10110\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 002 | loss: 107.10110 - R2: 0.0199 -- iter: 0750/1168\n",
      "Training Step: 40  | total loss: \u001b[1m\u001b[32m107.10110\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 002 | loss: 107.10110 - R2: 0.0199 -- iter: 0800/1168\n",
      "Training Step: 41  | total loss: \u001b[1m\u001b[32m105.37257\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 002 | loss: 105.37257 - R2: 0.0220 -- iter: 0850/1168\n",
      "Training Step: 42  | total loss: \u001b[1m\u001b[32m104.41809\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 002 | loss: 104.41809 - R2: 0.0232 -- iter: 0900/1168\n",
      "Training Step: 43  | total loss: \u001b[1m\u001b[32m104.41809\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 002 | loss: 104.41809 - R2: 0.0232 -- iter: 0950/1168\n",
      "Training Step: 44  | total loss: \u001b[1m\u001b[32m101.91952\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 002 | loss: 101.91952 - R2: 0.0257 -- iter: 1000/1168\n",
      "Training Step: 45  | total loss: \u001b[1m\u001b[32m101.51431\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 002 | loss: 101.51431 - R2: 0.0268 -- iter: 1050/1168\n",
      "Training Step: 46  | total loss: \u001b[1m\u001b[32m101.51431\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 002 | loss: 101.51431 - R2: 0.0268 -- iter: 1100/1168\n",
      "Training Step: 47  | total loss: \u001b[1m\u001b[32m99.57433\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 002 | loss: 99.57433 - R2: 0.0293 -- iter: 1150/1168\n",
      "Training Step: 48  | total loss: \u001b[1m\u001b[32m98.62059\u001b[0m\u001b[0m | time: 1.118s\n",
      "| SGD | epoch: 002 | loss: 98.62059 - R2: 0.0307 | val_loss: 93.64720 - val_acc: 0.0390 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 49  | total loss: \u001b[1m\u001b[32m98.62059\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 003 | loss: 98.62059 - R2: 0.0320 -- iter: 0050/1168\n",
      "Training Step: 50  | total loss: \u001b[1m\u001b[32m97.81812\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 003 | loss: 97.81812 - R2: 0.0320 -- iter: 0100/1168\n",
      "Training Step: 51  | total loss: \u001b[1m\u001b[32m96.23317\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 003 | loss: 96.23317 - R2: 0.0347 -- iter: 0150/1168\n",
      "Training Step: 52  | total loss: \u001b[1m\u001b[32m96.23317\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 003 | loss: 96.23317 - R2: 0.0347 -- iter: 0200/1168\n",
      "Training Step: 53  | total loss: \u001b[1m\u001b[32m95.36849\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 003 | loss: 95.36849 - R2: 0.0361 -- iter: 0250/1168\n",
      "Training Step: 54  | total loss: \u001b[1m\u001b[32m94.37862\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 003 | loss: 94.37862 - R2: 0.0376 -- iter: 0300/1168\n",
      "Training Step: 55  | total loss: \u001b[1m\u001b[32m92.33658\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 003 | loss: 92.33658 - R2: 0.0408 -- iter: 0350/1168\n",
      "Training Step: 56  | total loss: \u001b[1m\u001b[32m92.33658\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 003 | loss: 92.33658 - R2: 0.0408 -- iter: 0400/1168\n",
      "Training Step: 57  | total loss: \u001b[1m\u001b[32m91.26509\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 003 | loss: 91.26509 - R2: 0.0425 -- iter: 0450/1168\n",
      "Training Step: 58  | total loss: \u001b[1m\u001b[32m90.29823\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 003 | loss: 90.29823 - R2: 0.0442 -- iter: 0500/1168\n",
      "Training Step: 59  | total loss: \u001b[1m\u001b[32m89.52493\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 003 | loss: 89.52493 - R2: 0.0458 -- iter: 0550/1168\n",
      "Training Step: 60  | total loss: \u001b[1m\u001b[32m87.86414\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 003 | loss: 87.86414 - R2: 0.0492 -- iter: 0600/1168\n",
      "Training Step: 61  | total loss: \u001b[1m\u001b[32m87.05374\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 003 | loss: 87.05374 - R2: 0.0509 -- iter: 0650/1168\n",
      "Training Step: 62  | total loss: \u001b[1m\u001b[32m87.05374\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 003 | loss: 87.05374 - R2: 0.0509 -- iter: 0700/1168\n",
      "Training Step: 63  | total loss: \u001b[1m\u001b[32m86.43458\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 003 | loss: 86.43458 - R2: 0.0546 -- iter: 0750/1168\n",
      "Training Step: 64  | total loss: \u001b[1m\u001b[32m84.29594\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 003 | loss: 84.29594 - R2: 0.0567 -- iter: 0800/1168\n",
      "Training Step: 65  | total loss: \u001b[1m\u001b[32m84.29594\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 003 | loss: 84.29594 - R2: 0.0567 -- iter: 0850/1168\n",
      "Training Step: 66  | total loss: \u001b[1m\u001b[32m83.77589\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 003 | loss: 83.77589 - R2: 0.0585 -- iter: 0900/1168\n",
      "Training Step: 67  | total loss: \u001b[1m\u001b[32m82.83978\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 003 | loss: 82.83978 - R2: 0.0605 -- iter: 0950/1168\n",
      "Training Step: 68  | total loss: \u001b[1m\u001b[32m82.12325\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 003 | loss: 82.12325 - R2: 0.0625 -- iter: 1000/1168\n",
      "Training Step: 69  | total loss: \u001b[1m\u001b[32m81.18850\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 003 | loss: 81.18850 - R2: 0.0647 -- iter: 1050/1168\n",
      "Training Step: 70  | total loss: \u001b[1m\u001b[32m80.15781\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 003 | loss: 80.15781 - R2: 0.0670 -- iter: 1100/1168\n",
      "Training Step: 71  | total loss: \u001b[1m\u001b[32m78.42262\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 003 | loss: 78.42262 - R2: 0.0716 -- iter: 1150/1168\n",
      "Training Step: 72  | total loss: \u001b[1m\u001b[32m77.44548\u001b[0m\u001b[0m | time: 1.132s\n",
      "| SGD | epoch: 003 | loss: 77.44548 - R2: 0.0740 | val_loss: 69.12501 - val_acc: 0.0966 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 73  | total loss: \u001b[1m\u001b[32m76.57780\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 004 | loss: 76.57780 - R2: 0.0764 -- iter: 0050/1168\n",
      "Training Step: 74  | total loss: \u001b[1m\u001b[32m75.40105\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 004 | loss: 75.40105 - R2: 0.0793 -- iter: 0100/1168\n",
      "Training Step: 75  | total loss: \u001b[1m\u001b[32m74.24712\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 004 | loss: 74.24712 - R2: 0.0821 -- iter: 0150/1168\n",
      "Training Step: 76  | total loss: \u001b[1m\u001b[32m73.34504\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 004 | loss: 73.34504 - R2: 0.0848 -- iter: 0200/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 77  | total loss: \u001b[1m\u001b[32m73.34504\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 004 | loss: 73.34504 - R2: 0.0848 -- iter: 0250/1168\n",
      "Training Step: 78  | total loss: \u001b[1m\u001b[32m71.31438\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 004 | loss: 71.31438 - R2: 0.0906 -- iter: 0300/1168\n",
      "Training Step: 79  | total loss: \u001b[1m\u001b[32m70.28136\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 004 | loss: 70.28136 - R2: 0.0937 -- iter: 0350/1168\n",
      "Training Step: 80  | total loss: \u001b[1m\u001b[32m69.23603\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 004 | loss: 69.23603 - R2: 0.0969 -- iter: 0400/1168\n",
      "Training Step: 81  | total loss: \u001b[1m\u001b[32m68.14595\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 004 | loss: 68.14595 - R2: 0.1002 -- iter: 0450/1168\n",
      "Training Step: 82  | total loss: \u001b[1m\u001b[32m67.13463\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 004 | loss: 67.13463 - R2: 0.1035 -- iter: 0500/1168\n",
      "Training Step: 83  | total loss: \u001b[1m\u001b[32m67.13463\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 004 | loss: 67.13463 - R2: 0.1035 -- iter: 0550/1168\n",
      "Training Step: 84  | total loss: \u001b[1m\u001b[32m65.04190\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 004 | loss: 65.04190 - R2: 0.1106 -- iter: 0600/1168\n",
      "Training Step: 85  | total loss: \u001b[1m\u001b[32m65.04190\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 004 | loss: 65.04190 - R2: 0.1106 -- iter: 0650/1168\n",
      "Training Step: 86  | total loss: \u001b[1m\u001b[32m63.88946\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 004 | loss: 63.88946 - R2: 0.1146 -- iter: 0700/1168\n",
      "Training Step: 87  | total loss: \u001b[1m\u001b[32m62.05320\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 004 | loss: 62.05320 - R2: 0.1220 -- iter: 0750/1168\n",
      "Training Step: 88  | total loss: \u001b[1m\u001b[32m60.99800\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 004 | loss: 60.99800 - R2: 0.1261 -- iter: 0800/1168\n",
      "Training Step: 89  | total loss: \u001b[1m\u001b[32m59.95611\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 004 | loss: 59.95611 - R2: 0.1303 -- iter: 0850/1168\n",
      "Training Step: 90  | total loss: \u001b[1m\u001b[32m59.95611\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 004 | loss: 59.95611 - R2: 0.1303 -- iter: 0900/1168\n",
      "Training Step: 91  | total loss: \u001b[1m\u001b[32m58.88746\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 004 | loss: 58.88746 - R2: 0.1347 -- iter: 0950/1168\n",
      "Training Step: 92  | total loss: \u001b[1m\u001b[32m56.78438\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 004 | loss: 56.78438 - R2: 0.1439 -- iter: 1000/1168\n",
      "Training Step: 93  | total loss: \u001b[1m\u001b[32m55.71790\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 004 | loss: 55.71790 - R2: 0.1487 -- iter: 1050/1168\n",
      "Training Step: 94  | total loss: \u001b[1m\u001b[32m54.52996\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 004 | loss: 54.52996 - R2: 0.1540 -- iter: 1100/1168\n",
      "Training Step: 95  | total loss: \u001b[1m\u001b[32m53.50706\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 004 | loss: 53.50706 - R2: 0.1591 -- iter: 1150/1168\n",
      "Training Step: 96  | total loss: \u001b[1m\u001b[32m53.50706\u001b[0m\u001b[0m | time: 1.158s\n",
      "| SGD | epoch: 004 | loss: 53.50706 - R2: 0.1591 | val_loss: 40.78600 - val_acc: 0.2218 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 97  | total loss: \u001b[1m\u001b[32m51.19344\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 005 | loss: 51.19344 - R2: 0.1703 -- iter: 0050/1168\n",
      "Training Step: 98  | total loss: \u001b[1m\u001b[32m49.97738\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 005 | loss: 49.97738 - R2: 0.1765 -- iter: 0100/1168\n",
      "Training Step: 99  | total loss: \u001b[1m\u001b[32m48.80199\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 005 | loss: 48.80199 - R2: 0.1826 -- iter: 0150/1168\n",
      "Training Step: 100  | total loss: \u001b[1m\u001b[32m47.61687\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 005 | loss: 47.61687 - R2: 0.1889 -- iter: 0200/1168\n",
      "Training Step: 101  | total loss: \u001b[1m\u001b[32m46.32792\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 005 | loss: 46.32792 - R2: 0.1958 -- iter: 0250/1168\n",
      "Training Step: 102  | total loss: \u001b[1m\u001b[32m46.32792\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 005 | loss: 46.32792 - R2: 0.2025 -- iter: 0300/1168\n",
      "Training Step: 103  | total loss: \u001b[1m\u001b[32m43.93945\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 005 | loss: 43.93945 - R2: 0.2096 -- iter: 0350/1168\n",
      "Training Step: 104  | total loss: \u001b[1m\u001b[32m42.72849\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 005 | loss: 42.72849 - R2: 0.2169 -- iter: 0400/1168\n",
      "Training Step: 105  | total loss: \u001b[1m\u001b[32m41.50313\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 005 | loss: 41.50313 - R2: 0.2245 -- iter: 0450/1168\n",
      "Training Step: 106  | total loss: \u001b[1m\u001b[32m40.21184\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 005 | loss: 40.21184 - R2: 0.2327 -- iter: 0500/1168\n",
      "Training Step: 107  | total loss: \u001b[1m\u001b[32m39.08158\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 005 | loss: 39.08158 - R2: 0.2404 -- iter: 0550/1168\n",
      "Training Step: 108  | total loss: \u001b[1m\u001b[32m37.79789\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 005 | loss: 37.79789 - R2: 0.2491 -- iter: 0600/1168\n",
      "Training Step: 109  | total loss: \u001b[1m\u001b[32m37.79789\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 005 | loss: 37.79789 - R2: 0.2491 -- iter: 0650/1168\n",
      "Training Step: 110  | total loss: \u001b[1m\u001b[32m35.46294\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 005 | loss: 35.46294 - R2: 0.2662 -- iter: 0700/1168\n",
      "Training Step: 111  | total loss: \u001b[1m\u001b[32m35.46294\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 005 | loss: 35.46294 - R2: 0.2662 -- iter: 0750/1168\n",
      "Training Step: 112  | total loss: \u001b[1m\u001b[32m34.27270\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 005 | loss: 34.27270 - R2: 0.2752 -- iter: 0800/1168\n",
      "Training Step: 113  | total loss: \u001b[1m\u001b[32m33.02267\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 005 | loss: 33.02267 - R2: 0.2850 -- iter: 0850/1168\n",
      "Training Step: 114  | total loss: \u001b[1m\u001b[32m30.59373\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 005 | loss: 30.59373 - R2: 0.3053 -- iter: 0900/1168\n",
      "Training Step: 115  | total loss: \u001b[1m\u001b[32m30.59373\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 005 | loss: 30.59373 - R2: 0.3053 -- iter: 0950/1168\n",
      "Training Step: 116  | total loss: \u001b[1m\u001b[32m29.34025\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 005 | loss: 29.34025 - R2: 0.3163 -- iter: 1000/1168\n",
      "Training Step: 117  | total loss: \u001b[1m\u001b[32m28.15521\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 005 | loss: 28.15521 - R2: 0.3272 -- iter: 1050/1168\n",
      "Training Step: 118  | total loss: \u001b[1m\u001b[32m25.88973\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 005 | loss: 25.88973 - R2: 0.3495 -- iter: 1100/1168\n",
      "Training Step: 119  | total loss: \u001b[1m\u001b[32m25.88973\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 005 | loss: 25.88973 - R2: 0.3616 -- iter: 1150/1168\n",
      "Training Step: 120  | total loss: \u001b[1m\u001b[32m24.72864\u001b[0m\u001b[0m | time: 1.167s\n",
      "| SGD | epoch: 005 | loss: 24.72864 - R2: 0.3736 | val_loss: 12.84696 - val_acc: 0.4960 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 121  | total loss: \u001b[1m\u001b[32m23.62461\u001b[0m\u001b[0m | time: 0.016s\n",
      "| SGD | epoch: 006 | loss: 23.62461 - R2: 0.3736 -- iter: 0050/1168\n",
      "Training Step: 122  | total loss: \u001b[1m\u001b[32m21.35328\u001b[0m\u001b[0m | time: 0.020s\n",
      "| SGD | epoch: 006 | loss: 21.35328 - R2: 0.4002 -- iter: 0100/1168\n",
      "Training Step: 123  | total loss: \u001b[1m\u001b[32m21.35328\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 006 | loss: 21.35328 - R2: 0.4002 -- iter: 0150/1168\n",
      "Training Step: 124  | total loss: \u001b[1m\u001b[32m20.28550\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 006 | loss: 20.28550 - R2: 0.4133 -- iter: 0200/1168\n",
      "Training Step: 125  | total loss: \u001b[1m\u001b[32m19.16150\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 006 | loss: 19.16150 - R2: 0.4282 -- iter: 0250/1168\n",
      "Training Step: 126  | total loss: \u001b[1m\u001b[32m17.03537\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 006 | loss: 17.03537 - R2: 0.4583 -- iter: 0300/1168\n",
      "Training Step: 127  | total loss: \u001b[1m\u001b[32m16.02035\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 006 | loss: 16.02035 - R2: 0.4737 -- iter: 0350/1168\n",
      "Training Step: 128  | total loss: \u001b[1m\u001b[32m16.02035\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 006 | loss: 16.02035 - R2: 0.4737 -- iter: 0400/1168\n",
      "Training Step: 129  | total loss: \u001b[1m\u001b[32m15.10047\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 006 | loss: 15.10047 - R2: 0.4883 -- iter: 0450/1168\n",
      "Training Step: 130  | total loss: \u001b[1m\u001b[32m14.13866\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 006 | loss: 14.13866 - R2: 0.5045 -- iter: 0500/1168\n",
      "Training Step: 131  | total loss: \u001b[1m\u001b[32m13.28879\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 006 | loss: 13.28879 - R2: 0.5192 -- iter: 0550/1168\n",
      "Training Step: 132  | total loss: \u001b[1m\u001b[32m12.41696\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 006 | loss: 12.41696 - R2: 0.5354 -- iter: 0600/1168\n",
      "Training Step: 133  | total loss: \u001b[1m\u001b[32m10.73291\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 006 | loss: 10.73291 - R2: 0.5699 -- iter: 0650/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 134  | total loss: \u001b[1m\u001b[32m10.73291\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 006 | loss: 10.73291 - R2: 0.5880 -- iter: 0700/1168\n",
      "Training Step: 135  | total loss: \u001b[1m\u001b[32m9.15298\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 006 | loss: 9.15298 - R2: 0.6071 -- iter: 0750/1168\n",
      "Training Step: 136  | total loss: \u001b[1m\u001b[32m9.15298\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 006 | loss: 9.15298 - R2: 0.6071 -- iter: 0800/1168\n",
      "Training Step: 137  | total loss: \u001b[1m\u001b[32m7.69646\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 006 | loss: 7.69646 - R2: 0.6467 -- iter: 0850/1168\n",
      "Training Step: 138  | total loss: \u001b[1m\u001b[32m6.99029\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 006 | loss: 6.99029 - R2: 0.6702 -- iter: 0900/1168\n",
      "Training Step: 139  | total loss: \u001b[1m\u001b[32m6.34728\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 006 | loss: 6.34728 - R2: 0.6922 -- iter: 0950/1168\n",
      "Training Step: 140  | total loss: \u001b[1m\u001b[32m5.76537\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 006 | loss: 5.76537 - R2: 0.7130 -- iter: 1000/1168\n",
      "Training Step: 141  | total loss: \u001b[1m\u001b[32m5.20858\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 006 | loss: 5.20858 - R2: 0.7361 -- iter: 1050/1168\n",
      "Training Step: 142  | total loss: \u001b[1m\u001b[32m4.71162\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 006 | loss: 4.71162 - R2: 0.7582 -- iter: 1100/1168\n",
      "Training Step: 143  | total loss: \u001b[1m\u001b[32m4.25904\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 006 | loss: 4.25904 - R2: 0.7794 -- iter: 1150/1168\n",
      "Training Step: 144  | total loss: \u001b[1m\u001b[32m3.84971\u001b[0m\u001b[0m | time: 1.106s\n",
      "| SGD | epoch: 006 | loss: 3.84971 - R2: 0.7989 | val_loss: 0.14865 - val_acc: 0.9852 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 145  | total loss: \u001b[1m\u001b[32m3.47238\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 007 | loss: 3.47238 - R2: 0.8186 -- iter: 0050/1168\n",
      "Training Step: 146  | total loss: \u001b[1m\u001b[32m3.47238\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 007 | loss: 3.47238 - R2: 0.8186 -- iter: 0100/1168\n",
      "Training Step: 147  | total loss: \u001b[1m\u001b[32m2.82917\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 007 | loss: 2.82917 - R2: 0.8515 -- iter: 0150/1168\n",
      "Training Step: 148  | total loss: \u001b[1m\u001b[32m2.82917\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 007 | loss: 2.82917 - R2: 0.8515 -- iter: 0200/1168\n",
      "Training Step: 149  | total loss: \u001b[1m\u001b[32m2.56086\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 007 | loss: 2.56086 - R2: 0.8653 -- iter: 0250/1168\n",
      "Training Step: 150  | total loss: \u001b[1m\u001b[32m2.08811\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 007 | loss: 2.08811 - R2: 0.8908 -- iter: 0300/1168\n",
      "Training Step: 151  | total loss: \u001b[1m\u001b[32m2.08811\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 007 | loss: 2.08811 - R2: 0.8908 -- iter: 0350/1168\n",
      "Training Step: 152  | total loss: \u001b[1m\u001b[32m1.70592\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 007 | loss: 1.70592 - R2: 0.9120 -- iter: 0400/1168\n",
      "Training Step: 153  | total loss: \u001b[1m\u001b[32m1.54031\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 007 | loss: 1.54031 - R2: 0.9207 -- iter: 0450/1168\n",
      "Training Step: 154  | total loss: \u001b[1m\u001b[32m1.39061\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 007 | loss: 1.39061 - R2: 0.9286 -- iter: 0500/1168\n",
      "Training Step: 155  | total loss: \u001b[1m\u001b[32m1.25719\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 007 | loss: 1.25719 - R2: 0.9358 -- iter: 0550/1168\n",
      "Training Step: 156  | total loss: \u001b[1m\u001b[32m1.13570\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 007 | loss: 1.13570 - R2: 0.9421 -- iter: 0600/1168\n",
      "Training Step: 157  | total loss: \u001b[1m\u001b[32m1.13570\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 007 | loss: 1.13570 - R2: 0.9421 -- iter: 0650/1168\n",
      "Training Step: 158  | total loss: \u001b[1m\u001b[32m1.02616\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 007 | loss: 1.02616 - R2: 0.9470 -- iter: 0700/1168\n",
      "Training Step: 159  | total loss: \u001b[1m\u001b[32m0.92780\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 007 | loss: 0.92780 - R2: 0.9525 -- iter: 0750/1168\n",
      "Training Step: 160  | total loss: \u001b[1m\u001b[32m0.84220\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 007 | loss: 0.84220 - R2: 0.9574 -- iter: 0800/1168\n",
      "Training Step: 161  | total loss: \u001b[1m\u001b[32m0.76460\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 007 | loss: 0.76460 - R2: 0.9629 -- iter: 0850/1168\n",
      "Training Step: 162  | total loss: \u001b[1m\u001b[32m0.63242\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 007 | loss: 0.63242 - R2: 0.9700 -- iter: 0900/1168\n",
      "Training Step: 163  | total loss: \u001b[1m\u001b[32m0.57611\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 007 | loss: 0.57611 - R2: 0.9724 -- iter: 0950/1168\n",
      "Training Step: 164  | total loss: \u001b[1m\u001b[32m0.52291\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 007 | loss: 0.52291 - R2: 0.9754 -- iter: 1000/1168\n",
      "Training Step: 165  | total loss: \u001b[1m\u001b[32m0.47477\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 007 | loss: 0.47477 - R2: 0.9784 -- iter: 1050/1168\n",
      "Training Step: 166  | total loss: \u001b[1m\u001b[32m0.43239\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 007 | loss: 0.43239 - R2: 0.9807 -- iter: 1100/1168\n",
      "Training Step: 167  | total loss: \u001b[1m\u001b[32m0.43239\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 007 | loss: 0.43239 - R2: 0.9807 -- iter: 1150/1168\n",
      "Training Step: 168  | total loss: \u001b[1m\u001b[32m0.38592\u001b[0m\u001b[0m | time: 1.124s\n",
      "| SGD | epoch: 007 | loss: 0.38592 - R2: 0.9822 | val_loss: 0.05740 - val_acc: 0.9996 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 169  | total loss: \u001b[1m\u001b[32m0.35242\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 008 | loss: 0.35242 - R2: 0.9840 -- iter: 0050/1168\n",
      "Training Step: 170  | total loss: \u001b[1m\u001b[32m0.35242\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 008 | loss: 0.35242 - R2: 0.9840 -- iter: 0100/1168\n",
      "Training Step: 171  | total loss: \u001b[1m\u001b[32m0.32044\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 008 | loss: 0.32044 - R2: 0.9856 -- iter: 0150/1168\n",
      "Training Step: 172  | total loss: \u001b[1m\u001b[32m0.29054\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 008 | loss: 0.29054 - R2: 0.9873 -- iter: 0200/1168\n",
      "Training Step: 173  | total loss: \u001b[1m\u001b[32m0.26487\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 008 | loss: 0.26487 - R2: 0.9898 -- iter: 0250/1168\n",
      "Training Step: 174  | total loss: \u001b[1m\u001b[32m0.24130\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 008 | loss: 0.24130 - R2: 0.9903 -- iter: 0300/1168\n",
      "Training Step: 175  | total loss: \u001b[1m\u001b[32m0.20160\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 008 | loss: 0.20160 - R2: 0.9919 -- iter: 0350/1168\n",
      "Training Step: 176  | total loss: \u001b[1m\u001b[32m0.18476\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 008 | loss: 0.18476 - R2: 0.9926 -- iter: 0400/1168\n",
      "Training Step: 177  | total loss: \u001b[1m\u001b[32m0.16928\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 008 | loss: 0.16928 - R2: 0.9937 -- iter: 0450/1168\n",
      "Training Step: 178  | total loss: \u001b[1m\u001b[32m0.15483\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 008 | loss: 0.15483 - R2: 0.9950 -- iter: 0500/1168\n",
      "Training Step: 179  | total loss: \u001b[1m\u001b[32m0.14339\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 008 | loss: 0.14339 - R2: 0.9953 -- iter: 0550/1168\n",
      "Training Step: 180  | total loss: \u001b[1m\u001b[32m0.13215\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 008 | loss: 0.13215 - R2: 0.9962 -- iter: 0600/1168\n",
      "Training Step: 181  | total loss: \u001b[1m\u001b[32m0.12227\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 008 | loss: 0.12227 - R2: 0.9971 -- iter: 0650/1168\n",
      "Training Step: 182  | total loss: \u001b[1m\u001b[32m0.11226\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 008 | loss: 0.11226 - R2: 0.9965 -- iter: 0700/1168\n",
      "Training Step: 183  | total loss: \u001b[1m\u001b[32m0.11226\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 008 | loss: 0.11226 - R2: 0.9976 -- iter: 0750/1168\n",
      "Training Step: 184  | total loss: \u001b[1m\u001b[32m0.09483\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 008 | loss: 0.09483 - R2: 0.9973 -- iter: 0800/1168\n",
      "Training Step: 185  | total loss: \u001b[1m\u001b[32m0.08681\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 008 | loss: 0.08681 - R2: 0.9967 -- iter: 0850/1168\n",
      "Training Step: 186  | total loss: \u001b[1m\u001b[32m0.08192\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 008 | loss: 0.08192 - R2: 0.9968 -- iter: 0900/1168\n",
      "Training Step: 187  | total loss: \u001b[1m\u001b[32m0.08192\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 008 | loss: 0.08192 - R2: 0.9968 -- iter: 0950/1168\n",
      "Training Step: 188  | total loss: \u001b[1m\u001b[32m0.07047\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 008 | loss: 0.07047 - R2: 0.9967 -- iter: 1000/1168\n",
      "Training Step: 189  | total loss: \u001b[1m\u001b[32m0.07047\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 008 | loss: 0.07047 - R2: 0.9967 -- iter: 1050/1168\n",
      "Training Step: 190  | total loss: \u001b[1m\u001b[32m0.06531\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 008 | loss: 0.06531 - R2: 0.9978 -- iter: 1100/1168\n",
      "Training Step: 191  | total loss: \u001b[1m\u001b[32m0.06043\u001b[0m\u001b[0m | time: 0.172s\n",
      "| SGD | epoch: 008 | loss: 0.06043 - R2: 0.9978 -- iter: 1150/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 192  | total loss: \u001b[1m\u001b[32m0.05623\u001b[0m\u001b[0m | time: 1.176s\n",
      "| SGD | epoch: 008 | loss: 0.05623 - R2: 0.9982 | val_loss: 0.03310 - val_acc: 1.0004 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 193  | total loss: \u001b[1m\u001b[32m0.05263\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 009 | loss: 0.05263 - R2: 0.9985 -- iter: 0050/1168\n",
      "Training Step: 194  | total loss: \u001b[1m\u001b[32m0.05263\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 009 | loss: 0.05263 - R2: 0.9985 -- iter: 0100/1168\n",
      "Training Step: 195  | total loss: \u001b[1m\u001b[32m0.05148\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 009 | loss: 0.05148 - R2: 0.9992 -- iter: 0150/1168\n",
      "Training Step: 196  | total loss: \u001b[1m\u001b[32m0.04838\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 009 | loss: 0.04838 - R2: 0.9988 -- iter: 0200/1168\n",
      "Training Step: 197  | total loss: \u001b[1m\u001b[32m0.04301\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 009 | loss: 0.04301 - R2: 0.9989 -- iter: 0250/1168\n",
      "Training Step: 198  | total loss: \u001b[1m\u001b[32m0.04301\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 009 | loss: 0.04301 - R2: 0.9989 -- iter: 0300/1168\n",
      "Training Step: 199  | total loss: \u001b[1m\u001b[32m0.04023\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 009 | loss: 0.04023 - R2: 0.9991 -- iter: 0350/1168\n",
      "Training Step: 200  | total loss: \u001b[1m\u001b[32m0.03723\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 009 | loss: 0.03723 - R2: 0.9991 -- iter: 0400/1168\n",
      "Training Step: 201  | total loss: \u001b[1m\u001b[32m0.03437\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 009 | loss: 0.03437 - R2: 0.9990 -- iter: 0450/1168\n",
      "Training Step: 202  | total loss: \u001b[1m\u001b[32m0.03272\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 009 | loss: 0.03272 - R2: 0.9997 -- iter: 0500/1168\n",
      "Training Step: 203  | total loss: \u001b[1m\u001b[32m0.03157\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 009 | loss: 0.03157 - R2: 0.9998 -- iter: 0550/1168\n",
      "Training Step: 204  | total loss: \u001b[1m\u001b[32m0.03026\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 009 | loss: 0.03026 - R2: 1.0001 -- iter: 0600/1168\n",
      "Training Step: 205  | total loss: \u001b[1m\u001b[32m0.03049\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 009 | loss: 0.03049 - R2: 0.9999 -- iter: 0650/1168\n",
      "Training Step: 206  | total loss: \u001b[1m\u001b[32m0.03049\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 009 | loss: 0.03049 - R2: 0.9999 -- iter: 0700/1168\n",
      "Training Step: 207  | total loss: \u001b[1m\u001b[32m0.02885\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 009 | loss: 0.02885 - R2: 0.9999 -- iter: 0750/1168\n",
      "Training Step: 208  | total loss: \u001b[1m\u001b[32m0.02794\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 009 | loss: 0.02794 - R2: 0.9993 -- iter: 0800/1168\n",
      "Training Step: 209  | total loss: \u001b[1m\u001b[32m0.02794\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 009 | loss: 0.02794 - R2: 0.9993 -- iter: 0850/1168\n",
      "Training Step: 210  | total loss: \u001b[1m\u001b[32m0.02649\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 009 | loss: 0.02649 - R2: 0.9997 -- iter: 0900/1168\n",
      "Training Step: 211  | total loss: \u001b[1m\u001b[32m0.02651\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 009 | loss: 0.02651 - R2: 0.9999 -- iter: 0950/1168\n",
      "Training Step: 212  | total loss: \u001b[1m\u001b[32m0.02715\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 009 | loss: 0.02715 - R2: 1.0003 -- iter: 1000/1168\n",
      "Training Step: 213  | total loss: \u001b[1m\u001b[32m0.02715\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 009 | loss: 0.02715 - R2: 1.0003 -- iter: 1050/1168\n",
      "Training Step: 214  | total loss: \u001b[1m\u001b[32m0.02603\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 009 | loss: 0.02603 - R2: 1.0001 -- iter: 1100/1168\n",
      "Training Step: 215  | total loss: \u001b[1m\u001b[32m0.02603\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 009 | loss: 0.02603 - R2: 1.0001 -- iter: 1150/1168\n",
      "Training Step: 216  | total loss: \u001b[1m\u001b[32m0.02496\u001b[0m\u001b[0m | time: 1.124s\n",
      "| SGD | epoch: 009 | loss: 0.02496 - R2: 0.9992 | val_loss: 0.02752 - val_acc: 1.0005 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 217  | total loss: \u001b[1m\u001b[32m0.05319\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 010 | loss: 0.05319 - R2: 0.9980 -- iter: 0050/1168\n",
      "Training Step: 218  | total loss: \u001b[1m\u001b[32m0.05319\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 010 | loss: 0.05319 - R2: 0.9980 -- iter: 0100/1168\n",
      "Training Step: 219  | total loss: \u001b[1m\u001b[32m0.04982\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 010 | loss: 0.04982 - R2: 0.9992 -- iter: 0150/1168\n",
      "Training Step: 220  | total loss: \u001b[1m\u001b[32m0.04615\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 010 | loss: 0.04615 - R2: 0.9992 -- iter: 0200/1168\n",
      "Training Step: 221  | total loss: \u001b[1m\u001b[32m0.04141\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 010 | loss: 0.04141 - R2: 1.0002 -- iter: 0250/1168\n",
      "Training Step: 222  | total loss: \u001b[1m\u001b[32m0.03898\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 010 | loss: 0.03898 - R2: 0.9995 -- iter: 0300/1168\n",
      "Training Step: 223  | total loss: \u001b[1m\u001b[32m0.03898\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 010 | loss: 0.03898 - R2: 0.9995 -- iter: 0350/1168\n",
      "Training Step: 224  | total loss: \u001b[1m\u001b[32m0.03520\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 010 | loss: 0.03520 - R2: 0.9995 -- iter: 0400/1168\n",
      "Training Step: 225  | total loss: \u001b[1m\u001b[32m0.03520\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 010 | loss: 0.03520 - R2: 0.9995 -- iter: 0450/1168\n",
      "Training Step: 226  | total loss: \u001b[1m\u001b[32m0.03138\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 010 | loss: 0.03138 - R2: 0.9995 -- iter: 0500/1168\n",
      "Training Step: 227  | total loss: \u001b[1m\u001b[32m0.03360\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 010 | loss: 0.03360 - R2: 0.9995 -- iter: 0550/1168\n",
      "Training Step: 228  | total loss: \u001b[1m\u001b[32m0.03581\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 010 | loss: 0.03581 - R2: 1.0009 -- iter: 0600/1168\n",
      "Training Step: 229  | total loss: \u001b[1m\u001b[32m0.03531\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 010 | loss: 0.03531 - R2: 1.0005 -- iter: 0650/1168\n",
      "Training Step: 230  | total loss: \u001b[1m\u001b[32m0.03531\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 010 | loss: 0.03531 - R2: 1.0005 -- iter: 0700/1168\n",
      "Training Step: 231  | total loss: \u001b[1m\u001b[32m0.03259\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 010 | loss: 0.03259 - R2: 1.0004 -- iter: 0750/1168\n",
      "Training Step: 232  | total loss: \u001b[1m\u001b[32m0.02800\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 010 | loss: 0.02800 - R2: 1.0004 -- iter: 0800/1168\n",
      "Training Step: 233  | total loss: \u001b[1m\u001b[32m0.02821\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 010 | loss: 0.02821 - R2: 0.9996 -- iter: 0850/1168\n",
      "Training Step: 234  | total loss: \u001b[1m\u001b[32m0.02821\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 010 | loss: 0.02821 - R2: 0.9996 -- iter: 0900/1168\n",
      "Training Step: 235  | total loss: \u001b[1m\u001b[32m0.02542\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 010 | loss: 0.02542 - R2: 0.9998 -- iter: 0950/1168\n",
      "Training Step: 236  | total loss: \u001b[1m\u001b[32m0.02542\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 010 | loss: 0.02542 - R2: 0.9998 -- iter: 1000/1168\n",
      "Training Step: 237  | total loss: \u001b[1m\u001b[32m0.02473\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 010 | loss: 0.02473 - R2: 1.0000 -- iter: 1050/1168\n",
      "Training Step: 238  | total loss: \u001b[1m\u001b[32m0.02426\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 010 | loss: 0.02426 - R2: 0.9998 -- iter: 1100/1168\n",
      "Training Step: 239  | total loss: \u001b[1m\u001b[32m0.02426\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 010 | loss: 0.02426 - R2: 0.9998 -- iter: 1150/1168\n",
      "Training Step: 240  | total loss: \u001b[1m\u001b[32m0.02413\u001b[0m\u001b[0m | time: 1.120s\n",
      "| SGD | epoch: 010 | loss: 0.02413 - R2: 0.9996 | val_loss: 0.02625 - val_acc: 1.0002 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 241  | total loss: \u001b[1m\u001b[32m0.02450\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 011 | loss: 0.02450 - R2: 1.0003 -- iter: 0050/1168\n",
      "Training Step: 242  | total loss: \u001b[1m\u001b[32m0.02450\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 011 | loss: 0.02450 - R2: 1.0003 -- iter: 0100/1168\n",
      "Training Step: 243  | total loss: \u001b[1m\u001b[32m0.02329\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 011 | loss: 0.02329 - R2: 1.0004 -- iter: 0150/1168\n",
      "Training Step: 244  | total loss: \u001b[1m\u001b[32m0.02252\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 011 | loss: 0.02252 - R2: 0.9996 -- iter: 0200/1168\n",
      "Training Step: 245  | total loss: \u001b[1m\u001b[32m0.02046\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 011 | loss: 0.02046 - R2: 1.0002 -- iter: 0250/1168\n",
      "Training Step: 246  | total loss: \u001b[1m\u001b[32m0.02040\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 011 | loss: 0.02040 - R2: 1.0005 -- iter: 0300/1168\n",
      "Training Step: 247  | total loss: \u001b[1m\u001b[32m0.02002\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 011 | loss: 0.02002 - R2: 1.0002 -- iter: 0350/1168\n",
      "Training Step: 248  | total loss: \u001b[1m\u001b[32m0.01934\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 011 | loss: 0.01934 - R2: 0.9998 -- iter: 0400/1168\n",
      "Training Step: 249  | total loss: \u001b[1m\u001b[32m0.01934\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 011 | loss: 0.01934 - R2: 0.9998 -- iter: 0450/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 250  | total loss: \u001b[1m\u001b[32m0.01965\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 011 | loss: 0.01965 - R2: 1.0007 -- iter: 0500/1168\n",
      "Training Step: 251  | total loss: \u001b[1m\u001b[32m0.01922\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 011 | loss: 0.01922 - R2: 1.0012 -- iter: 0550/1168\n",
      "Training Step: 252  | total loss: \u001b[1m\u001b[32m0.01893\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 011 | loss: 0.01893 - R2: 1.0006 -- iter: 0600/1168\n",
      "Training Step: 253  | total loss: \u001b[1m\u001b[32m0.01800\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 011 | loss: 0.01800 - R2: 1.0006 -- iter: 0650/1168\n",
      "Training Step: 254  | total loss: \u001b[1m\u001b[32m0.02015\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 011 | loss: 0.02015 - R2: 1.0009 -- iter: 0700/1168\n",
      "Training Step: 255  | total loss: \u001b[1m\u001b[32m0.02241\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 011 | loss: 0.02241 - R2: 0.9999 -- iter: 0750/1168\n",
      "Training Step: 256  | total loss: \u001b[1m\u001b[32m0.02209\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 011 | loss: 0.02209 - R2: 1.0000 -- iter: 0800/1168\n",
      "Training Step: 257  | total loss: \u001b[1m\u001b[32m0.02209\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 011 | loss: 0.02209 - R2: 1.0000 -- iter: 0850/1168\n",
      "Training Step: 258  | total loss: \u001b[1m\u001b[32m0.02211\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 011 | loss: 0.02211 - R2: 0.9996 -- iter: 0900/1168\n",
      "Training Step: 259  | total loss: \u001b[1m\u001b[32m0.02208\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 011 | loss: 0.02208 - R2: 0.9992 -- iter: 0950/1168\n",
      "Training Step: 260  | total loss: \u001b[1m\u001b[32m0.02208\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 011 | loss: 0.02208 - R2: 0.9992 -- iter: 1000/1168\n",
      "Training Step: 261  | total loss: \u001b[1m\u001b[32m0.02140\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 011 | loss: 0.02140 - R2: 0.9990 -- iter: 1050/1168\n",
      "Training Step: 262  | total loss: \u001b[1m\u001b[32m0.02421\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 011 | loss: 0.02421 - R2: 0.9990 -- iter: 1100/1168\n",
      "Training Step: 263  | total loss: \u001b[1m\u001b[32m0.02282\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 011 | loss: 0.02282 - R2: 0.9991 -- iter: 1150/1168\n",
      "Training Step: 264  | total loss: \u001b[1m\u001b[32m0.02282\u001b[0m\u001b[0m | time: 1.126s\n",
      "| SGD | epoch: 011 | loss: 0.02282 - R2: 0.9991 | val_loss: 0.02514 - val_acc: 0.9997 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 265  | total loss: \u001b[1m\u001b[32m0.02267\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 012 | loss: 0.02267 - R2: 1.0000 -- iter: 0050/1168\n",
      "Training Step: 266  | total loss: \u001b[1m\u001b[32m0.02171\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 012 | loss: 0.02171 - R2: 0.9996 -- iter: 0100/1168\n",
      "Training Step: 267  | total loss: \u001b[1m\u001b[32m0.02120\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 012 | loss: 0.02120 - R2: 0.9999 -- iter: 0150/1168\n",
      "Training Step: 268  | total loss: \u001b[1m\u001b[32m0.02120\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 012 | loss: 0.02120 - R2: 1.0002 -- iter: 0200/1168\n",
      "Training Step: 269  | total loss: \u001b[1m\u001b[32m0.01932\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 012 | loss: 0.01932 - R2: 1.0007 -- iter: 0250/1168\n",
      "Training Step: 270  | total loss: \u001b[1m\u001b[32m0.01912\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 012 | loss: 0.01912 - R2: 1.0001 -- iter: 0300/1168\n",
      "Training Step: 271  | total loss: \u001b[1m\u001b[32m0.01961\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 012 | loss: 0.01961 - R2: 1.0000 -- iter: 0350/1168\n",
      "Training Step: 272  | total loss: \u001b[1m\u001b[32m0.01900\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 012 | loss: 0.01900 - R2: 0.9994 -- iter: 0400/1168\n",
      "Training Step: 273  | total loss: \u001b[1m\u001b[32m0.01889\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 012 | loss: 0.01889 - R2: 0.9998 -- iter: 0450/1168\n",
      "Training Step: 274  | total loss: \u001b[1m\u001b[32m0.01890\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 012 | loss: 0.01890 - R2: 0.9998 -- iter: 0500/1168\n",
      "Training Step: 275  | total loss: \u001b[1m\u001b[32m0.01890\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 012 | loss: 0.01890 - R2: 0.9998 -- iter: 0550/1168\n",
      "Training Step: 276  | total loss: \u001b[1m\u001b[32m0.01857\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 012 | loss: 0.01857 - R2: 0.9998 -- iter: 0600/1168\n",
      "Training Step: 277  | total loss: \u001b[1m\u001b[32m0.01861\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 012 | loss: 0.01861 - R2: 0.9996 -- iter: 0650/1168\n",
      "Training Step: 278  | total loss: \u001b[1m\u001b[32m0.01849\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 012 | loss: 0.01849 - R2: 0.9994 -- iter: 0700/1168\n",
      "Training Step: 279  | total loss: \u001b[1m\u001b[32m0.02323\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 012 | loss: 0.02323 - R2: 0.9999 -- iter: 0750/1168\n",
      "Training Step: 280  | total loss: \u001b[1m\u001b[32m0.02235\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 012 | loss: 0.02235 - R2: 0.9993 -- iter: 0800/1168\n",
      "Training Step: 281  | total loss: \u001b[1m\u001b[32m0.02105\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 012 | loss: 0.02105 - R2: 1.0000 -- iter: 0850/1168\n",
      "Training Step: 282  | total loss: \u001b[1m\u001b[32m0.02050\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 012 | loss: 0.02050 - R2: 0.9999 -- iter: 0900/1168\n",
      "Training Step: 283  | total loss: \u001b[1m\u001b[32m0.02257\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 012 | loss: 0.02257 - R2: 1.0002 -- iter: 0950/1168\n",
      "Training Step: 284  | total loss: \u001b[1m\u001b[32m0.02257\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 012 | loss: 0.02257 - R2: 1.0002 -- iter: 1000/1168\n",
      "Training Step: 285  | total loss: \u001b[1m\u001b[32m0.02212\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 012 | loss: 0.02212 - R2: 1.0001 -- iter: 1050/1168\n",
      "Training Step: 286  | total loss: \u001b[1m\u001b[32m0.02221\u001b[0m\u001b[0m | time: 0.165s\n",
      "| SGD | epoch: 012 | loss: 0.02221 - R2: 1.0001 -- iter: 1100/1168\n",
      "Training Step: 287  | total loss: \u001b[1m\u001b[32m0.02127\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 012 | loss: 0.02127 - R2: 0.9999 -- iter: 1150/1168\n",
      "Training Step: 288  | total loss: \u001b[1m\u001b[32m0.02141\u001b[0m\u001b[0m | time: 1.179s\n",
      "| SGD | epoch: 012 | loss: 0.02141 - R2: 0.9993 | val_loss: 0.02493 - val_acc: 1.0005 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 289  | total loss: \u001b[1m\u001b[32m0.02129\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 013 | loss: 0.02129 - R2: 0.9995 -- iter: 0050/1168\n",
      "Training Step: 290  | total loss: \u001b[1m\u001b[32m0.02114\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 013 | loss: 0.02114 - R2: 1.0002 -- iter: 0100/1168\n",
      "Training Step: 291  | total loss: \u001b[1m\u001b[32m0.02103\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 013 | loss: 0.02103 - R2: 0.9997 -- iter: 0150/1168\n",
      "Training Step: 292  | total loss: \u001b[1m\u001b[32m0.03366\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 013 | loss: 0.03366 - R2: 0.9996 -- iter: 0200/1168\n",
      "Training Step: 293  | total loss: \u001b[1m\u001b[32m0.03166\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 013 | loss: 0.03166 - R2: 0.9993 -- iter: 0250/1168\n",
      "Training Step: 294  | total loss: \u001b[1m\u001b[32m0.03014\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 013 | loss: 0.03014 - R2: 0.9990 -- iter: 0300/1168\n",
      "Training Step: 295  | total loss: \u001b[1m\u001b[32m0.02878\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 013 | loss: 0.02878 - R2: 0.9990 -- iter: 0350/1168\n",
      "Training Step: 296  | total loss: \u001b[1m\u001b[32m0.02878\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 013 | loss: 0.02878 - R2: 0.9989 -- iter: 0400/1168\n",
      "Training Step: 297  | total loss: \u001b[1m\u001b[32m0.02829\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 013 | loss: 0.02829 - R2: 0.9993 -- iter: 0450/1168\n",
      "Training Step: 298  | total loss: \u001b[1m\u001b[32m0.02695\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 013 | loss: 0.02695 - R2: 0.9993 -- iter: 0500/1168\n",
      "Training Step: 299  | total loss: \u001b[1m\u001b[32m0.03415\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 013 | loss: 0.03415 - R2: 1.0014 -- iter: 0550/1168\n",
      "Training Step: 300  | total loss: \u001b[1m\u001b[32m0.03267\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 013 | loss: 0.03267 - R2: 1.0014 -- iter: 0600/1168\n",
      "Training Step: 301  | total loss: \u001b[1m\u001b[32m0.03077\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 013 | loss: 0.03077 - R2: 1.0006 -- iter: 0650/1168\n",
      "Training Step: 302  | total loss: \u001b[1m\u001b[32m0.03003\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 013 | loss: 0.03003 - R2: 1.0003 -- iter: 0700/1168\n",
      "Training Step: 303  | total loss: \u001b[1m\u001b[32m0.02784\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 013 | loss: 0.02784 - R2: 1.0002 -- iter: 0750/1168\n",
      "Training Step: 304  | total loss: \u001b[1m\u001b[32m0.03020\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 013 | loss: 0.03020 - R2: 1.0004 -- iter: 0800/1168\n",
      "Training Step: 305  | total loss: \u001b[1m\u001b[32m0.03020\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 013 | loss: 0.03020 - R2: 1.0004 -- iter: 0850/1168\n",
      "Training Step: 306  | total loss: \u001b[1m\u001b[32m0.02866\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 013 | loss: 0.02866 - R2: 0.9998 -- iter: 0900/1168\n",
      "Training Step: 307  | total loss: \u001b[1m\u001b[32m0.03091\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 013 | loss: 0.03091 - R2: 0.9995 -- iter: 0950/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 308  | total loss: \u001b[1m\u001b[32m0.02952\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 013 | loss: 0.02952 - R2: 0.9994 -- iter: 1000/1168\n",
      "Training Step: 309  | total loss: \u001b[1m\u001b[32m0.02977\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 013 | loss: 0.02977 - R2: 0.9997 -- iter: 1050/1168\n",
      "Training Step: 310  | total loss: \u001b[1m\u001b[32m0.02895\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 013 | loss: 0.02895 - R2: 1.0000 -- iter: 1100/1168\n",
      "Training Step: 311  | total loss: \u001b[1m\u001b[32m0.02895\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 013 | loss: 0.02895 - R2: 1.0004 -- iter: 1150/1168\n",
      "Training Step: 312  | total loss: \u001b[1m\u001b[32m0.02637\u001b[0m\u001b[0m | time: 1.136s\n",
      "| SGD | epoch: 013 | loss: 0.02637 - R2: 1.0004 | val_loss: 0.02804 - val_acc: 0.9969 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 313  | total loss: \u001b[1m\u001b[32m0.02521\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 014 | loss: 0.02521 - R2: 0.9996 -- iter: 0050/1168\n",
      "Training Step: 314  | total loss: \u001b[1m\u001b[32m0.02528\u001b[0m\u001b[0m | time: 0.012s\n",
      "| SGD | epoch: 014 | loss: 0.02528 - R2: 0.9995 -- iter: 0100/1168\n",
      "Training Step: 315  | total loss: \u001b[1m\u001b[32m0.02506\u001b[0m\u001b[0m | time: 0.017s\n",
      "| SGD | epoch: 014 | loss: 0.02506 - R2: 0.9997 -- iter: 0150/1168\n",
      "Training Step: 316  | total loss: \u001b[1m\u001b[32m0.02506\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 014 | loss: 0.02506 - R2: 0.9997 -- iter: 0200/1168\n",
      "Training Step: 317  | total loss: \u001b[1m\u001b[32m0.02469\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 014 | loss: 0.02469 - R2: 1.0000 -- iter: 0250/1168\n",
      "Training Step: 318  | total loss: \u001b[1m\u001b[32m0.04014\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 014 | loss: 0.04014 - R2: 1.0003 -- iter: 0300/1168\n",
      "Training Step: 319  | total loss: \u001b[1m\u001b[32m0.03713\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 014 | loss: 0.03713 - R2: 1.0004 -- iter: 0350/1168\n",
      "Training Step: 320  | total loss: \u001b[1m\u001b[32m0.03567\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 014 | loss: 0.03567 - R2: 0.9997 -- iter: 0400/1168\n",
      "Training Step: 321  | total loss: \u001b[1m\u001b[32m0.03567\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 014 | loss: 0.03567 - R2: 0.9996 -- iter: 0450/1168\n",
      "Training Step: 322  | total loss: \u001b[1m\u001b[32m0.03283\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 014 | loss: 0.03283 - R2: 0.9995 -- iter: 0500/1168\n",
      "Training Step: 323  | total loss: \u001b[1m\u001b[32m0.03233\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 014 | loss: 0.03233 - R2: 0.9998 -- iter: 0550/1168\n",
      "Training Step: 324  | total loss: \u001b[1m\u001b[32m0.03010\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 014 | loss: 0.03010 - R2: 1.0001 -- iter: 0600/1168\n",
      "Training Step: 325  | total loss: \u001b[1m\u001b[32m0.02796\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 014 | loss: 0.02796 - R2: 1.0003 -- iter: 0650/1168\n",
      "Training Step: 326  | total loss: \u001b[1m\u001b[32m0.02796\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 014 | loss: 0.02796 - R2: 1.0003 -- iter: 0700/1168\n",
      "Training Step: 327  | total loss: \u001b[1m\u001b[32m0.02697\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 014 | loss: 0.02697 - R2: 1.0001 -- iter: 0750/1168\n",
      "Training Step: 328  | total loss: \u001b[1m\u001b[32m0.02533\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 014 | loss: 0.02533 - R2: 0.9989 -- iter: 0800/1168\n",
      "Training Step: 329  | total loss: \u001b[1m\u001b[32m0.02497\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 014 | loss: 0.02497 - R2: 0.9996 -- iter: 0850/1168\n",
      "Training Step: 330  | total loss: \u001b[1m\u001b[32m0.02357\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 014 | loss: 0.02357 - R2: 0.9996 -- iter: 0900/1168\n",
      "Training Step: 331  | total loss: \u001b[1m\u001b[32m0.02253\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 014 | loss: 0.02253 - R2: 0.9993 -- iter: 0950/1168\n",
      "Training Step: 332  | total loss: \u001b[1m\u001b[32m0.02123\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 014 | loss: 0.02123 - R2: 0.9995 -- iter: 1000/1168\n",
      "Training Step: 333  | total loss: \u001b[1m\u001b[32m0.02039\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 014 | loss: 0.02039 - R2: 1.0000 -- iter: 1050/1168\n",
      "Training Step: 334  | total loss: \u001b[1m\u001b[32m0.02039\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 014 | loss: 0.02039 - R2: 1.0000 -- iter: 1100/1168\n",
      "Training Step: 335  | total loss: \u001b[1m\u001b[32m0.01976\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 014 | loss: 0.01976 - R2: 1.0000 -- iter: 1150/1168\n",
      "Training Step: 336  | total loss: \u001b[1m\u001b[32m0.01976\u001b[0m\u001b[0m | time: 1.125s\n",
      "| SGD | epoch: 014 | loss: 0.01976 - R2: 1.0000 | val_loss: 0.02689 - val_acc: 0.9972 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 337  | total loss: \u001b[1m\u001b[32m0.02317\u001b[0m\u001b[0m | time: 0.018s\n",
      "| SGD | epoch: 015 | loss: 0.02317 - R2: 1.0004 -- iter: 0050/1168\n",
      "Training Step: 338  | total loss: \u001b[1m\u001b[32m0.02255\u001b[0m\u001b[0m | time: 0.026s\n",
      "| SGD | epoch: 015 | loss: 0.02255 - R2: 1.0003 -- iter: 0100/1168\n",
      "Training Step: 339  | total loss: \u001b[1m\u001b[32m0.02353\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 015 | loss: 0.02353 - R2: 0.9994 -- iter: 0150/1168\n",
      "Training Step: 340  | total loss: \u001b[1m\u001b[32m0.02337\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 015 | loss: 0.02337 - R2: 0.9992 -- iter: 0200/1168\n",
      "Training Step: 341  | total loss: \u001b[1m\u001b[32m0.02398\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 015 | loss: 0.02398 - R2: 0.9990 -- iter: 0250/1168\n",
      "Training Step: 342  | total loss: \u001b[1m\u001b[32m0.02347\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 015 | loss: 0.02347 - R2: 0.9992 -- iter: 0300/1168\n",
      "Training Step: 343  | total loss: \u001b[1m\u001b[32m0.02224\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 015 | loss: 0.02224 - R2: 0.9996 -- iter: 0350/1168\n",
      "Training Step: 344  | total loss: \u001b[1m\u001b[32m0.02147\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 015 | loss: 0.02147 - R2: 0.9995 -- iter: 0400/1168\n",
      "Training Step: 345  | total loss: \u001b[1m\u001b[32m0.02265\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 015 | loss: 0.02265 - R2: 1.0000 -- iter: 0450/1168\n",
      "Training Step: 346  | total loss: \u001b[1m\u001b[32m0.02265\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 015 | loss: 0.02265 - R2: 1.0000 -- iter: 0500/1168\n",
      "Training Step: 347  | total loss: \u001b[1m\u001b[32m0.02269\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 015 | loss: 0.02269 - R2: 0.9997 -- iter: 0550/1168\n",
      "Training Step: 348  | total loss: \u001b[1m\u001b[32m0.02064\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 015 | loss: 0.02064 - R2: 1.0000 -- iter: 0600/1168\n",
      "Training Step: 349  | total loss: \u001b[1m\u001b[32m0.02064\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 015 | loss: 0.02064 - R2: 1.0000 -- iter: 0650/1168\n",
      "Training Step: 350  | total loss: \u001b[1m\u001b[32m0.01935\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 015 | loss: 0.01935 - R2: 1.0008 -- iter: 0700/1168\n",
      "Training Step: 351  | total loss: \u001b[1m\u001b[32m0.01834\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 015 | loss: 0.01834 - R2: 1.0005 -- iter: 0750/1168\n",
      "Training Step: 352  | total loss: \u001b[1m\u001b[32m0.01828\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 015 | loss: 0.01828 - R2: 1.0002 -- iter: 0800/1168\n",
      "Training Step: 353  | total loss: \u001b[1m\u001b[32m0.01828\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 015 | loss: 0.01828 - R2: 1.0002 -- iter: 0850/1168\n",
      "Training Step: 354  | total loss: \u001b[1m\u001b[32m0.01788\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 015 | loss: 0.01788 - R2: 0.9999 -- iter: 0900/1168\n",
      "Training Step: 355  | total loss: \u001b[1m\u001b[32m0.01764\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 015 | loss: 0.01764 - R2: 0.9997 -- iter: 0950/1168\n",
      "Training Step: 356  | total loss: \u001b[1m\u001b[32m0.02436\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 015 | loss: 0.02436 - R2: 1.0009 -- iter: 1000/1168\n",
      "Training Step: 357  | total loss: \u001b[1m\u001b[32m0.02310\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 015 | loss: 0.02310 - R2: 1.0007 -- iter: 1050/1168\n",
      "Training Step: 358  | total loss: \u001b[1m\u001b[32m0.02120\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 015 | loss: 0.02120 - R2: 1.0002 -- iter: 1100/1168\n",
      "Training Step: 359  | total loss: \u001b[1m\u001b[32m0.02120\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 015 | loss: 0.02120 - R2: 0.9994 -- iter: 1150/1168\n",
      "Training Step: 360  | total loss: \u001b[1m\u001b[32m0.02122\u001b[0m\u001b[0m | time: 1.136s\n",
      "| SGD | epoch: 015 | loss: 0.02122 - R2: 0.9994 | val_loss: 0.02564 - val_acc: 0.9992 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 361  | total loss: \u001b[1m\u001b[32m0.02132\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 016 | loss: 0.02132 - R2: 0.9994 -- iter: 0050/1168\n",
      "Training Step: 362  | total loss: \u001b[1m\u001b[32m0.02062\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 016 | loss: 0.02062 - R2: 0.9995 -- iter: 0100/1168\n",
      "Training Step: 363  | total loss: \u001b[1m\u001b[32m0.01959\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 016 | loss: 0.01959 - R2: 0.9993 -- iter: 0150/1168\n",
      "Training Step: 364  | total loss: \u001b[1m\u001b[32m0.01908\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 016 | loss: 0.01908 - R2: 1.0000 -- iter: 0200/1168\n",
      "Training Step: 365  | total loss: \u001b[1m\u001b[32m0.01908\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 016 | loss: 0.01908 - R2: 1.0000 -- iter: 0250/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 366  | total loss: \u001b[1m\u001b[32m0.01821\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 016 | loss: 0.01821 - R2: 0.9994 -- iter: 0300/1168\n",
      "Training Step: 367  | total loss: \u001b[1m\u001b[32m0.01821\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 016 | loss: 0.01821 - R2: 0.9999 -- iter: 0350/1168\n",
      "Training Step: 368  | total loss: \u001b[1m\u001b[32m0.01821\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 016 | loss: 0.01821 - R2: 0.9994 -- iter: 0400/1168\n",
      "Training Step: 369  | total loss: \u001b[1m\u001b[32m0.01865\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 016 | loss: 0.01865 - R2: 0.9998 -- iter: 0450/1168\n",
      "Training Step: 370  | total loss: \u001b[1m\u001b[32m0.02051\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 016 | loss: 0.02051 - R2: 1.0002 -- iter: 0500/1168\n",
      "Training Step: 371  | total loss: \u001b[1m\u001b[32m0.02051\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 016 | loss: 0.02051 - R2: 1.0002 -- iter: 0550/1168\n",
      "Training Step: 372  | total loss: \u001b[1m\u001b[32m0.02039\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 016 | loss: 0.02039 - R2: 1.0000 -- iter: 0600/1168\n",
      "Training Step: 373  | total loss: \u001b[1m\u001b[32m0.02039\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 016 | loss: 0.02039 - R2: 1.0001 -- iter: 0650/1168\n",
      "Training Step: 374  | total loss: \u001b[1m\u001b[32m0.02084\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 016 | loss: 0.02084 - R2: 0.9990 -- iter: 0700/1168\n",
      "Training Step: 375  | total loss: \u001b[1m\u001b[32m0.01923\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 016 | loss: 0.01923 - R2: 0.9983 -- iter: 0750/1168\n",
      "Training Step: 376  | total loss: \u001b[1m\u001b[32m0.01870\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 016 | loss: 0.01870 - R2: 0.9988 -- iter: 0800/1168\n",
      "Training Step: 377  | total loss: \u001b[1m\u001b[32m0.01911\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 016 | loss: 0.01911 - R2: 0.9993 -- iter: 0850/1168\n",
      "Training Step: 378  | total loss: \u001b[1m\u001b[32m0.01867\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 016 | loss: 0.01867 - R2: 0.9995 -- iter: 0900/1168\n",
      "Training Step: 379  | total loss: \u001b[1m\u001b[32m0.01899\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 016 | loss: 0.01899 - R2: 0.9996 -- iter: 0950/1168\n",
      "Training Step: 380  | total loss: \u001b[1m\u001b[32m0.01899\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 016 | loss: 0.01899 - R2: 0.9996 -- iter: 1000/1168\n",
      "Training Step: 381  | total loss: \u001b[1m\u001b[32m0.01895\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 016 | loss: 0.01895 - R2: 0.9991 -- iter: 1050/1168\n",
      "Training Step: 382  | total loss: \u001b[1m\u001b[32m0.01943\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 016 | loss: 0.01943 - R2: 0.9995 -- iter: 1100/1168\n",
      "Training Step: 383  | total loss: \u001b[1m\u001b[32m0.01901\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 016 | loss: 0.01901 - R2: 0.9995 -- iter: 1150/1168\n",
      "Training Step: 384  | total loss: \u001b[1m\u001b[32m0.01708\u001b[0m\u001b[0m | time: 1.144s\n",
      "| SGD | epoch: 016 | loss: 0.01708 - R2: 1.0001 | val_loss: 0.02451 - val_acc: 1.0002 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 385  | total loss: \u001b[1m\u001b[32m0.01717\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 017 | loss: 0.01717 - R2: 1.0007 -- iter: 0050/1168\n",
      "Training Step: 386  | total loss: \u001b[1m\u001b[32m0.01717\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 017 | loss: 0.01717 - R2: 1.0007 -- iter: 0100/1168\n",
      "Training Step: 387  | total loss: \u001b[1m\u001b[32m0.01722\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 017 | loss: 0.01722 - R2: 1.0009 -- iter: 0150/1168\n",
      "Training Step: 388  | total loss: \u001b[1m\u001b[32m0.01801\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 017 | loss: 0.01801 - R2: 1.0011 -- iter: 0200/1168\n",
      "Training Step: 389  | total loss: \u001b[1m\u001b[32m0.02066\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 017 | loss: 0.02066 - R2: 1.0005 -- iter: 0250/1168\n",
      "Training Step: 390  | total loss: \u001b[1m\u001b[32m0.02073\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 017 | loss: 0.02073 - R2: 0.9997 -- iter: 0300/1168\n",
      "Training Step: 391  | total loss: \u001b[1m\u001b[32m0.01973\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 017 | loss: 0.01973 - R2: 1.0000 -- iter: 0350/1168\n",
      "Training Step: 392  | total loss: \u001b[1m\u001b[32m0.01973\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 017 | loss: 0.01973 - R2: 0.9996 -- iter: 0400/1168\n",
      "Training Step: 393  | total loss: \u001b[1m\u001b[32m0.01834\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 017 | loss: 0.01834 - R2: 0.9995 -- iter: 0450/1168\n",
      "Training Step: 394  | total loss: \u001b[1m\u001b[32m0.01834\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 017 | loss: 0.01834 - R2: 0.9995 -- iter: 0500/1168\n",
      "Training Step: 395  | total loss: \u001b[1m\u001b[32m0.02075\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 017 | loss: 0.02075 - R2: 0.9998 -- iter: 0550/1168\n",
      "Training Step: 396  | total loss: \u001b[1m\u001b[32m0.02397\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 017 | loss: 0.02397 - R2: 1.0002 -- iter: 0600/1168\n",
      "Training Step: 397  | total loss: \u001b[1m\u001b[32m0.02347\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 017 | loss: 0.02347 - R2: 0.9997 -- iter: 0650/1168\n",
      "Training Step: 398  | total loss: \u001b[1m\u001b[32m0.02347\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 017 | loss: 0.02347 - R2: 0.9997 -- iter: 0700/1168\n",
      "Training Step: 399  | total loss: \u001b[1m\u001b[32m0.02177\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 017 | loss: 0.02177 - R2: 1.0003 -- iter: 0750/1168\n",
      "Training Step: 400  | total loss: \u001b[1m\u001b[32m0.02101\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 017 | loss: 0.02101 - R2: 1.0004 -- iter: 0800/1168\n",
      "Training Step: 401  | total loss: \u001b[1m\u001b[32m0.02121\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 017 | loss: 0.02121 - R2: 1.0004 -- iter: 0850/1168\n",
      "Training Step: 402  | total loss: \u001b[1m\u001b[32m0.02045\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 017 | loss: 0.02045 - R2: 1.0000 -- iter: 0900/1168\n",
      "Training Step: 403  | total loss: \u001b[1m\u001b[32m0.02000\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 017 | loss: 0.02000 - R2: 1.0000 -- iter: 0950/1168\n",
      "Training Step: 404  | total loss: \u001b[1m\u001b[32m0.01969\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 017 | loss: 0.01969 - R2: 0.9993 -- iter: 1000/1168\n",
      "Training Step: 405  | total loss: \u001b[1m\u001b[32m0.01934\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 017 | loss: 0.01934 - R2: 0.9998 -- iter: 1050/1168\n",
      "Training Step: 406  | total loss: \u001b[1m\u001b[32m0.02194\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 017 | loss: 0.02194 - R2: 0.9997 -- iter: 1100/1168\n",
      "Training Step: 407  | total loss: \u001b[1m\u001b[32m0.02194\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 017 | loss: 0.02194 - R2: 0.9997 -- iter: 1150/1168\n",
      "Training Step: 408  | total loss: \u001b[1m\u001b[32m0.02113\u001b[0m\u001b[0m | time: 1.145s\n",
      "| SGD | epoch: 017 | loss: 0.02113 - R2: 1.0002 | val_loss: 0.02528 - val_acc: 0.9982 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 409  | total loss: \u001b[1m\u001b[32m0.02113\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 018 | loss: 0.02113 - R2: 1.0002 -- iter: 0050/1168\n",
      "Training Step: 410  | total loss: \u001b[1m\u001b[32m0.02029\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 018 | loss: 0.02029 - R2: 1.0003 -- iter: 0100/1168\n",
      "Training Step: 411  | total loss: \u001b[1m\u001b[32m0.02040\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 018 | loss: 0.02040 - R2: 0.9996 -- iter: 0150/1168\n",
      "Training Step: 412  | total loss: \u001b[1m\u001b[32m0.02038\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 018 | loss: 0.02038 - R2: 0.9991 -- iter: 0200/1168\n",
      "Training Step: 413  | total loss: \u001b[1m\u001b[32m0.02038\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 018 | loss: 0.02038 - R2: 0.9994 -- iter: 0250/1168\n",
      "Training Step: 414  | total loss: \u001b[1m\u001b[32m0.02019\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 018 | loss: 0.02019 - R2: 0.9995 -- iter: 0300/1168\n",
      "Training Step: 415  | total loss: \u001b[1m\u001b[32m0.01932\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 018 | loss: 0.01932 - R2: 0.9997 -- iter: 0350/1168\n",
      "Training Step: 416  | total loss: \u001b[1m\u001b[32m0.01899\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 018 | loss: 0.01899 - R2: 1.0001 -- iter: 0400/1168\n",
      "Training Step: 417  | total loss: \u001b[1m\u001b[32m0.01804\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 018 | loss: 0.01804 - R2: 0.9998 -- iter: 0450/1168\n",
      "Training Step: 418  | total loss: \u001b[1m\u001b[32m0.01808\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 018 | loss: 0.01808 - R2: 0.9996 -- iter: 0500/1168\n",
      "Training Step: 419  | total loss: \u001b[1m\u001b[32m0.01808\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 018 | loss: 0.01808 - R2: 0.9998 -- iter: 0550/1168\n",
      "Training Step: 420  | total loss: \u001b[1m\u001b[32m0.01798\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 018 | loss: 0.01798 - R2: 1.0000 -- iter: 0600/1168\n",
      "Training Step: 421  | total loss: \u001b[1m\u001b[32m0.01859\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 018 | loss: 0.01859 - R2: 1.0001 -- iter: 0650/1168\n",
      "Training Step: 422  | total loss: \u001b[1m\u001b[32m0.01859\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 018 | loss: 0.01859 - R2: 1.0001 -- iter: 0700/1168\n",
      "Training Step: 423  | total loss: \u001b[1m\u001b[32m0.01825\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 018 | loss: 0.01825 - R2: 1.0000 -- iter: 0750/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 424  | total loss: \u001b[1m\u001b[32m0.02189\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 018 | loss: 0.02189 - R2: 0.9999 -- iter: 0800/1168\n",
      "Training Step: 425  | total loss: \u001b[1m\u001b[32m0.02194\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 018 | loss: 0.02194 - R2: 1.0000 -- iter: 0850/1168\n",
      "Training Step: 426  | total loss: \u001b[1m\u001b[32m0.02178\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 018 | loss: 0.02178 - R2: 1.0001 -- iter: 0900/1168\n",
      "Training Step: 427  | total loss: \u001b[1m\u001b[32m0.02086\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 018 | loss: 0.02086 - R2: 0.9998 -- iter: 0950/1168\n",
      "Training Step: 428  | total loss: \u001b[1m\u001b[32m0.02282\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 018 | loss: 0.02282 - R2: 0.9999 -- iter: 1000/1168\n",
      "Training Step: 429  | total loss: \u001b[1m\u001b[32m0.02152\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 018 | loss: 0.02152 - R2: 0.9999 -- iter: 1050/1168\n",
      "Training Step: 430  | total loss: \u001b[1m\u001b[32m0.02152\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 018 | loss: 0.02152 - R2: 1.0003 -- iter: 1100/1168\n",
      "Training Step: 431  | total loss: \u001b[1m\u001b[32m0.02120\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 018 | loss: 0.02120 - R2: 0.9996 -- iter: 1150/1168\n",
      "Training Step: 432  | total loss: \u001b[1m\u001b[32m0.02120\u001b[0m\u001b[0m | time: 1.118s\n",
      "| SGD | epoch: 018 | loss: 0.02120 - R2: 0.9996 | val_loss: 0.02469 - val_acc: 0.9998 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 433  | total loss: \u001b[1m\u001b[32m0.02135\u001b[0m\u001b[0m | time: 0.005s\n",
      "| SGD | epoch: 019 | loss: 0.02135 - R2: 0.9996 -- iter: 0050/1168\n",
      "Training Step: 434  | total loss: \u001b[1m\u001b[32m0.02067\u001b[0m\u001b[0m | time: 0.009s\n",
      "| SGD | epoch: 019 | loss: 0.02067 - R2: 0.9994 -- iter: 0100/1168\n",
      "Training Step: 435  | total loss: \u001b[1m\u001b[32m0.01996\u001b[0m\u001b[0m | time: 0.013s\n",
      "| SGD | epoch: 019 | loss: 0.01996 - R2: 0.9994 -- iter: 0150/1168\n",
      "Training Step: 436  | total loss: \u001b[1m\u001b[32m0.02083\u001b[0m\u001b[0m | time: 0.016s\n",
      "| SGD | epoch: 019 | loss: 0.02083 - R2: 1.0002 -- iter: 0200/1168\n",
      "Training Step: 437  | total loss: \u001b[1m\u001b[32m0.02020\u001b[0m\u001b[0m | time: 0.020s\n",
      "| SGD | epoch: 019 | loss: 0.02020 - R2: 0.9998 -- iter: 0250/1168\n",
      "Training Step: 438  | total loss: \u001b[1m\u001b[32m0.01939\u001b[0m\u001b[0m | time: 0.023s\n",
      "| SGD | epoch: 019 | loss: 0.01939 - R2: 0.9995 -- iter: 0300/1168\n",
      "Training Step: 439  | total loss: \u001b[1m\u001b[32m0.01939\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 019 | loss: 0.01939 - R2: 0.9995 -- iter: 0350/1168\n",
      "Training Step: 440  | total loss: \u001b[1m\u001b[32m0.01836\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 019 | loss: 0.01836 - R2: 0.9996 -- iter: 0400/1168\n",
      "Training Step: 441  | total loss: \u001b[1m\u001b[32m0.01834\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 019 | loss: 0.01834 - R2: 1.0002 -- iter: 0450/1168\n",
      "Training Step: 442  | total loss: \u001b[1m\u001b[32m0.01836\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 019 | loss: 0.01836 - R2: 0.9997 -- iter: 0500/1168\n",
      "Training Step: 443  | total loss: \u001b[1m\u001b[32m0.01882\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 019 | loss: 0.01882 - R2: 0.9997 -- iter: 0550/1168\n",
      "Training Step: 444  | total loss: \u001b[1m\u001b[32m0.01882\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 019 | loss: 0.01882 - R2: 0.9997 -- iter: 0600/1168\n",
      "Training Step: 445  | total loss: \u001b[1m\u001b[32m0.01837\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 019 | loss: 0.01837 - R2: 0.9997 -- iter: 0650/1168\n",
      "Training Step: 446  | total loss: \u001b[1m\u001b[32m0.01906\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 019 | loss: 0.01906 - R2: 0.9997 -- iter: 0700/1168\n",
      "Training Step: 447  | total loss: \u001b[1m\u001b[32m0.01906\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 019 | loss: 0.01906 - R2: 0.9997 -- iter: 0750/1168\n",
      "Training Step: 448  | total loss: \u001b[1m\u001b[32m0.01839\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 019 | loss: 0.01839 - R2: 0.9993 -- iter: 0800/1168\n",
      "Training Step: 449  | total loss: \u001b[1m\u001b[32m0.01839\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 019 | loss: 0.01839 - R2: 0.9999 -- iter: 0850/1168\n",
      "Training Step: 450  | total loss: \u001b[1m\u001b[32m0.01601\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 019 | loss: 0.01601 - R2: 1.0003 -- iter: 0900/1168\n",
      "Training Step: 451  | total loss: \u001b[1m\u001b[32m0.01601\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 019 | loss: 0.01601 - R2: 1.0001 -- iter: 0950/1168\n",
      "Training Step: 452  | total loss: \u001b[1m\u001b[32m0.01559\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 019 | loss: 0.01559 - R2: 0.9999 -- iter: 1000/1168\n",
      "Training Step: 453  | total loss: \u001b[1m\u001b[32m0.01559\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 019 | loss: 0.01559 - R2: 0.9999 -- iter: 1050/1168\n",
      "Training Step: 454  | total loss: \u001b[1m\u001b[32m0.01552\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 019 | loss: 0.01552 - R2: 1.0000 -- iter: 1100/1168\n",
      "Training Step: 455  | total loss: \u001b[1m\u001b[32m0.01624\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 019 | loss: 0.01624 - R2: 1.0001 -- iter: 1150/1168\n",
      "Training Step: 456  | total loss: \u001b[1m\u001b[32m0.01624\u001b[0m\u001b[0m | time: 1.110s\n",
      "| SGD | epoch: 019 | loss: 0.01624 - R2: 1.0003 | val_loss: 0.02318 - val_acc: 0.9987 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 457  | total loss: \u001b[1m\u001b[32m0.01581\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 020 | loss: 0.01581 - R2: 1.0005 -- iter: 0050/1168\n",
      "Training Step: 458  | total loss: \u001b[1m\u001b[32m0.01527\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 020 | loss: 0.01527 - R2: 1.0005 -- iter: 0100/1168\n",
      "Training Step: 459  | total loss: \u001b[1m\u001b[32m0.01493\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 020 | loss: 0.01493 - R2: 0.9999 -- iter: 0150/1168\n",
      "Training Step: 460  | total loss: \u001b[1m\u001b[32m0.01479\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 020 | loss: 0.01479 - R2: 1.0001 -- iter: 0200/1168\n",
      "Training Step: 461  | total loss: \u001b[1m\u001b[32m0.01661\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 020 | loss: 0.01661 - R2: 1.0000 -- iter: 0250/1168\n",
      "Training Step: 462  | total loss: \u001b[1m\u001b[32m0.02470\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 020 | loss: 0.02470 - R2: 1.0004 -- iter: 0300/1168\n",
      "Training Step: 463  | total loss: \u001b[1m\u001b[32m0.02458\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 020 | loss: 0.02458 - R2: 0.9998 -- iter: 0350/1168\n",
      "Training Step: 464  | total loss: \u001b[1m\u001b[32m0.02205\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 020 | loss: 0.02205 - R2: 0.9995 -- iter: 0400/1168\n",
      "Training Step: 465  | total loss: \u001b[1m\u001b[32m0.02205\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 020 | loss: 0.02205 - R2: 1.0000 -- iter: 0450/1168\n",
      "Training Step: 466  | total loss: \u001b[1m\u001b[32m0.02114\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 020 | loss: 0.02114 - R2: 1.0000 -- iter: 0500/1168\n",
      "Training Step: 467  | total loss: \u001b[1m\u001b[32m0.02031\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 020 | loss: 0.02031 - R2: 0.9999 -- iter: 0550/1168\n",
      "Training Step: 468  | total loss: \u001b[1m\u001b[32m0.02054\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 020 | loss: 0.02054 - R2: 1.0003 -- iter: 0600/1168\n",
      "Training Step: 469  | total loss: \u001b[1m\u001b[32m0.02055\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 020 | loss: 0.02055 - R2: 0.9994 -- iter: 0650/1168\n",
      "Training Step: 470  | total loss: \u001b[1m\u001b[32m0.02341\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 020 | loss: 0.02341 - R2: 1.0000 -- iter: 0700/1168\n",
      "Training Step: 471  | total loss: \u001b[1m\u001b[32m0.02274\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 020 | loss: 0.02274 - R2: 1.0002 -- iter: 0750/1168\n",
      "Training Step: 472  | total loss: \u001b[1m\u001b[32m0.02197\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 020 | loss: 0.02197 - R2: 0.9996 -- iter: 0800/1168\n",
      "Training Step: 473  | total loss: \u001b[1m\u001b[32m0.02234\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 020 | loss: 0.02234 - R2: 0.9990 -- iter: 0850/1168\n",
      "Training Step: 474  | total loss: \u001b[1m\u001b[32m0.02152\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 020 | loss: 0.02152 - R2: 0.9991 -- iter: 0900/1168\n",
      "Training Step: 475  | total loss: \u001b[1m\u001b[32m0.02269\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 020 | loss: 0.02269 - R2: 0.9998 -- iter: 0950/1168\n",
      "Training Step: 476  | total loss: \u001b[1m\u001b[32m0.02332\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 020 | loss: 0.02332 - R2: 1.0002 -- iter: 1000/1168\n",
      "Training Step: 477  | total loss: \u001b[1m\u001b[32m0.02562\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 020 | loss: 0.02562 - R2: 1.0000 -- iter: 1050/1168\n",
      "Training Step: 478  | total loss: \u001b[1m\u001b[32m0.02483\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 020 | loss: 0.02483 - R2: 1.0000 -- iter: 1100/1168\n",
      "Training Step: 479  | total loss: \u001b[1m\u001b[32m0.02360\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 020 | loss: 0.02360 - R2: 1.0002 -- iter: 1150/1168\n",
      "Training Step: 480  | total loss: \u001b[1m\u001b[32m0.02285\u001b[0m\u001b[0m | time: 1.117s\n",
      "| SGD | epoch: 020 | loss: 0.02285 - R2: 1.0003 | val_loss: 0.02509 - val_acc: 0.9979 -- iter: 1168/1168\n",
      "--\n",
      "---------------------------------\n",
      "Run id: QKE5WH\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "INFO:tensorflow:Summary name StandardError/ (raw) is illegal; using StandardError/__raw_ instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Training samples: 1168\n",
      "Validation samples: 292\n",
      "--\n",
      "Training Step: 1  | time: 0.020s\n",
      "| SGD | epoch: 001 | loss: 0.00000 - R2: 0.0000 -- iter: 0064/1168\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m131.53171\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 001 | loss: 131.53171 - R2: 0.0000 -- iter: 0128/1168\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m142.00473\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 001 | loss: 142.00473 - R2: 0.0001 -- iter: 0192/1168\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m142.00473\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 001 | loss: 142.00473 - R2: 0.0001 -- iter: 0256/1168\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m141.43661\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 001 | loss: 141.43661 - R2: 0.0001 -- iter: 0320/1168\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m140.51355\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 001 | loss: 140.51355 - R2: 0.0002 -- iter: 0384/1168\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m138.87376\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 001 | loss: 138.87376 - R2: 0.0005 -- iter: 0448/1168\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m138.87376\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 001 | loss: 138.87376 - R2: 0.0005 -- iter: 0512/1168\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m136.52019\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 001 | loss: 136.52019 - R2: 0.0007 -- iter: 0576/1168\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m135.43445\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 001 | loss: 135.43445 - R2: 0.0009 -- iter: 0640/1168\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m135.00029\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 001 | loss: 135.00029 - R2: 0.0014 -- iter: 0704/1168\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m133.55298\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 001 | loss: 133.55298 - R2: 0.0017 -- iter: 0768/1168\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m133.55298\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 001 | loss: 133.55298 - R2: 0.0017 -- iter: 0832/1168\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m131.35719\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 001 | loss: 131.35719 - R2: 0.0023 -- iter: 0896/1168\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m130.35681\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 001 | loss: 130.35681 - R2: 0.0027 -- iter: 0960/1168\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m130.35681\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 001 | loss: 130.35681 - R2: 0.0027 -- iter: 1024/1168\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m129.54936\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 001 | loss: 129.54936 - R2: 0.0031 -- iter: 1088/1168\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m129.14726\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 001 | loss: 129.14726 - R2: 0.0035 -- iter: 1152/1168\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m127.53025\u001b[0m\u001b[0m | time: 1.129s\n",
      "| SGD | epoch: 001 | loss: 127.53025 - R2: 0.0040 | val_loss: 122.58027 - val_acc: 0.0062 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m126.25660\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 002 | loss: 126.25660 - R2: 0.0045 -- iter: 0064/1168\n",
      "Training Step: 21  | total loss: \u001b[1m\u001b[32m125.08324\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 002 | loss: 125.08324 - R2: 0.0050 -- iter: 0128/1168\n",
      "Training Step: 22  | total loss: \u001b[1m\u001b[32m124.17270\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 002 | loss: 124.17270 - R2: 0.0055 -- iter: 0192/1168\n",
      "Training Step: 23  | total loss: \u001b[1m\u001b[32m123.25267\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 002 | loss: 123.25267 - R2: 0.0061 -- iter: 0256/1168\n",
      "Training Step: 24  | total loss: \u001b[1m\u001b[32m120.84209\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 002 | loss: 120.84209 - R2: 0.0073 -- iter: 0320/1168\n",
      "Training Step: 25  | total loss: \u001b[1m\u001b[32m120.84209\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 002 | loss: 120.84209 - R2: 0.0073 -- iter: 0384/1168\n",
      "Training Step: 26  | total loss: \u001b[1m\u001b[32m119.89643\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 002 | loss: 119.89643 - R2: 0.0080 -- iter: 0448/1168\n",
      "Training Step: 27  | total loss: \u001b[1m\u001b[32m118.66632\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 002 | loss: 118.66632 - R2: 0.0093 -- iter: 0512/1168\n",
      "Training Step: 28  | total loss: \u001b[1m\u001b[32m117.65202\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 002 | loss: 117.65202 - R2: 0.0100 -- iter: 0576/1168\n",
      "Training Step: 29  | total loss: \u001b[1m\u001b[32m116.64180\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 002 | loss: 116.64180 - R2: 0.0108 -- iter: 0640/1168\n",
      "Training Step: 30  | total loss: \u001b[1m\u001b[32m116.64180\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 002 | loss: 116.64180 - R2: 0.0108 -- iter: 0704/1168\n",
      "Training Step: 31  | total loss: \u001b[1m\u001b[32m115.96650\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 002 | loss: 115.96650 - R2: 0.0116 -- iter: 0768/1168\n",
      "Training Step: 32  | total loss: \u001b[1m\u001b[32m113.63153\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 002 | loss: 113.63153 - R2: 0.0132 -- iter: 0832/1168\n",
      "Training Step: 33  | total loss: \u001b[1m\u001b[32m113.63153\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 002 | loss: 113.63153 - R2: 0.0141 -- iter: 0896/1168\n",
      "Training Step: 34  | total loss: \u001b[1m\u001b[32m112.40443\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 002 | loss: 112.40443 - R2: 0.0141 -- iter: 0960/1168\n",
      "Training Step: 35  | total loss: \u001b[1m\u001b[32m110.59608\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 002 | loss: 110.59608 - R2: 0.0159 -- iter: 1024/1168\n",
      "Training Step: 36  | total loss: \u001b[1m\u001b[32m110.59608\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 002 | loss: 110.59608 - R2: 0.0159 -- iter: 1088/1168\n",
      "Training Step: 37  | total loss: \u001b[1m\u001b[32m108.97238\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 002 | loss: 108.97238 - R2: 0.0178 -- iter: 1152/1168\n",
      "Training Step: 38  | total loss: \u001b[1m\u001b[32m108.17913\u001b[0m\u001b[0m | time: 1.107s\n",
      "| SGD | epoch: 002 | loss: 108.17913 - R2: 0.0188 | val_loss: 102.81495 - val_acc: 0.0244 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 39  | total loss: \u001b[1m\u001b[32m108.17913\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 003 | loss: 108.17913 - R2: 0.0188 -- iter: 0064/1168\n",
      "Training Step: 40  | total loss: \u001b[1m\u001b[32m106.71367\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 003 | loss: 106.71367 - R2: 0.0199 -- iter: 0128/1168\n",
      "Training Step: 41  | total loss: \u001b[1m\u001b[32m105.36656\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 003 | loss: 105.36656 - R2: 0.0211 -- iter: 0192/1168\n",
      "Training Step: 42  | total loss: \u001b[1m\u001b[32m104.89555\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 003 | loss: 104.89555 - R2: 0.0221 -- iter: 0256/1168\n",
      "Training Step: 43  | total loss: \u001b[1m\u001b[32m104.12458\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 003 | loss: 104.12458 - R2: 0.0232 -- iter: 0320/1168\n",
      "Training Step: 44  | total loss: \u001b[1m\u001b[32m103.23827\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 003 | loss: 103.23827 - R2: 0.0243 -- iter: 0384/1168\n",
      "Training Step: 45  | total loss: \u001b[1m\u001b[32m102.15807\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 003 | loss: 102.15807 - R2: 0.0255 -- iter: 0448/1168\n",
      "Training Step: 46  | total loss: \u001b[1m\u001b[32m101.41541\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 003 | loss: 101.41541 - R2: 0.0267 -- iter: 0512/1168\n",
      "Training Step: 47  | total loss: \u001b[1m\u001b[32m100.57687\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 003 | loss: 100.57687 - R2: 0.0279 -- iter: 0576/1168\n",
      "Training Step: 48  | total loss: \u001b[1m\u001b[32m99.88013\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 003 | loss: 99.88013 - R2: 0.0291 -- iter: 0640/1168\n",
      "Training Step: 49  | total loss: \u001b[1m\u001b[32m99.30641\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 003 | loss: 99.30641 - R2: 0.0303 -- iter: 0704/1168\n",
      "Training Step: 50  | total loss: \u001b[1m\u001b[32m97.17692\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 003 | loss: 97.17692 - R2: 0.0331 -- iter: 0768/1168\n",
      "Training Step: 51  | total loss: \u001b[1m\u001b[32m96.21329\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 003 | loss: 96.21329 - R2: 0.0345 -- iter: 0832/1168\n",
      "Training Step: 52  | total loss: \u001b[1m\u001b[32m96.21329\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 003 | loss: 96.21329 - R2: 0.0345 -- iter: 0896/1168\n",
      "Training Step: 53  | total loss: \u001b[1m\u001b[32m95.17699\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 003 | loss: 95.17699 - R2: 0.0359 -- iter: 0960/1168\n",
      "Training Step: 54  | total loss: \u001b[1m\u001b[32m94.16830\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 003 | loss: 94.16830 - R2: 0.0375 -- iter: 1024/1168\n",
      "Training Step: 55  | total loss: \u001b[1m\u001b[32m93.54933\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 003 | loss: 93.54933 - R2: 0.0388 -- iter: 1088/1168\n",
      "Training Step: 56  | total loss: \u001b[1m\u001b[32m92.07439\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 003 | loss: 92.07439 - R2: 0.0418 -- iter: 1152/1168\n",
      "Training Step: 57  | total loss: \u001b[1m\u001b[32m92.07439\u001b[0m\u001b[0m | time: 1.142s\n",
      "| SGD | epoch: 003 | loss: 92.07439 - R2: 0.0418 | val_loss: 84.31916 - val_acc: 0.0556 -- iter: 1168/1168\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 58  | total loss: \u001b[1m\u001b[32m91.21967\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 004 | loss: 91.21967 - R2: 0.0433 -- iter: 0064/1168\n",
      "Training Step: 59  | total loss: \u001b[1m\u001b[32m90.50325\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 004 | loss: 90.50325 - R2: 0.0449 -- iter: 0128/1168\n",
      "Training Step: 60  | total loss: \u001b[1m\u001b[32m88.18044\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 004 | loss: 88.18044 - R2: 0.0486 -- iter: 0192/1168\n",
      "Training Step: 61  | total loss: \u001b[1m\u001b[32m87.40452\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 004 | loss: 87.40452 - R2: 0.0502 -- iter: 0256/1168\n",
      "Training Step: 62  | total loss: \u001b[1m\u001b[32m87.40452\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 004 | loss: 87.40452 - R2: 0.0519 -- iter: 0320/1168\n",
      "Training Step: 63  | total loss: \u001b[1m\u001b[32m86.68460\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 004 | loss: 86.68460 - R2: 0.0519 -- iter: 0384/1168\n",
      "Training Step: 64  | total loss: \u001b[1m\u001b[32m85.01126\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 004 | loss: 85.01126 - R2: 0.0556 -- iter: 0448/1168\n",
      "Training Step: 65  | total loss: \u001b[1m\u001b[32m84.03973\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 004 | loss: 84.03973 - R2: 0.0576 -- iter: 0512/1168\n",
      "Training Step: 66  | total loss: \u001b[1m\u001b[32m84.03973\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 004 | loss: 84.03973 - R2: 0.0576 -- iter: 0576/1168\n",
      "Training Step: 67  | total loss: \u001b[1m\u001b[32m83.07783\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 004 | loss: 83.07783 - R2: 0.0596 -- iter: 0640/1168\n",
      "Training Step: 68  | total loss: \u001b[1m\u001b[32m82.27587\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 004 | loss: 82.27587 - R2: 0.0615 -- iter: 0704/1168\n",
      "Training Step: 69  | total loss: \u001b[1m\u001b[32m81.40093\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 004 | loss: 81.40093 - R2: 0.0636 -- iter: 0768/1168\n",
      "Training Step: 70  | total loss: \u001b[1m\u001b[32m79.65059\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 004 | loss: 79.65059 - R2: 0.0679 -- iter: 0832/1168\n",
      "Training Step: 71  | total loss: \u001b[1m\u001b[32m78.83269\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 004 | loss: 78.83269 - R2: 0.0701 -- iter: 0896/1168\n",
      "Training Step: 72  | total loss: \u001b[1m\u001b[32m78.83269\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 004 | loss: 78.83269 - R2: 0.0724 -- iter: 0960/1168\n",
      "Training Step: 73  | total loss: \u001b[1m\u001b[32m77.92149\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 004 | loss: 77.92149 - R2: 0.0724 -- iter: 1024/1168\n",
      "Training Step: 74  | total loss: \u001b[1m\u001b[32m76.93411\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 004 | loss: 76.93411 - R2: 0.0773 -- iter: 1088/1168\n",
      "Training Step: 75  | total loss: \u001b[1m\u001b[32m75.90897\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 004 | loss: 75.90897 - R2: 0.0773 -- iter: 1152/1168\n",
      "Training Step: 76  | total loss: \u001b[1m\u001b[32m74.15160\u001b[0m\u001b[0m | time: 1.122s\n",
      "| SGD | epoch: 004 | loss: 74.15160 - R2: 0.0823 | val_loss: 65.21529 - val_acc: 0.1076 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 77  | total loss: \u001b[1m\u001b[32m74.15160\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 005 | loss: 74.15160 - R2: 0.0823 -- iter: 0064/1168\n",
      "Training Step: 78  | total loss: \u001b[1m\u001b[32m73.32036\u001b[0m\u001b[0m | time: 0.006s\n",
      "| SGD | epoch: 005 | loss: 73.32036 - R2: 0.0849 -- iter: 0128/1168\n",
      "Training Step: 79  | total loss: \u001b[1m\u001b[32m71.40429\u001b[0m\u001b[0m | time: 0.014s\n",
      "| SGD | epoch: 005 | loss: 71.40429 - R2: 0.0904 -- iter: 0192/1168\n",
      "Training Step: 80  | total loss: \u001b[1m\u001b[32m71.40429\u001b[0m\u001b[0m | time: 0.018s\n",
      "| SGD | epoch: 005 | loss: 71.40429 - R2: 0.0904 -- iter: 0256/1168\n",
      "Training Step: 81  | total loss: \u001b[1m\u001b[32m70.45030\u001b[0m\u001b[0m | time: 0.020s\n",
      "| SGD | epoch: 005 | loss: 70.45030 - R2: 0.0933 -- iter: 0320/1168\n",
      "Training Step: 82  | total loss: \u001b[1m\u001b[32m68.59374\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 005 | loss: 68.59374 - R2: 0.0963 -- iter: 0384/1168\n",
      "Training Step: 83  | total loss: \u001b[1m\u001b[32m68.59374\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 005 | loss: 68.59374 - R2: 0.0993 -- iter: 0448/1168\n",
      "Training Step: 84  | total loss: \u001b[1m\u001b[32m67.76382\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 005 | loss: 67.76382 - R2: 0.1023 -- iter: 0512/1168\n",
      "Training Step: 85  | total loss: \u001b[1m\u001b[32m66.78666\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 005 | loss: 66.78666 - R2: 0.1091 -- iter: 0576/1168\n",
      "Training Step: 86  | total loss: \u001b[1m\u001b[32m64.73808\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 005 | loss: 64.73808 - R2: 0.1127 -- iter: 0640/1168\n",
      "Training Step: 87  | total loss: \u001b[1m\u001b[32m64.73808\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 005 | loss: 64.73808 - R2: 0.1127 -- iter: 0704/1168\n",
      "Training Step: 88  | total loss: \u001b[1m\u001b[32m63.61648\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 005 | loss: 63.61648 - R2: 0.1166 -- iter: 0768/1168\n",
      "Training Step: 89  | total loss: \u001b[1m\u001b[32m62.57367\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 005 | loss: 62.57367 - R2: 0.1204 -- iter: 0832/1168\n",
      "Training Step: 90  | total loss: \u001b[1m\u001b[32m60.53588\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 005 | loss: 60.53588 - R2: 0.1284 -- iter: 0896/1168\n",
      "Training Step: 91  | total loss: \u001b[1m\u001b[32m59.43211\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 005 | loss: 59.43211 - R2: 0.1328 -- iter: 0960/1168\n",
      "Training Step: 92  | total loss: \u001b[1m\u001b[32m58.35527\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 005 | loss: 58.35527 - R2: 0.1372 -- iter: 1024/1168\n",
      "Training Step: 93  | total loss: \u001b[1m\u001b[32m57.24689\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 005 | loss: 57.24689 - R2: 0.1418 -- iter: 1088/1168\n",
      "Training Step: 94  | total loss: \u001b[1m\u001b[32m56.17184\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 005 | loss: 56.17184 - R2: 0.1465 -- iter: 1152/1168\n",
      "Training Step: 95  | total loss: \u001b[1m\u001b[32m55.16579\u001b[0m\u001b[0m | time: 1.080s\n",
      "| SGD | epoch: 005 | loss: 55.16579 - R2: 0.1512 | val_loss: 43.66363 - val_acc: 0.2029 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 96  | total loss: \u001b[1m\u001b[32m55.16579\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 006 | loss: 55.16579 - R2: 0.1512 -- iter: 0064/1168\n",
      "Training Step: 97  | total loss: \u001b[1m\u001b[32m52.92786\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 006 | loss: 52.92786 - R2: 0.1616 -- iter: 0128/1168\n",
      "Training Step: 98  | total loss: \u001b[1m\u001b[32m51.83268\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 006 | loss: 51.83268 - R2: 0.1669 -- iter: 0192/1168\n",
      "Training Step: 99  | total loss: \u001b[1m\u001b[32m51.83268\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 006 | loss: 51.83268 - R2: 0.1669 -- iter: 0256/1168\n",
      "Training Step: 100  | total loss: \u001b[1m\u001b[32m49.50980\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 006 | loss: 49.50980 - R2: 0.1785 -- iter: 0320/1168\n",
      "Training Step: 101  | total loss: \u001b[1m\u001b[32m49.50980\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 006 | loss: 49.50980 - R2: 0.1785 -- iter: 0384/1168\n",
      "Training Step: 102  | total loss: \u001b[1m\u001b[32m48.29386\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 006 | loss: 48.29386 - R2: 0.1847 -- iter: 0448/1168\n",
      "Training Step: 103  | total loss: \u001b[1m\u001b[32m45.90500\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 006 | loss: 45.90500 - R2: 0.1976 -- iter: 0512/1168\n",
      "Training Step: 104  | total loss: \u001b[1m\u001b[32m44.83757\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 006 | loss: 44.83757 - R2: 0.2039 -- iter: 0576/1168\n",
      "Training Step: 105  | total loss: \u001b[1m\u001b[32m44.83757\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 006 | loss: 44.83757 - R2: 0.2039 -- iter: 0640/1168\n",
      "Training Step: 106  | total loss: \u001b[1m\u001b[32m42.43025\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 006 | loss: 42.43025 - R2: 0.2183 -- iter: 0704/1168\n",
      "Training Step: 107  | total loss: \u001b[1m\u001b[32m42.43025\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 006 | loss: 42.43025 - R2: 0.2183 -- iter: 0768/1168\n",
      "Training Step: 108  | total loss: \u001b[1m\u001b[32m39.93186\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 006 | loss: 39.93186 - R2: 0.2339 -- iter: 0832/1168\n",
      "Training Step: 109  | total loss: \u001b[1m\u001b[32m39.93186\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 006 | loss: 39.93186 - R2: 0.2339 -- iter: 0896/1168\n",
      "Training Step: 110  | total loss: \u001b[1m\u001b[32m37.62024\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 006 | loss: 37.62024 - R2: 0.2497 -- iter: 0960/1168\n",
      "Training Step: 111  | total loss: \u001b[1m\u001b[32m37.62024\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 006 | loss: 37.62024 - R2: 0.2584 -- iter: 1024/1168\n",
      "Training Step: 112  | total loss: \u001b[1m\u001b[32m36.40565\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 006 | loss: 36.40565 - R2: 0.2584 -- iter: 1088/1168\n",
      "Training Step: 113  | total loss: \u001b[1m\u001b[32m34.00656\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 006 | loss: 34.00656 - R2: 0.2764 -- iter: 1152/1168\n",
      "Training Step: 114  | total loss: \u001b[1m\u001b[32m34.00656\u001b[0m\u001b[0m | time: 1.148s\n",
      "| SGD | epoch: 006 | loss: 34.00656 - R2: 0.2764 | val_loss: 21.26925 - val_acc: 0.3806 -- iter: 1168/1168\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 115  | total loss: \u001b[1m\u001b[32m32.87838\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 007 | loss: 32.87838 - R2: 0.2854 -- iter: 0064/1168\n",
      "Training Step: 116  | total loss: \u001b[1m\u001b[32m31.72815\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 007 | loss: 31.72815 - R2: 0.2949 -- iter: 0128/1168\n",
      "Training Step: 117  | total loss: \u001b[1m\u001b[32m30.53178\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 007 | loss: 30.53178 - R2: 0.3051 -- iter: 0192/1168\n",
      "Training Step: 118  | total loss: \u001b[1m\u001b[32m29.44820\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 007 | loss: 29.44820 - R2: 0.3147 -- iter: 0256/1168\n",
      "Training Step: 119  | total loss: \u001b[1m\u001b[32m27.17241\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 007 | loss: 27.17241 - R2: 0.3361 -- iter: 0320/1168\n",
      "Training Step: 120  | total loss: \u001b[1m\u001b[32m27.17241\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 007 | loss: 27.17241 - R2: 0.3472 -- iter: 0384/1168\n",
      "Training Step: 121  | total loss: \u001b[1m\u001b[32m24.97681\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 007 | loss: 24.97681 - R2: 0.3583 -- iter: 0448/1168\n",
      "Training Step: 122  | total loss: \u001b[1m\u001b[32m23.87976\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 007 | loss: 23.87976 - R2: 0.3702 -- iter: 0512/1168\n",
      "Training Step: 123  | total loss: \u001b[1m\u001b[32m22.77221\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 007 | loss: 22.77221 - R2: 0.3828 -- iter: 0576/1168\n",
      "Training Step: 124  | total loss: \u001b[1m\u001b[32m21.74043\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 007 | loss: 21.74043 - R2: 0.3948 -- iter: 0640/1168\n",
      "Training Step: 125  | total loss: \u001b[1m\u001b[32m21.74043\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 007 | loss: 21.74043 - R2: 0.3948 -- iter: 0704/1168\n",
      "Training Step: 126  | total loss: \u001b[1m\u001b[32m20.74271\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 007 | loss: 20.74271 - R2: 0.4071 -- iter: 0768/1168\n",
      "Training Step: 127  | total loss: \u001b[1m\u001b[32m19.70223\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 007 | loss: 19.70223 - R2: 0.4342 -- iter: 0832/1168\n",
      "Training Step: 128  | total loss: \u001b[1m\u001b[32m17.63639\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 007 | loss: 17.63639 - R2: 0.4488 -- iter: 0896/1168\n",
      "Training Step: 129  | total loss: \u001b[1m\u001b[32m16.61521\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 007 | loss: 16.61521 - R2: 0.4641 -- iter: 0960/1168\n",
      "Training Step: 130  | total loss: \u001b[1m\u001b[32m15.59196\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 007 | loss: 15.59196 - R2: 0.4802 -- iter: 1024/1168\n",
      "Training Step: 131  | total loss: \u001b[1m\u001b[32m15.59196\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 007 | loss: 15.59196 - R2: 0.4802 -- iter: 1088/1168\n",
      "Training Step: 132  | total loss: \u001b[1m\u001b[32m13.69852\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 007 | loss: 13.69852 - R2: 0.5122 -- iter: 1152/1168\n",
      "Training Step: 133  | total loss: \u001b[1m\u001b[32m12.75308\u001b[0m\u001b[0m | time: 1.170s\n",
      "| SGD | epoch: 007 | loss: 12.75308 - R2: 0.5299 | val_loss: 3.91507 - val_acc: 0.7017 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 134  | total loss: \u001b[1m\u001b[32m11.85549\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 008 | loss: 11.85549 - R2: 0.5477 -- iter: 0064/1168\n",
      "Training Step: 135  | total loss: \u001b[1m\u001b[32m11.00593\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 008 | loss: 11.00593 - R2: 0.5652 -- iter: 0128/1168\n",
      "Training Step: 136  | total loss: \u001b[1m\u001b[32m10.17609\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 008 | loss: 10.17609 - R2: 0.5837 -- iter: 0192/1168\n",
      "Training Step: 137  | total loss: \u001b[1m\u001b[32m10.17609\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 008 | loss: 10.17609 - R2: 0.5837 -- iter: 0256/1168\n",
      "Training Step: 138  | total loss: \u001b[1m\u001b[32m8.62527\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 008 | loss: 8.62527 - R2: 0.6219 -- iter: 0320/1168\n",
      "Training Step: 139  | total loss: \u001b[1m\u001b[32m7.88018\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 008 | loss: 7.88018 - R2: 0.6429 -- iter: 0384/1168\n",
      "Training Step: 140  | total loss: \u001b[1m\u001b[32m7.17313\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 008 | loss: 7.17313 - R2: 0.6647 -- iter: 0448/1168\n",
      "Training Step: 141  | total loss: \u001b[1m\u001b[32m6.52984\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 008 | loss: 6.52984 - R2: 0.6859 -- iter: 0512/1168\n",
      "Training Step: 142  | total loss: \u001b[1m\u001b[32m6.52984\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 008 | loss: 6.52984 - R2: 0.6859 -- iter: 0576/1168\n",
      "Training Step: 143  | total loss: \u001b[1m\u001b[32m5.36611\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 008 | loss: 5.36611 - R2: 0.7297 -- iter: 0640/1168\n",
      "Training Step: 144  | total loss: \u001b[1m\u001b[32m5.36611\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 008 | loss: 5.36611 - R2: 0.7297 -- iter: 0704/1168\n",
      "Training Step: 145  | total loss: \u001b[1m\u001b[32m4.85564\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 008 | loss: 4.85564 - R2: 0.7511 -- iter: 0768/1168\n",
      "Training Step: 146  | total loss: \u001b[1m\u001b[32m4.38649\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 008 | loss: 4.38649 - R2: 0.7733 -- iter: 0832/1168\n",
      "Training Step: 147  | total loss: \u001b[1m\u001b[32m3.57825\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 008 | loss: 3.57825 - R2: 0.8136 -- iter: 0896/1168\n",
      "Training Step: 148  | total loss: \u001b[1m\u001b[32m3.22951\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 008 | loss: 3.22951 - R2: 0.8317 -- iter: 0960/1168\n",
      "Training Step: 149  | total loss: \u001b[1m\u001b[32m3.22951\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 008 | loss: 3.22951 - R2: 0.8317 -- iter: 1024/1168\n",
      "Training Step: 150  | total loss: \u001b[1m\u001b[32m2.91566\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 008 | loss: 2.91566 - R2: 0.8479 -- iter: 1088/1168\n",
      "Training Step: 151  | total loss: \u001b[1m\u001b[32m2.62946\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 008 | loss: 2.62946 - R2: 0.8633 -- iter: 1152/1168\n",
      "Training Step: 152  | total loss: \u001b[1m\u001b[32m2.37127\u001b[0m\u001b[0m | time: 1.128s\n",
      "| SGD | epoch: 008 | loss: 2.37127 - R2: 0.8893 | val_loss: 0.06207 - val_acc: 0.9927 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 153  | total loss: \u001b[1m\u001b[32m1.94119\u001b[0m\u001b[0m | time: 0.018s\n",
      "| SGD | epoch: 009 | loss: 1.94119 - R2: 0.8998 -- iter: 0064/1168\n",
      "Training Step: 154  | total loss: \u001b[1m\u001b[32m1.75396\u001b[0m\u001b[0m | time: 0.023s\n",
      "| SGD | epoch: 009 | loss: 1.75396 - R2: 0.9101 -- iter: 0128/1168\n",
      "Training Step: 155  | total loss: \u001b[1m\u001b[32m1.75396\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 009 | loss: 1.75396 - R2: 0.9101 -- iter: 0192/1168\n",
      "Training Step: 156  | total loss: \u001b[1m\u001b[32m1.43141\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 009 | loss: 1.43141 - R2: 0.9266 -- iter: 0256/1168\n",
      "Training Step: 157  | total loss: \u001b[1m\u001b[32m1.29380\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 009 | loss: 1.29380 - R2: 0.9343 -- iter: 0320/1168\n",
      "Training Step: 158  | total loss: \u001b[1m\u001b[32m1.16925\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 009 | loss: 1.16925 - R2: 0.9397 -- iter: 0384/1168\n",
      "Training Step: 159  | total loss: \u001b[1m\u001b[32m1.05775\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 009 | loss: 1.05775 - R2: 0.9460 -- iter: 0448/1168\n",
      "Training Step: 160  | total loss: \u001b[1m\u001b[32m1.05775\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 009 | loss: 1.05775 - R2: 0.9460 -- iter: 0512/1168\n",
      "Training Step: 161  | total loss: \u001b[1m\u001b[32m0.95576\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 009 | loss: 0.95576 - R2: 0.9516 -- iter: 0576/1168\n",
      "Training Step: 162  | total loss: \u001b[1m\u001b[32m0.78552\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 009 | loss: 0.78552 - R2: 0.9620 -- iter: 0640/1168\n",
      "Training Step: 163  | total loss: \u001b[1m\u001b[32m0.71044\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 009 | loss: 0.71044 - R2: 0.9645 -- iter: 0704/1168\n",
      "Training Step: 164  | total loss: \u001b[1m\u001b[32m0.71044\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 009 | loss: 0.71044 - R2: 0.9645 -- iter: 0768/1168\n",
      "Training Step: 165  | total loss: \u001b[1m\u001b[32m0.58029\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 009 | loss: 0.58029 - R2: 0.9709 -- iter: 0832/1168\n",
      "Training Step: 166  | total loss: \u001b[1m\u001b[32m0.52681\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 009 | loss: 0.52681 - R2: 0.9737 -- iter: 0896/1168\n",
      "Training Step: 167  | total loss: \u001b[1m\u001b[32m0.52681\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 009 | loss: 0.52681 - R2: 0.9737 -- iter: 0960/1168\n",
      "Training Step: 168  | total loss: \u001b[1m\u001b[32m0.50233\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 009 | loss: 0.50233 - R2: 0.9767 -- iter: 1024/1168\n",
      "Training Step: 169  | total loss: \u001b[1m\u001b[32m0.45566\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 009 | loss: 0.45566 - R2: 0.9793 -- iter: 1088/1168\n",
      "Training Step: 170  | total loss: \u001b[1m\u001b[32m0.37551\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 009 | loss: 0.37551 - R2: 0.9827 -- iter: 1152/1168\n",
      "Training Step: 171  | total loss: \u001b[1m\u001b[32m0.34132\u001b[0m\u001b[0m | time: 1.088s\n",
      "| SGD | epoch: 009 | loss: 0.34132 - R2: 0.9842 | val_loss: 0.02768 - val_acc: 1.0008 -- iter: 1168/1168\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 172  | total loss: \u001b[1m\u001b[32m0.30995\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 010 | loss: 0.30995 - R2: 0.9860 -- iter: 0064/1168\n",
      "Training Step: 173  | total loss: \u001b[1m\u001b[32m0.30995\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 010 | loss: 0.30995 - R2: 0.9860 -- iter: 0128/1168\n",
      "Training Step: 174  | total loss: \u001b[1m\u001b[32m0.25599\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 010 | loss: 0.25599 - R2: 0.9884 -- iter: 0192/1168\n",
      "Training Step: 175  | total loss: \u001b[1m\u001b[32m0.23286\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 010 | loss: 0.23286 - R2: 0.9892 -- iter: 0256/1168\n",
      "Training Step: 176  | total loss: \u001b[1m\u001b[32m0.21199\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 010 | loss: 0.21199 - R2: 0.9900 -- iter: 0320/1168\n",
      "Training Step: 177  | total loss: \u001b[1m\u001b[32m0.19780\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 010 | loss: 0.19780 - R2: 0.9914 -- iter: 0384/1168\n",
      "Training Step: 178  | total loss: \u001b[1m\u001b[32m0.18064\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 010 | loss: 0.18064 - R2: 0.9924 -- iter: 0448/1168\n",
      "Training Step: 179  | total loss: \u001b[1m\u001b[32m0.16441\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 010 | loss: 0.16441 - R2: 0.9941 -- iter: 0512/1168\n",
      "Training Step: 180  | total loss: \u001b[1m\u001b[32m0.14914\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 010 | loss: 0.14914 - R2: 0.9950 -- iter: 0576/1168\n",
      "Training Step: 181  | total loss: \u001b[1m\u001b[32m0.13729\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 010 | loss: 0.13729 - R2: 0.9954 -- iter: 0640/1168\n",
      "Training Step: 182  | total loss: \u001b[1m\u001b[32m0.12589\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 010 | loss: 0.12589 - R2: 0.9958 -- iter: 0704/1168\n",
      "Training Step: 183  | total loss: \u001b[1m\u001b[32m0.11555\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 010 | loss: 0.11555 - R2: 0.9961 -- iter: 0768/1168\n",
      "Training Step: 184  | total loss: \u001b[1m\u001b[32m0.10858\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 010 | loss: 0.10858 - R2: 0.9963 -- iter: 0832/1168\n",
      "Training Step: 185  | total loss: \u001b[1m\u001b[32m0.10858\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 010 | loss: 0.10858 - R2: 0.9968 -- iter: 0896/1168\n",
      "Training Step: 186  | total loss: \u001b[1m\u001b[32m0.10004\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 010 | loss: 0.10004 - R2: 0.9971 -- iter: 0960/1168\n",
      "Training Step: 187  | total loss: \u001b[1m\u001b[32m0.10673\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 010 | loss: 0.10673 - R2: 0.9979 -- iter: 1024/1168\n",
      "Training Step: 188  | total loss: \u001b[1m\u001b[32m0.10024\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 010 | loss: 0.10024 - R2: 0.9970 -- iter: 1088/1168\n",
      "Training Step: 189  | total loss: \u001b[1m\u001b[32m0.09198\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 010 | loss: 0.09198 - R2: 0.9975 -- iter: 1152/1168\n",
      "Training Step: 190  | total loss: \u001b[1m\u001b[32m0.08563\u001b[0m\u001b[0m | time: 1.164s\n",
      "| SGD | epoch: 010 | loss: 0.08563 - R2: 0.9976 | val_loss: 0.02084 - val_acc: 1.0000 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 191  | total loss: \u001b[1m\u001b[32m0.08285\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 011 | loss: 0.08285 - R2: 0.9978 -- iter: 0064/1168\n",
      "Training Step: 192  | total loss: \u001b[1m\u001b[32m0.07574\u001b[0m\u001b[0m | time: 0.005s\n",
      "| SGD | epoch: 011 | loss: 0.07574 - R2: 0.9978 -- iter: 0128/1168\n",
      "Training Step: 193  | total loss: \u001b[1m\u001b[32m0.07187\u001b[0m\u001b[0m | time: 0.010s\n",
      "| SGD | epoch: 011 | loss: 0.07187 - R2: 0.9977 -- iter: 0192/1168\n",
      "Training Step: 194  | total loss: \u001b[1m\u001b[32m0.06659\u001b[0m\u001b[0m | time: 0.013s\n",
      "| SGD | epoch: 011 | loss: 0.06659 - R2: 0.9977 -- iter: 0256/1168\n",
      "Training Step: 195  | total loss: \u001b[1m\u001b[32m0.06659\u001b[0m\u001b[0m | time: 0.018s\n",
      "| SGD | epoch: 011 | loss: 0.06659 - R2: 0.9977 -- iter: 0320/1168\n",
      "Training Step: 196  | total loss: \u001b[1m\u001b[32m0.06162\u001b[0m\u001b[0m | time: 0.020s\n",
      "| SGD | epoch: 011 | loss: 0.06162 - R2: 0.9979 -- iter: 0384/1168\n",
      "Training Step: 197  | total loss: \u001b[1m\u001b[32m0.05698\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 011 | loss: 0.05698 - R2: 0.9983 -- iter: 0448/1168\n",
      "Training Step: 198  | total loss: \u001b[1m\u001b[32m0.05196\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 011 | loss: 0.05196 - R2: 0.9991 -- iter: 0512/1168\n",
      "Training Step: 199  | total loss: \u001b[1m\u001b[32m0.05196\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 011 | loss: 0.05196 - R2: 0.9991 -- iter: 0576/1168\n",
      "Training Step: 200  | total loss: \u001b[1m\u001b[32m0.04486\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 011 | loss: 0.04486 - R2: 0.9980 -- iter: 0640/1168\n",
      "Training Step: 201  | total loss: \u001b[1m\u001b[32m0.04197\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 011 | loss: 0.04197 - R2: 0.9985 -- iter: 0704/1168\n",
      "Training Step: 202  | total loss: \u001b[1m\u001b[32m0.04269\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 011 | loss: 0.04269 - R2: 0.9992 -- iter: 0768/1168\n",
      "Training Step: 203  | total loss: \u001b[1m\u001b[32m0.04034\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 011 | loss: 0.04034 - R2: 0.9991 -- iter: 0832/1168\n",
      "Training Step: 204  | total loss: \u001b[1m\u001b[32m0.03875\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 011 | loss: 0.03875 - R2: 0.9991 -- iter: 0896/1168\n",
      "Training Step: 205  | total loss: \u001b[1m\u001b[32m0.03875\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 011 | loss: 0.03875 - R2: 0.9998 -- iter: 0960/1168\n",
      "Training Step: 206  | total loss: \u001b[1m\u001b[32m0.03393\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 011 | loss: 0.03393 - R2: 0.9999 -- iter: 1024/1168\n",
      "Training Step: 207  | total loss: \u001b[1m\u001b[32m0.03393\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 011 | loss: 0.03393 - R2: 0.9999 -- iter: 1088/1168\n",
      "Training Step: 208  | total loss: \u001b[1m\u001b[32m0.03170\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 011 | loss: 0.03170 - R2: 0.9999 -- iter: 1152/1168\n",
      "Training Step: 209  | total loss: \u001b[1m\u001b[32m0.03055\u001b[0m\u001b[0m | time: 1.082s\n",
      "| SGD | epoch: 011 | loss: 0.03055 - R2: 0.9997 | val_loss: 0.01736 - val_acc: 1.0014 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 210  | total loss: \u001b[1m\u001b[32m0.02886\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 012 | loss: 0.02886 - R2: 0.9997 -- iter: 0064/1168\n",
      "Training Step: 211  | total loss: \u001b[1m\u001b[32m0.02810\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 012 | loss: 0.02810 - R2: 0.9994 -- iter: 0128/1168\n",
      "Training Step: 212  | total loss: \u001b[1m\u001b[32m0.02876\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 012 | loss: 0.02876 - R2: 0.9999 -- iter: 0192/1168\n",
      "Training Step: 213  | total loss: \u001b[1m\u001b[32m0.02662\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 012 | loss: 0.02662 - R2: 0.9996 -- iter: 0256/1168\n",
      "Training Step: 214  | total loss: \u001b[1m\u001b[32m0.02619\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 012 | loss: 0.02619 - R2: 0.9995 -- iter: 0320/1168\n",
      "Training Step: 215  | total loss: \u001b[1m\u001b[32m0.02486\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 012 | loss: 0.02486 - R2: 0.9995 -- iter: 0384/1168\n",
      "Training Step: 216  | total loss: \u001b[1m\u001b[32m0.02732\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 012 | loss: 0.02732 - R2: 1.0001 -- iter: 0448/1168\n",
      "Training Step: 217  | total loss: \u001b[1m\u001b[32m0.02647\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 012 | loss: 0.02647 - R2: 1.0000 -- iter: 0512/1168\n",
      "Training Step: 218  | total loss: \u001b[1m\u001b[32m0.02552\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 012 | loss: 0.02552 - R2: 0.9999 -- iter: 0576/1168\n",
      "Training Step: 219  | total loss: \u001b[1m\u001b[32m0.02506\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 012 | loss: 0.02506 - R2: 1.0003 -- iter: 0640/1168\n",
      "Training Step: 220  | total loss: \u001b[1m\u001b[32m0.02763\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 012 | loss: 0.02763 - R2: 1.0022 -- iter: 0704/1168\n",
      "Training Step: 221  | total loss: \u001b[1m\u001b[32m0.02763\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 012 | loss: 0.02763 - R2: 1.0022 -- iter: 0768/1168\n",
      "Training Step: 222  | total loss: \u001b[1m\u001b[32m0.02624\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 012 | loss: 0.02624 - R2: 1.0004 -- iter: 0832/1168\n",
      "Training Step: 223  | total loss: \u001b[1m\u001b[32m0.02693\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 012 | loss: 0.02693 - R2: 1.0010 -- iter: 0896/1168\n",
      "Training Step: 224  | total loss: \u001b[1m\u001b[32m0.02582\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 012 | loss: 0.02582 - R2: 1.0005 -- iter: 0960/1168\n",
      "Training Step: 225  | total loss: \u001b[1m\u001b[32m0.02582\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 012 | loss: 0.02582 - R2: 1.0005 -- iter: 1024/1168\n",
      "Training Step: 226  | total loss: \u001b[1m\u001b[32m0.02591\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 012 | loss: 0.02591 - R2: 0.9993 -- iter: 1088/1168\n",
      "Training Step: 227  | total loss: \u001b[1m\u001b[32m0.05883\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 012 | loss: 0.05883 - R2: 0.9979 -- iter: 1152/1168\n",
      "Training Step: 228  | total loss: \u001b[1m\u001b[32m0.05883\u001b[0m\u001b[0m | time: 1.141s\n",
      "| SGD | epoch: 012 | loss: 0.05883 - R2: 0.9979 | val_loss: 0.01794 - val_acc: 1.0032 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 229  | total loss: \u001b[1m\u001b[32m0.05424\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 013 | loss: 0.05424 - R2: 0.9984 -- iter: 0064/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 230  | total loss: \u001b[1m\u001b[32m0.04833\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 013 | loss: 0.04833 - R2: 0.9989 -- iter: 0128/1168\n",
      "Training Step: 231  | total loss: \u001b[1m\u001b[32m0.04833\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 013 | loss: 0.04833 - R2: 0.9989 -- iter: 0192/1168\n",
      "Training Step: 232  | total loss: \u001b[1m\u001b[32m0.04267\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 013 | loss: 0.04267 - R2: 0.9993 -- iter: 0256/1168\n",
      "Training Step: 233  | total loss: \u001b[1m\u001b[32m0.04267\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 013 | loss: 0.04267 - R2: 0.9998 -- iter: 0320/1168\n",
      "Training Step: 234  | total loss: \u001b[1m\u001b[32m0.04208\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 013 | loss: 0.04208 - R2: 0.9998 -- iter: 0384/1168\n",
      "Training Step: 235  | total loss: \u001b[1m\u001b[32m0.04147\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 013 | loss: 0.04147 - R2: 0.9996 -- iter: 0448/1168\n",
      "Training Step: 236  | total loss: \u001b[1m\u001b[32m0.03880\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 013 | loss: 0.03880 - R2: 0.9998 -- iter: 0512/1168\n",
      "Training Step: 237  | total loss: \u001b[1m\u001b[32m0.03672\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 013 | loss: 0.03672 - R2: 0.9999 -- iter: 0576/1168\n",
      "Training Step: 238  | total loss: \u001b[1m\u001b[32m0.03486\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 013 | loss: 0.03486 - R2: 0.9998 -- iter: 0640/1168\n",
      "Training Step: 239  | total loss: \u001b[1m\u001b[32m0.03338\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 013 | loss: 0.03338 - R2: 0.9999 -- iter: 0704/1168\n",
      "Training Step: 240  | total loss: \u001b[1m\u001b[32m0.03088\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 013 | loss: 0.03088 - R2: 1.0005 -- iter: 0768/1168\n",
      "Training Step: 241  | total loss: \u001b[1m\u001b[32m0.02720\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 013 | loss: 0.02720 - R2: 1.0001 -- iter: 0832/1168\n",
      "Training Step: 242  | total loss: \u001b[1m\u001b[32m0.02619\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 013 | loss: 0.02619 - R2: 0.9996 -- iter: 0896/1168\n",
      "Training Step: 243  | total loss: \u001b[1m\u001b[32m0.02667\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 013 | loss: 0.02667 - R2: 0.9997 -- iter: 0960/1168\n",
      "Training Step: 244  | total loss: \u001b[1m\u001b[32m0.02667\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 013 | loss: 0.02667 - R2: 0.9997 -- iter: 1024/1168\n",
      "Training Step: 245  | total loss: \u001b[1m\u001b[32m0.02838\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 013 | loss: 0.02838 - R2: 0.9998 -- iter: 1088/1168\n",
      "Training Step: 246  | total loss: \u001b[1m\u001b[32m0.02681\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 013 | loss: 0.02681 - R2: 0.9997 -- iter: 1152/1168\n",
      "Training Step: 247  | total loss: \u001b[1m\u001b[32m0.02681\u001b[0m\u001b[0m | time: 1.137s\n",
      "| SGD | epoch: 013 | loss: 0.02681 - R2: 0.9996 | val_loss: 0.01657 - val_acc: 1.0012 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 248  | total loss: \u001b[1m\u001b[32m0.02666\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 014 | loss: 0.02666 - R2: 0.9996 -- iter: 0064/1168\n",
      "Training Step: 249  | total loss: \u001b[1m\u001b[32m0.05511\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 014 | loss: 0.05511 - R2: 0.9993 -- iter: 0128/1168\n",
      "Training Step: 250  | total loss: \u001b[1m\u001b[32m0.05195\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 014 | loss: 0.05195 - R2: 0.9996 -- iter: 0192/1168\n",
      "Training Step: 251  | total loss: \u001b[1m\u001b[32m0.04943\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 014 | loss: 0.04943 - R2: 1.0000 -- iter: 0256/1168\n",
      "Training Step: 252  | total loss: \u001b[1m\u001b[32m0.04432\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 014 | loss: 0.04432 - R2: 1.0001 -- iter: 0320/1168\n",
      "Training Step: 253  | total loss: \u001b[1m\u001b[32m0.04432\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 014 | loss: 0.04432 - R2: 1.0001 -- iter: 0384/1168\n",
      "Training Step: 254  | total loss: \u001b[1m\u001b[32m0.04267\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 014 | loss: 0.04267 - R2: 0.9999 -- iter: 0448/1168\n",
      "Training Step: 255  | total loss: \u001b[1m\u001b[32m0.04267\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 014 | loss: 0.04267 - R2: 0.9999 -- iter: 0512/1168\n",
      "Training Step: 256  | total loss: \u001b[1m\u001b[32m0.03806\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 014 | loss: 0.03806 - R2: 0.9996 -- iter: 0576/1168\n",
      "Training Step: 257  | total loss: \u001b[1m\u001b[32m0.03806\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 014 | loss: 0.03806 - R2: 0.9996 -- iter: 0640/1168\n",
      "Training Step: 258  | total loss: \u001b[1m\u001b[32m0.03628\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 014 | loss: 0.03628 - R2: 0.9992 -- iter: 0704/1168\n",
      "Training Step: 259  | total loss: \u001b[1m\u001b[32m0.03504\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 014 | loss: 0.03504 - R2: 0.9986 -- iter: 0768/1168\n",
      "Training Step: 260  | total loss: \u001b[1m\u001b[32m0.03124\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 014 | loss: 0.03124 - R2: 0.9993 -- iter: 0832/1168\n",
      "Training Step: 261  | total loss: \u001b[1m\u001b[32m0.03124\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 014 | loss: 0.03124 - R2: 0.9993 -- iter: 0896/1168\n",
      "Training Step: 262  | total loss: \u001b[1m\u001b[32m0.03035\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 014 | loss: 0.03035 - R2: 0.9998 -- iter: 0960/1168\n",
      "Training Step: 263  | total loss: \u001b[1m\u001b[32m0.02931\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 014 | loss: 0.02931 - R2: 0.9998 -- iter: 1024/1168\n",
      "Training Step: 264  | total loss: \u001b[1m\u001b[32m0.02907\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 014 | loss: 0.02907 - R2: 0.9997 -- iter: 1088/1168\n",
      "Training Step: 265  | total loss: \u001b[1m\u001b[32m0.02608\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 014 | loss: 0.02608 - R2: 0.9996 -- iter: 1152/1168\n",
      "Training Step: 266  | total loss: \u001b[1m\u001b[32m0.02608\u001b[0m\u001b[0m | time: 1.157s\n",
      "| SGD | epoch: 014 | loss: 0.02608 - R2: 0.9996 | val_loss: 0.01656 - val_acc: 1.0022 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 267  | total loss: \u001b[1m\u001b[32m0.02587\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 015 | loss: 0.02587 - R2: 0.9997 -- iter: 0064/1168\n",
      "Training Step: 268  | total loss: \u001b[1m\u001b[32m0.05881\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 015 | loss: 0.05881 - R2: 0.9988 -- iter: 0128/1168\n",
      "Training Step: 269  | total loss: \u001b[1m\u001b[32m0.05517\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 015 | loss: 0.05517 - R2: 0.9989 -- iter: 0192/1168\n",
      "Training Step: 270  | total loss: \u001b[1m\u001b[32m0.05103\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 015 | loss: 0.05103 - R2: 0.9992 -- iter: 0256/1168\n",
      "Training Step: 271  | total loss: \u001b[1m\u001b[32m0.04733\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 015 | loss: 0.04733 - R2: 0.9997 -- iter: 0320/1168\n",
      "Training Step: 272  | total loss: \u001b[1m\u001b[32m0.04452\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 015 | loss: 0.04452 - R2: 1.0003 -- iter: 0384/1168\n",
      "Training Step: 273  | total loss: \u001b[1m\u001b[32m0.04452\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 015 | loss: 0.04452 - R2: 1.0003 -- iter: 0448/1168\n",
      "Training Step: 274  | total loss: \u001b[1m\u001b[32m0.04020\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 015 | loss: 0.04020 - R2: 0.9998 -- iter: 0512/1168\n",
      "Training Step: 275  | total loss: \u001b[1m\u001b[32m0.03833\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 015 | loss: 0.03833 - R2: 0.9996 -- iter: 0576/1168\n",
      "Training Step: 276  | total loss: \u001b[1m\u001b[32m0.03833\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 015 | loss: 0.03833 - R2: 0.9996 -- iter: 0640/1168\n",
      "Training Step: 277  | total loss: \u001b[1m\u001b[32m0.03619\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 015 | loss: 0.03619 - R2: 0.9999 -- iter: 0704/1168\n",
      "Training Step: 278  | total loss: \u001b[1m\u001b[32m0.03468\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 015 | loss: 0.03468 - R2: 0.9999 -- iter: 0768/1168\n",
      "Training Step: 279  | total loss: \u001b[1m\u001b[32m0.03238\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 015 | loss: 0.03238 - R2: 0.9994 -- iter: 0832/1168\n",
      "Training Step: 280  | total loss: \u001b[1m\u001b[32m0.03165\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 015 | loss: 0.03165 - R2: 0.9996 -- iter: 0896/1168\n",
      "Training Step: 281  | total loss: \u001b[1m\u001b[32m0.03165\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 015 | loss: 0.03165 - R2: 0.9996 -- iter: 0960/1168\n",
      "Training Step: 282  | total loss: \u001b[1m\u001b[32m0.03023\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 015 | loss: 0.03023 - R2: 0.9996 -- iter: 1024/1168\n",
      "Training Step: 283  | total loss: \u001b[1m\u001b[32m0.02806\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 015 | loss: 0.02806 - R2: 0.9999 -- iter: 1088/1168\n",
      "Training Step: 284  | total loss: \u001b[1m\u001b[32m0.02938\u001b[0m\u001b[0m | time: 0.171s\n",
      "| SGD | epoch: 015 | loss: 0.02938 - R2: 1.0001 -- iter: 1152/1168\n",
      "Training Step: 285  | total loss: \u001b[1m\u001b[32m0.02725\u001b[0m\u001b[0m | time: 1.179s\n",
      "| SGD | epoch: 015 | loss: 0.02725 - R2: 0.9999 | val_loss: 0.01705 - val_acc: 1.0019 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 286  | total loss: \u001b[1m\u001b[32m0.02725\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 016 | loss: 0.02725 - R2: 0.9999 -- iter: 0064/1168\n",
      "Training Step: 287  | total loss: \u001b[1m\u001b[32m0.02695\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 016 | loss: 0.02695 - R2: 0.9999 -- iter: 0128/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 288  | total loss: \u001b[1m\u001b[32m0.05178\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 016 | loss: 0.05178 - R2: 1.0011 -- iter: 0192/1168\n",
      "Training Step: 289  | total loss: \u001b[1m\u001b[32m0.05178\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 016 | loss: 0.05178 - R2: 1.0011 -- iter: 0256/1168\n",
      "Training Step: 290  | total loss: \u001b[1m\u001b[32m0.04821\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 016 | loss: 0.04821 - R2: 1.0003 -- iter: 0320/1168\n",
      "Training Step: 291  | total loss: \u001b[1m\u001b[32m0.04708\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 016 | loss: 0.04708 - R2: 1.0002 -- iter: 0384/1168\n",
      "Training Step: 292  | total loss: \u001b[1m\u001b[32m0.04409\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 016 | loss: 0.04409 - R2: 0.9998 -- iter: 0448/1168\n",
      "Training Step: 293  | total loss: \u001b[1m\u001b[32m0.04100\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 016 | loss: 0.04100 - R2: 0.9998 -- iter: 0512/1168\n",
      "Training Step: 294  | total loss: \u001b[1m\u001b[32m0.03865\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 016 | loss: 0.03865 - R2: 0.9996 -- iter: 0576/1168\n",
      "Training Step: 295  | total loss: \u001b[1m\u001b[32m0.03865\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 016 | loss: 0.03865 - R2: 0.9996 -- iter: 0640/1168\n",
      "Training Step: 296  | total loss: \u001b[1m\u001b[32m0.03777\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 016 | loss: 0.03777 - R2: 0.9994 -- iter: 0704/1168\n",
      "Training Step: 297  | total loss: \u001b[1m\u001b[32m0.03498\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 016 | loss: 0.03498 - R2: 0.9998 -- iter: 0768/1168\n",
      "Training Step: 298  | total loss: \u001b[1m\u001b[32m0.03355\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 016 | loss: 0.03355 - R2: 0.9996 -- iter: 0832/1168\n",
      "Training Step: 299  | total loss: \u001b[1m\u001b[32m0.03355\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 016 | loss: 0.03355 - R2: 0.9996 -- iter: 0896/1168\n",
      "Training Step: 300  | total loss: \u001b[1m\u001b[32m0.03118\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 016 | loss: 0.03118 - R2: 0.9993 -- iter: 0960/1168\n",
      "Training Step: 301  | total loss: \u001b[1m\u001b[32m0.02784\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 016 | loss: 0.02784 - R2: 0.9993 -- iter: 1024/1168\n",
      "Training Step: 302  | total loss: \u001b[1m\u001b[32m0.02595\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 016 | loss: 0.02595 - R2: 0.9994 -- iter: 1088/1168\n",
      "Training Step: 303  | total loss: \u001b[1m\u001b[32m0.02595\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 016 | loss: 0.02595 - R2: 0.9994 -- iter: 1152/1168\n",
      "Training Step: 304  | total loss: \u001b[1m\u001b[32m0.02875\u001b[0m\u001b[0m | time: 1.146s\n",
      "| SGD | epoch: 016 | loss: 0.02875 - R2: 0.9993 | val_loss: 0.01772 - val_acc: 1.0024 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 305  | total loss: \u001b[1m\u001b[32m0.02767\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 017 | loss: 0.02767 - R2: 0.9994 -- iter: 0064/1168\n",
      "Training Step: 306  | total loss: \u001b[1m\u001b[32m0.02671\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 017 | loss: 0.02671 - R2: 0.9996 -- iter: 0128/1168\n",
      "Training Step: 307  | total loss: \u001b[1m\u001b[32m0.02551\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 017 | loss: 0.02551 - R2: 0.9993 -- iter: 0192/1168\n",
      "Training Step: 308  | total loss: \u001b[1m\u001b[32m0.02455\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 017 | loss: 0.02455 - R2: 0.9992 -- iter: 0256/1168\n",
      "Training Step: 309  | total loss: \u001b[1m\u001b[32m0.02580\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 017 | loss: 0.02580 - R2: 0.9996 -- iter: 0320/1168\n",
      "Training Step: 310  | total loss: \u001b[1m\u001b[32m0.02580\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 017 | loss: 0.02580 - R2: 0.9995 -- iter: 0384/1168\n",
      "Training Step: 311  | total loss: \u001b[1m\u001b[32m0.02922\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 017 | loss: 0.02922 - R2: 1.0004 -- iter: 0448/1168\n",
      "Training Step: 312  | total loss: \u001b[1m\u001b[32m0.02786\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 017 | loss: 0.02786 - R2: 1.0000 -- iter: 0512/1168\n",
      "Training Step: 313  | total loss: \u001b[1m\u001b[32m0.02786\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 017 | loss: 0.02786 - R2: 1.0003 -- iter: 0576/1168\n",
      "Training Step: 314  | total loss: \u001b[1m\u001b[32m0.02633\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 017 | loss: 0.02633 - R2: 1.0003 -- iter: 0640/1168\n",
      "Training Step: 315  | total loss: \u001b[1m\u001b[32m0.02499\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 017 | loss: 0.02499 - R2: 1.0003 -- iter: 0704/1168\n",
      "Training Step: 316  | total loss: \u001b[1m\u001b[32m0.02658\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 017 | loss: 0.02658 - R2: 1.0003 -- iter: 0768/1168\n",
      "Training Step: 317  | total loss: \u001b[1m\u001b[32m0.02511\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 017 | loss: 0.02511 - R2: 1.0003 -- iter: 0832/1168\n",
      "Training Step: 318  | total loss: \u001b[1m\u001b[32m0.02512\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 017 | loss: 0.02512 - R2: 1.0005 -- iter: 0896/1168\n",
      "Training Step: 319  | total loss: \u001b[1m\u001b[32m0.02391\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 017 | loss: 0.02391 - R2: 1.0005 -- iter: 0960/1168\n",
      "Training Step: 320  | total loss: \u001b[1m\u001b[32m0.02261\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 017 | loss: 0.02261 - R2: 1.0006 -- iter: 1024/1168\n",
      "Training Step: 321  | total loss: \u001b[1m\u001b[32m0.02261\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 017 | loss: 0.02261 - R2: 1.0006 -- iter: 1088/1168\n",
      "Training Step: 322  | total loss: \u001b[1m\u001b[32m0.02145\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 017 | loss: 0.02145 - R2: 0.9998 -- iter: 1152/1168\n",
      "Training Step: 323  | total loss: \u001b[1m\u001b[32m0.02136\u001b[0m\u001b[0m | time: 1.151s\n",
      "| SGD | epoch: 017 | loss: 0.02136 - R2: 0.9995 | val_loss: 0.01662 - val_acc: 1.0021 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 324  | total loss: \u001b[1m\u001b[32m0.02136\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 018 | loss: 0.02136 - R2: 0.9995 -- iter: 0064/1168\n",
      "Training Step: 325  | total loss: \u001b[1m\u001b[32m0.02049\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 018 | loss: 0.02049 - R2: 0.9993 -- iter: 0128/1168\n",
      "Training Step: 326  | total loss: \u001b[1m\u001b[32m0.02056\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 018 | loss: 0.02056 - R2: 0.9995 -- iter: 0192/1168\n",
      "Training Step: 327  | total loss: \u001b[1m\u001b[32m0.04103\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 018 | loss: 0.04103 - R2: 0.9998 -- iter: 0256/1168\n",
      "Training Step: 328  | total loss: \u001b[1m\u001b[32m0.04001\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 018 | loss: 0.04001 - R2: 1.0006 -- iter: 0320/1168\n",
      "Training Step: 329  | total loss: \u001b[1m\u001b[32m0.04001\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 018 | loss: 0.04001 - R2: 1.0005 -- iter: 0384/1168\n",
      "Training Step: 330  | total loss: \u001b[1m\u001b[32m0.03599\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 018 | loss: 0.03599 - R2: 1.0000 -- iter: 0448/1168\n",
      "Training Step: 331  | total loss: \u001b[1m\u001b[32m0.03518\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 018 | loss: 0.03518 - R2: 0.9996 -- iter: 0512/1168\n",
      "Training Step: 332  | total loss: \u001b[1m\u001b[32m0.03518\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 018 | loss: 0.03518 - R2: 0.9996 -- iter: 0576/1168\n",
      "Training Step: 333  | total loss: \u001b[1m\u001b[32m0.03291\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 018 | loss: 0.03291 - R2: 0.9996 -- iter: 0640/1168\n",
      "Training Step: 334  | total loss: \u001b[1m\u001b[32m0.03175\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 018 | loss: 0.03175 - R2: 0.9995 -- iter: 0704/1168\n",
      "Training Step: 335  | total loss: \u001b[1m\u001b[32m0.03014\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 018 | loss: 0.03014 - R2: 0.9996 -- iter: 0768/1168\n",
      "Training Step: 336  | total loss: \u001b[1m\u001b[32m0.03014\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 018 | loss: 0.03014 - R2: 0.9996 -- iter: 0832/1168\n",
      "Training Step: 337  | total loss: \u001b[1m\u001b[32m0.02807\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 018 | loss: 0.02807 - R2: 0.9990 -- iter: 0896/1168\n",
      "Training Step: 338  | total loss: \u001b[1m\u001b[32m0.02768\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 018 | loss: 0.02768 - R2: 0.9994 -- iter: 0960/1168\n",
      "Training Step: 339  | total loss: \u001b[1m\u001b[32m0.02936\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 018 | loss: 0.02936 - R2: 0.9998 -- iter: 1024/1168\n",
      "Training Step: 340  | total loss: \u001b[1m\u001b[32m0.02936\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 018 | loss: 0.02936 - R2: 0.9998 -- iter: 1088/1168\n",
      "Training Step: 341  | total loss: \u001b[1m\u001b[32m0.02596\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 018 | loss: 0.02596 - R2: 0.9999 -- iter: 1152/1168\n",
      "Training Step: 342  | total loss: \u001b[1m\u001b[32m0.02600\u001b[0m\u001b[0m | time: 1.166s\n",
      "| SGD | epoch: 018 | loss: 0.02600 - R2: 1.0000 | val_loss: 0.01723 - val_acc: 1.0001 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 343  | total loss: \u001b[1m\u001b[32m0.02600\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 019 | loss: 0.02600 - R2: 1.0000 -- iter: 0064/1168\n",
      "Training Step: 344  | total loss: \u001b[1m\u001b[32m0.02518\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 019 | loss: 0.02518 - R2: 1.0002 -- iter: 0128/1168\n",
      "Training Step: 345  | total loss: \u001b[1m\u001b[32m0.02566\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 019 | loss: 0.02566 - R2: 0.9999 -- iter: 0192/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 346  | total loss: \u001b[1m\u001b[32m0.02480\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 019 | loss: 0.02480 - R2: 0.9991 -- iter: 0256/1168\n",
      "Training Step: 347  | total loss: \u001b[1m\u001b[32m0.02424\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 019 | loss: 0.02424 - R2: 0.9991 -- iter: 0320/1168\n",
      "Training Step: 348  | total loss: \u001b[1m\u001b[32m0.02365\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 019 | loss: 0.02365 - R2: 0.9995 -- iter: 0384/1168\n",
      "Training Step: 349  | total loss: \u001b[1m\u001b[32m0.02380\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 019 | loss: 0.02380 - R2: 0.9995 -- iter: 0448/1168\n",
      "Training Step: 350  | total loss: \u001b[1m\u001b[32m0.02503\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 019 | loss: 0.02503 - R2: 1.0002 -- iter: 0512/1168\n",
      "Training Step: 351  | total loss: \u001b[1m\u001b[32m0.02395\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 019 | loss: 0.02395 - R2: 1.0001 -- iter: 0576/1168\n",
      "Training Step: 352  | total loss: \u001b[1m\u001b[32m0.02317\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 019 | loss: 0.02317 - R2: 0.9992 -- iter: 0640/1168\n",
      "Training Step: 353  | total loss: \u001b[1m\u001b[32m0.02317\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 019 | loss: 0.02317 - R2: 0.9989 -- iter: 0704/1168\n",
      "Training Step: 354  | total loss: \u001b[1m\u001b[32m0.02163\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 019 | loss: 0.02163 - R2: 0.9997 -- iter: 0768/1168\n",
      "Training Step: 355  | total loss: \u001b[1m\u001b[32m0.02108\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 019 | loss: 0.02108 - R2: 0.9995 -- iter: 0832/1168\n",
      "Training Step: 356  | total loss: \u001b[1m\u001b[32m0.02108\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 019 | loss: 0.02108 - R2: 0.9997 -- iter: 0896/1168\n",
      "Training Step: 357  | total loss: \u001b[1m\u001b[32m0.02019\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 019 | loss: 0.02019 - R2: 0.9997 -- iter: 0960/1168\n",
      "Training Step: 358  | total loss: \u001b[1m\u001b[32m0.02056\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 019 | loss: 0.02056 - R2: 1.0000 -- iter: 1024/1168\n",
      "Training Step: 359  | total loss: \u001b[1m\u001b[32m0.02056\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 019 | loss: 0.02056 - R2: 1.0000 -- iter: 1088/1168\n",
      "Training Step: 360  | total loss: \u001b[1m\u001b[32m0.01828\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 019 | loss: 0.01828 - R2: 1.0006 -- iter: 1152/1168\n",
      "Training Step: 361  | total loss: \u001b[1m\u001b[32m0.01810\u001b[0m\u001b[0m | time: 1.107s\n",
      "| SGD | epoch: 019 | loss: 0.01810 - R2: 1.0004 | val_loss: 0.01559 - val_acc: 0.9998 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 362  | total loss: \u001b[1m\u001b[32m0.01812\u001b[0m\u001b[0m | time: 0.026s\n",
      "| SGD | epoch: 020 | loss: 0.01812 - R2: 1.0003 -- iter: 0064/1168\n",
      "Training Step: 363  | total loss: \u001b[1m\u001b[32m0.01822\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 020 | loss: 0.01822 - R2: 1.0000 -- iter: 0128/1168\n",
      "Training Step: 364  | total loss: \u001b[1m\u001b[32m0.01822\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 020 | loss: 0.01822 - R2: 1.0002 -- iter: 0192/1168\n",
      "Training Step: 365  | total loss: \u001b[1m\u001b[32m0.01988\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 020 | loss: 0.01988 - R2: 1.0003 -- iter: 0256/1168\n",
      "Training Step: 366  | total loss: \u001b[1m\u001b[32m0.02383\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 020 | loss: 0.02383 - R2: 1.0003 -- iter: 0320/1168\n",
      "Training Step: 367  | total loss: \u001b[1m\u001b[32m0.02284\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 020 | loss: 0.02284 - R2: 1.0001 -- iter: 0384/1168\n",
      "Training Step: 368  | total loss: \u001b[1m\u001b[32m0.02284\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 020 | loss: 0.02284 - R2: 1.0001 -- iter: 0448/1168\n",
      "Training Step: 369  | total loss: \u001b[1m\u001b[32m0.02560\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 020 | loss: 0.02560 - R2: 1.0002 -- iter: 0512/1168\n",
      "Training Step: 370  | total loss: \u001b[1m\u001b[32m0.02536\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 020 | loss: 0.02536 - R2: 1.0001 -- iter: 0576/1168\n",
      "Training Step: 371  | total loss: \u001b[1m\u001b[32m0.02471\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 020 | loss: 0.02471 - R2: 0.9996 -- iter: 0640/1168\n",
      "Training Step: 372  | total loss: \u001b[1m\u001b[32m0.02471\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 020 | loss: 0.02471 - R2: 0.9996 -- iter: 0704/1168\n",
      "Training Step: 373  | total loss: \u001b[1m\u001b[32m0.02335\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 020 | loss: 0.02335 - R2: 1.0000 -- iter: 0768/1168\n",
      "Training Step: 374  | total loss: \u001b[1m\u001b[32m0.02315\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 020 | loss: 0.02315 - R2: 0.9996 -- iter: 0832/1168\n",
      "Training Step: 375  | total loss: \u001b[1m\u001b[32m0.02296\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 020 | loss: 0.02296 - R2: 0.9997 -- iter: 0896/1168\n",
      "Training Step: 376  | total loss: \u001b[1m\u001b[32m0.02321\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 020 | loss: 0.02321 - R2: 0.9994 -- iter: 0960/1168\n",
      "Training Step: 377  | total loss: \u001b[1m\u001b[32m0.02228\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 020 | loss: 0.02228 - R2: 0.9996 -- iter: 1024/1168\n",
      "Training Step: 378  | total loss: \u001b[1m\u001b[32m0.02188\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 020 | loss: 0.02188 - R2: 0.9997 -- iter: 1088/1168\n",
      "Training Step: 379  | total loss: \u001b[1m\u001b[32m0.02188\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 020 | loss: 0.02188 - R2: 0.9993 -- iter: 1152/1168\n",
      "Training Step: 380  | total loss: \u001b[1m\u001b[32m0.02063\u001b[0m\u001b[0m | time: 1.081s\n",
      "| SGD | epoch: 020 | loss: 0.02063 - R2: 0.9993 | val_loss: 0.01663 - val_acc: 1.0031 -- iter: 1168/1168\n",
      "--\n",
      "---------------------------------\n",
      "Run id: QNGFPL\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "INFO:tensorflow:Summary name StandardError/ (raw) is illegal; using StandardError/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 1168\n",
      "Validation samples: 292\n",
      "--\n",
      "Training Step: 1  | total loss: \u001b[1m\u001b[32m130.37653\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 001 | loss: 130.37653 - R2: 0.0000 -- iter: 0064/1168\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m130.37653\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 001 | loss: 130.37653 - R2: 0.0000 -- iter: 0128/1168\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m142.88940\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 001 | loss: 142.88940 - R2: 0.0001 -- iter: 0192/1168\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m142.88940\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 001 | loss: 142.88940 - R2: 0.0001 -- iter: 0256/1168\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m140.49599\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 001 | loss: 140.49599 - R2: 0.0002 -- iter: 0320/1168\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m140.49599\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 001 | loss: 140.49599 - R2: 0.0002 -- iter: 0384/1168\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m138.48912\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 001 | loss: 138.48912 - R2: 0.0005 -- iter: 0448/1168\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m138.48912\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 001 | loss: 138.48912 - R2: 0.0005 -- iter: 0512/1168\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m136.09097\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 001 | loss: 136.09097 - R2: 0.0009 -- iter: 0576/1168\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m136.09097\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 001 | loss: 136.09097 - R2: 0.0011 -- iter: 0640/1168\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m134.08035\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 001 | loss: 134.08035 - R2: 0.0014 -- iter: 0704/1168\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m132.71265\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 001 | loss: 132.71265 - R2: 0.0017 -- iter: 0768/1168\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m131.02423\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 001 | loss: 131.02423 - R2: 0.0020 -- iter: 0832/1168\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m131.02423\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 001 | loss: 131.02423 - R2: 0.0020 -- iter: 0896/1168\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m130.47307\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 001 | loss: 130.47307 - R2: 0.0024 -- iter: 0960/1168\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m128.75034\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 001 | loss: 128.75034 - R2: 0.0031 -- iter: 1024/1168\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m128.75034\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 001 | loss: 128.75034 - R2: 0.0031 -- iter: 1088/1168\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m128.37996\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 001 | loss: 128.37996 - R2: 0.0035 -- iter: 1152/1168\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m126.92690\u001b[0m\u001b[0m | time: 1.114s\n",
      "| SGD | epoch: 001 | loss: 126.92690 - R2: 0.0045 | val_loss: 123.64600 - val_acc: 0.0061 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m126.92690\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 002 | loss: 126.92690 - R2: 0.0045 -- iter: 0064/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 21  | total loss: \u001b[1m\u001b[32m125.28558\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 002 | loss: 125.28558 - R2: 0.0055 -- iter: 0128/1168\n",
      "Training Step: 22  | total loss: \u001b[1m\u001b[32m123.73078\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 002 | loss: 123.73078 - R2: 0.0061 -- iter: 0192/1168\n",
      "Training Step: 23  | total loss: \u001b[1m\u001b[32m123.73078\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 002 | loss: 123.73078 - R2: 0.0061 -- iter: 0256/1168\n",
      "Training Step: 24  | total loss: \u001b[1m\u001b[32m122.69962\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 002 | loss: 122.69962 - R2: 0.0067 -- iter: 0320/1168\n",
      "Training Step: 25  | total loss: \u001b[1m\u001b[32m121.28933\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 002 | loss: 121.28933 - R2: 0.0073 -- iter: 0384/1168\n",
      "Training Step: 26  | total loss: \u001b[1m\u001b[32m120.18050\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 002 | loss: 120.18050 - R2: 0.0080 -- iter: 0448/1168\n",
      "Training Step: 27  | total loss: \u001b[1m\u001b[32m119.08270\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 002 | loss: 119.08270 - R2: 0.0087 -- iter: 0512/1168\n",
      "Training Step: 28  | total loss: \u001b[1m\u001b[32m118.16174\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 002 | loss: 118.16174 - R2: 0.0094 -- iter: 0576/1168\n",
      "Training Step: 29  | total loss: \u001b[1m\u001b[32m116.09399\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 002 | loss: 116.09399 - R2: 0.0109 -- iter: 0640/1168\n",
      "Training Step: 30  | total loss: \u001b[1m\u001b[32m115.54945\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 002 | loss: 115.54945 - R2: 0.0116 -- iter: 0704/1168\n",
      "Training Step: 31  | total loss: \u001b[1m\u001b[32m115.54945\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 002 | loss: 115.54945 - R2: 0.0116 -- iter: 0768/1168\n",
      "Training Step: 32  | total loss: \u001b[1m\u001b[32m113.37889\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 002 | loss: 113.37889 - R2: 0.0133 -- iter: 0832/1168\n",
      "Training Step: 33  | total loss: \u001b[1m\u001b[32m113.37889\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 002 | loss: 113.37889 - R2: 0.0133 -- iter: 0896/1168\n",
      "Training Step: 34  | total loss: \u001b[1m\u001b[32m111.25259\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 002 | loss: 111.25259 - R2: 0.0151 -- iter: 0960/1168\n",
      "Training Step: 35  | total loss: \u001b[1m\u001b[32m111.25259\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 002 | loss: 111.25259 - R2: 0.0151 -- iter: 1024/1168\n",
      "Training Step: 36  | total loss: \u001b[1m\u001b[32m110.50350\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 002 | loss: 110.50350 - R2: 0.0160 -- iter: 1088/1168\n",
      "Training Step: 37  | total loss: \u001b[1m\u001b[32m109.44611\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 002 | loss: 109.44611 - R2: 0.0169 -- iter: 1152/1168\n",
      "Training Step: 38  | total loss: \u001b[1m\u001b[32m108.53302\u001b[0m\u001b[0m | time: 1.114s\n",
      "| SGD | epoch: 002 | loss: 108.53302 - R2: 0.0179 | val_loss: 103.76269 - val_acc: 0.0242 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 39  | total loss: \u001b[1m\u001b[32m106.71072\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 003 | loss: 106.71072 - R2: 0.0199 -- iter: 0064/1168\n",
      "Training Step: 40  | total loss: \u001b[1m\u001b[32m106.71072\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 003 | loss: 106.71072 - R2: 0.0199 -- iter: 0128/1168\n",
      "Training Step: 41  | total loss: \u001b[1m\u001b[32m106.06070\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 003 | loss: 106.06070 - R2: 0.0210 -- iter: 0192/1168\n",
      "Training Step: 42  | total loss: \u001b[1m\u001b[32m105.20065\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 003 | loss: 105.20065 - R2: 0.0220 -- iter: 0256/1168\n",
      "Training Step: 43  | total loss: \u001b[1m\u001b[32m104.58759\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 003 | loss: 104.58759 - R2: 0.0242 -- iter: 0320/1168\n",
      "Training Step: 44  | total loss: \u001b[1m\u001b[32m102.56092\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 003 | loss: 102.56092 - R2: 0.0255 -- iter: 0384/1168\n",
      "Training Step: 45  | total loss: \u001b[1m\u001b[32m102.56092\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 003 | loss: 102.56092 - R2: 0.0266 -- iter: 0448/1168\n",
      "Training Step: 46  | total loss: \u001b[1m\u001b[32m101.72143\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 003 | loss: 101.72143 - R2: 0.0266 -- iter: 0512/1168\n",
      "Training Step: 47  | total loss: \u001b[1m\u001b[32m100.77990\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 003 | loss: 100.77990 - R2: 0.0279 -- iter: 0576/1168\n",
      "Training Step: 48  | total loss: \u001b[1m\u001b[32m99.74483\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 003 | loss: 99.74483 - R2: 0.0292 -- iter: 0640/1168\n",
      "Training Step: 49  | total loss: \u001b[1m\u001b[32m98.97669\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 003 | loss: 98.97669 - R2: 0.0304 -- iter: 0704/1168\n",
      "Training Step: 50  | total loss: \u001b[1m\u001b[32m97.85587\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 003 | loss: 97.85587 - R2: 0.0319 -- iter: 0768/1168\n",
      "Training Step: 51  | total loss: \u001b[1m\u001b[32m97.14999\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 003 | loss: 97.14999 - R2: 0.0331 -- iter: 0832/1168\n",
      "Training Step: 52  | total loss: \u001b[1m\u001b[32m96.32325\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 003 | loss: 96.32325 - R2: 0.0345 -- iter: 0896/1168\n",
      "Training Step: 53  | total loss: \u001b[1m\u001b[32m95.54945\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 003 | loss: 95.54945 - R2: 0.0359 -- iter: 0960/1168\n",
      "Training Step: 54  | total loss: \u001b[1m\u001b[32m94.60556\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 003 | loss: 94.60556 - R2: 0.0374 -- iter: 1024/1168\n",
      "Training Step: 55  | total loss: \u001b[1m\u001b[32m93.59285\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 003 | loss: 93.59285 - R2: 0.0389 -- iter: 1088/1168\n",
      "Training Step: 56  | total loss: \u001b[1m\u001b[32m92.52382\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 003 | loss: 92.52382 - R2: 0.0405 -- iter: 1152/1168\n",
      "Training Step: 57  | total loss: \u001b[1m\u001b[32m91.71462\u001b[0m\u001b[0m | time: 1.140s\n",
      "| SGD | epoch: 003 | loss: 91.71462 - R2: 0.0436 | val_loss: 85.05387 - val_acc: 0.0555 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 58  | total loss: \u001b[1m\u001b[32m90.89370\u001b[0m\u001b[0m | time: 0.008s\n",
      "| SGD | epoch: 004 | loss: 90.89370 - R2: 0.0436 -- iter: 0064/1168\n",
      "Training Step: 59  | total loss: \u001b[1m\u001b[32m90.06413\u001b[0m\u001b[0m | time: 0.016s\n",
      "| SGD | epoch: 004 | loss: 90.06413 - R2: 0.0453 -- iter: 0128/1168\n",
      "Training Step: 60  | total loss: \u001b[1m\u001b[32m89.04351\u001b[0m\u001b[0m | time: 0.019s\n",
      "| SGD | epoch: 004 | loss: 89.04351 - R2: 0.0488 -- iter: 0192/1168\n",
      "Training Step: 61  | total loss: \u001b[1m\u001b[32m88.04285\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 004 | loss: 88.04285 - R2: 0.0488 -- iter: 0256/1168\n",
      "Training Step: 62  | total loss: \u001b[1m\u001b[32m87.31999\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 004 | loss: 87.31999 - R2: 0.0505 -- iter: 0320/1168\n",
      "Training Step: 63  | total loss: \u001b[1m\u001b[32m85.41071\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 004 | loss: 85.41071 - R2: 0.0542 -- iter: 0384/1168\n",
      "Training Step: 64  | total loss: \u001b[1m\u001b[32m85.41071\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 004 | loss: 85.41071 - R2: 0.0542 -- iter: 0448/1168\n",
      "Training Step: 65  | total loss: \u001b[1m\u001b[32m84.60440\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 004 | loss: 84.60440 - R2: 0.0561 -- iter: 0512/1168\n",
      "Training Step: 66  | total loss: \u001b[1m\u001b[32m83.81982\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 004 | loss: 83.81982 - R2: 0.0580 -- iter: 0576/1168\n",
      "Training Step: 67  | total loss: \u001b[1m\u001b[32m82.82005\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 004 | loss: 82.82005 - R2: 0.0601 -- iter: 0640/1168\n",
      "Training Step: 68  | total loss: \u001b[1m\u001b[32m82.02060\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 004 | loss: 82.02060 - R2: 0.0620 -- iter: 0704/1168\n",
      "Training Step: 69  | total loss: \u001b[1m\u001b[32m81.10342\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 004 | loss: 81.10342 - R2: 0.0642 -- iter: 0768/1168\n",
      "Training Step: 70  | total loss: \u001b[1m\u001b[32m80.01607\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 004 | loss: 80.01607 - R2: 0.0665 -- iter: 0832/1168\n",
      "Training Step: 71  | total loss: \u001b[1m\u001b[32m79.14036\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 004 | loss: 79.14036 - R2: 0.0687 -- iter: 0896/1168\n",
      "Training Step: 72  | total loss: \u001b[1m\u001b[32m78.25093\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 004 | loss: 78.25093 - R2: 0.0710 -- iter: 0960/1168\n",
      "Training Step: 73  | total loss: \u001b[1m\u001b[32m77.40421\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 004 | loss: 77.40421 - R2: 0.0733 -- iter: 1024/1168\n",
      "Training Step: 74  | total loss: \u001b[1m\u001b[32m76.49451\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 004 | loss: 76.49451 - R2: 0.0757 -- iter: 1088/1168\n",
      "Training Step: 75  | total loss: \u001b[1m\u001b[32m75.56801\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 004 | loss: 75.56801 - R2: 0.0782 -- iter: 1152/1168\n",
      "Training Step: 76  | total loss: \u001b[1m\u001b[32m74.65848\u001b[0m\u001b[0m | time: 1.068s\n",
      "| SGD | epoch: 004 | loss: 74.65848 - R2: 0.0807 | val_loss: 65.53282 - val_acc: 0.1083 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 77  | total loss: \u001b[1m\u001b[32m72.82771\u001b[0m\u001b[0m | time: 0.006s\n",
      "| SGD | epoch: 005 | loss: 72.82771 - R2: 0.0860 -- iter: 0064/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 78  | total loss: \u001b[1m\u001b[32m72.82771\u001b[0m\u001b[0m | time: 0.012s\n",
      "| SGD | epoch: 005 | loss: 72.82771 - R2: 0.0860 -- iter: 0128/1168\n",
      "Training Step: 79  | total loss: \u001b[1m\u001b[32m71.97852\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 005 | loss: 71.97852 - R2: 0.0887 -- iter: 0192/1168\n",
      "Training Step: 80  | total loss: \u001b[1m\u001b[32m70.23663\u001b[0m\u001b[0m | time: 0.020s\n",
      "| SGD | epoch: 005 | loss: 70.23663 - R2: 0.0942 -- iter: 0256/1168\n",
      "Training Step: 81  | total loss: \u001b[1m\u001b[32m69.35363\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 005 | loss: 69.35363 - R2: 0.0972 -- iter: 0320/1168\n",
      "Training Step: 82  | total loss: \u001b[1m\u001b[32m68.32396\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 005 | loss: 68.32396 - R2: 0.1004 -- iter: 0384/1168\n",
      "Training Step: 83  | total loss: \u001b[1m\u001b[32m68.32396\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 005 | loss: 68.32396 - R2: 0.1004 -- iter: 0448/1168\n",
      "Training Step: 84  | total loss: \u001b[1m\u001b[32m66.25510\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 005 | loss: 66.25510 - R2: 0.1072 -- iter: 0512/1168\n",
      "Training Step: 85  | total loss: \u001b[1m\u001b[32m66.25510\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 005 | loss: 66.25510 - R2: 0.1108 -- iter: 0576/1168\n",
      "Training Step: 86  | total loss: \u001b[1m\u001b[32m65.23466\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 005 | loss: 65.23466 - R2: 0.1108 -- iter: 0640/1168\n",
      "Training Step: 87  | total loss: \u001b[1m\u001b[32m64.15086\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 005 | loss: 64.15086 - R2: 0.1146 -- iter: 0704/1168\n",
      "Training Step: 88  | total loss: \u001b[1m\u001b[32m62.11739\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 005 | loss: 62.11739 - R2: 0.1223 -- iter: 0768/1168\n",
      "Training Step: 89  | total loss: \u001b[1m\u001b[32m61.04818\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 005 | loss: 61.04818 - R2: 0.1264 -- iter: 0832/1168\n",
      "Training Step: 90  | total loss: \u001b[1m\u001b[32m61.04818\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 005 | loss: 61.04818 - R2: 0.1264 -- iter: 0896/1168\n",
      "Training Step: 91  | total loss: \u001b[1m\u001b[32m59.85808\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 005 | loss: 59.85808 - R2: 0.1353 -- iter: 0960/1168\n",
      "Training Step: 92  | total loss: \u001b[1m\u001b[32m58.77755\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 005 | loss: 58.77755 - R2: 0.1353 -- iter: 1024/1168\n",
      "Training Step: 93  | total loss: \u001b[1m\u001b[32m57.58361\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 005 | loss: 57.58361 - R2: 0.1401 -- iter: 1088/1168\n",
      "Training Step: 94  | total loss: \u001b[1m\u001b[32m55.52436\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 005 | loss: 55.52436 - R2: 0.1494 -- iter: 1152/1168\n",
      "Training Step: 95  | total loss: \u001b[1m\u001b[32m54.27832\u001b[0m\u001b[0m | time: 1.073s\n",
      "| SGD | epoch: 005 | loss: 54.27832 - R2: 0.1548 | val_loss: 43.47075 - val_acc: 0.2060 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 96  | total loss: \u001b[1m\u001b[32m53.12410\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 006 | loss: 53.12410 - R2: 0.1601 -- iter: 0064/1168\n",
      "Training Step: 97  | total loss: \u001b[1m\u001b[32m53.12410\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 006 | loss: 53.12410 - R2: 0.1601 -- iter: 0128/1168\n",
      "Training Step: 98  | total loss: \u001b[1m\u001b[32m52.05017\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 006 | loss: 52.05017 - R2: 0.1710 -- iter: 0192/1168\n",
      "Training Step: 99  | total loss: \u001b[1m\u001b[32m49.53729\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 006 | loss: 49.53729 - R2: 0.1775 -- iter: 0256/1168\n",
      "Training Step: 100  | total loss: \u001b[1m\u001b[32m48.19176\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 006 | loss: 48.19176 - R2: 0.1842 -- iter: 0320/1168\n",
      "Training Step: 101  | total loss: \u001b[1m\u001b[32m46.99091\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 006 | loss: 46.99091 - R2: 0.1905 -- iter: 0384/1168\n",
      "Training Step: 102  | total loss: \u001b[1m\u001b[32m45.85704\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 006 | loss: 45.85704 - R2: 0.1968 -- iter: 0448/1168\n",
      "Training Step: 103  | total loss: \u001b[1m\u001b[32m44.71629\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 006 | loss: 44.71629 - R2: 0.2034 -- iter: 0512/1168\n",
      "Training Step: 104  | total loss: \u001b[1m\u001b[32m43.64949\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 006 | loss: 43.64949 - R2: 0.2099 -- iter: 0576/1168\n",
      "Training Step: 105  | total loss: \u001b[1m\u001b[32m43.64949\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 006 | loss: 43.64949 - R2: 0.2099 -- iter: 0640/1168\n",
      "Training Step: 106  | total loss: \u001b[1m\u001b[32m42.44901\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 006 | loss: 42.44901 - R2: 0.2171 -- iter: 0704/1168\n",
      "Training Step: 107  | total loss: \u001b[1m\u001b[32m41.36225\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 006 | loss: 41.36225 - R2: 0.2241 -- iter: 0768/1168\n",
      "Training Step: 108  | total loss: \u001b[1m\u001b[32m39.03263\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 006 | loss: 39.03263 - R2: 0.2395 -- iter: 0832/1168\n",
      "Training Step: 109  | total loss: \u001b[1m\u001b[32m37.83247\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 006 | loss: 37.83247 - R2: 0.2477 -- iter: 0896/1168\n",
      "Training Step: 110  | total loss: \u001b[1m\u001b[32m36.58537\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 006 | loss: 36.58537 - R2: 0.2564 -- iter: 0960/1168\n",
      "Training Step: 111  | total loss: \u001b[1m\u001b[32m36.58537\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 006 | loss: 36.58537 - R2: 0.2564 -- iter: 1024/1168\n",
      "Training Step: 112  | total loss: \u001b[1m\u001b[32m34.30980\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 006 | loss: 34.30980 - R2: 0.2737 -- iter: 1088/1168\n",
      "Training Step: 113  | total loss: \u001b[1m\u001b[32m34.30980\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 006 | loss: 34.30980 - R2: 0.2737 -- iter: 1152/1168\n",
      "Training Step: 114  | total loss: \u001b[1m\u001b[32m33.19423\u001b[0m\u001b[0m | time: 1.125s\n",
      "| SGD | epoch: 006 | loss: 33.19423 - R2: 0.2826 | val_loss: 20.79522 - val_acc: 0.3880 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 115  | total loss: \u001b[1m\u001b[32m30.90305\u001b[0m\u001b[0m | time: 0.019s\n",
      "| SGD | epoch: 007 | loss: 30.90305 - R2: 0.3018 -- iter: 0064/1168\n",
      "Training Step: 116  | total loss: \u001b[1m\u001b[32m30.90305\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 007 | loss: 30.90305 - R2: 0.3018 -- iter: 0128/1168\n",
      "Training Step: 117  | total loss: \u001b[1m\u001b[32m29.70983\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 007 | loss: 29.70983 - R2: 0.3123 -- iter: 0192/1168\n",
      "Training Step: 118  | total loss: \u001b[1m\u001b[32m27.45762\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 007 | loss: 27.45762 - R2: 0.3334 -- iter: 0256/1168\n",
      "Training Step: 119  | total loss: \u001b[1m\u001b[32m26.28191\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 007 | loss: 26.28191 - R2: 0.3450 -- iter: 0320/1168\n",
      "Training Step: 120  | total loss: \u001b[1m\u001b[32m25.11479\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 007 | loss: 25.11479 - R2: 0.3571 -- iter: 0384/1168\n",
      "Training Step: 121  | total loss: \u001b[1m\u001b[32m25.11479\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 007 | loss: 25.11479 - R2: 0.3571 -- iter: 0448/1168\n",
      "Training Step: 122  | total loss: \u001b[1m\u001b[32m22.93541\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 007 | loss: 22.93541 - R2: 0.3808 -- iter: 0512/1168\n",
      "Training Step: 123  | total loss: \u001b[1m\u001b[32m21.86657\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 007 | loss: 21.86657 - R2: 0.3932 -- iter: 0576/1168\n",
      "Training Step: 124  | total loss: \u001b[1m\u001b[32m20.81446\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 007 | loss: 20.81446 - R2: 0.4059 -- iter: 0640/1168\n",
      "Training Step: 125  | total loss: \u001b[1m\u001b[32m20.81446\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 007 | loss: 20.81446 - R2: 0.4059 -- iter: 0704/1168\n",
      "Training Step: 126  | total loss: \u001b[1m\u001b[32m19.75224\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 007 | loss: 19.75224 - R2: 0.4195 -- iter: 0768/1168\n",
      "Training Step: 127  | total loss: \u001b[1m\u001b[32m18.69260\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 007 | loss: 18.69260 - R2: 0.4337 -- iter: 0832/1168\n",
      "Training Step: 128  | total loss: \u001b[1m\u001b[32m17.73191\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 007 | loss: 17.73191 - R2: 0.4469 -- iter: 0896/1168\n",
      "Training Step: 129  | total loss: \u001b[1m\u001b[32m15.78458\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 007 | loss: 15.78458 - R2: 0.4762 -- iter: 0960/1168\n",
      "Training Step: 130  | total loss: \u001b[1m\u001b[32m14.79025\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 007 | loss: 14.79025 - R2: 0.4926 -- iter: 1024/1168\n",
      "Training Step: 131  | total loss: \u001b[1m\u001b[32m13.87542\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 007 | loss: 13.87542 - R2: 0.5082 -- iter: 1088/1168\n",
      "Training Step: 132  | total loss: \u001b[1m\u001b[32m12.95448\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 007 | loss: 12.95448 - R2: 0.5252 -- iter: 1152/1168\n",
      "Training Step: 133  | total loss: \u001b[1m\u001b[32m12.95448\u001b[0m\u001b[0m | time: 1.094s\n",
      "| SGD | epoch: 007 | loss: 12.95448 - R2: 0.5252 | val_loss: 3.68765 - val_acc: 0.7120 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 134  | total loss: \u001b[1m\u001b[32m11.19360\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 008 | loss: 11.19360 - R2: 0.5605 -- iter: 0064/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 135  | total loss: \u001b[1m\u001b[32m11.19360\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 008 | loss: 11.19360 - R2: 0.5605 -- iter: 0128/1168\n",
      "Training Step: 136  | total loss: \u001b[1m\u001b[32m9.58802\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 008 | loss: 9.58802 - R2: 0.5971 -- iter: 0192/1168\n",
      "Training Step: 137  | total loss: \u001b[1m\u001b[32m9.58802\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 008 | loss: 9.58802 - R2: 0.5971 -- iter: 0256/1168\n",
      "Training Step: 138  | total loss: \u001b[1m\u001b[32m8.07572\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 008 | loss: 8.07572 - R2: 0.6369 -- iter: 0320/1168\n",
      "Training Step: 139  | total loss: \u001b[1m\u001b[32m7.39605\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 008 | loss: 7.39605 - R2: 0.6561 -- iter: 0384/1168\n",
      "Training Step: 140  | total loss: \u001b[1m\u001b[32m6.74696\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 008 | loss: 6.74696 - R2: 0.6761 -- iter: 0448/1168\n",
      "Training Step: 141  | total loss: \u001b[1m\u001b[32m6.74696\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 008 | loss: 6.74696 - R2: 0.6761 -- iter: 0512/1168\n",
      "Training Step: 142  | total loss: \u001b[1m\u001b[32m5.55438\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 008 | loss: 5.55438 - R2: 0.7202 -- iter: 0576/1168\n",
      "Training Step: 143  | total loss: \u001b[1m\u001b[32m5.03145\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 008 | loss: 5.03145 - R2: 0.7422 -- iter: 0640/1168\n",
      "Training Step: 144  | total loss: \u001b[1m\u001b[32m4.54806\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 008 | loss: 4.54806 - R2: 0.7641 -- iter: 0704/1168\n",
      "Training Step: 145  | total loss: \u001b[1m\u001b[32m4.54806\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 008 | loss: 4.54806 - R2: 0.7641 -- iter: 0768/1168\n",
      "Training Step: 146  | total loss: \u001b[1m\u001b[32m3.71409\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 008 | loss: 3.71409 - R2: 0.8042 -- iter: 0832/1168\n",
      "Training Step: 147  | total loss: \u001b[1m\u001b[32m3.71409\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 008 | loss: 3.71409 - R2: 0.8042 -- iter: 0896/1168\n",
      "Training Step: 148  | total loss: \u001b[1m\u001b[32m3.36352\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 008 | loss: 3.36352 - R2: 0.8235 -- iter: 0960/1168\n",
      "Training Step: 149  | total loss: \u001b[1m\u001b[32m3.04172\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 008 | loss: 3.04172 - R2: 0.8410 -- iter: 1024/1168\n",
      "Training Step: 150  | total loss: \u001b[1m\u001b[32m2.74668\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 008 | loss: 2.74668 - R2: 0.8556 -- iter: 1088/1168\n",
      "Training Step: 151  | total loss: \u001b[1m\u001b[32m2.48596\u001b[0m\u001b[0m | time: 0.165s\n",
      "| SGD | epoch: 008 | loss: 2.48596 - R2: 0.8689 -- iter: 1152/1168\n",
      "Training Step: 152  | total loss: \u001b[1m\u001b[32m2.02851\u001b[0m\u001b[0m | time: 1.188s\n",
      "| SGD | epoch: 008 | loss: 2.02851 - R2: 0.8944 | val_loss: 0.09809 - val_acc: 0.9865 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 153  | total loss: \u001b[1m\u001b[32m2.02851\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 009 | loss: 2.02851 - R2: 0.8944 -- iter: 0064/1168\n",
      "Training Step: 154  | total loss: \u001b[1m\u001b[32m1.83657\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 009 | loss: 1.83657 - R2: 0.9032 -- iter: 0128/1168\n",
      "Training Step: 155  | total loss: \u001b[1m\u001b[32m1.66242\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 009 | loss: 1.66242 - R2: 0.9125 -- iter: 0192/1168\n",
      "Training Step: 156  | total loss: \u001b[1m\u001b[32m1.36641\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 009 | loss: 1.36641 - R2: 0.9302 -- iter: 0256/1168\n",
      "Training Step: 157  | total loss: \u001b[1m\u001b[32m1.23764\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 009 | loss: 1.23764 - R2: 0.9360 -- iter: 0320/1168\n",
      "Training Step: 158  | total loss: \u001b[1m\u001b[32m1.23764\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 009 | loss: 1.23764 - R2: 0.9427 -- iter: 0384/1168\n",
      "Training Step: 159  | total loss: \u001b[1m\u001b[32m1.11991\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 009 | loss: 1.11991 - R2: 0.9427 -- iter: 0448/1168\n",
      "Training Step: 160  | total loss: \u001b[1m\u001b[32m1.01273\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 009 | loss: 1.01273 - R2: 0.9554 -- iter: 0512/1168\n",
      "Training Step: 161  | total loss: \u001b[1m\u001b[32m0.91534\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 009 | loss: 0.91534 - R2: 0.9554 -- iter: 0576/1168\n",
      "Training Step: 162  | total loss: \u001b[1m\u001b[32m0.75705\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 009 | loss: 0.75705 - R2: 0.9630 -- iter: 0640/1168\n",
      "Training Step: 163  | total loss: \u001b[1m\u001b[32m0.75705\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 009 | loss: 0.75705 - R2: 0.9630 -- iter: 0704/1168\n",
      "Training Step: 164  | total loss: \u001b[1m\u001b[32m0.68595\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 009 | loss: 0.68595 - R2: 0.9657 -- iter: 0768/1168\n",
      "Training Step: 165  | total loss: \u001b[1m\u001b[32m0.56557\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 009 | loss: 0.56557 - R2: 0.9724 -- iter: 0832/1168\n",
      "Training Step: 166  | total loss: \u001b[1m\u001b[32m0.51421\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 009 | loss: 0.51421 - R2: 0.9752 -- iter: 0896/1168\n",
      "Training Step: 167  | total loss: \u001b[1m\u001b[32m0.51421\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 009 | loss: 0.51421 - R2: 0.9752 -- iter: 0960/1168\n",
      "Training Step: 168  | total loss: \u001b[1m\u001b[32m0.46887\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 009 | loss: 0.46887 - R2: 0.9783 -- iter: 1024/1168\n",
      "Training Step: 169  | total loss: \u001b[1m\u001b[32m0.42500\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 009 | loss: 0.42500 - R2: 0.9802 -- iter: 1088/1168\n",
      "Training Step: 170  | total loss: \u001b[1m\u001b[32m0.38830\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 009 | loss: 0.38830 - R2: 0.9821 -- iter: 1152/1168\n",
      "Training Step: 171  | total loss: \u001b[1m\u001b[32m0.35295\u001b[0m\u001b[0m | time: 1.136s\n",
      "| SGD | epoch: 009 | loss: 0.35295 - R2: 0.9835 | val_loss: 0.04500 - val_acc: 0.9911 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 172  | total loss: \u001b[1m\u001b[32m0.32258\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 010 | loss: 0.32258 - R2: 0.9859 -- iter: 0064/1168\n",
      "Training Step: 173  | total loss: \u001b[1m\u001b[32m0.29445\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 010 | loss: 0.29445 - R2: 0.9865 -- iter: 0128/1168\n",
      "Training Step: 174  | total loss: \u001b[1m\u001b[32m0.26864\u001b[0m\u001b[0m | time: 0.026s\n",
      "| SGD | epoch: 010 | loss: 0.26864 - R2: 0.9884 -- iter: 0192/1168\n",
      "Training Step: 175  | total loss: \u001b[1m\u001b[32m0.24751\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 010 | loss: 0.24751 - R2: 0.9898 -- iter: 0256/1168\n",
      "Training Step: 176  | total loss: \u001b[1m\u001b[32m0.22776\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 010 | loss: 0.22776 - R2: 0.9908 -- iter: 0320/1168\n",
      "Training Step: 177  | total loss: \u001b[1m\u001b[32m0.20946\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 010 | loss: 0.20946 - R2: 0.9913 -- iter: 0384/1168\n",
      "Training Step: 178  | total loss: \u001b[1m\u001b[32m0.19188\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 010 | loss: 0.19188 - R2: 0.9916 -- iter: 0448/1168\n",
      "Training Step: 179  | total loss: \u001b[1m\u001b[32m0.16352\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 010 | loss: 0.16352 - R2: 0.9939 -- iter: 0512/1168\n",
      "Training Step: 180  | total loss: \u001b[1m\u001b[32m0.16352\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 010 | loss: 0.16352 - R2: 0.9948 -- iter: 0576/1168\n",
      "Training Step: 181  | total loss: \u001b[1m\u001b[32m0.14989\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 010 | loss: 0.14989 - R2: 0.9946 -- iter: 0640/1168\n",
      "Training Step: 182  | total loss: \u001b[1m\u001b[32m0.12704\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 010 | loss: 0.12704 - R2: 0.9952 -- iter: 0704/1168\n",
      "Training Step: 183  | total loss: \u001b[1m\u001b[32m0.12704\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 010 | loss: 0.12704 - R2: 0.9952 -- iter: 0768/1168\n",
      "Training Step: 184  | total loss: \u001b[1m\u001b[32m0.10607\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 010 | loss: 0.10607 - R2: 0.9962 -- iter: 0832/1168\n",
      "Training Step: 185  | total loss: \u001b[1m\u001b[32m0.10607\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 010 | loss: 0.10607 - R2: 0.9962 -- iter: 0896/1168\n",
      "Training Step: 186  | total loss: \u001b[1m\u001b[32m0.09269\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 010 | loss: 0.09269 - R2: 0.9965 -- iter: 0960/1168\n",
      "Training Step: 187  | total loss: \u001b[1m\u001b[32m0.09269\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 010 | loss: 0.09269 - R2: 0.9965 -- iter: 1024/1168\n",
      "Training Step: 188  | total loss: \u001b[1m\u001b[32m0.08180\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 010 | loss: 0.08180 - R2: 0.9968 -- iter: 1088/1168\n",
      "Training Step: 189  | total loss: \u001b[1m\u001b[32m0.08180\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 010 | loss: 0.08180 - R2: 0.9968 -- iter: 1152/1168\n",
      "Training Step: 190  | total loss: \u001b[1m\u001b[32m0.07591\u001b[0m\u001b[0m | time: 1.069s\n",
      "| SGD | epoch: 010 | loss: 0.07591 - R2: 0.9967 | val_loss: 0.02486 - val_acc: 0.9960 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 191  | total loss: \u001b[1m\u001b[32m0.06693\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 011 | loss: 0.06693 - R2: 0.9984 -- iter: 0064/1168\n",
      "Training Step: 192  | total loss: \u001b[1m\u001b[32m0.06693\u001b[0m\u001b[0m | time: 0.007s\n",
      "| SGD | epoch: 011 | loss: 0.06693 - R2: 0.9984 -- iter: 0128/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 193  | total loss: \u001b[1m\u001b[32m0.05801\u001b[0m\u001b[0m | time: 0.011s\n",
      "| SGD | epoch: 011 | loss: 0.05801 - R2: 0.9985 -- iter: 0192/1168\n",
      "Training Step: 194  | total loss: \u001b[1m\u001b[32m0.05457\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 011 | loss: 0.05457 - R2: 0.9988 -- iter: 0256/1168\n",
      "Training Step: 195  | total loss: \u001b[1m\u001b[32m0.05457\u001b[0m\u001b[0m | time: 0.018s\n",
      "| SGD | epoch: 011 | loss: 0.05457 - R2: 0.9988 -- iter: 0320/1168\n",
      "Training Step: 196  | total loss: \u001b[1m\u001b[32m0.05096\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 011 | loss: 0.05096 - R2: 0.9992 -- iter: 0384/1168\n",
      "Training Step: 197  | total loss: \u001b[1m\u001b[32m0.04819\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 011 | loss: 0.04819 - R2: 0.9987 -- iter: 0448/1168\n",
      "Training Step: 198  | total loss: \u001b[1m\u001b[32m0.04537\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 011 | loss: 0.04537 - R2: 0.9988 -- iter: 0512/1168\n",
      "Training Step: 199  | total loss: \u001b[1m\u001b[32m0.04537\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 011 | loss: 0.04537 - R2: 0.9988 -- iter: 0576/1168\n",
      "Training Step: 200  | total loss: \u001b[1m\u001b[32m0.04214\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 011 | loss: 0.04214 - R2: 0.9987 -- iter: 0640/1168\n",
      "Training Step: 201  | total loss: \u001b[1m\u001b[32m0.03899\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 011 | loss: 0.03899 - R2: 0.9987 -- iter: 0704/1168\n",
      "Training Step: 202  | total loss: \u001b[1m\u001b[32m0.03716\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 011 | loss: 0.03716 - R2: 0.9999 -- iter: 0768/1168\n",
      "Training Step: 203  | total loss: \u001b[1m\u001b[32m0.03573\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 011 | loss: 0.03573 - R2: 1.0004 -- iter: 0832/1168\n",
      "Training Step: 204  | total loss: \u001b[1m\u001b[32m0.03376\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 011 | loss: 0.03376 - R2: 0.9997 -- iter: 0896/1168\n",
      "Training Step: 205  | total loss: \u001b[1m\u001b[32m0.03376\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 011 | loss: 0.03376 - R2: 0.9998 -- iter: 0960/1168\n",
      "Training Step: 206  | total loss: \u001b[1m\u001b[32m0.03240\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 011 | loss: 0.03240 - R2: 0.9998 -- iter: 1024/1168\n",
      "Training Step: 207  | total loss: \u001b[1m\u001b[32m0.05783\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 011 | loss: 0.05783 - R2: 1.0011 -- iter: 1088/1168\n",
      "Training Step: 208  | total loss: \u001b[1m\u001b[32m0.05783\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 011 | loss: 0.05783 - R2: 1.0011 -- iter: 1152/1168\n",
      "Training Step: 209  | total loss: \u001b[1m\u001b[32m0.05359\u001b[0m\u001b[0m | time: 1.060s\n",
      "| SGD | epoch: 011 | loss: 0.05359 - R2: 1.0005 | val_loss: 0.02556 - val_acc: 0.9899 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 210  | total loss: \u001b[1m\u001b[32m0.05145\u001b[0m\u001b[0m | time: 0.014s\n",
      "| SGD | epoch: 012 | loss: 0.05145 - R2: 1.0009 -- iter: 0064/1168\n",
      "Training Step: 211  | total loss: \u001b[1m\u001b[32m0.04702\u001b[0m\u001b[0m | time: 0.016s\n",
      "| SGD | epoch: 012 | loss: 0.04702 - R2: 1.0004 -- iter: 0128/1168\n",
      "Training Step: 212  | total loss: \u001b[1m\u001b[32m0.04317\u001b[0m\u001b[0m | time: 0.020s\n",
      "| SGD | epoch: 012 | loss: 0.04317 - R2: 0.9996 -- iter: 0192/1168\n",
      "Training Step: 213  | total loss: \u001b[1m\u001b[32m0.04217\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 012 | loss: 0.04217 - R2: 0.9994 -- iter: 0256/1168\n",
      "Training Step: 214  | total loss: \u001b[1m\u001b[32m0.04134\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 012 | loss: 0.04134 - R2: 0.9988 -- iter: 0320/1168\n",
      "Training Step: 215  | total loss: \u001b[1m\u001b[32m0.03904\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 012 | loss: 0.03904 - R2: 0.9988 -- iter: 0384/1168\n",
      "Training Step: 216  | total loss: \u001b[1m\u001b[32m0.03904\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 012 | loss: 0.03904 - R2: 0.9988 -- iter: 0448/1168\n",
      "Training Step: 217  | total loss: \u001b[1m\u001b[32m0.03859\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 012 | loss: 0.03859 - R2: 0.9994 -- iter: 0512/1168\n",
      "Training Step: 218  | total loss: \u001b[1m\u001b[32m0.03624\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 012 | loss: 0.03624 - R2: 0.9992 -- iter: 0576/1168\n",
      "Training Step: 219  | total loss: \u001b[1m\u001b[32m0.03422\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 012 | loss: 0.03422 - R2: 0.9990 -- iter: 0640/1168\n",
      "Training Step: 220  | total loss: \u001b[1m\u001b[32m0.03231\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 012 | loss: 0.03231 - R2: 0.9982 -- iter: 0704/1168\n",
      "Training Step: 221  | total loss: \u001b[1m\u001b[32m0.03059\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 012 | loss: 0.03059 - R2: 0.9986 -- iter: 0768/1168\n",
      "Training Step: 222  | total loss: \u001b[1m\u001b[32m0.03059\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 012 | loss: 0.03059 - R2: 0.9986 -- iter: 0832/1168\n",
      "Training Step: 223  | total loss: \u001b[1m\u001b[32m0.03052\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 012 | loss: 0.03052 - R2: 1.0000 -- iter: 0896/1168\n",
      "Training Step: 224  | total loss: \u001b[1m\u001b[32m0.02890\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 012 | loss: 0.02890 - R2: 1.0003 -- iter: 0960/1168\n",
      "Training Step: 225  | total loss: \u001b[1m\u001b[32m0.02893\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 012 | loss: 0.02893 - R2: 0.9999 -- iter: 1024/1168\n",
      "Training Step: 226  | total loss: \u001b[1m\u001b[32m0.02844\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 012 | loss: 0.02844 - R2: 1.0002 -- iter: 1088/1168\n",
      "Training Step: 227  | total loss: \u001b[1m\u001b[32m0.02844\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 012 | loss: 0.02844 - R2: 1.0002 -- iter: 1152/1168\n",
      "Training Step: 228  | total loss: \u001b[1m\u001b[32m0.02722\u001b[0m\u001b[0m | time: 1.064s\n",
      "| SGD | epoch: 012 | loss: 0.02722 - R2: 1.0004 | val_loss: 0.01781 - val_acc: 0.9978 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 229  | total loss: \u001b[1m\u001b[32m0.02740\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 013 | loss: 0.02740 - R2: 0.9998 -- iter: 0064/1168\n",
      "Training Step: 230  | total loss: \u001b[1m\u001b[32m0.02639\u001b[0m\u001b[0m | time: 0.023s\n",
      "| SGD | epoch: 013 | loss: 0.02639 - R2: 1.0001 -- iter: 0128/1168\n",
      "Training Step: 231  | total loss: \u001b[1m\u001b[32m0.02653\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 013 | loss: 0.02653 - R2: 1.0003 -- iter: 0192/1168\n",
      "Training Step: 232  | total loss: \u001b[1m\u001b[32m0.02616\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 013 | loss: 0.02616 - R2: 1.0008 -- iter: 0256/1168\n",
      "Training Step: 233  | total loss: \u001b[1m\u001b[32m0.02630\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 013 | loss: 0.02630 - R2: 1.0004 -- iter: 0320/1168\n",
      "Training Step: 234  | total loss: \u001b[1m\u001b[32m0.02744\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 013 | loss: 0.02744 - R2: 1.0004 -- iter: 0384/1168\n",
      "Training Step: 235  | total loss: \u001b[1m\u001b[32m0.02678\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 013 | loss: 0.02678 - R2: 1.0000 -- iter: 0448/1168\n",
      "Training Step: 236  | total loss: \u001b[1m\u001b[32m0.02886\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 013 | loss: 0.02886 - R2: 1.0008 -- iter: 0512/1168\n",
      "Training Step: 237  | total loss: \u001b[1m\u001b[32m0.02810\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 013 | loss: 0.02810 - R2: 1.0004 -- iter: 0576/1168\n",
      "Training Step: 238  | total loss: \u001b[1m\u001b[32m0.02700\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 013 | loss: 0.02700 - R2: 0.9998 -- iter: 0640/1168\n",
      "Training Step: 239  | total loss: \u001b[1m\u001b[32m0.02559\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 013 | loss: 0.02559 - R2: 0.9998 -- iter: 0704/1168\n",
      "Training Step: 240  | total loss: \u001b[1m\u001b[32m0.02359\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 013 | loss: 0.02359 - R2: 1.0001 -- iter: 0768/1168\n",
      "Training Step: 241  | total loss: \u001b[1m\u001b[32m0.02170\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 013 | loss: 0.02170 - R2: 1.0001 -- iter: 0832/1168\n",
      "Training Step: 242  | total loss: \u001b[1m\u001b[32m0.02122\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 013 | loss: 0.02122 - R2: 0.9998 -- iter: 0896/1168\n",
      "Training Step: 243  | total loss: \u001b[1m\u001b[32m0.02165\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 013 | loss: 0.02165 - R2: 1.0000 -- iter: 0960/1168\n",
      "Training Step: 244  | total loss: \u001b[1m\u001b[32m0.02165\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 013 | loss: 0.02165 - R2: 0.9998 -- iter: 1024/1168\n",
      "Training Step: 245  | total loss: \u001b[1m\u001b[32m0.02048\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 013 | loss: 0.02048 - R2: 1.0000 -- iter: 1088/1168\n",
      "Training Step: 246  | total loss: \u001b[1m\u001b[32m0.02024\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 013 | loss: 0.02024 - R2: 1.0001 -- iter: 1152/1168\n",
      "Training Step: 247  | total loss: \u001b[1m\u001b[32m0.02024\u001b[0m\u001b[0m | time: 1.070s\n",
      "| SGD | epoch: 013 | loss: 0.02024 - R2: 1.0001 | val_loss: 0.01942 - val_acc: 0.9936 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 248  | total loss: \u001b[1m\u001b[32m0.04277\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 014 | loss: 0.04277 - R2: 1.0003 -- iter: 0064/1168\n",
      "Training Step: 249  | total loss: \u001b[1m\u001b[32m0.04277\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 014 | loss: 0.04277 - R2: 1.0003 -- iter: 0128/1168\n",
      "Training Step: 250  | total loss: \u001b[1m\u001b[32m0.04001\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 014 | loss: 0.04001 - R2: 0.9999 -- iter: 0192/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 251  | total loss: \u001b[1m\u001b[32m0.03781\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 014 | loss: 0.03781 - R2: 1.0002 -- iter: 0256/1168\n",
      "Training Step: 252  | total loss: \u001b[1m\u001b[32m0.03514\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 014 | loss: 0.03514 - R2: 0.9997 -- iter: 0320/1168\n",
      "Training Step: 253  | total loss: \u001b[1m\u001b[32m0.03514\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 014 | loss: 0.03514 - R2: 0.9997 -- iter: 0384/1168\n",
      "Training Step: 254  | total loss: \u001b[1m\u001b[32m0.03290\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 014 | loss: 0.03290 - R2: 0.9998 -- iter: 0448/1168\n",
      "Training Step: 255  | total loss: \u001b[1m\u001b[32m0.03096\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 014 | loss: 0.03096 - R2: 0.9991 -- iter: 0512/1168\n",
      "Training Step: 256  | total loss: \u001b[1m\u001b[32m0.03048\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 014 | loss: 0.03048 - R2: 0.9994 -- iter: 0576/1168\n",
      "Training Step: 257  | total loss: \u001b[1m\u001b[32m0.03048\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 014 | loss: 0.03048 - R2: 0.9994 -- iter: 0640/1168\n",
      "Training Step: 258  | total loss: \u001b[1m\u001b[32m0.02881\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 014 | loss: 0.02881 - R2: 0.9997 -- iter: 0704/1168\n",
      "Training Step: 259  | total loss: \u001b[1m\u001b[32m0.02881\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 014 | loss: 0.02881 - R2: 0.9997 -- iter: 0768/1168\n",
      "Training Step: 260  | total loss: \u001b[1m\u001b[32m0.02752\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 014 | loss: 0.02752 - R2: 1.0003 -- iter: 0832/1168\n",
      "Training Step: 261  | total loss: \u001b[1m\u001b[32m0.02823\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 014 | loss: 0.02823 - R2: 1.0004 -- iter: 0896/1168\n",
      "Training Step: 262  | total loss: \u001b[1m\u001b[32m0.02654\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 014 | loss: 0.02654 - R2: 1.0003 -- iter: 0960/1168\n",
      "Training Step: 263  | total loss: \u001b[1m\u001b[32m0.02573\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 014 | loss: 0.02573 - R2: 1.0001 -- iter: 1024/1168\n",
      "Training Step: 264  | total loss: \u001b[1m\u001b[32m0.02573\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 014 | loss: 0.02573 - R2: 1.0001 -- iter: 1088/1168\n",
      "Training Step: 265  | total loss: \u001b[1m\u001b[32m0.02516\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 014 | loss: 0.02516 - R2: 0.9994 -- iter: 1152/1168\n",
      "Training Step: 266  | total loss: \u001b[1m\u001b[32m0.02497\u001b[0m\u001b[0m | time: 1.118s\n",
      "| SGD | epoch: 014 | loss: 0.02497 - R2: 0.9996 | val_loss: 0.01793 - val_acc: 0.9967 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 267  | total loss: \u001b[1m\u001b[32m0.02424\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 015 | loss: 0.02424 - R2: 0.9990 -- iter: 0064/1168\n",
      "Training Step: 268  | total loss: \u001b[1m\u001b[32m0.02393\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 015 | loss: 0.02393 - R2: 0.9995 -- iter: 0128/1168\n",
      "Training Step: 269  | total loss: \u001b[1m\u001b[32m0.02393\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 015 | loss: 0.02393 - R2: 0.9995 -- iter: 0192/1168\n",
      "Training Step: 270  | total loss: \u001b[1m\u001b[32m0.02357\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 015 | loss: 0.02357 - R2: 0.9990 -- iter: 0256/1168\n",
      "Training Step: 271  | total loss: \u001b[1m\u001b[32m0.02528\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 015 | loss: 0.02528 - R2: 0.9994 -- iter: 0320/1168\n",
      "Training Step: 272  | total loss: \u001b[1m\u001b[32m0.02439\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 015 | loss: 0.02439 - R2: 1.0000 -- iter: 0384/1168\n",
      "Training Step: 273  | total loss: \u001b[1m\u001b[32m0.02374\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 015 | loss: 0.02374 - R2: 0.9996 -- iter: 0448/1168\n",
      "Training Step: 274  | total loss: \u001b[1m\u001b[32m0.02281\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 015 | loss: 0.02281 - R2: 0.9993 -- iter: 0512/1168\n",
      "Training Step: 275  | total loss: \u001b[1m\u001b[32m0.02203\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 015 | loss: 0.02203 - R2: 0.9998 -- iter: 0576/1168\n",
      "Training Step: 276  | total loss: \u001b[1m\u001b[32m0.02203\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 015 | loss: 0.02203 - R2: 0.9998 -- iter: 0640/1168\n",
      "Training Step: 277  | total loss: \u001b[1m\u001b[32m0.02412\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 015 | loss: 0.02412 - R2: 0.9998 -- iter: 0704/1168\n",
      "Training Step: 278  | total loss: \u001b[1m\u001b[32m0.02334\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 015 | loss: 0.02334 - R2: 1.0002 -- iter: 0768/1168\n",
      "Training Step: 279  | total loss: \u001b[1m\u001b[32m0.02334\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 015 | loss: 0.02334 - R2: 1.0002 -- iter: 0832/1168\n",
      "Training Step: 280  | total loss: \u001b[1m\u001b[32m0.01991\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 015 | loss: 0.01991 - R2: 1.0004 -- iter: 0896/1168\n",
      "Training Step: 281  | total loss: \u001b[1m\u001b[32m0.01991\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 015 | loss: 0.01991 - R2: 1.0004 -- iter: 0960/1168\n",
      "Training Step: 282  | total loss: \u001b[1m\u001b[32m0.02023\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 015 | loss: 0.02023 - R2: 1.0005 -- iter: 1024/1168\n",
      "Training Step: 283  | total loss: \u001b[1m\u001b[32m0.01981\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 015 | loss: 0.01981 - R2: 0.9997 -- iter: 1088/1168\n",
      "Training Step: 284  | total loss: \u001b[1m\u001b[32m0.02046\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 015 | loss: 0.02046 - R2: 1.0000 -- iter: 1152/1168\n",
      "Training Step: 285  | total loss: \u001b[1m\u001b[32m0.01957\u001b[0m\u001b[0m | time: 1.148s\n",
      "| SGD | epoch: 015 | loss: 0.01957 - R2: 0.9998 | val_loss: 0.01768 - val_acc: 0.9969 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 286  | total loss: \u001b[1m\u001b[32m0.01944\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 016 | loss: 0.01944 - R2: 0.9998 -- iter: 0064/1168\n",
      "Training Step: 287  | total loss: \u001b[1m\u001b[32m0.01944\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 016 | loss: 0.01944 - R2: 0.9998 -- iter: 0128/1168\n",
      "Training Step: 288  | total loss: \u001b[1m\u001b[32m0.02061\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 016 | loss: 0.02061 - R2: 0.9999 -- iter: 0192/1168\n",
      "Training Step: 289  | total loss: \u001b[1m\u001b[32m0.01996\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 016 | loss: 0.01996 - R2: 0.9992 -- iter: 0256/1168\n",
      "Training Step: 290  | total loss: \u001b[1m\u001b[32m0.01996\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 016 | loss: 0.01996 - R2: 0.9992 -- iter: 0320/1168\n",
      "Training Step: 291  | total loss: \u001b[1m\u001b[32m0.01886\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 016 | loss: 0.01886 - R2: 1.0001 -- iter: 0384/1168\n",
      "Training Step: 292  | total loss: \u001b[1m\u001b[32m0.01886\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 016 | loss: 0.01886 - R2: 1.0001 -- iter: 0448/1168\n",
      "Training Step: 293  | total loss: \u001b[1m\u001b[32m0.01906\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 016 | loss: 0.01906 - R2: 1.0000 -- iter: 0512/1168\n",
      "Training Step: 294  | total loss: \u001b[1m\u001b[32m0.01910\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 016 | loss: 0.01910 - R2: 0.9999 -- iter: 0576/1168\n",
      "Training Step: 295  | total loss: \u001b[1m\u001b[32m0.01879\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 016 | loss: 0.01879 - R2: 0.9997 -- iter: 0640/1168\n",
      "Training Step: 296  | total loss: \u001b[1m\u001b[32m0.01900\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 016 | loss: 0.01900 - R2: 0.9997 -- iter: 0704/1168\n",
      "Training Step: 297  | total loss: \u001b[1m\u001b[32m0.01916\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 016 | loss: 0.01916 - R2: 0.9999 -- iter: 0768/1168\n",
      "Training Step: 298  | total loss: \u001b[1m\u001b[32m0.01869\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 016 | loss: 0.01869 - R2: 0.9998 -- iter: 0832/1168\n",
      "Training Step: 299  | total loss: \u001b[1m\u001b[32m0.01852\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 016 | loss: 0.01852 - R2: 0.9997 -- iter: 0896/1168\n",
      "Training Step: 300  | total loss: \u001b[1m\u001b[32m0.01816\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 016 | loss: 0.01816 - R2: 0.9991 -- iter: 0960/1168\n",
      "Training Step: 301  | total loss: \u001b[1m\u001b[32m0.01816\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 016 | loss: 0.01816 - R2: 0.9991 -- iter: 1024/1168\n",
      "Training Step: 302  | total loss: \u001b[1m\u001b[32m0.01878\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 016 | loss: 0.01878 - R2: 0.9995 -- iter: 1088/1168\n",
      "Training Step: 303  | total loss: \u001b[1m\u001b[32m0.01841\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 016 | loss: 0.01841 - R2: 0.9995 -- iter: 1152/1168\n",
      "Training Step: 304  | total loss: \u001b[1m\u001b[32m0.01996\u001b[0m\u001b[0m | time: 1.129s\n",
      "| SGD | epoch: 016 | loss: 0.01996 - R2: 0.9998 | val_loss: 0.01795 - val_acc: 0.9957 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 305  | total loss: \u001b[1m\u001b[32m0.02317\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 017 | loss: 0.02317 - R2: 1.0007 -- iter: 0064/1168\n",
      "Training Step: 306  | total loss: \u001b[1m\u001b[32m0.02296\u001b[0m\u001b[0m | time: 0.006s\n",
      "| SGD | epoch: 017 | loss: 0.02296 - R2: 1.0003 -- iter: 0128/1168\n",
      "Training Step: 307  | total loss: \u001b[1m\u001b[32m0.02532\u001b[0m\u001b[0m | time: 0.014s\n",
      "| SGD | epoch: 017 | loss: 0.02532 - R2: 1.0005 -- iter: 0192/1168\n",
      "Training Step: 308  | total loss: \u001b[1m\u001b[32m0.04829\u001b[0m\u001b[0m | time: 0.016s\n",
      "| SGD | epoch: 017 | loss: 0.04829 - R2: 1.0020 -- iter: 0256/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 309  | total loss: \u001b[1m\u001b[32m0.04570\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 017 | loss: 0.04570 - R2: 1.0007 -- iter: 0320/1168\n",
      "Training Step: 310  | total loss: \u001b[1m\u001b[32m0.04269\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 017 | loss: 0.04269 - R2: 1.0000 -- iter: 0384/1168\n",
      "Training Step: 311  | total loss: \u001b[1m\u001b[32m0.03958\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 017 | loss: 0.03958 - R2: 0.9995 -- iter: 0448/1168\n",
      "Training Step: 312  | total loss: \u001b[1m\u001b[32m0.03958\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 017 | loss: 0.03958 - R2: 0.9995 -- iter: 0512/1168\n",
      "Training Step: 313  | total loss: \u001b[1m\u001b[32m0.03832\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 017 | loss: 0.03832 - R2: 0.9993 -- iter: 0576/1168\n",
      "Training Step: 314  | total loss: \u001b[1m\u001b[32m0.03369\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 017 | loss: 0.03369 - R2: 0.9998 -- iter: 0640/1168\n",
      "Training Step: 315  | total loss: \u001b[1m\u001b[32m0.03241\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 017 | loss: 0.03241 - R2: 0.9995 -- iter: 0704/1168\n",
      "Training Step: 316  | total loss: \u001b[1m\u001b[32m0.03155\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 017 | loss: 0.03155 - R2: 0.9995 -- iter: 0768/1168\n",
      "Training Step: 317  | total loss: \u001b[1m\u001b[32m0.03155\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 017 | loss: 0.03155 - R2: 0.9995 -- iter: 0832/1168\n",
      "Training Step: 318  | total loss: \u001b[1m\u001b[32m0.02927\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 017 | loss: 0.02927 - R2: 0.9997 -- iter: 0896/1168\n",
      "Training Step: 319  | total loss: \u001b[1m\u001b[32m0.02931\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 017 | loss: 0.02931 - R2: 0.9998 -- iter: 0960/1168\n",
      "Training Step: 320  | total loss: \u001b[1m\u001b[32m0.02677\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 017 | loss: 0.02677 - R2: 0.9994 -- iter: 1024/1168\n",
      "Training Step: 321  | total loss: \u001b[1m\u001b[32m0.02677\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 017 | loss: 0.02677 - R2: 0.9994 -- iter: 1088/1168\n",
      "Training Step: 322  | total loss: \u001b[1m\u001b[32m0.02510\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 017 | loss: 0.02510 - R2: 0.9998 -- iter: 1152/1168\n",
      "Training Step: 323  | total loss: \u001b[1m\u001b[32m0.02510\u001b[0m\u001b[0m | time: 1.068s\n",
      "| SGD | epoch: 017 | loss: 0.02510 - R2: 0.9998 | val_loss: 0.01799 - val_acc: 0.9964 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 324  | total loss: \u001b[1m\u001b[32m0.02767\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 018 | loss: 0.02767 - R2: 0.9999 -- iter: 0064/1168\n",
      "Training Step: 325  | total loss: \u001b[1m\u001b[32m0.02660\u001b[0m\u001b[0m | time: 0.007s\n",
      "| SGD | epoch: 018 | loss: 0.02660 - R2: 0.9989 -- iter: 0128/1168\n",
      "Training Step: 326  | total loss: \u001b[1m\u001b[32m0.02690\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 018 | loss: 0.02690 - R2: 0.9996 -- iter: 0192/1168\n",
      "Training Step: 327  | total loss: \u001b[1m\u001b[32m0.05282\u001b[0m\u001b[0m | time: 0.019s\n",
      "| SGD | epoch: 018 | loss: 0.05282 - R2: 0.9995 -- iter: 0256/1168\n",
      "Training Step: 328  | total loss: \u001b[1m\u001b[32m0.04939\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 018 | loss: 0.04939 - R2: 0.9996 -- iter: 0320/1168\n",
      "Training Step: 329  | total loss: \u001b[1m\u001b[32m0.04939\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 018 | loss: 0.04939 - R2: 0.9996 -- iter: 0384/1168\n",
      "Training Step: 330  | total loss: \u001b[1m\u001b[32m0.04613\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 018 | loss: 0.04613 - R2: 0.9996 -- iter: 0448/1168\n",
      "Training Step: 331  | total loss: \u001b[1m\u001b[32m0.04223\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 018 | loss: 0.04223 - R2: 0.9998 -- iter: 0512/1168\n",
      "Training Step: 332  | total loss: \u001b[1m\u001b[32m0.04223\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 018 | loss: 0.04223 - R2: 0.9998 -- iter: 0576/1168\n",
      "Training Step: 333  | total loss: \u001b[1m\u001b[32m0.04001\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 018 | loss: 0.04001 - R2: 0.9998 -- iter: 0640/1168\n",
      "Training Step: 334  | total loss: \u001b[1m\u001b[32m0.03601\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 018 | loss: 0.03601 - R2: 0.9992 -- iter: 0704/1168\n",
      "Training Step: 335  | total loss: \u001b[1m\u001b[32m0.03601\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 018 | loss: 0.03601 - R2: 0.9992 -- iter: 0768/1168\n",
      "Training Step: 336  | total loss: \u001b[1m\u001b[32m0.03404\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 018 | loss: 0.03404 - R2: 0.9997 -- iter: 0832/1168\n",
      "Training Step: 337  | total loss: \u001b[1m\u001b[32m0.03358\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 018 | loss: 0.03358 - R2: 0.9997 -- iter: 0896/1168\n",
      "Training Step: 338  | total loss: \u001b[1m\u001b[32m0.03133\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 018 | loss: 0.03133 - R2: 0.9997 -- iter: 0960/1168\n",
      "Training Step: 339  | total loss: \u001b[1m\u001b[32m0.03027\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 018 | loss: 0.03027 - R2: 0.9997 -- iter: 1024/1168\n",
      "Training Step: 340  | total loss: \u001b[1m\u001b[32m0.02769\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 018 | loss: 0.02769 - R2: 0.9996 -- iter: 1088/1168\n",
      "Training Step: 341  | total loss: \u001b[1m\u001b[32m0.02543\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 018 | loss: 0.02543 - R2: 0.9997 -- iter: 1152/1168\n",
      "Training Step: 342  | total loss: \u001b[1m\u001b[32m0.02543\u001b[0m\u001b[0m | time: 1.070s\n",
      "| SGD | epoch: 018 | loss: 0.02543 - R2: 0.9997 | val_loss: 0.01670 - val_acc: 0.9989 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 343  | total loss: \u001b[1m\u001b[32m0.02466\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 019 | loss: 0.02466 - R2: 0.9994 -- iter: 0064/1168\n",
      "Training Step: 344  | total loss: \u001b[1m\u001b[32m0.02424\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 019 | loss: 0.02424 - R2: 0.9997 -- iter: 0128/1168\n",
      "Training Step: 345  | total loss: \u001b[1m\u001b[32m0.02307\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 019 | loss: 0.02307 - R2: 0.9998 -- iter: 0192/1168\n",
      "Training Step: 346  | total loss: \u001b[1m\u001b[32m0.02357\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 019 | loss: 0.02357 - R2: 1.0003 -- iter: 0256/1168\n",
      "Training Step: 347  | total loss: \u001b[1m\u001b[32m0.02357\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 019 | loss: 0.02357 - R2: 1.0003 -- iter: 0320/1168\n",
      "Training Step: 348  | total loss: \u001b[1m\u001b[32m0.05610\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 019 | loss: 0.05610 - R2: 1.0006 -- iter: 0384/1168\n",
      "Training Step: 349  | total loss: \u001b[1m\u001b[32m0.05610\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 019 | loss: 0.05610 - R2: 1.0006 -- iter: 0448/1168\n",
      "Training Step: 350  | total loss: \u001b[1m\u001b[32m0.05199\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 019 | loss: 0.05199 - R2: 1.0006 -- iter: 0512/1168\n",
      "Training Step: 351  | total loss: \u001b[1m\u001b[32m0.04875\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 019 | loss: 0.04875 - R2: 1.0005 -- iter: 0576/1168\n",
      "Training Step: 352  | total loss: \u001b[1m\u001b[32m0.04626\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 019 | loss: 0.04626 - R2: 1.0001 -- iter: 0640/1168\n",
      "Training Step: 353  | total loss: \u001b[1m\u001b[32m0.04091\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 019 | loss: 0.04091 - R2: 0.9996 -- iter: 0704/1168\n",
      "Training Step: 354  | total loss: \u001b[1m\u001b[32m0.03914\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 019 | loss: 0.03914 - R2: 0.9993 -- iter: 0768/1168\n",
      "Training Step: 355  | total loss: \u001b[1m\u001b[32m0.03914\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 019 | loss: 0.03914 - R2: 0.9993 -- iter: 0832/1168\n",
      "Training Step: 356  | total loss: \u001b[1m\u001b[32m0.03715\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 019 | loss: 0.03715 - R2: 0.9997 -- iter: 0896/1168\n",
      "Training Step: 357  | total loss: \u001b[1m\u001b[32m0.03277\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 019 | loss: 0.03277 - R2: 0.9988 -- iter: 0960/1168\n",
      "Training Step: 358  | total loss: \u001b[1m\u001b[32m0.03518\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 019 | loss: 0.03518 - R2: 0.9998 -- iter: 1024/1168\n",
      "Training Step: 359  | total loss: \u001b[1m\u001b[32m0.03518\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 019 | loss: 0.03518 - R2: 0.9998 -- iter: 1088/1168\n",
      "Training Step: 360  | total loss: \u001b[1m\u001b[32m0.02967\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 019 | loss: 0.02967 - R2: 0.9999 -- iter: 1152/1168\n",
      "Training Step: 361  | total loss: \u001b[1m\u001b[32m0.02967\u001b[0m\u001b[0m | time: 1.132s\n",
      "| SGD | epoch: 019 | loss: 0.02967 - R2: 1.0000 | val_loss: 0.02027 - val_acc: 0.9943 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 362  | total loss: \u001b[1m\u001b[32m0.03167\u001b[0m\u001b[0m | time: 0.018s\n",
      "| SGD | epoch: 020 | loss: 0.03167 - R2: 1.0005 -- iter: 0064/1168\n",
      "Training Step: 363  | total loss: \u001b[1m\u001b[32m0.02886\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 020 | loss: 0.02886 - R2: 1.0001 -- iter: 0128/1168\n",
      "Training Step: 364  | total loss: \u001b[1m\u001b[32m0.02886\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 020 | loss: 0.02886 - R2: 0.9998 -- iter: 0192/1168\n",
      "Training Step: 365  | total loss: \u001b[1m\u001b[32m0.02613\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 020 | loss: 0.02613 - R2: 0.9992 -- iter: 0256/1168\n",
      "Training Step: 366  | total loss: \u001b[1m\u001b[32m0.02613\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 020 | loss: 0.02613 - R2: 0.9992 -- iter: 0320/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 367  | total loss: \u001b[1m\u001b[32m0.05014\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 020 | loss: 0.05014 - R2: 0.9990 -- iter: 0384/1168\n",
      "Training Step: 368  | total loss: \u001b[1m\u001b[32m0.04738\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 020 | loss: 0.04738 - R2: 0.9985 -- iter: 0448/1168\n",
      "Training Step: 369  | total loss: \u001b[1m\u001b[32m0.04738\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 020 | loss: 0.04738 - R2: 0.9985 -- iter: 0512/1168\n",
      "Training Step: 370  | total loss: \u001b[1m\u001b[32m0.04129\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 020 | loss: 0.04129 - R2: 0.9989 -- iter: 0576/1168\n",
      "Training Step: 371  | total loss: \u001b[1m\u001b[32m0.03986\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 020 | loss: 0.03986 - R2: 0.9995 -- iter: 0640/1168\n",
      "Training Step: 372  | total loss: \u001b[1m\u001b[32m0.03787\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 020 | loss: 0.03787 - R2: 0.9995 -- iter: 0704/1168\n",
      "Training Step: 373  | total loss: \u001b[1m\u001b[32m0.03832\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 020 | loss: 0.03832 - R2: 0.9997 -- iter: 0768/1168\n",
      "Training Step: 374  | total loss: \u001b[1m\u001b[32m0.03577\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 020 | loss: 0.03577 - R2: 0.9997 -- iter: 0832/1168\n",
      "Training Step: 375  | total loss: \u001b[1m\u001b[32m0.03447\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 020 | loss: 0.03447 - R2: 0.9995 -- iter: 0896/1168\n",
      "Training Step: 376  | total loss: \u001b[1m\u001b[32m0.03308\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 020 | loss: 0.03308 - R2: 0.9998 -- iter: 0960/1168\n",
      "Training Step: 377  | total loss: \u001b[1m\u001b[32m0.03152\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 020 | loss: 0.03152 - R2: 0.9998 -- iter: 1024/1168\n",
      "Training Step: 378  | total loss: \u001b[1m\u001b[32m0.03010\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 020 | loss: 0.03010 - R2: 0.9998 -- iter: 1088/1168\n",
      "Training Step: 379  | total loss: \u001b[1m\u001b[32m0.03031\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 020 | loss: 0.03031 - R2: 1.0010 -- iter: 1152/1168\n",
      "Training Step: 380  | total loss: \u001b[1m\u001b[32m0.02991\u001b[0m\u001b[0m | time: 1.098s\n",
      "| SGD | epoch: 020 | loss: 0.02991 - R2: 1.0017 | val_loss: 0.01929 - val_acc: 0.9927 -- iter: 1168/1168\n",
      "--\n",
      "---------------------------------\n",
      "Run id: RADK4P\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "INFO:tensorflow:Summary name StandardError/ (raw) is illegal; using StandardError/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 1168\n",
      "Validation samples: 292\n",
      "--\n",
      "Training Step: 1  | time: 0.033s\n",
      "| SGD | epoch: 001 | loss: 0.00000 - R2: 0.0000 -- iter: 0064/1168\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m130.34830\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 001 | loss: 130.34830 - R2: 0.0000 -- iter: 0128/1168\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m139.21304\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 001 | loss: 139.21304 - R2: 0.0001 -- iter: 0192/1168\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m140.33932\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 001 | loss: 140.33932 - R2: 0.0001 -- iter: 0256/1168\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m141.42564\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 001 | loss: 141.42564 - R2: 0.0001 -- iter: 0320/1168\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m140.68651\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 001 | loss: 140.68651 - R2: 0.0002 -- iter: 0384/1168\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m139.47522\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 001 | loss: 139.47522 - R2: 0.0003 -- iter: 0448/1168\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m139.07101\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 001 | loss: 139.07101 - R2: 0.0005 -- iter: 0512/1168\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m137.62582\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 001 | loss: 137.62582 - R2: 0.0009 -- iter: 0576/1168\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m135.88405\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 001 | loss: 135.88405 - R2: 0.0011 -- iter: 0640/1168\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m134.77754\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 001 | loss: 134.77754 - R2: 0.0014 -- iter: 0704/1168\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m133.33463\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 001 | loss: 133.33463 - R2: 0.0017 -- iter: 0768/1168\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m131.91122\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 001 | loss: 131.91122 - R2: 0.0020 -- iter: 0832/1168\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m130.62462\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 001 | loss: 130.62462 - R2: 0.0024 -- iter: 0896/1168\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m130.62462\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 001 | loss: 130.62462 - R2: 0.0028 -- iter: 0960/1168\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m129.16692\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 001 | loss: 129.16692 - R2: 0.0028 -- iter: 1024/1168\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m129.09225\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 001 | loss: 129.09225 - R2: 0.0031 -- iter: 1088/1168\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m127.82381\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 001 | loss: 127.82381 - R2: 0.0036 -- iter: 1152/1168\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m126.87914\u001b[0m\u001b[0m | time: 1.112s\n",
      "| SGD | epoch: 001 | loss: 126.87914 - R2: 0.0040 | val_loss: 123.46846 - val_acc: 0.0061 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m125.28340\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 002 | loss: 125.28340 - R2: 0.0050 -- iter: 0064/1168\n",
      "Training Step: 21  | total loss: \u001b[1m\u001b[32m123.98079\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 002 | loss: 123.98079 - R2: 0.0056 -- iter: 0128/1168\n",
      "Training Step: 22  | total loss: \u001b[1m\u001b[32m123.98079\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 002 | loss: 123.98079 - R2: 0.0062 -- iter: 0192/1168\n",
      "Training Step: 23  | total loss: \u001b[1m\u001b[32m122.61053\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 002 | loss: 122.61053 - R2: 0.0062 -- iter: 0256/1168\n",
      "Training Step: 24  | total loss: \u001b[1m\u001b[32m121.92407\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 002 | loss: 121.92407 - R2: 0.0067 -- iter: 0320/1168\n",
      "Training Step: 25  | total loss: \u001b[1m\u001b[32m120.02494\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 002 | loss: 120.02494 - R2: 0.0080 -- iter: 0384/1168\n",
      "Training Step: 26  | total loss: \u001b[1m\u001b[32m120.02494\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 002 | loss: 120.02494 - R2: 0.0080 -- iter: 0448/1168\n",
      "Training Step: 27  | total loss: \u001b[1m\u001b[32m119.08933\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 002 | loss: 119.08933 - R2: 0.0087 -- iter: 0512/1168\n",
      "Training Step: 28  | total loss: \u001b[1m\u001b[32m118.05983\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 002 | loss: 118.05983 - R2: 0.0094 -- iter: 0576/1168\n",
      "Training Step: 29  | total loss: \u001b[1m\u001b[32m117.24723\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 002 | loss: 117.24723 - R2: 0.0101 -- iter: 0640/1168\n",
      "Training Step: 30  | total loss: \u001b[1m\u001b[32m116.41953\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 002 | loss: 116.41953 - R2: 0.0108 -- iter: 0704/1168\n",
      "Training Step: 31  | total loss: \u001b[1m\u001b[32m114.55311\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 002 | loss: 114.55311 - R2: 0.0124 -- iter: 0768/1168\n",
      "Training Step: 32  | total loss: \u001b[1m\u001b[32m113.56076\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 002 | loss: 113.56076 - R2: 0.0133 -- iter: 0832/1168\n",
      "Training Step: 33  | total loss: \u001b[1m\u001b[32m113.56076\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 002 | loss: 113.56076 - R2: 0.0133 -- iter: 0896/1168\n",
      "Training Step: 34  | total loss: \u001b[1m\u001b[32m112.60392\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 002 | loss: 112.60392 - R2: 0.0141 -- iter: 0960/1168\n",
      "Training Step: 35  | total loss: \u001b[1m\u001b[32m110.43378\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 002 | loss: 110.43378 - R2: 0.0160 -- iter: 1024/1168\n",
      "Training Step: 36  | total loss: \u001b[1m\u001b[32m109.67123\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 002 | loss: 109.67123 - R2: 0.0169 -- iter: 1088/1168\n",
      "Training Step: 37  | total loss: \u001b[1m\u001b[32m109.67123\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 002 | loss: 109.67123 - R2: 0.0169 -- iter: 1152/1168\n",
      "Training Step: 38  | total loss: \u001b[1m\u001b[32m108.84864\u001b[0m\u001b[0m | time: 1.136s\n",
      "| SGD | epoch: 002 | loss: 108.84864 - R2: 0.0179 | val_loss: 103.56575 - val_acc: 0.0243 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 39  | total loss: \u001b[1m\u001b[32m107.86915\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 003 | loss: 107.86915 - R2: 0.0198 -- iter: 0064/1168\n",
      "Training Step: 40  | total loss: \u001b[1m\u001b[32m107.86915\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 003 | loss: 107.86915 - R2: 0.0198 -- iter: 0128/1168\n",
      "Training Step: 41  | total loss: \u001b[1m\u001b[32m107.52617\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 003 | loss: 107.52617 - R2: 0.0207 -- iter: 0192/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 42  | total loss: \u001b[1m\u001b[32m106.20991\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 003 | loss: 106.20991 - R2: 0.0219 -- iter: 0256/1168\n",
      "Training Step: 43  | total loss: \u001b[1m\u001b[32m103.97104\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 003 | loss: 103.97104 - R2: 0.0242 -- iter: 0320/1168\n",
      "Training Step: 44  | total loss: \u001b[1m\u001b[32m103.97104\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 003 | loss: 103.97104 - R2: 0.0242 -- iter: 0384/1168\n",
      "Training Step: 45  | total loss: \u001b[1m\u001b[32m102.86520\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 003 | loss: 102.86520 - R2: 0.0255 -- iter: 0448/1168\n",
      "Training Step: 46  | total loss: \u001b[1m\u001b[32m100.74028\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 003 | loss: 100.74028 - R2: 0.0280 -- iter: 0512/1168\n",
      "Training Step: 47  | total loss: \u001b[1m\u001b[32m99.62471\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 003 | loss: 99.62471 - R2: 0.0293 -- iter: 0576/1168\n",
      "Training Step: 48  | total loss: \u001b[1m\u001b[32m98.63583\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 003 | loss: 98.63583 - R2: 0.0307 -- iter: 0640/1168\n",
      "Training Step: 49  | total loss: \u001b[1m\u001b[32m97.56651\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 003 | loss: 97.56651 - R2: 0.0321 -- iter: 0704/1168\n",
      "Training Step: 50  | total loss: \u001b[1m\u001b[32m97.56651\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 003 | loss: 97.56651 - R2: 0.0321 -- iter: 0768/1168\n",
      "Training Step: 51  | total loss: \u001b[1m\u001b[32m96.60375\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 003 | loss: 96.60375 - R2: 0.0334 -- iter: 0832/1168\n",
      "Training Step: 52  | total loss: \u001b[1m\u001b[32m95.98019\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 003 | loss: 95.98019 - R2: 0.0362 -- iter: 0896/1168\n",
      "Training Step: 53  | total loss: \u001b[1m\u001b[32m94.96152\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 003 | loss: 94.96152 - R2: 0.0376 -- iter: 0960/1168\n",
      "Training Step: 54  | total loss: \u001b[1m\u001b[32m94.28220\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 003 | loss: 94.28220 - R2: 0.0376 -- iter: 1024/1168\n",
      "Training Step: 55  | total loss: \u001b[1m\u001b[32m93.51151\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 003 | loss: 93.51151 - R2: 0.0391 -- iter: 1088/1168\n",
      "Training Step: 56  | total loss: \u001b[1m\u001b[32m91.33524\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 003 | loss: 91.33524 - R2: 0.0408 -- iter: 1152/1168\n",
      "Training Step: 57  | total loss: \u001b[1m\u001b[32m91.33524\u001b[0m\u001b[0m | time: 1.140s\n",
      "| SGD | epoch: 003 | loss: 91.33524 - R2: 0.0424 | val_loss: 84.72470 - val_acc: 0.0560 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 58  | total loss: \u001b[1m\u001b[32m90.34709\u001b[0m\u001b[0m | time: 0.010s\n",
      "| SGD | epoch: 004 | loss: 90.34709 - R2: 0.0441 -- iter: 0064/1168\n",
      "Training Step: 59  | total loss: \u001b[1m\u001b[32m88.60370\u001b[0m\u001b[0m | time: 0.014s\n",
      "| SGD | epoch: 004 | loss: 88.60370 - R2: 0.0475 -- iter: 0128/1168\n",
      "Training Step: 60  | total loss: \u001b[1m\u001b[32m87.45850\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 004 | loss: 87.45850 - R2: 0.0475 -- iter: 0192/1168\n",
      "Training Step: 61  | total loss: \u001b[1m\u001b[32m87.45850\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 004 | loss: 87.45850 - R2: 0.0494 -- iter: 0256/1168\n",
      "Training Step: 62  | total loss: \u001b[1m\u001b[32m86.67239\u001b[0m\u001b[0m | time: 0.026s\n",
      "| SGD | epoch: 004 | loss: 86.67239 - R2: 0.0511 -- iter: 0320/1168\n",
      "Training Step: 63  | total loss: \u001b[1m\u001b[32m85.37389\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 004 | loss: 85.37389 - R2: 0.0528 -- iter: 0384/1168\n",
      "Training Step: 64  | total loss: \u001b[1m\u001b[32m85.37389\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 004 | loss: 85.37389 - R2: 0.0545 -- iter: 0448/1168\n",
      "Training Step: 65  | total loss: \u001b[1m\u001b[32m84.44028\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 004 | loss: 84.44028 - R2: 0.0565 -- iter: 0512/1168\n",
      "Training Step: 66  | total loss: \u001b[1m\u001b[32m82.79901\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 004 | loss: 82.79901 - R2: 0.0604 -- iter: 0576/1168\n",
      "Training Step: 67  | total loss: \u001b[1m\u001b[32m82.79901\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 004 | loss: 82.79901 - R2: 0.0604 -- iter: 0640/1168\n",
      "Training Step: 68  | total loss: \u001b[1m\u001b[32m81.75833\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 004 | loss: 81.75833 - R2: 0.0626 -- iter: 0704/1168\n",
      "Training Step: 69  | total loss: \u001b[1m\u001b[32m80.92883\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 004 | loss: 80.92883 - R2: 0.0647 -- iter: 0768/1168\n",
      "Training Step: 70  | total loss: \u001b[1m\u001b[32m78.83769\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 004 | loss: 78.83769 - R2: 0.0694 -- iter: 0832/1168\n",
      "Training Step: 71  | total loss: \u001b[1m\u001b[32m78.83769\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 004 | loss: 78.83769 - R2: 0.0694 -- iter: 0896/1168\n",
      "Training Step: 72  | total loss: \u001b[1m\u001b[32m77.04272\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 004 | loss: 77.04272 - R2: 0.0741 -- iter: 0960/1168\n",
      "Training Step: 73  | total loss: \u001b[1m\u001b[32m77.04272\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 004 | loss: 77.04272 - R2: 0.0766 -- iter: 1024/1168\n",
      "Training Step: 74  | total loss: \u001b[1m\u001b[32m75.22216\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 004 | loss: 75.22216 - R2: 0.0792 -- iter: 1088/1168\n",
      "Training Step: 75  | total loss: \u001b[1m\u001b[32m74.45641\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 004 | loss: 74.45641 - R2: 0.0816 -- iter: 1152/1168\n",
      "Training Step: 76  | total loss: \u001b[1m\u001b[32m74.45641\u001b[0m\u001b[0m | time: 1.077s\n",
      "| SGD | epoch: 004 | loss: 74.45641 - R2: 0.0843 | val_loss: 64.83634 - val_acc: 0.1104 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 77  | total loss: \u001b[1m\u001b[32m72.50926\u001b[0m\u001b[0m | time: 0.010s\n",
      "| SGD | epoch: 005 | loss: 72.50926 - R2: 0.0871 -- iter: 0064/1168\n",
      "Training Step: 78  | total loss: \u001b[1m\u001b[32m71.53815\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 005 | loss: 71.53815 - R2: 0.0900 -- iter: 0128/1168\n",
      "Training Step: 79  | total loss: \u001b[1m\u001b[32m71.53815\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 005 | loss: 71.53815 - R2: 0.0900 -- iter: 0192/1168\n",
      "Training Step: 80  | total loss: \u001b[1m\u001b[32m70.56699\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 005 | loss: 70.56699 - R2: 0.0930 -- iter: 0256/1168\n",
      "Training Step: 81  | total loss: \u001b[1m\u001b[32m69.58868\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 005 | loss: 69.58868 - R2: 0.0961 -- iter: 0320/1168\n",
      "Training Step: 82  | total loss: \u001b[1m\u001b[32m68.50794\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 005 | loss: 68.50794 - R2: 0.0994 -- iter: 0384/1168\n",
      "Training Step: 83  | total loss: \u001b[1m\u001b[32m66.48037\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 005 | loss: 66.48037 - R2: 0.1061 -- iter: 0448/1168\n",
      "Training Step: 84  | total loss: \u001b[1m\u001b[32m65.54919\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 005 | loss: 65.54919 - R2: 0.1095 -- iter: 0512/1168\n",
      "Training Step: 85  | total loss: \u001b[1m\u001b[32m65.54919\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 005 | loss: 65.54919 - R2: 0.1095 -- iter: 0576/1168\n",
      "Training Step: 86  | total loss: \u001b[1m\u001b[32m64.45188\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 005 | loss: 64.45188 - R2: 0.1132 -- iter: 0640/1168\n",
      "Training Step: 87  | total loss: \u001b[1m\u001b[32m63.56237\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 005 | loss: 63.56237 - R2: 0.1168 -- iter: 0704/1168\n",
      "Training Step: 88  | total loss: \u001b[1m\u001b[32m62.54139\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 005 | loss: 62.54139 - R2: 0.1207 -- iter: 0768/1168\n",
      "Training Step: 89  | total loss: \u001b[1m\u001b[32m60.26481\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 005 | loss: 60.26481 - R2: 0.1293 -- iter: 0832/1168\n",
      "Training Step: 90  | total loss: \u001b[1m\u001b[32m59.09664\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 005 | loss: 59.09664 - R2: 0.1339 -- iter: 0896/1168\n",
      "Training Step: 91  | total loss: \u001b[1m\u001b[32m58.05391\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 005 | loss: 58.05391 - R2: 0.1383 -- iter: 0960/1168\n",
      "Training Step: 92  | total loss: \u001b[1m\u001b[32m56.98909\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 005 | loss: 56.98909 - R2: 0.1430 -- iter: 1024/1168\n",
      "Training Step: 93  | total loss: \u001b[1m\u001b[32m55.82646\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 005 | loss: 55.82646 - R2: 0.1480 -- iter: 1088/1168\n",
      "Training Step: 94  | total loss: \u001b[1m\u001b[32m54.65376\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 005 | loss: 54.65376 - R2: 0.1532 -- iter: 1152/1168\n",
      "Training Step: 95  | total loss: \u001b[1m\u001b[32m54.65376\u001b[0m\u001b[0m | time: 1.104s\n",
      "| SGD | epoch: 005 | loss: 54.65376 - R2: 0.1589 | val_loss: 42.32710 - val_acc: 0.2123 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 96  | total loss: \u001b[1m\u001b[32m53.36522\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 006 | loss: 53.36522 - R2: 0.1589 -- iter: 0064/1168\n",
      "Training Step: 97  | total loss: \u001b[1m\u001b[32m51.16222\u001b[0m\u001b[0m | time: 0.007s\n",
      "| SGD | epoch: 006 | loss: 51.16222 - R2: 0.1697 -- iter: 0128/1168\n",
      "Training Step: 98  | total loss: \u001b[1m\u001b[32m50.07973\u001b[0m\u001b[0m | time: 0.017s\n",
      "| SGD | epoch: 006 | loss: 50.07973 - R2: 0.1754 -- iter: 0192/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 99  | total loss: \u001b[1m\u001b[32m48.87430\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 006 | loss: 48.87430 - R2: 0.1815 -- iter: 0256/1168\n",
      "Training Step: 100  | total loss: \u001b[1m\u001b[32m47.66395\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 006 | loss: 47.66395 - R2: 0.1880 -- iter: 0320/1168\n",
      "Training Step: 101  | total loss: \u001b[1m\u001b[32m46.44298\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 006 | loss: 46.44298 - R2: 0.1946 -- iter: 0384/1168\n",
      "Training Step: 102  | total loss: \u001b[1m\u001b[32m45.28674\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 006 | loss: 45.28674 - R2: 0.2012 -- iter: 0448/1168\n",
      "Training Step: 103  | total loss: \u001b[1m\u001b[32m45.28674\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 006 | loss: 45.28674 - R2: 0.2081 -- iter: 0512/1168\n",
      "Training Step: 104  | total loss: \u001b[1m\u001b[32m44.10451\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 006 | loss: 44.10451 - R2: 0.2081 -- iter: 0576/1168\n",
      "Training Step: 105  | total loss: \u001b[1m\u001b[32m42.95579\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 006 | loss: 42.95579 - R2: 0.2230 -- iter: 0640/1168\n",
      "Training Step: 106  | total loss: \u001b[1m\u001b[32m40.47351\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 006 | loss: 40.47351 - R2: 0.2307 -- iter: 0704/1168\n",
      "Training Step: 107  | total loss: \u001b[1m\u001b[32m39.28761\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 006 | loss: 39.28761 - R2: 0.2386 -- iter: 0768/1168\n",
      "Training Step: 108  | total loss: \u001b[1m\u001b[32m38.07886\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 006 | loss: 38.07886 - R2: 0.2468 -- iter: 0832/1168\n",
      "Training Step: 109  | total loss: \u001b[1m\u001b[32m38.07886\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 006 | loss: 38.07886 - R2: 0.2468 -- iter: 0896/1168\n",
      "Training Step: 110  | total loss: \u001b[1m\u001b[32m36.94825\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 006 | loss: 36.94825 - R2: 0.2639 -- iter: 0960/1168\n",
      "Training Step: 111  | total loss: \u001b[1m\u001b[32m34.57501\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 006 | loss: 34.57501 - R2: 0.2727 -- iter: 1024/1168\n",
      "Training Step: 112  | total loss: \u001b[1m\u001b[32m33.42311\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 006 | loss: 33.42311 - R2: 0.2819 -- iter: 1088/1168\n",
      "Training Step: 113  | total loss: \u001b[1m\u001b[32m33.42311\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 006 | loss: 33.42311 - R2: 0.2919 -- iter: 1152/1168\n",
      "Training Step: 114  | total loss: \u001b[1m\u001b[32m32.18166\u001b[0m\u001b[0m | time: 1.064s\n",
      "| SGD | epoch: 006 | loss: 32.18166 - R2: 0.2919 | val_loss: 19.49998 - val_acc: 0.4028 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 115  | total loss: \u001b[1m\u001b[32m29.91316\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 007 | loss: 29.91316 - R2: 0.3115 -- iter: 0064/1168\n",
      "Training Step: 116  | total loss: \u001b[1m\u001b[32m29.91316\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 007 | loss: 29.91316 - R2: 0.3115 -- iter: 0128/1168\n",
      "Training Step: 117  | total loss: \u001b[1m\u001b[32m28.80137\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 007 | loss: 28.80137 - R2: 0.3217 -- iter: 0192/1168\n",
      "Training Step: 118  | total loss: \u001b[1m\u001b[32m26.35248\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 007 | loss: 26.35248 - R2: 0.3452 -- iter: 0256/1168\n",
      "Training Step: 119  | total loss: \u001b[1m\u001b[32m25.23563\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 007 | loss: 25.23563 - R2: 0.3565 -- iter: 0320/1168\n",
      "Training Step: 120  | total loss: \u001b[1m\u001b[32m25.23563\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 007 | loss: 25.23563 - R2: 0.3565 -- iter: 0384/1168\n",
      "Training Step: 121  | total loss: \u001b[1m\u001b[32m23.03988\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 007 | loss: 23.03988 - R2: 0.3804 -- iter: 0448/1168\n",
      "Training Step: 122  | total loss: \u001b[1m\u001b[32m23.03988\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 007 | loss: 23.03988 - R2: 0.3804 -- iter: 0512/1168\n",
      "Training Step: 123  | total loss: \u001b[1m\u001b[32m20.78636\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 007 | loss: 20.78636 - R2: 0.4073 -- iter: 0576/1168\n",
      "Training Step: 124  | total loss: \u001b[1m\u001b[32m20.78636\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 007 | loss: 20.78636 - R2: 0.4073 -- iter: 0640/1168\n",
      "Training Step: 125  | total loss: \u001b[1m\u001b[32m18.66745\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 007 | loss: 18.66745 - R2: 0.4350 -- iter: 0704/1168\n",
      "Training Step: 126  | total loss: \u001b[1m\u001b[32m17.56453\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 007 | loss: 17.56453 - R2: 0.4506 -- iter: 0768/1168\n",
      "Training Step: 127  | total loss: \u001b[1m\u001b[32m17.56453\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 007 | loss: 17.56453 - R2: 0.4506 -- iter: 0832/1168\n",
      "Training Step: 128  | total loss: \u001b[1m\u001b[32m15.56164\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 007 | loss: 15.56164 - R2: 0.4810 -- iter: 0896/1168\n",
      "Training Step: 129  | total loss: \u001b[1m\u001b[32m14.62248\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 007 | loss: 14.62248 - R2: 0.4964 -- iter: 0960/1168\n",
      "Training Step: 130  | total loss: \u001b[1m\u001b[32m14.62248\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 007 | loss: 14.62248 - R2: 0.4964 -- iter: 1024/1168\n",
      "Training Step: 131  | total loss: \u001b[1m\u001b[32m13.69563\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 007 | loss: 13.69563 - R2: 0.5124 -- iter: 1088/1168\n",
      "Training Step: 132  | total loss: \u001b[1m\u001b[32m11.91745\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 007 | loss: 11.91745 - R2: 0.5287 -- iter: 1152/1168\n",
      "Training Step: 133  | total loss: \u001b[1m\u001b[32m11.91745\u001b[0m\u001b[0m | time: 1.100s\n",
      "| SGD | epoch: 007 | loss: 11.91745 - R2: 0.5460 | val_loss: 2.87283 - val_acc: 0.7441 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 134  | total loss: \u001b[1m\u001b[32m10.24379\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 008 | loss: 10.24379 - R2: 0.5818 -- iter: 0064/1168\n",
      "Training Step: 135  | total loss: \u001b[1m\u001b[32m10.24379\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 008 | loss: 10.24379 - R2: 0.5818 -- iter: 0128/1168\n",
      "Training Step: 136  | total loss: \u001b[1m\u001b[32m9.41529\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 008 | loss: 9.41529 - R2: 0.6022 -- iter: 0192/1168\n",
      "Training Step: 137  | total loss: \u001b[1m\u001b[32m8.65200\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 008 | loss: 8.65200 - R2: 0.6435 -- iter: 0256/1168\n",
      "Training Step: 138  | total loss: \u001b[1m\u001b[32m7.90246\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 008 | loss: 7.90246 - R2: 0.6435 -- iter: 0320/1168\n",
      "Training Step: 139  | total loss: \u001b[1m\u001b[32m6.55628\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 008 | loss: 6.55628 - R2: 0.6850 -- iter: 0384/1168\n",
      "Training Step: 140  | total loss: \u001b[1m\u001b[32m6.55628\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 008 | loss: 6.55628 - R2: 0.6850 -- iter: 0448/1168\n",
      "Training Step: 141  | total loss: \u001b[1m\u001b[32m5.95076\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 008 | loss: 5.95076 - R2: 0.7279 -- iter: 0512/1168\n",
      "Training Step: 142  | total loss: \u001b[1m\u001b[32m4.86960\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 008 | loss: 4.86960 - R2: 0.7511 -- iter: 0576/1168\n",
      "Training Step: 143  | total loss: \u001b[1m\u001b[32m4.40468\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 008 | loss: 4.40468 - R2: 0.7729 -- iter: 0640/1168\n",
      "Training Step: 144  | total loss: \u001b[1m\u001b[32m4.40468\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 008 | loss: 4.40468 - R2: 0.7729 -- iter: 0704/1168\n",
      "Training Step: 145  | total loss: \u001b[1m\u001b[32m3.98620\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 008 | loss: 3.98620 - R2: 0.7932 -- iter: 0768/1168\n",
      "Training Step: 146  | total loss: \u001b[1m\u001b[32m3.24466\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 008 | loss: 3.24466 - R2: 0.8301 -- iter: 0832/1168\n",
      "Training Step: 147  | total loss: \u001b[1m\u001b[32m2.93972\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 008 | loss: 2.93972 - R2: 0.8475 -- iter: 0896/1168\n",
      "Training Step: 148  | total loss: \u001b[1m\u001b[32m2.93972\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 008 | loss: 2.93972 - R2: 0.8475 -- iter: 0960/1168\n",
      "Training Step: 149  | total loss: \u001b[1m\u001b[32m2.65446\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 008 | loss: 2.65446 - R2: 0.8619 -- iter: 1024/1168\n",
      "Training Step: 150  | total loss: \u001b[1m\u001b[32m2.39626\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 008 | loss: 2.39626 - R2: 0.8757 -- iter: 1088/1168\n",
      "Training Step: 151  | total loss: \u001b[1m\u001b[32m1.95578\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 008 | loss: 1.95578 - R2: 0.8986 -- iter: 1152/1168\n",
      "Training Step: 152  | total loss: \u001b[1m\u001b[32m1.95578\u001b[0m\u001b[0m | time: 1.160s\n",
      "| SGD | epoch: 008 | loss: 1.95578 - R2: 0.8986 | val_loss: 0.07887 - val_acc: 0.9923 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 153  | total loss: \u001b[1m\u001b[32m1.77065\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 009 | loss: 1.77065 - R2: 0.9088 -- iter: 0064/1168\n",
      "Training Step: 154  | total loss: \u001b[1m\u001b[32m1.60122\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 009 | loss: 1.60122 - R2: 0.9171 -- iter: 0128/1168\n",
      "Training Step: 155  | total loss: \u001b[1m\u001b[32m1.45053\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 009 | loss: 1.45053 - R2: 0.9262 -- iter: 0192/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 156  | total loss: \u001b[1m\u001b[32m1.31199\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 009 | loss: 1.31199 - R2: 0.9332 -- iter: 0256/1168\n",
      "Training Step: 157  | total loss: \u001b[1m\u001b[32m1.18835\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 009 | loss: 1.18835 - R2: 0.9399 -- iter: 0320/1168\n",
      "Training Step: 158  | total loss: \u001b[1m\u001b[32m1.07782\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 009 | loss: 1.07782 - R2: 0.9443 -- iter: 0384/1168\n",
      "Training Step: 159  | total loss: \u001b[1m\u001b[32m0.97596\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 009 | loss: 0.97596 - R2: 0.9548 -- iter: 0448/1168\n",
      "Training Step: 160  | total loss: \u001b[1m\u001b[32m0.88095\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 009 | loss: 0.88095 - R2: 0.9548 -- iter: 0512/1168\n",
      "Training Step: 161  | total loss: \u001b[1m\u001b[32m0.79489\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 009 | loss: 0.79489 - R2: 0.9590 -- iter: 0576/1168\n",
      "Training Step: 162  | total loss: \u001b[1m\u001b[32m0.65455\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 009 | loss: 0.65455 - R2: 0.9680 -- iter: 0640/1168\n",
      "Training Step: 163  | total loss: \u001b[1m\u001b[32m0.65455\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 009 | loss: 0.65455 - R2: 0.9680 -- iter: 0704/1168\n",
      "Training Step: 164  | total loss: \u001b[1m\u001b[32m0.59511\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 009 | loss: 0.59511 - R2: 0.9712 -- iter: 0768/1168\n",
      "Training Step: 165  | total loss: \u001b[1m\u001b[32m0.54127\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 009 | loss: 0.54127 - R2: 0.9749 -- iter: 0832/1168\n",
      "Training Step: 166  | total loss: \u001b[1m\u001b[32m0.49246\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 009 | loss: 0.49246 - R2: 0.9774 -- iter: 0896/1168\n",
      "Training Step: 167  | total loss: \u001b[1m\u001b[32m0.40564\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 009 | loss: 0.40564 - R2: 0.9814 -- iter: 0960/1168\n",
      "Training Step: 168  | total loss: \u001b[1m\u001b[32m0.40564\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 009 | loss: 0.40564 - R2: 0.9814 -- iter: 1024/1168\n",
      "Training Step: 169  | total loss: \u001b[1m\u001b[32m0.36938\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 009 | loss: 0.36938 - R2: 0.9831 -- iter: 1088/1168\n",
      "Training Step: 170  | total loss: \u001b[1m\u001b[32m0.30520\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 009 | loss: 0.30520 - R2: 0.9867 -- iter: 1152/1168\n",
      "Training Step: 171  | total loss: \u001b[1m\u001b[32m0.30520\u001b[0m\u001b[0m | time: 1.123s\n",
      "| SGD | epoch: 009 | loss: 0.30520 - R2: 0.9867 | val_loss: 0.03629 - val_acc: 0.9972 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 172  | total loss: \u001b[1m\u001b[32m0.28055\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 010 | loss: 0.28055 - R2: 0.9878 -- iter: 0064/1168\n",
      "Training Step: 173  | total loss: \u001b[1m\u001b[32m0.25738\u001b[0m\u001b[0m | time: 0.026s\n",
      "| SGD | epoch: 010 | loss: 0.25738 - R2: 0.9905 -- iter: 0128/1168\n",
      "Training Step: 174  | total loss: \u001b[1m\u001b[32m0.21248\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 010 | loss: 0.21248 - R2: 0.9905 -- iter: 0192/1168\n",
      "Training Step: 175  | total loss: \u001b[1m\u001b[32m0.19445\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 010 | loss: 0.19445 - R2: 0.9924 -- iter: 0256/1168\n",
      "Training Step: 176  | total loss: \u001b[1m\u001b[32m0.17768\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 010 | loss: 0.17768 - R2: 0.9927 -- iter: 0320/1168\n",
      "Training Step: 177  | total loss: \u001b[1m\u001b[32m0.17768\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 010 | loss: 0.17768 - R2: 0.9928 -- iter: 0384/1168\n",
      "Training Step: 178  | total loss: \u001b[1m\u001b[32m0.14980\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 010 | loss: 0.14980 - R2: 0.9934 -- iter: 0448/1168\n",
      "Training Step: 179  | total loss: \u001b[1m\u001b[32m0.14980\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 010 | loss: 0.14980 - R2: 0.9957 -- iter: 0512/1168\n",
      "Training Step: 180  | total loss: \u001b[1m\u001b[32m0.13886\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 010 | loss: 0.13886 - R2: 0.9957 -- iter: 0576/1168\n",
      "Training Step: 181  | total loss: \u001b[1m\u001b[32m0.11795\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 010 | loss: 0.11795 - R2: 0.9965 -- iter: 0640/1168\n",
      "Training Step: 182  | total loss: \u001b[1m\u001b[32m0.10999\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 010 | loss: 0.10999 - R2: 0.9965 -- iter: 0704/1168\n",
      "Training Step: 183  | total loss: \u001b[1m\u001b[32m0.10999\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 010 | loss: 0.10999 - R2: 0.9965 -- iter: 0768/1168\n",
      "Training Step: 184  | total loss: \u001b[1m\u001b[32m0.09536\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 010 | loss: 0.09536 - R2: 0.9974 -- iter: 0832/1168\n",
      "Training Step: 185  | total loss: \u001b[1m\u001b[32m0.08810\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 010 | loss: 0.08810 - R2: 0.9970 -- iter: 0896/1168\n",
      "Training Step: 186  | total loss: \u001b[1m\u001b[32m0.08176\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 010 | loss: 0.08176 - R2: 0.9973 -- iter: 0960/1168\n",
      "Training Step: 187  | total loss: \u001b[1m\u001b[32m0.07562\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 010 | loss: 0.07562 - R2: 0.9978 -- iter: 1024/1168\n",
      "Training Step: 188  | total loss: \u001b[1m\u001b[32m0.06981\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 010 | loss: 0.06981 - R2: 0.9974 -- iter: 1088/1168\n",
      "Training Step: 189  | total loss: \u001b[1m\u001b[32m0.06981\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 010 | loss: 0.06981 - R2: 0.9978 -- iter: 1152/1168\n",
      "Training Step: 190  | total loss: \u001b[1m\u001b[32m0.06160\u001b[0m\u001b[0m | time: 1.104s\n",
      "| SGD | epoch: 010 | loss: 0.06160 - R2: 0.9985 | val_loss: 0.02678 - val_acc: 0.9981 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 191  | total loss: \u001b[1m\u001b[32m0.05950\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 011 | loss: 0.05950 - R2: 0.9987 -- iter: 0064/1168\n",
      "Training Step: 192  | total loss: \u001b[1m\u001b[32m0.05950\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 011 | loss: 0.05950 - R2: 0.9987 -- iter: 0128/1168\n",
      "Training Step: 193  | total loss: \u001b[1m\u001b[32m0.05283\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 011 | loss: 0.05283 - R2: 0.9984 -- iter: 0192/1168\n",
      "Training Step: 194  | total loss: \u001b[1m\u001b[32m0.05283\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 011 | loss: 0.05283 - R2: 0.9984 -- iter: 0256/1168\n",
      "Training Step: 195  | total loss: \u001b[1m\u001b[32m0.04934\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 011 | loss: 0.04934 - R2: 0.9983 -- iter: 0320/1168\n",
      "Training Step: 196  | total loss: \u001b[1m\u001b[32m0.04687\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 011 | loss: 0.04687 - R2: 0.9987 -- iter: 0384/1168\n",
      "Training Step: 197  | total loss: \u001b[1m\u001b[32m0.04088\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 011 | loss: 0.04088 - R2: 0.9985 -- iter: 0448/1168\n",
      "Training Step: 198  | total loss: \u001b[1m\u001b[32m0.04088\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 011 | loss: 0.04088 - R2: 0.9985 -- iter: 0512/1168\n",
      "Training Step: 199  | total loss: \u001b[1m\u001b[32m0.03928\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 011 | loss: 0.03928 - R2: 0.9995 -- iter: 0576/1168\n",
      "Training Step: 200  | total loss: \u001b[1m\u001b[32m0.03790\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 011 | loss: 0.03790 - R2: 1.0000 -- iter: 0640/1168\n",
      "Training Step: 201  | total loss: \u001b[1m\u001b[32m0.03362\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 011 | loss: 0.03362 - R2: 0.9998 -- iter: 0704/1168\n",
      "Training Step: 202  | total loss: \u001b[1m\u001b[32m0.03193\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 011 | loss: 0.03193 - R2: 0.9999 -- iter: 0768/1168\n",
      "Training Step: 203  | total loss: \u001b[1m\u001b[32m0.03047\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 011 | loss: 0.03047 - R2: 0.9997 -- iter: 0832/1168\n",
      "Training Step: 204  | total loss: \u001b[1m\u001b[32m0.03047\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 011 | loss: 0.03047 - R2: 0.9997 -- iter: 0896/1168\n",
      "Training Step: 205  | total loss: \u001b[1m\u001b[32m0.03148\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 011 | loss: 0.03148 - R2: 1.0006 -- iter: 0960/1168\n",
      "Training Step: 206  | total loss: \u001b[1m\u001b[32m0.02976\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 011 | loss: 0.02976 - R2: 1.0004 -- iter: 1024/1168\n",
      "Training Step: 207  | total loss: \u001b[1m\u001b[32m0.02976\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 011 | loss: 0.02976 - R2: 1.0004 -- iter: 1088/1168\n",
      "Training Step: 208  | total loss: \u001b[1m\u001b[32m0.06058\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 011 | loss: 0.06058 - R2: 0.9992 -- iter: 1152/1168\n",
      "Training Step: 209  | total loss: \u001b[1m\u001b[32m0.05275\u001b[0m\u001b[0m | time: 1.150s\n",
      "| SGD | epoch: 011 | loss: 0.05275 - R2: 0.9990 | val_loss: 0.02580 - val_acc: 1.0005 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 210  | total loss: \u001b[1m\u001b[32m0.05077\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 012 | loss: 0.05077 - R2: 0.9995 -- iter: 0064/1168\n",
      "Training Step: 211  | total loss: \u001b[1m\u001b[32m0.05077\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 012 | loss: 0.05077 - R2: 0.9995 -- iter: 0128/1168\n",
      "Training Step: 212  | total loss: \u001b[1m\u001b[32m0.04752\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 012 | loss: 0.04752 - R2: 0.9994 -- iter: 0192/1168\n",
      "Training Step: 213  | total loss: \u001b[1m\u001b[32m0.04266\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 012 | loss: 0.04266 - R2: 0.9993 -- iter: 0256/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 214  | total loss: \u001b[1m\u001b[32m0.03913\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 012 | loss: 0.03913 - R2: 0.9996 -- iter: 0320/1168\n",
      "Training Step: 215  | total loss: \u001b[1m\u001b[32m0.03655\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 012 | loss: 0.03655 - R2: 0.9998 -- iter: 0384/1168\n",
      "Training Step: 216  | total loss: \u001b[1m\u001b[32m0.03571\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 012 | loss: 0.03571 - R2: 0.9996 -- iter: 0448/1168\n",
      "Training Step: 217  | total loss: \u001b[1m\u001b[32m0.03374\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 012 | loss: 0.03374 - R2: 0.9996 -- iter: 0512/1168\n",
      "Training Step: 218  | total loss: \u001b[1m\u001b[32m0.03374\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 012 | loss: 0.03374 - R2: 0.9996 -- iter: 0576/1168\n",
      "Training Step: 219  | total loss: \u001b[1m\u001b[32m0.03100\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 012 | loss: 0.03100 - R2: 1.0002 -- iter: 0640/1168\n",
      "Training Step: 220  | total loss: \u001b[1m\u001b[32m0.02898\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 012 | loss: 0.02898 - R2: 1.0004 -- iter: 0704/1168\n",
      "Training Step: 221  | total loss: \u001b[1m\u001b[32m0.02777\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 012 | loss: 0.02777 - R2: 1.0002 -- iter: 0768/1168\n",
      "Training Step: 222  | total loss: \u001b[1m\u001b[32m0.02732\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 012 | loss: 0.02732 - R2: 1.0000 -- iter: 0832/1168\n",
      "Training Step: 223  | total loss: \u001b[1m\u001b[32m0.02731\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 012 | loss: 0.02731 - R2: 1.0000 -- iter: 0896/1168\n",
      "Training Step: 224  | total loss: \u001b[1m\u001b[32m0.02712\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 012 | loss: 0.02712 - R2: 1.0001 -- iter: 0960/1168\n",
      "Training Step: 225  | total loss: \u001b[1m\u001b[32m0.02712\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 012 | loss: 0.02712 - R2: 1.0001 -- iter: 1024/1168\n",
      "Training Step: 226  | total loss: \u001b[1m\u001b[32m0.02511\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 012 | loss: 0.02511 - R2: 0.9999 -- iter: 1088/1168\n",
      "Training Step: 227  | total loss: \u001b[1m\u001b[32m0.04994\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 012 | loss: 0.04994 - R2: 1.0024 -- iter: 1152/1168\n",
      "Training Step: 228  | total loss: \u001b[1m\u001b[32m0.04716\u001b[0m\u001b[0m | time: 1.142s\n",
      "| SGD | epoch: 012 | loss: 0.04716 - R2: 1.0009 | val_loss: 0.02673 - val_acc: 0.9946 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 229  | total loss: \u001b[1m\u001b[32m0.04384\u001b[0m\u001b[0m | time: 0.010s\n",
      "| SGD | epoch: 013 | loss: 0.04384 - R2: 1.0005 -- iter: 0064/1168\n",
      "Training Step: 230  | total loss: \u001b[1m\u001b[32m0.04384\u001b[0m\u001b[0m | time: 0.012s\n",
      "| SGD | epoch: 013 | loss: 0.04384 - R2: 1.0005 -- iter: 0128/1168\n",
      "Training Step: 231  | total loss: \u001b[1m\u001b[32m0.04305\u001b[0m\u001b[0m | time: 0.016s\n",
      "| SGD | epoch: 013 | loss: 0.04305 - R2: 1.0001 -- iter: 0192/1168\n",
      "Training Step: 232  | total loss: \u001b[1m\u001b[32m0.04103\u001b[0m\u001b[0m | time: 0.018s\n",
      "| SGD | epoch: 013 | loss: 0.04103 - R2: 0.9995 -- iter: 0256/1168\n",
      "Training Step: 233  | total loss: \u001b[1m\u001b[32m0.03765\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 013 | loss: 0.03765 - R2: 0.9988 -- iter: 0320/1168\n",
      "Training Step: 234  | total loss: \u001b[1m\u001b[32m0.03765\u001b[0m\u001b[0m | time: 0.026s\n",
      "| SGD | epoch: 013 | loss: 0.03765 - R2: 0.9988 -- iter: 0384/1168\n",
      "Training Step: 235  | total loss: \u001b[1m\u001b[32m0.03309\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 013 | loss: 0.03309 - R2: 0.9987 -- iter: 0448/1168\n",
      "Training Step: 236  | total loss: \u001b[1m\u001b[32m0.03360\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 013 | loss: 0.03360 - R2: 0.9999 -- iter: 0512/1168\n",
      "Training Step: 237  | total loss: \u001b[1m\u001b[32m0.03360\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 013 | loss: 0.03360 - R2: 0.9999 -- iter: 0576/1168\n",
      "Training Step: 238  | total loss: \u001b[1m\u001b[32m0.03188\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 013 | loss: 0.03188 - R2: 0.9995 -- iter: 0640/1168\n",
      "Training Step: 239  | total loss: \u001b[1m\u001b[32m0.03253\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 013 | loss: 0.03253 - R2: 1.0004 -- iter: 0704/1168\n",
      "Training Step: 240  | total loss: \u001b[1m\u001b[32m0.03405\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 013 | loss: 0.03405 - R2: 1.0006 -- iter: 0768/1168\n",
      "Training Step: 241  | total loss: \u001b[1m\u001b[32m0.03405\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 013 | loss: 0.03405 - R2: 1.0001 -- iter: 0832/1168\n",
      "Training Step: 242  | total loss: \u001b[1m\u001b[32m0.03199\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 013 | loss: 0.03199 - R2: 1.0001 -- iter: 0896/1168\n",
      "Training Step: 243  | total loss: \u001b[1m\u001b[32m0.02899\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 013 | loss: 0.02899 - R2: 0.9999 -- iter: 0960/1168\n",
      "Training Step: 244  | total loss: \u001b[1m\u001b[32m0.02899\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 013 | loss: 0.02899 - R2: 0.9999 -- iter: 1024/1168\n",
      "Training Step: 245  | total loss: \u001b[1m\u001b[32m0.02750\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 013 | loss: 0.02750 - R2: 0.9996 -- iter: 1088/1168\n",
      "Training Step: 246  | total loss: \u001b[1m\u001b[32m0.02585\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 013 | loss: 0.02585 - R2: 0.9996 -- iter: 1152/1168\n",
      "Training Step: 247  | total loss: \u001b[1m\u001b[32m0.02610\u001b[0m\u001b[0m | time: 1.069s\n",
      "| SGD | epoch: 013 | loss: 0.02610 - R2: 0.9999 | val_loss: 0.02681 - val_acc: 1.0006 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 248  | total loss: \u001b[1m\u001b[32m0.05716\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 014 | loss: 0.05716 - R2: 0.9989 -- iter: 0064/1168\n",
      "Training Step: 249  | total loss: \u001b[1m\u001b[32m0.05379\u001b[0m\u001b[0m | time: 0.008s\n",
      "| SGD | epoch: 014 | loss: 0.05379 - R2: 0.9990 -- iter: 0128/1168\n",
      "Training Step: 250  | total loss: \u001b[1m\u001b[32m0.05020\u001b[0m\u001b[0m | time: 0.014s\n",
      "| SGD | epoch: 014 | loss: 0.05020 - R2: 0.9988 -- iter: 0192/1168\n",
      "Training Step: 251  | total loss: \u001b[1m\u001b[32m0.04760\u001b[0m\u001b[0m | time: 0.017s\n",
      "| SGD | epoch: 014 | loss: 0.04760 - R2: 0.9993 -- iter: 0256/1168\n",
      "Training Step: 252  | total loss: \u001b[1m\u001b[32m0.04491\u001b[0m\u001b[0m | time: 0.020s\n",
      "| SGD | epoch: 014 | loss: 0.04491 - R2: 0.9996 -- iter: 0320/1168\n",
      "Training Step: 253  | total loss: \u001b[1m\u001b[32m0.04208\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 014 | loss: 0.04208 - R2: 0.9992 -- iter: 0384/1168\n",
      "Training Step: 254  | total loss: \u001b[1m\u001b[32m0.04006\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 014 | loss: 0.04006 - R2: 0.9995 -- iter: 0448/1168\n",
      "Training Step: 255  | total loss: \u001b[1m\u001b[32m0.03808\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 014 | loss: 0.03808 - R2: 1.0003 -- iter: 0512/1168\n",
      "Training Step: 256  | total loss: \u001b[1m\u001b[32m0.03731\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 014 | loss: 0.03731 - R2: 1.0002 -- iter: 0576/1168\n",
      "Training Step: 257  | total loss: \u001b[1m\u001b[32m0.03667\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 014 | loss: 0.03667 - R2: 1.0002 -- iter: 0640/1168\n",
      "Training Step: 258  | total loss: \u001b[1m\u001b[32m0.03232\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 014 | loss: 0.03232 - R2: 1.0002 -- iter: 0704/1168\n",
      "Training Step: 259  | total loss: \u001b[1m\u001b[32m0.03104\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 014 | loss: 0.03104 - R2: 0.9995 -- iter: 0768/1168\n",
      "Training Step: 260  | total loss: \u001b[1m\u001b[32m0.03104\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 014 | loss: 0.03104 - R2: 0.9995 -- iter: 0832/1168\n",
      "Training Step: 261  | total loss: \u001b[1m\u001b[32m0.02843\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 014 | loss: 0.02843 - R2: 0.9991 -- iter: 0896/1168\n",
      "Training Step: 262  | total loss: \u001b[1m\u001b[32m0.02843\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 014 | loss: 0.02843 - R2: 0.9991 -- iter: 0960/1168\n",
      "Training Step: 263  | total loss: \u001b[1m\u001b[32m0.02681\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 014 | loss: 0.02681 - R2: 0.9993 -- iter: 1024/1168\n",
      "Training Step: 264  | total loss: \u001b[1m\u001b[32m0.02681\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 014 | loss: 0.02681 - R2: 0.9993 -- iter: 1088/1168\n",
      "Training Step: 265  | total loss: \u001b[1m\u001b[32m0.02618\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 014 | loss: 0.02618 - R2: 0.9996 -- iter: 1152/1168\n",
      "Training Step: 266  | total loss: \u001b[1m\u001b[32m0.02453\u001b[0m\u001b[0m | time: 1.063s\n",
      "| SGD | epoch: 014 | loss: 0.02453 - R2: 0.9996 | val_loss: 0.02707 - val_acc: 1.0014 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 267  | total loss: \u001b[1m\u001b[32m0.05402\u001b[0m\u001b[0m | time: 0.010s\n",
      "| SGD | epoch: 015 | loss: 0.05402 - R2: 0.9985 -- iter: 0064/1168\n",
      "Training Step: 268  | total loss: \u001b[1m\u001b[32m0.05402\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 015 | loss: 0.05402 - R2: 0.9985 -- iter: 0128/1168\n",
      "Training Step: 269  | total loss: \u001b[1m\u001b[32m0.04764\u001b[0m\u001b[0m | time: 0.018s\n",
      "| SGD | epoch: 015 | loss: 0.04764 - R2: 0.9987 -- iter: 0192/1168\n",
      "Training Step: 270  | total loss: \u001b[1m\u001b[32m0.04458\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 015 | loss: 0.04458 - R2: 0.9988 -- iter: 0256/1168\n",
      "Training Step: 271  | total loss: \u001b[1m\u001b[32m0.04458\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 015 | loss: 0.04458 - R2: 0.9995 -- iter: 0320/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 272  | total loss: \u001b[1m\u001b[32m0.03990\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 015 | loss: 0.03990 - R2: 0.9995 -- iter: 0384/1168\n",
      "Training Step: 273  | total loss: \u001b[1m\u001b[32m0.03735\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 015 | loss: 0.03735 - R2: 0.9997 -- iter: 0448/1168\n",
      "Training Step: 274  | total loss: \u001b[1m\u001b[32m0.03735\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 015 | loss: 0.03735 - R2: 0.9997 -- iter: 0512/1168\n",
      "Training Step: 275  | total loss: \u001b[1m\u001b[32m0.03352\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 015 | loss: 0.03352 - R2: 0.9998 -- iter: 0576/1168\n",
      "Training Step: 276  | total loss: \u001b[1m\u001b[32m0.03195\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 015 | loss: 0.03195 - R2: 1.0001 -- iter: 0640/1168\n",
      "Training Step: 277  | total loss: \u001b[1m\u001b[32m0.03195\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 015 | loss: 0.03195 - R2: 1.0001 -- iter: 0704/1168\n",
      "Training Step: 278  | total loss: \u001b[1m\u001b[32m0.03058\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 015 | loss: 0.03058 - R2: 1.0001 -- iter: 0768/1168\n",
      "Training Step: 279  | total loss: \u001b[1m\u001b[32m0.02955\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 015 | loss: 0.02955 - R2: 1.0011 -- iter: 0832/1168\n",
      "Training Step: 280  | total loss: \u001b[1m\u001b[32m0.02778\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 015 | loss: 0.02778 - R2: 1.0011 -- iter: 0896/1168\n",
      "Training Step: 281  | total loss: \u001b[1m\u001b[32m0.02598\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 015 | loss: 0.02598 - R2: 1.0013 -- iter: 0960/1168\n",
      "Training Step: 282  | total loss: \u001b[1m\u001b[32m0.02505\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 015 | loss: 0.02505 - R2: 1.0007 -- iter: 1024/1168\n",
      "Training Step: 283  | total loss: \u001b[1m\u001b[32m0.02400\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 015 | loss: 0.02400 - R2: 1.0002 -- iter: 1088/1168\n",
      "Training Step: 284  | total loss: \u001b[1m\u001b[32m0.02390\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 015 | loss: 0.02390 - R2: 1.0004 -- iter: 1152/1168\n",
      "Training Step: 285  | total loss: \u001b[1m\u001b[32m0.02632\u001b[0m\u001b[0m | time: 1.083s\n",
      "| SGD | epoch: 015 | loss: 0.02632 - R2: 1.0013 | val_loss: 0.02728 - val_acc: 0.9970 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 286  | total loss: \u001b[1m\u001b[32m0.02543\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 016 | loss: 0.02543 - R2: 1.0009 -- iter: 0064/1168\n",
      "Training Step: 287  | total loss: \u001b[1m\u001b[32m0.02504\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 016 | loss: 0.02504 - R2: 1.0009 -- iter: 0128/1168\n",
      "Training Step: 288  | total loss: \u001b[1m\u001b[32m0.05173\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 016 | loss: 0.05173 - R2: 1.0008 -- iter: 0192/1168\n",
      "Training Step: 289  | total loss: \u001b[1m\u001b[32m0.04847\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 016 | loss: 0.04847 - R2: 0.9996 -- iter: 0256/1168\n",
      "Training Step: 290  | total loss: \u001b[1m\u001b[32m0.04526\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 016 | loss: 0.04526 - R2: 0.9996 -- iter: 0320/1168\n",
      "Training Step: 291  | total loss: \u001b[1m\u001b[32m0.04244\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 016 | loss: 0.04244 - R2: 0.9994 -- iter: 0384/1168\n",
      "Training Step: 292  | total loss: \u001b[1m\u001b[32m0.03990\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 016 | loss: 0.03990 - R2: 0.9990 -- iter: 0448/1168\n",
      "Training Step: 293  | total loss: \u001b[1m\u001b[32m0.03797\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 016 | loss: 0.03797 - R2: 0.9998 -- iter: 0512/1168\n",
      "Training Step: 294  | total loss: \u001b[1m\u001b[32m0.03639\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 016 | loss: 0.03639 - R2: 0.9998 -- iter: 0576/1168\n",
      "Training Step: 295  | total loss: \u001b[1m\u001b[32m0.03435\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 016 | loss: 0.03435 - R2: 0.9996 -- iter: 0640/1168\n",
      "Training Step: 296  | total loss: \u001b[1m\u001b[32m0.03279\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 016 | loss: 0.03279 - R2: 0.9995 -- iter: 0704/1168\n",
      "Training Step: 297  | total loss: \u001b[1m\u001b[32m0.03279\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 016 | loss: 0.03279 - R2: 0.9998 -- iter: 0768/1168\n",
      "Training Step: 298  | total loss: \u001b[1m\u001b[32m0.03004\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 016 | loss: 0.03004 - R2: 0.9990 -- iter: 0832/1168\n",
      "Training Step: 299  | total loss: \u001b[1m\u001b[32m0.03004\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 016 | loss: 0.03004 - R2: 0.9990 -- iter: 0896/1168\n",
      "Training Step: 300  | total loss: \u001b[1m\u001b[32m0.02805\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 016 | loss: 0.02805 - R2: 0.9997 -- iter: 0960/1168\n",
      "Training Step: 301  | total loss: \u001b[1m\u001b[32m0.02607\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 016 | loss: 0.02607 - R2: 1.0000 -- iter: 1024/1168\n",
      "Training Step: 302  | total loss: \u001b[1m\u001b[32m0.02563\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 016 | loss: 0.02563 - R2: 0.9992 -- iter: 1088/1168\n",
      "Training Step: 303  | total loss: \u001b[1m\u001b[32m0.02566\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 016 | loss: 0.02566 - R2: 0.9999 -- iter: 1152/1168\n",
      "Training Step: 304  | total loss: \u001b[1m\u001b[32m0.02603\u001b[0m\u001b[0m | time: 1.154s\n",
      "| SGD | epoch: 016 | loss: 0.02603 - R2: 0.9997 | val_loss: 0.02764 - val_acc: 1.0000 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 305  | total loss: \u001b[1m\u001b[32m0.02535\u001b[0m\u001b[0m | time: 0.007s\n",
      "| SGD | epoch: 017 | loss: 0.02535 - R2: 0.9999 -- iter: 0064/1168\n",
      "Training Step: 306  | total loss: \u001b[1m\u001b[32m0.02462\u001b[0m\u001b[0m | time: 0.011s\n",
      "| SGD | epoch: 017 | loss: 0.02462 - R2: 1.0002 -- iter: 0128/1168\n",
      "Training Step: 307  | total loss: \u001b[1m\u001b[32m0.02371\u001b[0m\u001b[0m | time: 0.013s\n",
      "| SGD | epoch: 017 | loss: 0.02371 - R2: 1.0004 -- iter: 0192/1168\n",
      "Training Step: 308  | total loss: \u001b[1m\u001b[32m0.04770\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 017 | loss: 0.04770 - R2: 0.9992 -- iter: 0256/1168\n",
      "Training Step: 309  | total loss: \u001b[1m\u001b[32m0.04414\u001b[0m\u001b[0m | time: 0.017s\n",
      "| SGD | epoch: 017 | loss: 0.04414 - R2: 0.9995 -- iter: 0320/1168\n",
      "Training Step: 310  | total loss: \u001b[1m\u001b[32m0.04409\u001b[0m\u001b[0m | time: 0.019s\n",
      "| SGD | epoch: 017 | loss: 0.04409 - R2: 0.9997 -- iter: 0384/1168\n",
      "Training Step: 311  | total loss: \u001b[1m\u001b[32m0.04157\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 017 | loss: 0.04157 - R2: 0.9998 -- iter: 0448/1168\n",
      "Training Step: 312  | total loss: \u001b[1m\u001b[32m0.03650\u001b[0m\u001b[0m | time: 0.026s\n",
      "| SGD | epoch: 017 | loss: 0.03650 - R2: 0.9998 -- iter: 0512/1168\n",
      "Training Step: 313  | total loss: \u001b[1m\u001b[32m0.03440\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 017 | loss: 0.03440 - R2: 0.9995 -- iter: 0576/1168\n",
      "Training Step: 314  | total loss: \u001b[1m\u001b[32m0.03413\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 017 | loss: 0.03413 - R2: 0.9996 -- iter: 0640/1168\n",
      "Training Step: 315  | total loss: \u001b[1m\u001b[32m0.03282\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 017 | loss: 0.03282 - R2: 0.9996 -- iter: 0704/1168\n",
      "Training Step: 316  | total loss: \u001b[1m\u001b[32m0.03053\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 017 | loss: 0.03053 - R2: 0.9996 -- iter: 0768/1168\n",
      "Training Step: 317  | total loss: \u001b[1m\u001b[32m0.03053\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 017 | loss: 0.03053 - R2: 0.9996 -- iter: 0832/1168\n",
      "Training Step: 318  | total loss: \u001b[1m\u001b[32m0.02977\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 017 | loss: 0.02977 - R2: 0.9999 -- iter: 0896/1168\n",
      "Training Step: 319  | total loss: \u001b[1m\u001b[32m0.02767\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 017 | loss: 0.02767 - R2: 1.0006 -- iter: 0960/1168\n",
      "Training Step: 320  | total loss: \u001b[1m\u001b[32m0.02767\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 017 | loss: 0.02767 - R2: 1.0006 -- iter: 1024/1168\n",
      "Training Step: 321  | total loss: \u001b[1m\u001b[32m0.02495\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 017 | loss: 0.02495 - R2: 1.0010 -- iter: 1088/1168\n",
      "Training Step: 322  | total loss: \u001b[1m\u001b[32m0.02495\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 017 | loss: 0.02495 - R2: 1.0003 -- iter: 1152/1168\n",
      "Training Step: 323  | total loss: \u001b[1m\u001b[32m0.02511\u001b[0m\u001b[0m | time: 1.064s\n",
      "| SGD | epoch: 017 | loss: 0.02511 - R2: 1.0000 | val_loss: 0.02694 - val_acc: 0.9996 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 324  | total loss: \u001b[1m\u001b[32m0.02342\u001b[0m\u001b[0m | time: 0.012s\n",
      "| SGD | epoch: 018 | loss: 0.02342 - R2: 0.9996 -- iter: 0064/1168\n",
      "Training Step: 325  | total loss: \u001b[1m\u001b[32m0.02306\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 018 | loss: 0.02306 - R2: 0.9997 -- iter: 0128/1168\n",
      "Training Step: 326  | total loss: \u001b[1m\u001b[32m0.02306\u001b[0m\u001b[0m | time: 0.018s\n",
      "| SGD | epoch: 018 | loss: 0.02306 - R2: 0.9997 -- iter: 0192/1168\n",
      "Training Step: 327  | total loss: \u001b[1m\u001b[32m0.02244\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 018 | loss: 0.02244 - R2: 0.9993 -- iter: 0256/1168\n",
      "Training Step: 328  | total loss: \u001b[1m\u001b[32m0.04759\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 018 | loss: 0.04759 - R2: 1.0006 -- iter: 0320/1168\n",
      "Training Step: 329  | total loss: \u001b[1m\u001b[32m0.04509\u001b[0m\u001b[0m | time: 0.026s\n",
      "| SGD | epoch: 018 | loss: 0.04509 - R2: 1.0003 -- iter: 0384/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 330  | total loss: \u001b[1m\u001b[32m0.04258\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 018 | loss: 0.04258 - R2: 1.0001 -- iter: 0448/1168\n",
      "Training Step: 331  | total loss: \u001b[1m\u001b[32m0.04009\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 018 | loss: 0.04009 - R2: 0.9996 -- iter: 0512/1168\n",
      "Training Step: 332  | total loss: \u001b[1m\u001b[32m0.03571\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 018 | loss: 0.03571 - R2: 0.9995 -- iter: 0576/1168\n",
      "Training Step: 333  | total loss: \u001b[1m\u001b[32m0.03355\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 018 | loss: 0.03355 - R2: 0.9991 -- iter: 0640/1168\n",
      "Training Step: 334  | total loss: \u001b[1m\u001b[32m0.03205\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 018 | loss: 0.03205 - R2: 0.9991 -- iter: 0704/1168\n",
      "Training Step: 335  | total loss: \u001b[1m\u001b[32m0.03205\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 018 | loss: 0.03205 - R2: 0.9991 -- iter: 0768/1168\n",
      "Training Step: 336  | total loss: \u001b[1m\u001b[32m0.03074\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 018 | loss: 0.03074 - R2: 0.9991 -- iter: 0832/1168\n",
      "Training Step: 337  | total loss: \u001b[1m\u001b[32m0.02914\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 018 | loss: 0.02914 - R2: 0.9991 -- iter: 0896/1168\n",
      "Training Step: 338  | total loss: \u001b[1m\u001b[32m0.02914\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 018 | loss: 0.02914 - R2: 0.9991 -- iter: 0960/1168\n",
      "Training Step: 339  | total loss: \u001b[1m\u001b[32m0.02764\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 018 | loss: 0.02764 - R2: 0.9996 -- iter: 1024/1168\n",
      "Training Step: 340  | total loss: \u001b[1m\u001b[32m0.02613\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 018 | loss: 0.02613 - R2: 1.0002 -- iter: 1088/1168\n",
      "Training Step: 341  | total loss: \u001b[1m\u001b[32m0.02442\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 018 | loss: 0.02442 - R2: 1.0007 -- iter: 1152/1168\n",
      "Training Step: 342  | total loss: \u001b[1m\u001b[32m0.02381\u001b[0m\u001b[0m | time: 1.065s\n",
      "| SGD | epoch: 018 | loss: 0.02381 - R2: 1.0005 | val_loss: 0.02758 - val_acc: 0.9980 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 343  | total loss: \u001b[1m\u001b[32m0.02346\u001b[0m\u001b[0m | time: 0.023s\n",
      "| SGD | epoch: 019 | loss: 0.02346 - R2: 1.0003 -- iter: 0064/1168\n",
      "Training Step: 344  | total loss: \u001b[1m\u001b[32m0.02473\u001b[0m\u001b[0m | time: 0.026s\n",
      "| SGD | epoch: 019 | loss: 0.02473 - R2: 1.0003 -- iter: 0128/1168\n",
      "Training Step: 345  | total loss: \u001b[1m\u001b[32m0.02543\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 019 | loss: 0.02543 - R2: 1.0005 -- iter: 0192/1168\n",
      "Training Step: 346  | total loss: \u001b[1m\u001b[32m0.02407\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 019 | loss: 0.02407 - R2: 1.0001 -- iter: 0256/1168\n",
      "Training Step: 347  | total loss: \u001b[1m\u001b[32m0.05385\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 019 | loss: 0.05385 - R2: 1.0010 -- iter: 0320/1168\n",
      "Training Step: 348  | total loss: \u001b[1m\u001b[32m0.05385\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 019 | loss: 0.05385 - R2: 1.0010 -- iter: 0384/1168\n",
      "Training Step: 349  | total loss: \u001b[1m\u001b[32m0.05036\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 019 | loss: 0.05036 - R2: 1.0007 -- iter: 0448/1168\n",
      "Training Step: 350  | total loss: \u001b[1m\u001b[32m0.04969\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 019 | loss: 0.04969 - R2: 1.0007 -- iter: 0512/1168\n",
      "Training Step: 351  | total loss: \u001b[1m\u001b[32m0.04596\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 019 | loss: 0.04596 - R2: 1.0003 -- iter: 0576/1168\n",
      "Training Step: 352  | total loss: \u001b[1m\u001b[32m0.04279\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 019 | loss: 0.04279 - R2: 0.9994 -- iter: 0640/1168\n",
      "Training Step: 353  | total loss: \u001b[1m\u001b[32m0.04046\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 019 | loss: 0.04046 - R2: 0.9988 -- iter: 0704/1168\n",
      "Training Step: 354  | total loss: \u001b[1m\u001b[32m0.03855\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 019 | loss: 0.03855 - R2: 0.9993 -- iter: 0768/1168\n",
      "Training Step: 355  | total loss: \u001b[1m\u001b[32m0.03590\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 019 | loss: 0.03590 - R2: 0.9996 -- iter: 0832/1168\n",
      "Training Step: 356  | total loss: \u001b[1m\u001b[32m0.03590\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 019 | loss: 0.03590 - R2: 0.9996 -- iter: 0896/1168\n",
      "Training Step: 357  | total loss: \u001b[1m\u001b[32m0.03390\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 019 | loss: 0.03390 - R2: 0.9992 -- iter: 0960/1168\n",
      "Training Step: 358  | total loss: \u001b[1m\u001b[32m0.03182\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 019 | loss: 0.03182 - R2: 0.9993 -- iter: 1024/1168\n",
      "Training Step: 359  | total loss: \u001b[1m\u001b[32m0.02978\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 019 | loss: 0.02978 - R2: 0.9994 -- iter: 1088/1168\n",
      "Training Step: 360  | total loss: \u001b[1m\u001b[32m0.02817\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 019 | loss: 0.02817 - R2: 0.9994 -- iter: 1152/1168\n",
      "Training Step: 361  | total loss: \u001b[1m\u001b[32m0.02652\u001b[0m\u001b[0m | time: 1.066s\n",
      "| SGD | epoch: 019 | loss: 0.02652 - R2: 0.9997 | val_loss: 0.02736 - val_acc: 1.0010 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 362  | total loss: \u001b[1m\u001b[32m0.02481\u001b[0m\u001b[0m | time: 0.012s\n",
      "| SGD | epoch: 020 | loss: 0.02481 - R2: 0.9989 -- iter: 0064/1168\n",
      "Training Step: 363  | total loss: \u001b[1m\u001b[32m0.02481\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 020 | loss: 0.02481 - R2: 0.9989 -- iter: 0128/1168\n",
      "Training Step: 364  | total loss: \u001b[1m\u001b[32m0.02439\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 020 | loss: 0.02439 - R2: 0.9992 -- iter: 0192/1168\n",
      "Training Step: 365  | total loss: \u001b[1m\u001b[32m0.02358\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 020 | loss: 0.02358 - R2: 0.9997 -- iter: 0256/1168\n",
      "Training Step: 366  | total loss: \u001b[1m\u001b[32m0.02314\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 020 | loss: 0.02314 - R2: 0.9999 -- iter: 0320/1168\n",
      "Training Step: 367  | total loss: \u001b[1m\u001b[32m0.02294\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 020 | loss: 0.02294 - R2: 0.9998 -- iter: 0384/1168\n",
      "Training Step: 368  | total loss: \u001b[1m\u001b[32m0.04359\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 020 | loss: 0.04359 - R2: 0.9994 -- iter: 0448/1168\n",
      "Training Step: 369  | total loss: \u001b[1m\u001b[32m0.04199\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 020 | loss: 0.04199 - R2: 0.9994 -- iter: 0512/1168\n",
      "Training Step: 370  | total loss: \u001b[1m\u001b[32m0.04018\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 020 | loss: 0.04018 - R2: 0.9997 -- iter: 0576/1168\n",
      "Training Step: 371  | total loss: \u001b[1m\u001b[32m0.03819\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 020 | loss: 0.03819 - R2: 0.9998 -- iter: 0640/1168\n",
      "Training Step: 372  | total loss: \u001b[1m\u001b[32m0.03553\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 020 | loss: 0.03553 - R2: 1.0003 -- iter: 0704/1168\n",
      "Training Step: 373  | total loss: \u001b[1m\u001b[32m0.03520\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 020 | loss: 0.03520 - R2: 1.0003 -- iter: 0768/1168\n",
      "Training Step: 374  | total loss: \u001b[1m\u001b[32m0.03186\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 020 | loss: 0.03186 - R2: 1.0001 -- iter: 0832/1168\n",
      "Training Step: 375  | total loss: \u001b[1m\u001b[32m0.03250\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 020 | loss: 0.03250 - R2: 1.0006 -- iter: 0896/1168\n",
      "Training Step: 376  | total loss: \u001b[1m\u001b[32m0.03040\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 020 | loss: 0.03040 - R2: 1.0004 -- iter: 0960/1168\n",
      "Training Step: 377  | total loss: \u001b[1m\u001b[32m0.03040\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 020 | loss: 0.03040 - R2: 1.0004 -- iter: 1024/1168\n",
      "Training Step: 378  | total loss: \u001b[1m\u001b[32m0.02868\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 020 | loss: 0.02868 - R2: 1.0003 -- iter: 1088/1168\n",
      "Training Step: 379  | total loss: \u001b[1m\u001b[32m0.02845\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 020 | loss: 0.02845 - R2: 0.9996 -- iter: 1152/1168\n",
      "Training Step: 380  | total loss: \u001b[1m\u001b[32m0.02786\u001b[0m\u001b[0m | time: 1.087s\n",
      "| SGD | epoch: 020 | loss: 0.02786 - R2: 0.9992 | val_loss: 0.02824 - val_acc: 1.0022 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 381  | total loss: \u001b[1m\u001b[32m0.02699\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 021 | loss: 0.02699 - R2: 0.9990 -- iter: 0064/1168\n",
      "Training Step: 382  | total loss: \u001b[1m\u001b[32m0.02688\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 021 | loss: 0.02688 - R2: 0.9993 -- iter: 0128/1168\n",
      "Training Step: 383  | total loss: \u001b[1m\u001b[32m0.02507\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 021 | loss: 0.02507 - R2: 0.9996 -- iter: 0192/1168\n",
      "Training Step: 384  | total loss: \u001b[1m\u001b[32m0.02409\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 021 | loss: 0.02409 - R2: 0.9994 -- iter: 0256/1168\n",
      "Training Step: 385  | total loss: \u001b[1m\u001b[32m0.02345\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 021 | loss: 0.02345 - R2: 0.9990 -- iter: 0320/1168\n",
      "Training Step: 386  | total loss: \u001b[1m\u001b[32m0.02275\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 021 | loss: 0.02275 - R2: 0.9990 -- iter: 0384/1168\n",
      "Training Step: 387  | total loss: \u001b[1m\u001b[32m0.02148\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 021 | loss: 0.02148 - R2: 0.9994 -- iter: 0448/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 388  | total loss: \u001b[1m\u001b[32m0.02187\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 021 | loss: 0.02187 - R2: 0.9995 -- iter: 0512/1168\n",
      "Training Step: 389  | total loss: \u001b[1m\u001b[32m0.02203\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 021 | loss: 0.02203 - R2: 1.0000 -- iter: 0576/1168\n",
      "Training Step: 390  | total loss: \u001b[1m\u001b[32m0.02192\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 021 | loss: 0.02192 - R2: 1.0000 -- iter: 0640/1168\n",
      "Training Step: 391  | total loss: \u001b[1m\u001b[32m0.02192\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 021 | loss: 0.02192 - R2: 1.0000 -- iter: 0704/1168\n",
      "Training Step: 392  | total loss: \u001b[1m\u001b[32m0.02288\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 021 | loss: 0.02288 - R2: 1.0005 -- iter: 0768/1168\n",
      "Training Step: 393  | total loss: \u001b[1m\u001b[32m0.02288\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 021 | loss: 0.02288 - R2: 1.0005 -- iter: 0832/1168\n",
      "Training Step: 394  | total loss: \u001b[1m\u001b[32m0.02222\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 021 | loss: 0.02222 - R2: 1.0001 -- iter: 0896/1168\n",
      "Training Step: 395  | total loss: \u001b[1m\u001b[32m0.02151\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 021 | loss: 0.02151 - R2: 0.9996 -- iter: 0960/1168\n",
      "Training Step: 396  | total loss: \u001b[1m\u001b[32m0.02051\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 021 | loss: 0.02051 - R2: 0.9998 -- iter: 1024/1168\n",
      "Training Step: 397  | total loss: \u001b[1m\u001b[32m0.02051\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 021 | loss: 0.02051 - R2: 0.9998 -- iter: 1088/1168\n",
      "Training Step: 398  | total loss: \u001b[1m\u001b[32m0.02027\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 021 | loss: 0.02027 - R2: 1.0003 -- iter: 1152/1168\n",
      "Training Step: 399  | total loss: \u001b[1m\u001b[32m0.02012\u001b[0m\u001b[0m | time: 1.146s\n",
      "| SGD | epoch: 021 | loss: 0.02012 - R2: 1.0005 | val_loss: 0.02891 - val_acc: 0.9993 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 400  | total loss: \u001b[1m\u001b[32m0.01950\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 022 | loss: 0.01950 - R2: 1.0006 -- iter: 0064/1168\n",
      "Training Step: 401  | total loss: \u001b[1m\u001b[32m0.01861\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 022 | loss: 0.01861 - R2: 1.0007 -- iter: 0128/1168\n",
      "Training Step: 402  | total loss: \u001b[1m\u001b[32m0.01814\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 022 | loss: 0.01814 - R2: 1.0005 -- iter: 0192/1168\n",
      "Training Step: 403  | total loss: \u001b[1m\u001b[32m0.01758\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 022 | loss: 0.01758 - R2: 1.0003 -- iter: 0256/1168\n",
      "Training Step: 404  | total loss: \u001b[1m\u001b[32m0.01788\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 022 | loss: 0.01788 - R2: 1.0002 -- iter: 0320/1168\n",
      "Training Step: 405  | total loss: \u001b[1m\u001b[32m0.01755\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 022 | loss: 0.01755 - R2: 1.0004 -- iter: 0384/1168\n",
      "Training Step: 406  | total loss: \u001b[1m\u001b[32m0.01942\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 022 | loss: 0.01942 - R2: 1.0000 -- iter: 0448/1168\n",
      "Training Step: 407  | total loss: \u001b[1m\u001b[32m0.01942\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 022 | loss: 0.01942 - R2: 0.9999 -- iter: 0512/1168\n",
      "Training Step: 408  | total loss: \u001b[1m\u001b[32m0.04244\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 022 | loss: 0.04244 - R2: 0.9999 -- iter: 0576/1168\n",
      "Training Step: 409  | total loss: \u001b[1m\u001b[32m0.03730\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 022 | loss: 0.03730 - R2: 0.9999 -- iter: 0640/1168\n",
      "Training Step: 410  | total loss: \u001b[1m\u001b[32m0.03483\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 022 | loss: 0.03483 - R2: 0.9996 -- iter: 0704/1168\n",
      "Training Step: 411  | total loss: \u001b[1m\u001b[32m0.03266\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 022 | loss: 0.03266 - R2: 0.9997 -- iter: 0768/1168\n",
      "Training Step: 412  | total loss: \u001b[1m\u001b[32m0.03266\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 022 | loss: 0.03266 - R2: 0.9997 -- iter: 0832/1168\n",
      "Training Step: 413  | total loss: \u001b[1m\u001b[32m0.03368\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 022 | loss: 0.03368 - R2: 0.9993 -- iter: 0896/1168\n",
      "Training Step: 414  | total loss: \u001b[1m\u001b[32m0.03070\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 022 | loss: 0.03070 - R2: 0.9995 -- iter: 0960/1168\n",
      "Training Step: 415  | total loss: \u001b[1m\u001b[32m0.02889\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 022 | loss: 0.02889 - R2: 0.9997 -- iter: 1024/1168\n",
      "Training Step: 416  | total loss: \u001b[1m\u001b[32m0.02806\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 022 | loss: 0.02806 - R2: 0.9995 -- iter: 1088/1168\n",
      "Training Step: 417  | total loss: \u001b[1m\u001b[32m0.02672\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 022 | loss: 0.02672 - R2: 0.9999 -- iter: 1152/1168\n",
      "Training Step: 418  | total loss: \u001b[1m\u001b[32m0.02533\u001b[0m\u001b[0m | time: 1.149s\n",
      "| SGD | epoch: 022 | loss: 0.02533 - R2: 0.9995 | val_loss: 0.02760 - val_acc: 1.0005 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 419  | total loss: \u001b[1m\u001b[32m0.02828\u001b[0m\u001b[0m | time: 0.014s\n",
      "| SGD | epoch: 023 | loss: 0.02828 - R2: 1.0004 -- iter: 0064/1168\n",
      "Training Step: 420  | total loss: \u001b[1m\u001b[32m0.02991\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 023 | loss: 0.02991 - R2: 1.0010 -- iter: 0128/1168\n",
      "Training Step: 421  | total loss: \u001b[1m\u001b[32m0.02920\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 023 | loss: 0.02920 - R2: 1.0006 -- iter: 0192/1168\n",
      "Training Step: 422  | total loss: \u001b[1m\u001b[32m0.02827\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 023 | loss: 0.02827 - R2: 0.9999 -- iter: 0256/1168\n",
      "Training Step: 423  | total loss: \u001b[1m\u001b[32m0.02827\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 023 | loss: 0.02827 - R2: 0.9999 -- iter: 0320/1168\n",
      "Training Step: 424  | total loss: \u001b[1m\u001b[32m0.02707\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 023 | loss: 0.02707 - R2: 0.9996 -- iter: 0384/1168\n",
      "Training Step: 425  | total loss: \u001b[1m\u001b[32m0.02740\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 023 | loss: 0.02740 - R2: 1.0001 -- iter: 0448/1168\n",
      "Training Step: 426  | total loss: \u001b[1m\u001b[32m0.02740\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 023 | loss: 0.02740 - R2: 1.0001 -- iter: 0512/1168\n",
      "Training Step: 427  | total loss: \u001b[1m\u001b[32m0.04681\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 023 | loss: 0.04681 - R2: 0.9986 -- iter: 0576/1168\n",
      "Training Step: 428  | total loss: \u001b[1m\u001b[32m0.04681\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 023 | loss: 0.04681 - R2: 0.9986 -- iter: 0640/1168\n",
      "Training Step: 429  | total loss: \u001b[1m\u001b[32m0.04361\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 023 | loss: 0.04361 - R2: 0.9988 -- iter: 0704/1168\n",
      "Training Step: 430  | total loss: \u001b[1m\u001b[32m0.03755\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 023 | loss: 0.03755 - R2: 0.9991 -- iter: 0768/1168\n",
      "Training Step: 431  | total loss: \u001b[1m\u001b[32m0.03755\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 023 | loss: 0.03755 - R2: 0.9991 -- iter: 0832/1168\n",
      "Training Step: 432  | total loss: \u001b[1m\u001b[32m0.03744\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 023 | loss: 0.03744 - R2: 0.9997 -- iter: 0896/1168\n",
      "Training Step: 433  | total loss: \u001b[1m\u001b[32m0.03423\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 023 | loss: 0.03423 - R2: 0.9995 -- iter: 0960/1168\n",
      "Training Step: 434  | total loss: \u001b[1m\u001b[32m0.03423\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 023 | loss: 0.03423 - R2: 0.9995 -- iter: 1024/1168\n",
      "Training Step: 435  | total loss: \u001b[1m\u001b[32m0.03240\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 023 | loss: 0.03240 - R2: 0.9998 -- iter: 1088/1168\n",
      "Training Step: 436  | total loss: \u001b[1m\u001b[32m0.03035\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 023 | loss: 0.03035 - R2: 0.9999 -- iter: 1152/1168\n",
      "Training Step: 437  | total loss: \u001b[1m\u001b[32m0.02971\u001b[0m\u001b[0m | time: 1.072s\n",
      "| SGD | epoch: 023 | loss: 0.02971 - R2: 0.9993 | val_loss: 0.02890 - val_acc: 1.0023 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 438  | total loss: \u001b[1m\u001b[32m0.02882\u001b[0m\u001b[0m | time: 0.005s\n",
      "| SGD | epoch: 024 | loss: 0.02882 - R2: 0.9991 -- iter: 0064/1168\n",
      "Training Step: 439  | total loss: \u001b[1m\u001b[32m0.02882\u001b[0m\u001b[0m | time: 0.009s\n",
      "| SGD | epoch: 024 | loss: 0.02882 - R2: 0.9991 -- iter: 0128/1168\n",
      "Training Step: 440  | total loss: \u001b[1m\u001b[32m0.02799\u001b[0m\u001b[0m | time: 0.013s\n",
      "| SGD | epoch: 024 | loss: 0.02799 - R2: 1.0005 -- iter: 0192/1168\n",
      "Training Step: 441  | total loss: \u001b[1m\u001b[32m0.02618\u001b[0m\u001b[0m | time: 0.017s\n",
      "| SGD | epoch: 024 | loss: 0.02618 - R2: 1.0009 -- iter: 0256/1168\n",
      "Training Step: 442  | total loss: \u001b[1m\u001b[32m0.02512\u001b[0m\u001b[0m | time: 0.023s\n",
      "| SGD | epoch: 024 | loss: 0.02512 - R2: 1.0004 -- iter: 0320/1168\n",
      "Training Step: 443  | total loss: \u001b[1m\u001b[32m0.02512\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 024 | loss: 0.02512 - R2: 1.0006 -- iter: 0384/1168\n",
      "Training Step: 444  | total loss: \u001b[1m\u001b[32m0.02433\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 024 | loss: 0.02433 - R2: 1.0006 -- iter: 0448/1168\n",
      "Training Step: 445  | total loss: \u001b[1m\u001b[32m0.02406\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 024 | loss: 0.02406 - R2: 1.0009 -- iter: 0512/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 446  | total loss: \u001b[1m\u001b[32m0.02309\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 024 | loss: 0.02309 - R2: 1.0008 -- iter: 0576/1168\n",
      "Training Step: 447  | total loss: \u001b[1m\u001b[32m0.02297\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 024 | loss: 0.02297 - R2: 1.0004 -- iter: 0640/1168\n",
      "Training Step: 448  | total loss: \u001b[1m\u001b[32m0.02285\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 024 | loss: 0.02285 - R2: 1.0001 -- iter: 0704/1168\n",
      "Training Step: 449  | total loss: \u001b[1m\u001b[32m0.02149\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 024 | loss: 0.02149 - R2: 0.9995 -- iter: 0768/1168\n",
      "Training Step: 450  | total loss: \u001b[1m\u001b[32m0.02107\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 024 | loss: 0.02107 - R2: 0.9997 -- iter: 0832/1168\n",
      "Training Step: 451  | total loss: \u001b[1m\u001b[32m0.02107\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 024 | loss: 0.02107 - R2: 0.9997 -- iter: 0896/1168\n",
      "Training Step: 452  | total loss: \u001b[1m\u001b[32m0.02049\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 024 | loss: 0.02049 - R2: 0.9996 -- iter: 0960/1168\n",
      "Training Step: 453  | total loss: \u001b[1m\u001b[32m0.01989\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 024 | loss: 0.01989 - R2: 0.9996 -- iter: 1024/1168\n",
      "Training Step: 454  | total loss: \u001b[1m\u001b[32m0.01918\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 024 | loss: 0.01918 - R2: 0.9991 -- iter: 1088/1168\n",
      "Training Step: 455  | total loss: \u001b[1m\u001b[32m0.02046\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 024 | loss: 0.02046 - R2: 1.0001 -- iter: 1152/1168\n",
      "Training Step: 456  | total loss: \u001b[1m\u001b[32m0.02046\u001b[0m\u001b[0m | time: 1.083s\n",
      "| SGD | epoch: 024 | loss: 0.02046 - R2: 1.0001 | val_loss: 0.02834 - val_acc: 1.0007 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 457  | total loss: \u001b[1m\u001b[32m0.01998\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 025 | loss: 0.01998 - R2: 1.0000 -- iter: 0064/1168\n",
      "Training Step: 458  | total loss: \u001b[1m\u001b[32m0.01967\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 025 | loss: 0.01967 - R2: 0.9997 -- iter: 0128/1168\n",
      "Training Step: 459  | total loss: \u001b[1m\u001b[32m0.02067\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 025 | loss: 0.02067 - R2: 1.0008 -- iter: 0192/1168\n",
      "Training Step: 460  | total loss: \u001b[1m\u001b[32m0.02120\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 025 | loss: 0.02120 - R2: 1.0015 -- iter: 0256/1168\n",
      "Training Step: 461  | total loss: \u001b[1m\u001b[32m0.02082\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 025 | loss: 0.02082 - R2: 1.0006 -- iter: 0320/1168\n",
      "Training Step: 462  | total loss: \u001b[1m\u001b[32m0.02082\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 025 | loss: 0.02082 - R2: 1.0006 -- iter: 0384/1168\n",
      "Training Step: 463  | total loss: \u001b[1m\u001b[32m0.02072\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 025 | loss: 0.02072 - R2: 1.0005 -- iter: 0448/1168\n",
      "Training Step: 464  | total loss: \u001b[1m\u001b[32m0.02173\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 025 | loss: 0.02173 - R2: 1.0005 -- iter: 0512/1168\n",
      "Training Step: 465  | total loss: \u001b[1m\u001b[32m0.02148\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 025 | loss: 0.02148 - R2: 1.0003 -- iter: 0576/1168\n",
      "Training Step: 466  | total loss: \u001b[1m\u001b[32m0.02295\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 025 | loss: 0.02295 - R2: 1.0000 -- iter: 0640/1168\n",
      "Training Step: 467  | total loss: \u001b[1m\u001b[32m0.06056\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 025 | loss: 0.06056 - R2: 1.0001 -- iter: 0704/1168\n",
      "Training Step: 468  | total loss: \u001b[1m\u001b[32m0.05638\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 025 | loss: 0.05638 - R2: 1.0000 -- iter: 0768/1168\n",
      "Training Step: 469  | total loss: \u001b[1m\u001b[32m0.05638\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 025 | loss: 0.05638 - R2: 1.0000 -- iter: 0832/1168\n",
      "Training Step: 470  | total loss: \u001b[1m\u001b[32m0.04956\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 025 | loss: 0.04956 - R2: 0.9996 -- iter: 0896/1168\n",
      "Training Step: 471  | total loss: \u001b[1m\u001b[32m0.04956\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 025 | loss: 0.04956 - R2: 0.9996 -- iter: 0960/1168\n",
      "Training Step: 472  | total loss: \u001b[1m\u001b[32m0.04695\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 025 | loss: 0.04695 - R2: 0.9996 -- iter: 1024/1168\n",
      "Training Step: 473  | total loss: \u001b[1m\u001b[32m0.04331\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 025 | loss: 0.04331 - R2: 0.9997 -- iter: 1088/1168\n",
      "Training Step: 474  | total loss: \u001b[1m\u001b[32m0.04020\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 025 | loss: 0.04020 - R2: 0.9995 -- iter: 1152/1168\n",
      "Training Step: 475  | total loss: \u001b[1m\u001b[32m0.03776\u001b[0m\u001b[0m | time: 1.183s\n",
      "| SGD | epoch: 025 | loss: 0.03776 - R2: 0.9997 | val_loss: 0.02828 - val_acc: 0.9988 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 476  | total loss: \u001b[1m\u001b[32m0.03776\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 026 | loss: 0.03776 - R2: 0.9997 -- iter: 0064/1168\n",
      "Training Step: 477  | total loss: \u001b[1m\u001b[32m0.03528\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 026 | loss: 0.03528 - R2: 0.9997 -- iter: 0128/1168\n",
      "Training Step: 478  | total loss: \u001b[1m\u001b[32m0.03183\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 026 | loss: 0.03183 - R2: 0.9992 -- iter: 0192/1168\n",
      "Training Step: 479  | total loss: \u001b[1m\u001b[32m0.03048\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 026 | loss: 0.03048 - R2: 0.9988 -- iter: 0256/1168\n",
      "Training Step: 480  | total loss: \u001b[1m\u001b[32m0.03048\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 026 | loss: 0.03048 - R2: 0.9987 -- iter: 0320/1168\n",
      "Training Step: 481  | total loss: \u001b[1m\u001b[32m0.02832\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 026 | loss: 0.02832 - R2: 0.9990 -- iter: 0384/1168\n",
      "Training Step: 482  | total loss: \u001b[1m\u001b[32m0.02721\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 026 | loss: 0.02721 - R2: 0.9993 -- iter: 0448/1168\n",
      "Training Step: 483  | total loss: \u001b[1m\u001b[32m0.02858\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 026 | loss: 0.02858 - R2: 0.9998 -- iter: 0512/1168\n",
      "Training Step: 484  | total loss: \u001b[1m\u001b[32m0.02731\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 026 | loss: 0.02731 - R2: 0.9996 -- iter: 0576/1168\n",
      "Training Step: 485  | total loss: \u001b[1m\u001b[32m0.02635\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 026 | loss: 0.02635 - R2: 0.9991 -- iter: 0640/1168\n",
      "Training Step: 486  | total loss: \u001b[1m\u001b[32m0.02635\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 026 | loss: 0.02635 - R2: 0.9991 -- iter: 0704/1168\n",
      "Training Step: 487  | total loss: \u001b[1m\u001b[32m0.05981\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 026 | loss: 0.05981 - R2: 0.9981 -- iter: 0768/1168\n",
      "Training Step: 488  | total loss: \u001b[1m\u001b[32m0.05517\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 026 | loss: 0.05517 - R2: 0.9981 -- iter: 0832/1168\n",
      "Training Step: 489  | total loss: \u001b[1m\u001b[32m0.05517\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 026 | loss: 0.05517 - R2: 0.9981 -- iter: 0896/1168\n",
      "Training Step: 490  | total loss: \u001b[1m\u001b[32m0.04879\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 026 | loss: 0.04879 - R2: 0.9984 -- iter: 0960/1168\n",
      "Training Step: 491  | total loss: \u001b[1m\u001b[32m0.04522\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 026 | loss: 0.04522 - R2: 0.9985 -- iter: 1024/1168\n",
      "Training Step: 492  | total loss: \u001b[1m\u001b[32m0.04232\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 026 | loss: 0.04232 - R2: 0.9989 -- iter: 1088/1168\n",
      "Training Step: 493  | total loss: \u001b[1m\u001b[32m0.04232\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 026 | loss: 0.04232 - R2: 0.9989 -- iter: 1152/1168\n",
      "Training Step: 494  | total loss: \u001b[1m\u001b[32m0.04175\u001b[0m\u001b[0m | time: 1.137s\n",
      "| SGD | epoch: 026 | loss: 0.04175 - R2: 0.9998 | val_loss: 0.02717 - val_acc: 1.0005 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 495  | total loss: \u001b[1m\u001b[32m0.03763\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 027 | loss: 0.03763 - R2: 1.0005 -- iter: 0064/1168\n",
      "Training Step: 496  | total loss: \u001b[1m\u001b[32m0.03763\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 027 | loss: 0.03763 - R2: 1.0005 -- iter: 0128/1168\n",
      "Training Step: 497  | total loss: \u001b[1m\u001b[32m0.03594\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 027 | loss: 0.03594 - R2: 1.0003 -- iter: 0192/1168\n",
      "Training Step: 498  | total loss: \u001b[1m\u001b[32m0.03215\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 027 | loss: 0.03215 - R2: 1.0003 -- iter: 0256/1168\n",
      "Training Step: 499  | total loss: \u001b[1m\u001b[32m0.03215\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 027 | loss: 0.03215 - R2: 1.0003 -- iter: 0320/1168\n",
      "Training Step: 500  | total loss: \u001b[1m\u001b[32m0.02828\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 027 | loss: 0.02828 - R2: 1.0005 -- iter: 0384/1168\n",
      "Training Step: 501  | total loss: \u001b[1m\u001b[32m0.02828\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 027 | loss: 0.02828 - R2: 1.0005 -- iter: 0448/1168\n",
      "Training Step: 502  | total loss: \u001b[1m\u001b[32m0.02834\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 027 | loss: 0.02834 - R2: 1.0003 -- iter: 0512/1168\n",
      "Training Step: 503  | total loss: \u001b[1m\u001b[32m0.02673\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 027 | loss: 0.02673 - R2: 0.9998 -- iter: 0576/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 504  | total loss: \u001b[1m\u001b[32m0.02460\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 027 | loss: 0.02460 - R2: 0.9997 -- iter: 0640/1168\n",
      "Training Step: 505  | total loss: \u001b[1m\u001b[32m0.02335\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 027 | loss: 0.02335 - R2: 1.0001 -- iter: 0704/1168\n",
      "Training Step: 506  | total loss: \u001b[1m\u001b[32m0.02436\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 027 | loss: 0.02436 - R2: 1.0006 -- iter: 0768/1168\n",
      "Training Step: 507  | total loss: \u001b[1m\u001b[32m0.02369\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 027 | loss: 0.02369 - R2: 1.0005 -- iter: 0832/1168\n",
      "Training Step: 508  | total loss: \u001b[1m\u001b[32m0.02369\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 027 | loss: 0.02369 - R2: 1.0005 -- iter: 0896/1168\n",
      "Training Step: 509  | total loss: \u001b[1m\u001b[32m0.02257\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 027 | loss: 0.02257 - R2: 0.9995 -- iter: 0960/1168\n",
      "Training Step: 510  | total loss: \u001b[1m\u001b[32m0.02257\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 027 | loss: 0.02257 - R2: 0.9995 -- iter: 1024/1168\n",
      "Training Step: 511  | total loss: \u001b[1m\u001b[32m0.02153\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 027 | loss: 0.02153 - R2: 0.9997 -- iter: 1088/1168\n",
      "Training Step: 512  | total loss: \u001b[1m\u001b[32m0.02057\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 027 | loss: 0.02057 - R2: 0.9997 -- iter: 1152/1168\n",
      "Training Step: 513  | total loss: \u001b[1m\u001b[32m0.02057\u001b[0m\u001b[0m | time: 1.145s\n",
      "| SGD | epoch: 027 | loss: 0.02057 - R2: 0.9997 | val_loss: 0.02813 - val_acc: 1.0017 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 514  | total loss: \u001b[1m\u001b[32m0.01963\u001b[0m\u001b[0m | time: 0.016s\n",
      "| SGD | epoch: 028 | loss: 0.01963 - R2: 0.9991 -- iter: 0064/1168\n",
      "Training Step: 515  | total loss: \u001b[1m\u001b[32m0.01963\u001b[0m\u001b[0m | time: 0.019s\n",
      "| SGD | epoch: 028 | loss: 0.01963 - R2: 0.9991 -- iter: 0128/1168\n",
      "Training Step: 516  | total loss: \u001b[1m\u001b[32m0.02171\u001b[0m\u001b[0m | time: 0.023s\n",
      "| SGD | epoch: 028 | loss: 0.02171 - R2: 0.9993 -- iter: 0192/1168\n",
      "Training Step: 517  | total loss: \u001b[1m\u001b[32m0.02189\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 028 | loss: 0.02189 - R2: 1.0002 -- iter: 0256/1168\n",
      "Training Step: 518  | total loss: \u001b[1m\u001b[32m0.02189\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 028 | loss: 0.02189 - R2: 1.0002 -- iter: 0320/1168\n",
      "Training Step: 519  | total loss: \u001b[1m\u001b[32m0.02093\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 028 | loss: 0.02093 - R2: 1.0003 -- iter: 0384/1168\n",
      "Training Step: 520  | total loss: \u001b[1m\u001b[32m0.02085\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 028 | loss: 0.02085 - R2: 1.0003 -- iter: 0448/1168\n",
      "Training Step: 521  | total loss: \u001b[1m\u001b[32m0.01961\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 028 | loss: 0.01961 - R2: 1.0001 -- iter: 0512/1168\n",
      "Training Step: 522  | total loss: \u001b[1m\u001b[32m0.01954\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 028 | loss: 0.01954 - R2: 1.0002 -- iter: 0576/1168\n",
      "Training Step: 523  | total loss: \u001b[1m\u001b[32m0.01954\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 028 | loss: 0.01954 - R2: 1.0002 -- iter: 0640/1168\n",
      "Training Step: 524  | total loss: \u001b[1m\u001b[32m0.01946\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 028 | loss: 0.01946 - R2: 1.0008 -- iter: 0704/1168\n",
      "Training Step: 525  | total loss: \u001b[1m\u001b[32m0.01946\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 028 | loss: 0.01946 - R2: 1.0008 -- iter: 0768/1168\n",
      "Training Step: 526  | total loss: \u001b[1m\u001b[32m0.02002\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 028 | loss: 0.02002 - R2: 1.0003 -- iter: 0832/1168\n",
      "Training Step: 527  | total loss: \u001b[1m\u001b[32m0.02130\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 028 | loss: 0.02130 - R2: 1.0002 -- iter: 0896/1168\n",
      "Training Step: 528  | total loss: \u001b[1m\u001b[32m0.02127\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 028 | loss: 0.02127 - R2: 0.9997 -- iter: 0960/1168\n",
      "Training Step: 529  | total loss: \u001b[1m\u001b[32m0.01892\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 028 | loss: 0.01892 - R2: 1.0000 -- iter: 1024/1168\n",
      "Training Step: 530  | total loss: \u001b[1m\u001b[32m0.01892\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 028 | loss: 0.01892 - R2: 1.0000 -- iter: 1088/1168\n",
      "Training Step: 531  | total loss: \u001b[1m\u001b[32m0.01873\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 028 | loss: 0.01873 - R2: 1.0001 -- iter: 1152/1168\n",
      "Training Step: 532  | total loss: \u001b[1m\u001b[32m0.02072\u001b[0m\u001b[0m | time: 1.075s\n",
      "| SGD | epoch: 028 | loss: 0.02072 - R2: 0.9998 | val_loss: 0.02847 - val_acc: 1.0002 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 533  | total loss: \u001b[1m\u001b[32m0.02072\u001b[0m\u001b[0m | time: 0.009s\n",
      "| SGD | epoch: 029 | loss: 0.02072 - R2: 0.9998 -- iter: 0064/1168\n",
      "Training Step: 534  | total loss: \u001b[1m\u001b[32m0.02041\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 029 | loss: 0.02041 - R2: 0.9996 -- iter: 0128/1168\n",
      "Training Step: 535  | total loss: \u001b[1m\u001b[32m0.02041\u001b[0m\u001b[0m | time: 0.017s\n",
      "| SGD | epoch: 029 | loss: 0.02041 - R2: 0.9996 -- iter: 0192/1168\n",
      "Training Step: 536  | total loss: \u001b[1m\u001b[32m0.01998\u001b[0m\u001b[0m | time: 0.020s\n",
      "| SGD | epoch: 029 | loss: 0.01998 - R2: 0.9993 -- iter: 0256/1168\n",
      "Training Step: 537  | total loss: \u001b[1m\u001b[32m0.01935\u001b[0m\u001b[0m | time: 0.023s\n",
      "| SGD | epoch: 029 | loss: 0.01935 - R2: 0.9994 -- iter: 0320/1168\n",
      "Training Step: 538  | total loss: \u001b[1m\u001b[32m0.01918\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 029 | loss: 0.01918 - R2: 0.9996 -- iter: 0384/1168\n",
      "Training Step: 539  | total loss: \u001b[1m\u001b[32m0.01815\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 029 | loss: 0.01815 - R2: 0.9996 -- iter: 0448/1168\n",
      "Training Step: 540  | total loss: \u001b[1m\u001b[32m0.01709\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 029 | loss: 0.01709 - R2: 0.9993 -- iter: 0512/1168\n",
      "Training Step: 541  | total loss: \u001b[1m\u001b[32m0.01709\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 029 | loss: 0.01709 - R2: 0.9993 -- iter: 0576/1168\n",
      "Training Step: 542  | total loss: \u001b[1m\u001b[32m0.01672\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 029 | loss: 0.01672 - R2: 0.9993 -- iter: 0640/1168\n",
      "Training Step: 543  | total loss: \u001b[1m\u001b[32m0.01949\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 029 | loss: 0.01949 - R2: 0.9998 -- iter: 0704/1168\n",
      "Training Step: 544  | total loss: \u001b[1m\u001b[32m0.01897\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 029 | loss: 0.01897 - R2: 0.9999 -- iter: 0768/1168\n",
      "Training Step: 545  | total loss: \u001b[1m\u001b[32m0.01896\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 029 | loss: 0.01896 - R2: 1.0000 -- iter: 0832/1168\n",
      "Training Step: 546  | total loss: \u001b[1m\u001b[32m0.01896\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 029 | loss: 0.01896 - R2: 1.0003 -- iter: 0896/1168\n",
      "Training Step: 547  | total loss: \u001b[1m\u001b[32m0.02019\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 029 | loss: 0.02019 - R2: 1.0010 -- iter: 0960/1168\n",
      "Training Step: 548  | total loss: \u001b[1m\u001b[32m0.02019\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 029 | loss: 0.02019 - R2: 1.0010 -- iter: 1024/1168\n",
      "Training Step: 549  | total loss: \u001b[1m\u001b[32m0.02010\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 029 | loss: 0.02010 - R2: 1.0002 -- iter: 1088/1168\n",
      "Training Step: 550  | total loss: \u001b[1m\u001b[32m0.02031\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 029 | loss: 0.02031 - R2: 1.0003 -- iter: 1152/1168\n",
      "Training Step: 551  | total loss: \u001b[1m\u001b[32m0.01982\u001b[0m\u001b[0m | time: 1.079s\n",
      "| SGD | epoch: 029 | loss: 0.01982 - R2: 1.0001 | val_loss: 0.02793 - val_acc: 1.0013 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 552  | total loss: \u001b[1m\u001b[32m0.01905\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 030 | loss: 0.01905 - R2: 0.9994 -- iter: 0064/1168\n",
      "Training Step: 553  | total loss: \u001b[1m\u001b[32m0.01976\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 030 | loss: 0.01976 - R2: 0.9996 -- iter: 0128/1168\n",
      "Training Step: 554  | total loss: \u001b[1m\u001b[32m0.01954\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 030 | loss: 0.01954 - R2: 1.0004 -- iter: 0192/1168\n",
      "Training Step: 555  | total loss: \u001b[1m\u001b[32m0.01954\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 030 | loss: 0.01954 - R2: 1.0004 -- iter: 0256/1168\n",
      "Training Step: 556  | total loss: \u001b[1m\u001b[32m0.01798\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 030 | loss: 0.01798 - R2: 0.9998 -- iter: 0320/1168\n",
      "Training Step: 557  | total loss: \u001b[1m\u001b[32m0.01774\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 030 | loss: 0.01774 - R2: 0.9997 -- iter: 0384/1168\n",
      "Training Step: 558  | total loss: \u001b[1m\u001b[32m0.01778\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 030 | loss: 0.01778 - R2: 0.9994 -- iter: 0448/1168\n",
      "Training Step: 559  | total loss: \u001b[1m\u001b[32m0.01734\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 030 | loss: 0.01734 - R2: 0.9990 -- iter: 0512/1168\n",
      "Training Step: 560  | total loss: \u001b[1m\u001b[32m0.01676\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 030 | loss: 0.01676 - R2: 0.9988 -- iter: 0576/1168\n",
      "Training Step: 561  | total loss: \u001b[1m\u001b[32m0.01676\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 030 | loss: 0.01676 - R2: 0.9988 -- iter: 0640/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 562  | total loss: \u001b[1m\u001b[32m0.01686\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 030 | loss: 0.01686 - R2: 0.9992 -- iter: 0704/1168\n",
      "Training Step: 563  | total loss: \u001b[1m\u001b[32m0.01686\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 030 | loss: 0.01686 - R2: 0.9992 -- iter: 0768/1168\n",
      "Training Step: 564  | total loss: \u001b[1m\u001b[32m0.01803\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 030 | loss: 0.01803 - R2: 1.0000 -- iter: 0832/1168\n",
      "Training Step: 565  | total loss: \u001b[1m\u001b[32m0.01974\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 030 | loss: 0.01974 - R2: 1.0001 -- iter: 0896/1168\n",
      "Training Step: 566  | total loss: \u001b[1m\u001b[32m0.01974\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 030 | loss: 0.01974 - R2: 1.0001 -- iter: 0960/1168\n",
      "Training Step: 567  | total loss: \u001b[1m\u001b[32m0.01979\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 030 | loss: 0.01979 - R2: 1.0008 -- iter: 1024/1168\n",
      "Training Step: 568  | total loss: \u001b[1m\u001b[32m0.04184\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 030 | loss: 0.04184 - R2: 1.0006 -- iter: 1088/1168\n",
      "Training Step: 569  | total loss: \u001b[1m\u001b[32m0.04184\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 030 | loss: 0.04184 - R2: 1.0000 -- iter: 1152/1168\n",
      "Training Step: 570  | total loss: \u001b[1m\u001b[32m0.03891\u001b[0m\u001b[0m | time: 1.160s\n",
      "| SGD | epoch: 030 | loss: 0.03891 - R2: 1.0001 | val_loss: 0.02817 - val_acc: 1.0006 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 571  | total loss: \u001b[1m\u001b[32m0.03667\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 031 | loss: 0.03667 - R2: 0.9997 -- iter: 0064/1168\n",
      "Training Step: 572  | total loss: \u001b[1m\u001b[32m0.03561\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 031 | loss: 0.03561 - R2: 1.0003 -- iter: 0128/1168\n",
      "Training Step: 573  | total loss: \u001b[1m\u001b[32m0.03347\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 031 | loss: 0.03347 - R2: 1.0003 -- iter: 0192/1168\n",
      "Training Step: 574  | total loss: \u001b[1m\u001b[32m0.03156\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 031 | loss: 0.03156 - R2: 0.9995 -- iter: 0256/1168\n",
      "Training Step: 575  | total loss: \u001b[1m\u001b[32m0.02973\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 031 | loss: 0.02973 - R2: 0.9996 -- iter: 0320/1168\n",
      "Training Step: 576  | total loss: \u001b[1m\u001b[32m0.02836\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 031 | loss: 0.02836 - R2: 0.9999 -- iter: 0384/1168\n",
      "Training Step: 577  | total loss: \u001b[1m\u001b[32m0.02792\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 031 | loss: 0.02792 - R2: 1.0000 -- iter: 0448/1168\n",
      "Training Step: 578  | total loss: \u001b[1m\u001b[32m0.02792\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 031 | loss: 0.02792 - R2: 1.0000 -- iter: 0512/1168\n",
      "Training Step: 579  | total loss: \u001b[1m\u001b[32m0.02894\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 031 | loss: 0.02894 - R2: 1.0003 -- iter: 0576/1168\n",
      "Training Step: 580  | total loss: \u001b[1m\u001b[32m0.02511\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 031 | loss: 0.02511 - R2: 1.0003 -- iter: 0640/1168\n",
      "Training Step: 581  | total loss: \u001b[1m\u001b[32m0.02399\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 031 | loss: 0.02399 - R2: 1.0000 -- iter: 0704/1168\n",
      "Training Step: 582  | total loss: \u001b[1m\u001b[32m0.02399\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 031 | loss: 0.02399 - R2: 1.0000 -- iter: 0768/1168\n",
      "Training Step: 583  | total loss: \u001b[1m\u001b[32m0.02249\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 031 | loss: 0.02249 - R2: 0.9999 -- iter: 0832/1168\n",
      "Training Step: 584  | total loss: \u001b[1m\u001b[32m0.02249\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 031 | loss: 0.02249 - R2: 0.9999 -- iter: 0896/1168\n",
      "Training Step: 585  | total loss: \u001b[1m\u001b[32m0.02324\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 031 | loss: 0.02324 - R2: 0.9999 -- iter: 0960/1168\n",
      "Training Step: 586  | total loss: \u001b[1m\u001b[32m0.02238\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 031 | loss: 0.02238 - R2: 0.9995 -- iter: 1024/1168\n",
      "Training Step: 587  | total loss: \u001b[1m\u001b[32m0.02273\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 031 | loss: 0.02273 - R2: 1.0000 -- iter: 1088/1168\n",
      "Training Step: 588  | total loss: \u001b[1m\u001b[32m0.02161\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 031 | loss: 0.02161 - R2: 1.0002 -- iter: 1152/1168\n",
      "Training Step: 589  | total loss: \u001b[1m\u001b[32m0.02183\u001b[0m\u001b[0m | time: 1.162s\n",
      "| SGD | epoch: 031 | loss: 0.02183 - R2: 1.0000 | val_loss: 0.02821 - val_acc: 1.0008 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 590  | total loss: \u001b[1m\u001b[32m0.02138\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 032 | loss: 0.02138 - R2: 0.9995 -- iter: 0064/1168\n",
      "Training Step: 591  | total loss: \u001b[1m\u001b[32m0.02067\u001b[0m\u001b[0m | time: 0.005s\n",
      "| SGD | epoch: 032 | loss: 0.02067 - R2: 0.9997 -- iter: 0128/1168\n",
      "Training Step: 592  | total loss: \u001b[1m\u001b[32m0.02007\u001b[0m\u001b[0m | time: 0.008s\n",
      "| SGD | epoch: 032 | loss: 0.02007 - R2: 0.9996 -- iter: 0192/1168\n",
      "Training Step: 593  | total loss: \u001b[1m\u001b[32m0.02015\u001b[0m\u001b[0m | time: 0.011s\n",
      "| SGD | epoch: 032 | loss: 0.02015 - R2: 0.9995 -- iter: 0256/1168\n",
      "Training Step: 594  | total loss: \u001b[1m\u001b[32m0.02084\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 032 | loss: 0.02084 - R2: 0.9994 -- iter: 0320/1168\n",
      "Training Step: 595  | total loss: \u001b[1m\u001b[32m0.02245\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 032 | loss: 0.02245 - R2: 0.9997 -- iter: 0384/1168\n",
      "Training Step: 596  | total loss: \u001b[1m\u001b[32m0.02242\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 032 | loss: 0.02242 - R2: 1.0000 -- iter: 0448/1168\n",
      "Training Step: 597  | total loss: \u001b[1m\u001b[32m0.02243\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 032 | loss: 0.02243 - R2: 0.9998 -- iter: 0512/1168\n",
      "Training Step: 598  | total loss: \u001b[1m\u001b[32m0.02219\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 032 | loss: 0.02219 - R2: 1.0004 -- iter: 0576/1168\n",
      "Training Step: 599  | total loss: \u001b[1m\u001b[32m0.02085\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 032 | loss: 0.02085 - R2: 1.0006 -- iter: 0640/1168\n",
      "Training Step: 600  | total loss: \u001b[1m\u001b[32m0.01942\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 032 | loss: 0.01942 - R2: 1.0006 -- iter: 0704/1168\n",
      "Training Step: 601  | total loss: \u001b[1m\u001b[32m0.01907\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 032 | loss: 0.01907 - R2: 1.0006 -- iter: 0768/1168\n",
      "Training Step: 602  | total loss: \u001b[1m\u001b[32m0.01940\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 032 | loss: 0.01940 - R2: 1.0001 -- iter: 0832/1168\n",
      "Training Step: 603  | total loss: \u001b[1m\u001b[32m0.01940\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 032 | loss: 0.01940 - R2: 1.0001 -- iter: 0896/1168\n",
      "Training Step: 604  | total loss: \u001b[1m\u001b[32m0.01860\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 032 | loss: 0.01860 - R2: 0.9998 -- iter: 0960/1168\n",
      "Training Step: 605  | total loss: \u001b[1m\u001b[32m0.01828\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 032 | loss: 0.01828 - R2: 0.9995 -- iter: 1024/1168\n",
      "Training Step: 606  | total loss: \u001b[1m\u001b[32m0.01828\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 032 | loss: 0.01828 - R2: 0.9995 -- iter: 1088/1168\n",
      "Training Step: 607  | total loss: \u001b[1m\u001b[32m0.01726\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 032 | loss: 0.01726 - R2: 1.0001 -- iter: 1152/1168\n",
      "Training Step: 608  | total loss: \u001b[1m\u001b[32m0.01721\u001b[0m\u001b[0m | time: 1.086s\n",
      "| SGD | epoch: 032 | loss: 0.01721 - R2: 1.0005 | val_loss: 0.02837 - val_acc: 0.9986 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 609  | total loss: \u001b[1m\u001b[32m0.01769\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 033 | loss: 0.01769 - R2: 1.0001 -- iter: 0064/1168\n",
      "Training Step: 610  | total loss: \u001b[1m\u001b[32m0.01769\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 033 | loss: 0.01769 - R2: 1.0001 -- iter: 0128/1168\n",
      "Training Step: 611  | total loss: \u001b[1m\u001b[32m0.01912\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 033 | loss: 0.01912 - R2: 0.9999 -- iter: 0192/1168\n",
      "Training Step: 612  | total loss: \u001b[1m\u001b[32m0.01912\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 033 | loss: 0.01912 - R2: 0.9999 -- iter: 0256/1168\n",
      "Training Step: 613  | total loss: \u001b[1m\u001b[32m0.01853\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 033 | loss: 0.01853 - R2: 1.0001 -- iter: 0320/1168\n",
      "Training Step: 614  | total loss: \u001b[1m\u001b[32m0.01727\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 033 | loss: 0.01727 - R2: 0.9998 -- iter: 0384/1168\n",
      "Training Step: 615  | total loss: \u001b[1m\u001b[32m0.01731\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 033 | loss: 0.01731 - R2: 0.9998 -- iter: 0448/1168\n",
      "Training Step: 616  | total loss: \u001b[1m\u001b[32m0.01700\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 033 | loss: 0.01700 - R2: 1.0000 -- iter: 0512/1168\n",
      "Training Step: 617  | total loss: \u001b[1m\u001b[32m0.01700\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 033 | loss: 0.01700 - R2: 0.9995 -- iter: 0576/1168\n",
      "Training Step: 618  | total loss: \u001b[1m\u001b[32m0.01659\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 033 | loss: 0.01659 - R2: 0.9995 -- iter: 0640/1168\n",
      "Training Step: 619  | total loss: \u001b[1m\u001b[32m0.01624\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 033 | loss: 0.01624 - R2: 0.9993 -- iter: 0704/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 620  | total loss: \u001b[1m\u001b[32m0.01613\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 033 | loss: 0.01613 - R2: 0.9991 -- iter: 0768/1168\n",
      "Training Step: 621  | total loss: \u001b[1m\u001b[32m0.01709\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 033 | loss: 0.01709 - R2: 0.9985 -- iter: 0832/1168\n",
      "Training Step: 622  | total loss: \u001b[1m\u001b[32m0.01709\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 033 | loss: 0.01709 - R2: 0.9987 -- iter: 0896/1168\n",
      "Training Step: 623  | total loss: \u001b[1m\u001b[32m0.01642\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 033 | loss: 0.01642 - R2: 0.9990 -- iter: 0960/1168\n",
      "Training Step: 624  | total loss: \u001b[1m\u001b[32m0.01642\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 033 | loss: 0.01642 - R2: 0.9992 -- iter: 1024/1168\n",
      "Training Step: 625  | total loss: \u001b[1m\u001b[32m0.01904\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 033 | loss: 0.01904 - R2: 0.9997 -- iter: 1088/1168\n",
      "Training Step: 626  | total loss: \u001b[1m\u001b[32m0.02055\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 033 | loss: 0.02055 - R2: 1.0001 -- iter: 1152/1168\n",
      "Training Step: 627  | total loss: \u001b[1m\u001b[32m0.05069\u001b[0m\u001b[0m | time: 1.168s\n",
      "| SGD | epoch: 033 | loss: 0.05069 - R2: 1.0015 | val_loss: 0.02833 - val_acc: 0.9968 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 628  | total loss: \u001b[1m\u001b[32m0.04803\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 034 | loss: 0.04803 - R2: 1.0006 -- iter: 0064/1168\n",
      "Training Step: 629  | total loss: \u001b[1m\u001b[32m0.04547\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 034 | loss: 0.04547 - R2: 1.0002 -- iter: 0128/1168\n",
      "Training Step: 630  | total loss: \u001b[1m\u001b[32m0.04286\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 034 | loss: 0.04286 - R2: 0.9999 -- iter: 0192/1168\n",
      "Training Step: 631  | total loss: \u001b[1m\u001b[32m0.04176\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 034 | loss: 0.04176 - R2: 1.0004 -- iter: 0256/1168\n",
      "Training Step: 632  | total loss: \u001b[1m\u001b[32m0.04176\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 034 | loss: 0.04176 - R2: 1.0004 -- iter: 0320/1168\n",
      "Training Step: 633  | total loss: \u001b[1m\u001b[32m0.03779\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 034 | loss: 0.03779 - R2: 1.0003 -- iter: 0384/1168\n",
      "Training Step: 634  | total loss: \u001b[1m\u001b[32m0.03613\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 034 | loss: 0.03613 - R2: 1.0002 -- iter: 0448/1168\n",
      "Training Step: 635  | total loss: \u001b[1m\u001b[32m0.03449\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 034 | loss: 0.03449 - R2: 0.9998 -- iter: 0512/1168\n",
      "Training Step: 636  | total loss: \u001b[1m\u001b[32m0.03234\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 034 | loss: 0.03234 - R2: 0.9996 -- iter: 0576/1168\n",
      "Training Step: 637  | total loss: \u001b[1m\u001b[32m0.03016\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 034 | loss: 0.03016 - R2: 0.9994 -- iter: 0640/1168\n",
      "Training Step: 638  | total loss: \u001b[1m\u001b[32m0.02860\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 034 | loss: 0.02860 - R2: 0.9995 -- iter: 0704/1168\n",
      "Training Step: 639  | total loss: \u001b[1m\u001b[32m0.02860\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 034 | loss: 0.02860 - R2: 0.9992 -- iter: 0768/1168\n",
      "Training Step: 640  | total loss: \u001b[1m\u001b[32m0.02429\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 034 | loss: 0.02429 - R2: 0.9990 -- iter: 0832/1168\n",
      "Training Step: 641  | total loss: \u001b[1m\u001b[32m0.02392\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 034 | loss: 0.02392 - R2: 0.9988 -- iter: 0896/1168\n",
      "Training Step: 642  | total loss: \u001b[1m\u001b[32m0.02392\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 034 | loss: 0.02392 - R2: 0.9988 -- iter: 0960/1168\n",
      "Training Step: 643  | total loss: \u001b[1m\u001b[32m0.02364\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 034 | loss: 0.02364 - R2: 0.9990 -- iter: 1024/1168\n",
      "Training Step: 644  | total loss: \u001b[1m\u001b[32m0.02251\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 034 | loss: 0.02251 - R2: 0.9995 -- iter: 1088/1168\n",
      "Training Step: 645  | total loss: \u001b[1m\u001b[32m0.02251\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 034 | loss: 0.02251 - R2: 0.9995 -- iter: 1152/1168\n",
      "Training Step: 646  | total loss: \u001b[1m\u001b[32m0.02140\u001b[0m\u001b[0m | time: 1.159s\n",
      "| SGD | epoch: 034 | loss: 0.02140 - R2: 0.9995 | val_loss: 0.02798 - val_acc: 1.0004 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 647  | total loss: \u001b[1m\u001b[32m0.02123\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 035 | loss: 0.02123 - R2: 1.0001 -- iter: 0064/1168\n",
      "Training Step: 648  | total loss: \u001b[1m\u001b[32m0.05388\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 035 | loss: 0.05388 - R2: 0.9971 -- iter: 0128/1168\n",
      "Training Step: 649  | total loss: \u001b[1m\u001b[32m0.04699\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 035 | loss: 0.04699 - R2: 0.9988 -- iter: 0192/1168\n",
      "Training Step: 650  | total loss: \u001b[1m\u001b[32m0.04699\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 035 | loss: 0.04699 - R2: 0.9988 -- iter: 0256/1168\n",
      "Training Step: 651  | total loss: \u001b[1m\u001b[32m0.04523\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 035 | loss: 0.04523 - R2: 0.9994 -- iter: 0320/1168\n",
      "Training Step: 652  | total loss: \u001b[1m\u001b[32m0.04289\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 035 | loss: 0.04289 - R2: 0.9996 -- iter: 0384/1168\n",
      "Training Step: 653  | total loss: \u001b[1m\u001b[32m0.04129\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 035 | loss: 0.04129 - R2: 0.9998 -- iter: 0448/1168\n",
      "Training Step: 654  | total loss: \u001b[1m\u001b[32m0.03546\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 035 | loss: 0.03546 - R2: 0.9997 -- iter: 0512/1168\n",
      "Training Step: 655  | total loss: \u001b[1m\u001b[32m0.03515\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 035 | loss: 0.03515 - R2: 1.0000 -- iter: 0576/1168\n",
      "Training Step: 656  | total loss: \u001b[1m\u001b[32m0.03515\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 035 | loss: 0.03515 - R2: 1.0000 -- iter: 0640/1168\n",
      "Training Step: 657  | total loss: \u001b[1m\u001b[32m0.03209\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 035 | loss: 0.03209 - R2: 0.9999 -- iter: 0704/1168\n",
      "Training Step: 658  | total loss: \u001b[1m\u001b[32m0.03126\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 035 | loss: 0.03126 - R2: 1.0006 -- iter: 0768/1168\n",
      "Training Step: 659  | total loss: \u001b[1m\u001b[32m0.03005\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 035 | loss: 0.03005 - R2: 1.0006 -- iter: 0832/1168\n",
      "Training Step: 660  | total loss: \u001b[1m\u001b[32m0.03005\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 035 | loss: 0.03005 - R2: 1.0006 -- iter: 0896/1168\n",
      "Training Step: 661  | total loss: \u001b[1m\u001b[32m0.02880\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 035 | loss: 0.02880 - R2: 1.0005 -- iter: 0960/1168\n",
      "Training Step: 662  | total loss: \u001b[1m\u001b[32m0.02786\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 035 | loss: 0.02786 - R2: 1.0005 -- iter: 1024/1168\n",
      "Training Step: 663  | total loss: \u001b[1m\u001b[32m0.02712\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 035 | loss: 0.02712 - R2: 1.0006 -- iter: 1088/1168\n",
      "Training Step: 664  | total loss: \u001b[1m\u001b[32m0.02674\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 035 | loss: 0.02674 - R2: 1.0007 -- iter: 1152/1168\n",
      "Training Step: 665  | total loss: \u001b[1m\u001b[32m0.02505\u001b[0m\u001b[0m | time: 1.176s\n",
      "| SGD | epoch: 035 | loss: 0.02505 - R2: 1.0000 | val_loss: 0.02785 - val_acc: 1.0013 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 666  | total loss: \u001b[1m\u001b[32m0.02405\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 036 | loss: 0.02405 - R2: 0.9995 -- iter: 0064/1168\n",
      "Training Step: 667  | total loss: \u001b[1m\u001b[32m0.02330\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 036 | loss: 0.02330 - R2: 0.9996 -- iter: 0128/1168\n",
      "Training Step: 668  | total loss: \u001b[1m\u001b[32m0.05154\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 036 | loss: 0.05154 - R2: 1.0009 -- iter: 0192/1168\n",
      "Training Step: 669  | total loss: \u001b[1m\u001b[32m0.05176\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 036 | loss: 0.05176 - R2: 1.0005 -- iter: 0256/1168\n",
      "Training Step: 670  | total loss: \u001b[1m\u001b[32m0.04917\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 036 | loss: 0.04917 - R2: 0.9996 -- iter: 0320/1168\n",
      "Training Step: 671  | total loss: \u001b[1m\u001b[32m0.04530\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 036 | loss: 0.04530 - R2: 0.9996 -- iter: 0384/1168\n",
      "Training Step: 672  | total loss: \u001b[1m\u001b[32m0.04283\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 036 | loss: 0.04283 - R2: 1.0002 -- iter: 0448/1168\n",
      "Training Step: 673  | total loss: \u001b[1m\u001b[32m0.04041\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 036 | loss: 0.04041 - R2: 1.0002 -- iter: 0512/1168\n",
      "Training Step: 674  | total loss: \u001b[1m\u001b[32m0.03758\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 036 | loss: 0.03758 - R2: 1.0000 -- iter: 0576/1168\n",
      "Training Step: 675  | total loss: \u001b[1m\u001b[32m0.03562\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 036 | loss: 0.03562 - R2: 1.0000 -- iter: 0640/1168\n",
      "Training Step: 676  | total loss: \u001b[1m\u001b[32m0.03354\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 036 | loss: 0.03354 - R2: 0.9998 -- iter: 0704/1168\n",
      "Training Step: 677  | total loss: \u001b[1m\u001b[32m0.03181\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 036 | loss: 0.03181 - R2: 0.9995 -- iter: 0768/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 678  | total loss: \u001b[1m\u001b[32m0.03063\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 036 | loss: 0.03063 - R2: 0.9992 -- iter: 0832/1168\n",
      "Training Step: 679  | total loss: \u001b[1m\u001b[32m0.02906\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 036 | loss: 0.02906 - R2: 0.9991 -- iter: 0896/1168\n",
      "Training Step: 680  | total loss: \u001b[1m\u001b[32m0.02698\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 036 | loss: 0.02698 - R2: 0.9999 -- iter: 0960/1168\n",
      "Training Step: 681  | total loss: \u001b[1m\u001b[32m0.02366\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 036 | loss: 0.02366 - R2: 1.0002 -- iter: 1024/1168\n",
      "Training Step: 682  | total loss: \u001b[1m\u001b[32m0.02366\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 036 | loss: 0.02366 - R2: 1.0002 -- iter: 1088/1168\n",
      "Training Step: 683  | total loss: \u001b[1m\u001b[32m0.02349\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 036 | loss: 0.02349 - R2: 1.0006 -- iter: 1152/1168\n",
      "Training Step: 684  | total loss: \u001b[1m\u001b[32m0.02222\u001b[0m\u001b[0m | time: 1.135s\n",
      "| SGD | epoch: 036 | loss: 0.02222 - R2: 1.0004 | val_loss: 0.02782 - val_acc: 0.9996 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 685  | total loss: \u001b[1m\u001b[32m0.02242\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 037 | loss: 0.02242 - R2: 0.9999 -- iter: 0064/1168\n",
      "Training Step: 686  | total loss: \u001b[1m\u001b[32m0.02249\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 037 | loss: 0.02249 - R2: 1.0001 -- iter: 0128/1168\n",
      "Training Step: 687  | total loss: \u001b[1m\u001b[32m0.02249\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 037 | loss: 0.02249 - R2: 0.9997 -- iter: 0192/1168\n",
      "Training Step: 688  | total loss: \u001b[1m\u001b[32m0.04639\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 037 | loss: 0.04639 - R2: 1.0008 -- iter: 0256/1168\n",
      "Training Step: 689  | total loss: \u001b[1m\u001b[32m0.04318\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 037 | loss: 0.04318 - R2: 1.0003 -- iter: 0320/1168\n",
      "Training Step: 690  | total loss: \u001b[1m\u001b[32m0.04070\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 037 | loss: 0.04070 - R2: 1.0003 -- iter: 0384/1168\n",
      "Training Step: 691  | total loss: \u001b[1m\u001b[32m0.03850\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 037 | loss: 0.03850 - R2: 0.9998 -- iter: 0448/1168\n",
      "Training Step: 692  | total loss: \u001b[1m\u001b[32m0.03600\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 037 | loss: 0.03600 - R2: 1.0000 -- iter: 0512/1168\n",
      "Training Step: 693  | total loss: \u001b[1m\u001b[32m0.03600\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 037 | loss: 0.03600 - R2: 1.0000 -- iter: 0576/1168\n",
      "Training Step: 694  | total loss: \u001b[1m\u001b[32m0.03244\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 037 | loss: 0.03244 - R2: 1.0002 -- iter: 0640/1168\n",
      "Training Step: 695  | total loss: \u001b[1m\u001b[32m0.03244\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 037 | loss: 0.03244 - R2: 1.0002 -- iter: 0704/1168\n",
      "Training Step: 696  | total loss: \u001b[1m\u001b[32m0.03071\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 037 | loss: 0.03071 - R2: 0.9992 -- iter: 0768/1168\n",
      "Training Step: 697  | total loss: \u001b[1m\u001b[32m0.03034\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 037 | loss: 0.03034 - R2: 0.9993 -- iter: 0832/1168\n",
      "Training Step: 698  | total loss: \u001b[1m\u001b[32m0.02910\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 037 | loss: 0.02910 - R2: 0.9993 -- iter: 0896/1168\n",
      "Training Step: 699  | total loss: \u001b[1m\u001b[32m0.02757\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 037 | loss: 0.02757 - R2: 0.9994 -- iter: 0960/1168\n",
      "Training Step: 700  | total loss: \u001b[1m\u001b[32m0.02628\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 037 | loss: 0.02628 - R2: 0.9987 -- iter: 1024/1168\n",
      "Training Step: 701  | total loss: \u001b[1m\u001b[32m0.02487\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 037 | loss: 0.02487 - R2: 0.9983 -- iter: 1088/1168\n",
      "Training Step: 702  | total loss: \u001b[1m\u001b[32m0.02598\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 037 | loss: 0.02598 - R2: 0.9990 -- iter: 1152/1168\n",
      "Training Step: 703  | total loss: \u001b[1m\u001b[32m0.02598\u001b[0m\u001b[0m | time: 1.107s\n",
      "| SGD | epoch: 037 | loss: 0.02598 - R2: 0.9990 | val_loss: 0.02873 - val_acc: 1.0021 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 704  | total loss: \u001b[1m\u001b[32m0.02438\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 038 | loss: 0.02438 - R2: 0.9990 -- iter: 0064/1168\n",
      "Training Step: 705  | total loss: \u001b[1m\u001b[32m0.02438\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 038 | loss: 0.02438 - R2: 0.9990 -- iter: 0128/1168\n",
      "Training Step: 706  | total loss: \u001b[1m\u001b[32m0.02318\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 038 | loss: 0.02318 - R2: 0.9991 -- iter: 0192/1168\n",
      "Training Step: 707  | total loss: \u001b[1m\u001b[32m0.04815\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 038 | loss: 0.04815 - R2: 0.9991 -- iter: 0256/1168\n",
      "Training Step: 708  | total loss: \u001b[1m\u001b[32m0.04815\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 038 | loss: 0.04815 - R2: 0.9991 -- iter: 0320/1168\n",
      "Training Step: 709  | total loss: \u001b[1m\u001b[32m0.04487\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 038 | loss: 0.04487 - R2: 0.9991 -- iter: 0384/1168\n",
      "Training Step: 710  | total loss: \u001b[1m\u001b[32m0.03876\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 038 | loss: 0.03876 - R2: 0.9995 -- iter: 0448/1168\n",
      "Training Step: 711  | total loss: \u001b[1m\u001b[32m0.03876\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 038 | loss: 0.03876 - R2: 0.9995 -- iter: 0512/1168\n",
      "Training Step: 712  | total loss: \u001b[1m\u001b[32m0.03690\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 038 | loss: 0.03690 - R2: 0.9991 -- iter: 0576/1168\n",
      "Training Step: 713  | total loss: \u001b[1m\u001b[32m0.03545\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 038 | loss: 0.03545 - R2: 0.9997 -- iter: 0640/1168\n",
      "Training Step: 714  | total loss: \u001b[1m\u001b[32m0.03419\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 038 | loss: 0.03419 - R2: 0.9993 -- iter: 0704/1168\n",
      "Training Step: 715  | total loss: \u001b[1m\u001b[32m0.03366\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 038 | loss: 0.03366 - R2: 0.9997 -- iter: 0768/1168\n",
      "Training Step: 716  | total loss: \u001b[1m\u001b[32m0.03000\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 038 | loss: 0.03000 - R2: 0.9994 -- iter: 0832/1168\n",
      "Training Step: 717  | total loss: \u001b[1m\u001b[32m0.02833\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 038 | loss: 0.02833 - R2: 1.0000 -- iter: 0896/1168\n",
      "Training Step: 718  | total loss: \u001b[1m\u001b[32m0.02833\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 038 | loss: 0.02833 - R2: 1.0000 -- iter: 0960/1168\n",
      "Training Step: 719  | total loss: \u001b[1m\u001b[32m0.02603\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 038 | loss: 0.02603 - R2: 1.0000 -- iter: 1024/1168\n",
      "Training Step: 720  | total loss: \u001b[1m\u001b[32m0.02603\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 038 | loss: 0.02603 - R2: 1.0000 -- iter: 1088/1168\n",
      "Training Step: 721  | total loss: \u001b[1m\u001b[32m0.02325\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 038 | loss: 0.02325 - R2: 1.0005 -- iter: 1152/1168\n",
      "Training Step: 722  | total loss: \u001b[1m\u001b[32m0.02284\u001b[0m\u001b[0m | time: 1.134s\n",
      "| SGD | epoch: 038 | loss: 0.02284 - R2: 1.0005 | val_loss: 0.02761 - val_acc: 1.0002 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 723  | total loss: \u001b[1m\u001b[32m0.02284\u001b[0m\u001b[0m | time: 0.010s\n",
      "| SGD | epoch: 039 | loss: 0.02284 - R2: 1.0005 -- iter: 0064/1168\n",
      "Training Step: 724  | total loss: \u001b[1m\u001b[32m0.02152\u001b[0m\u001b[0m | time: 0.014s\n",
      "| SGD | epoch: 039 | loss: 0.02152 - R2: 1.0007 -- iter: 0128/1168\n",
      "Training Step: 725  | total loss: \u001b[1m\u001b[32m0.02262\u001b[0m\u001b[0m | time: 0.017s\n",
      "| SGD | epoch: 039 | loss: 0.02262 - R2: 1.0007 -- iter: 0192/1168\n",
      "Training Step: 726  | total loss: \u001b[1m\u001b[32m0.02187\u001b[0m\u001b[0m | time: 0.020s\n",
      "| SGD | epoch: 039 | loss: 0.02187 - R2: 1.0004 -- iter: 0256/1168\n",
      "Training Step: 727  | total loss: \u001b[1m\u001b[32m0.02227\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 039 | loss: 0.02227 - R2: 1.0010 -- iter: 0320/1168\n",
      "Training Step: 728  | total loss: \u001b[1m\u001b[32m0.04912\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 039 | loss: 0.04912 - R2: 0.9995 -- iter: 0384/1168\n",
      "Training Step: 729  | total loss: \u001b[1m\u001b[32m0.04540\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 039 | loss: 0.04540 - R2: 0.9995 -- iter: 0448/1168\n",
      "Training Step: 730  | total loss: \u001b[1m\u001b[32m0.04384\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 039 | loss: 0.04384 - R2: 0.9994 -- iter: 0512/1168\n",
      "Training Step: 731  | total loss: \u001b[1m\u001b[32m0.04099\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 039 | loss: 0.04099 - R2: 0.9994 -- iter: 0576/1168\n",
      "Training Step: 732  | total loss: \u001b[1m\u001b[32m0.03987\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 039 | loss: 0.03987 - R2: 0.9999 -- iter: 0640/1168\n",
      "Training Step: 733  | total loss: \u001b[1m\u001b[32m0.03749\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 039 | loss: 0.03749 - R2: 0.9996 -- iter: 0704/1168\n",
      "Training Step: 734  | total loss: \u001b[1m\u001b[32m0.03637\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 039 | loss: 0.03637 - R2: 1.0002 -- iter: 0768/1168\n",
      "Training Step: 735  | total loss: \u001b[1m\u001b[32m0.03384\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 039 | loss: 0.03384 - R2: 1.0004 -- iter: 0832/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 736  | total loss: \u001b[1m\u001b[32m0.03384\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 039 | loss: 0.03384 - R2: 1.0004 -- iter: 0896/1168\n",
      "Training Step: 737  | total loss: \u001b[1m\u001b[32m0.03069\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 039 | loss: 0.03069 - R2: 1.0001 -- iter: 0960/1168\n",
      "Training Step: 738  | total loss: \u001b[1m\u001b[32m0.02920\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 039 | loss: 0.02920 - R2: 0.9999 -- iter: 1024/1168\n",
      "Training Step: 739  | total loss: \u001b[1m\u001b[32m0.02920\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 039 | loss: 0.02920 - R2: 0.9999 -- iter: 1088/1168\n",
      "Training Step: 740  | total loss: \u001b[1m\u001b[32m0.02754\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 039 | loss: 0.02754 - R2: 1.0000 -- iter: 1152/1168\n",
      "Training Step: 741  | total loss: \u001b[1m\u001b[32m0.02472\u001b[0m\u001b[0m | time: 1.083s\n",
      "| SGD | epoch: 039 | loss: 0.02472 - R2: 0.9998 | val_loss: 0.02767 - val_acc: 1.0002 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 742  | total loss: \u001b[1m\u001b[32m0.02472\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 040 | loss: 0.02472 - R2: 0.9998 -- iter: 0064/1168\n",
      "Training Step: 743  | total loss: \u001b[1m\u001b[32m0.02323\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 040 | loss: 0.02323 - R2: 0.9999 -- iter: 0128/1168\n",
      "Training Step: 744  | total loss: \u001b[1m\u001b[32m0.02313\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 040 | loss: 0.02313 - R2: 1.0001 -- iter: 0192/1168\n",
      "Training Step: 745  | total loss: \u001b[1m\u001b[32m0.02313\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 040 | loss: 0.02313 - R2: 1.0001 -- iter: 0256/1168\n",
      "Training Step: 746  | total loss: \u001b[1m\u001b[32m0.02244\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 040 | loss: 0.02244 - R2: 0.9999 -- iter: 0320/1168\n",
      "Training Step: 747  | total loss: \u001b[1m\u001b[32m0.04859\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 040 | loss: 0.04859 - R2: 1.0009 -- iter: 0384/1168\n",
      "Training Step: 748  | total loss: \u001b[1m\u001b[32m0.04859\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 040 | loss: 0.04859 - R2: 1.0009 -- iter: 0448/1168\n",
      "Training Step: 749  | total loss: \u001b[1m\u001b[32m0.04393\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 040 | loss: 0.04393 - R2: 1.0001 -- iter: 0512/1168\n",
      "Training Step: 750  | total loss: \u001b[1m\u001b[32m0.04123\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 040 | loss: 0.04123 - R2: 1.0000 -- iter: 0576/1168\n",
      "Training Step: 751  | total loss: \u001b[1m\u001b[32m0.04024\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 040 | loss: 0.04024 - R2: 1.0005 -- iter: 0640/1168\n",
      "Training Step: 752  | total loss: \u001b[1m\u001b[32m0.03762\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 040 | loss: 0.03762 - R2: 1.0005 -- iter: 0704/1168\n",
      "Training Step: 753  | total loss: \u001b[1m\u001b[32m0.03565\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 040 | loss: 0.03565 - R2: 0.9998 -- iter: 0768/1168\n",
      "Training Step: 754  | total loss: \u001b[1m\u001b[32m0.03341\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 040 | loss: 0.03341 - R2: 0.9996 -- iter: 0832/1168\n",
      "Training Step: 755  | total loss: \u001b[1m\u001b[32m0.03341\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 040 | loss: 0.03341 - R2: 0.9996 -- iter: 0896/1168\n",
      "Training Step: 756  | total loss: \u001b[1m\u001b[32m0.03145\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 040 | loss: 0.03145 - R2: 0.9995 -- iter: 0960/1168\n",
      "Training Step: 757  | total loss: \u001b[1m\u001b[32m0.03042\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 040 | loss: 0.03042 - R2: 0.9991 -- iter: 1024/1168\n",
      "Training Step: 758  | total loss: \u001b[1m\u001b[32m0.02718\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 040 | loss: 0.02718 - R2: 0.9988 -- iter: 1088/1168\n",
      "Training Step: 759  | total loss: \u001b[1m\u001b[32m0.02589\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 040 | loss: 0.02589 - R2: 0.9999 -- iter: 1152/1168\n",
      "Training Step: 760  | total loss: \u001b[1m\u001b[32m0.02589\u001b[0m\u001b[0m | time: 1.153s\n",
      "| SGD | epoch: 040 | loss: 0.02589 - R2: 0.9999 | val_loss: 0.02756 - val_acc: 0.9981 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 761  | total loss: \u001b[1m\u001b[32m0.02359\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 041 | loss: 0.02359 - R2: 1.0003 -- iter: 0064/1168\n",
      "Training Step: 762  | total loss: \u001b[1m\u001b[32m0.02279\u001b[0m\u001b[0m | time: 0.009s\n",
      "| SGD | epoch: 041 | loss: 0.02279 - R2: 1.0000 -- iter: 0128/1168\n",
      "Training Step: 763  | total loss: \u001b[1m\u001b[32m0.02452\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 041 | loss: 0.02452 - R2: 1.0000 -- iter: 0192/1168\n",
      "Training Step: 764  | total loss: \u001b[1m\u001b[32m0.02493\u001b[0m\u001b[0m | time: 0.019s\n",
      "| SGD | epoch: 041 | loss: 0.02493 - R2: 1.0001 -- iter: 0256/1168\n",
      "Training Step: 765  | total loss: \u001b[1m\u001b[32m0.02493\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 041 | loss: 0.02493 - R2: 1.0001 -- iter: 0320/1168\n",
      "Training Step: 766  | total loss: \u001b[1m\u001b[32m0.02348\u001b[0m\u001b[0m | time: 0.026s\n",
      "| SGD | epoch: 041 | loss: 0.02348 - R2: 0.9998 -- iter: 0384/1168\n",
      "Training Step: 767  | total loss: \u001b[1m\u001b[32m0.02348\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 041 | loss: 0.02348 - R2: 0.9998 -- iter: 0448/1168\n",
      "Training Step: 768  | total loss: \u001b[1m\u001b[32m0.04935\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 041 | loss: 0.04935 - R2: 0.9998 -- iter: 0512/1168\n",
      "Training Step: 769  | total loss: \u001b[1m\u001b[32m0.04608\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 041 | loss: 0.04608 - R2: 0.9994 -- iter: 0576/1168\n",
      "Training Step: 770  | total loss: \u001b[1m\u001b[32m0.04312\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 041 | loss: 0.04312 - R2: 0.9994 -- iter: 0640/1168\n",
      "Training Step: 771  | total loss: \u001b[1m\u001b[32m0.04044\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 041 | loss: 0.04044 - R2: 0.9994 -- iter: 0704/1168\n",
      "Training Step: 772  | total loss: \u001b[1m\u001b[32m0.03792\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 041 | loss: 0.03792 - R2: 0.9997 -- iter: 0768/1168\n",
      "Training Step: 773  | total loss: \u001b[1m\u001b[32m0.03595\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 041 | loss: 0.03595 - R2: 1.0000 -- iter: 0832/1168\n",
      "Training Step: 774  | total loss: \u001b[1m\u001b[32m0.03343\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 041 | loss: 0.03343 - R2: 0.9999 -- iter: 0896/1168\n",
      "Training Step: 775  | total loss: \u001b[1m\u001b[32m0.03253\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 041 | loss: 0.03253 - R2: 0.9996 -- iter: 0960/1168\n",
      "Training Step: 776  | total loss: \u001b[1m\u001b[32m0.03078\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 041 | loss: 0.03078 - R2: 0.9997 -- iter: 1024/1168\n",
      "Training Step: 777  | total loss: \u001b[1m\u001b[32m0.02960\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 041 | loss: 0.02960 - R2: 0.9996 -- iter: 1088/1168\n",
      "Training Step: 778  | total loss: \u001b[1m\u001b[32m0.02960\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 041 | loss: 0.02960 - R2: 0.9996 -- iter: 1152/1168\n",
      "Training Step: 779  | total loss: \u001b[1m\u001b[32m0.02841\u001b[0m\u001b[0m | time: 1.076s\n",
      "| SGD | epoch: 041 | loss: 0.02841 - R2: 1.0002 | val_loss: 0.02830 - val_acc: 0.9989 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 780  | total loss: \u001b[1m\u001b[32m0.03114\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 042 | loss: 0.03114 - R2: 1.0002 -- iter: 0064/1168\n",
      "Training Step: 781  | total loss: \u001b[1m\u001b[32m0.03336\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 042 | loss: 0.03336 - R2: 1.0005 -- iter: 0128/1168\n",
      "Training Step: 782  | total loss: \u001b[1m\u001b[32m0.02957\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 042 | loss: 0.02957 - R2: 0.9999 -- iter: 0192/1168\n",
      "Training Step: 783  | total loss: \u001b[1m\u001b[32m0.02793\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 042 | loss: 0.02793 - R2: 0.9994 -- iter: 0256/1168\n",
      "Training Step: 784  | total loss: \u001b[1m\u001b[32m0.02708\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 042 | loss: 0.02708 - R2: 0.9993 -- iter: 0320/1168\n",
      "Training Step: 785  | total loss: \u001b[1m\u001b[32m0.02687\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 042 | loss: 0.02687 - R2: 0.9990 -- iter: 0384/1168\n",
      "Training Step: 786  | total loss: \u001b[1m\u001b[32m0.02629\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 042 | loss: 0.02629 - R2: 0.9993 -- iter: 0448/1168\n",
      "Training Step: 787  | total loss: \u001b[1m\u001b[32m0.02680\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 042 | loss: 0.02680 - R2: 0.9999 -- iter: 0512/1168\n",
      "Training Step: 788  | total loss: \u001b[1m\u001b[32m0.02593\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 042 | loss: 0.02593 - R2: 0.9996 -- iter: 0576/1168\n",
      "Training Step: 789  | total loss: \u001b[1m\u001b[32m0.02563\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 042 | loss: 0.02563 - R2: 0.9998 -- iter: 0640/1168\n",
      "Training Step: 790  | total loss: \u001b[1m\u001b[32m0.02577\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 042 | loss: 0.02577 - R2: 0.9999 -- iter: 0704/1168\n",
      "Training Step: 791  | total loss: \u001b[1m\u001b[32m0.02548\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 042 | loss: 0.02548 - R2: 0.9999 -- iter: 0768/1168\n",
      "Training Step: 792  | total loss: \u001b[1m\u001b[32m0.02548\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 042 | loss: 0.02548 - R2: 0.9999 -- iter: 0832/1168\n",
      "Training Step: 793  | total loss: \u001b[1m\u001b[32m0.02599\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 042 | loss: 0.02599 - R2: 1.0003 -- iter: 0896/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 794  | total loss: \u001b[1m\u001b[32m0.02480\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 042 | loss: 0.02480 - R2: 1.0000 -- iter: 0960/1168\n",
      "Training Step: 795  | total loss: \u001b[1m\u001b[32m0.02447\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 042 | loss: 0.02447 - R2: 1.0000 -- iter: 1024/1168\n",
      "Training Step: 796  | total loss: \u001b[1m\u001b[32m0.02365\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 042 | loss: 0.02365 - R2: 0.9999 -- iter: 1088/1168\n",
      "Training Step: 797  | total loss: \u001b[1m\u001b[32m0.02333\u001b[0m\u001b[0m | time: 0.182s\n",
      "| SGD | epoch: 042 | loss: 0.02333 - R2: 0.9999 -- iter: 1152/1168\n",
      "Training Step: 798  | total loss: \u001b[1m\u001b[32m0.02270\u001b[0m\u001b[0m | time: 1.187s\n",
      "| SGD | epoch: 042 | loss: 0.02270 - R2: 1.0000 | val_loss: 0.02835 - val_acc: 1.0000 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 799  | total loss: \u001b[1m\u001b[32m0.02134\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 043 | loss: 0.02134 - R2: 0.9996 -- iter: 0064/1168\n",
      "Training Step: 800  | total loss: \u001b[1m\u001b[32m0.02001\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 043 | loss: 0.02001 - R2: 0.9994 -- iter: 0128/1168\n",
      "Training Step: 801  | total loss: \u001b[1m\u001b[32m0.01980\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 043 | loss: 0.01980 - R2: 0.9992 -- iter: 0192/1168\n",
      "Training Step: 802  | total loss: \u001b[1m\u001b[32m0.01901\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 043 | loss: 0.01901 - R2: 0.9994 -- iter: 0256/1168\n",
      "Training Step: 803  | total loss: \u001b[1m\u001b[32m0.01901\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 043 | loss: 0.01901 - R2: 0.9993 -- iter: 0320/1168\n",
      "Training Step: 804  | total loss: \u001b[1m\u001b[32m0.01858\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 043 | loss: 0.01858 - R2: 0.9993 -- iter: 0384/1168\n",
      "Training Step: 805  | total loss: \u001b[1m\u001b[32m0.01834\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 043 | loss: 0.01834 - R2: 1.0000 -- iter: 0448/1168\n",
      "Training Step: 806  | total loss: \u001b[1m\u001b[32m0.01802\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 043 | loss: 0.01802 - R2: 0.9998 -- iter: 0512/1168\n",
      "Training Step: 807  | total loss: \u001b[1m\u001b[32m0.03972\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 043 | loss: 0.03972 - R2: 0.9997 -- iter: 0576/1168\n",
      "Training Step: 808  | total loss: \u001b[1m\u001b[32m0.03972\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 043 | loss: 0.03972 - R2: 0.9997 -- iter: 0640/1168\n",
      "Training Step: 809  | total loss: \u001b[1m\u001b[32m0.03615\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 043 | loss: 0.03615 - R2: 1.0001 -- iter: 0704/1168\n",
      "Training Step: 810  | total loss: \u001b[1m\u001b[32m0.03530\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 043 | loss: 0.03530 - R2: 1.0004 -- iter: 0768/1168\n",
      "Training Step: 811  | total loss: \u001b[1m\u001b[32m0.03530\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 043 | loss: 0.03530 - R2: 1.0004 -- iter: 0832/1168\n",
      "Training Step: 812  | total loss: \u001b[1m\u001b[32m0.03305\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 043 | loss: 0.03305 - R2: 0.9999 -- iter: 0896/1168\n",
      "Training Step: 813  | total loss: \u001b[1m\u001b[32m0.03213\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 043 | loss: 0.03213 - R2: 0.9995 -- iter: 0960/1168\n",
      "Training Step: 814  | total loss: \u001b[1m\u001b[32m0.03085\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 043 | loss: 0.03085 - R2: 0.9992 -- iter: 1024/1168\n",
      "Training Step: 815  | total loss: \u001b[1m\u001b[32m0.02890\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 043 | loss: 0.02890 - R2: 0.9997 -- iter: 1088/1168\n",
      "Training Step: 816  | total loss: \u001b[1m\u001b[32m0.02800\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 043 | loss: 0.02800 - R2: 0.9997 -- iter: 1152/1168\n",
      "Training Step: 817  | total loss: \u001b[1m\u001b[32m0.02701\u001b[0m\u001b[0m | time: 1.172s\n",
      "| SGD | epoch: 043 | loss: 0.02701 - R2: 0.9996 | val_loss: 0.02828 - val_acc: 1.0012 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 818  | total loss: \u001b[1m\u001b[32m0.02701\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 044 | loss: 0.02701 - R2: 0.9996 -- iter: 0064/1168\n",
      "Training Step: 819  | total loss: \u001b[1m\u001b[32m0.02560\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 044 | loss: 0.02560 - R2: 1.0003 -- iter: 0128/1168\n",
      "Training Step: 820  | total loss: \u001b[1m\u001b[32m0.02407\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 044 | loss: 0.02407 - R2: 1.0003 -- iter: 0192/1168\n",
      "Training Step: 821  | total loss: \u001b[1m\u001b[32m0.02248\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 044 | loss: 0.02248 - R2: 1.0011 -- iter: 0256/1168\n",
      "Training Step: 822  | total loss: \u001b[1m\u001b[32m0.02128\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 044 | loss: 0.02128 - R2: 1.0008 -- iter: 0320/1168\n",
      "Training Step: 823  | total loss: \u001b[1m\u001b[32m0.02305\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 044 | loss: 0.02305 - R2: 1.0010 -- iter: 0384/1168\n",
      "Training Step: 824  | total loss: \u001b[1m\u001b[32m0.02211\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 044 | loss: 0.02211 - R2: 1.0002 -- iter: 0448/1168\n",
      "Training Step: 825  | total loss: \u001b[1m\u001b[32m0.02182\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 044 | loss: 0.02182 - R2: 1.0007 -- iter: 0512/1168\n",
      "Training Step: 826  | total loss: \u001b[1m\u001b[32m0.02140\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 044 | loss: 0.02140 - R2: 1.0006 -- iter: 0576/1168\n",
      "Training Step: 827  | total loss: \u001b[1m\u001b[32m0.02024\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 044 | loss: 0.02024 - R2: 1.0003 -- iter: 0640/1168\n",
      "Training Step: 828  | total loss: \u001b[1m\u001b[32m0.01959\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 044 | loss: 0.01959 - R2: 0.9996 -- iter: 0704/1168\n",
      "Training Step: 829  | total loss: \u001b[1m\u001b[32m0.01959\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 044 | loss: 0.01959 - R2: 0.9996 -- iter: 0768/1168\n",
      "Training Step: 830  | total loss: \u001b[1m\u001b[32m0.02150\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 044 | loss: 0.02150 - R2: 0.9999 -- iter: 0832/1168\n",
      "Training Step: 831  | total loss: \u001b[1m\u001b[32m0.02150\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 044 | loss: 0.02150 - R2: 0.9999 -- iter: 0896/1168\n",
      "Training Step: 832  | total loss: \u001b[1m\u001b[32m0.02039\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 044 | loss: 0.02039 - R2: 0.9998 -- iter: 0960/1168\n",
      "Training Step: 833  | total loss: \u001b[1m\u001b[32m0.01994\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 044 | loss: 0.01994 - R2: 0.9998 -- iter: 1024/1168\n",
      "Training Step: 834  | total loss: \u001b[1m\u001b[32m0.01956\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 044 | loss: 0.01956 - R2: 0.9998 -- iter: 1088/1168\n",
      "Training Step: 835  | total loss: \u001b[1m\u001b[32m0.01956\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 044 | loss: 0.01956 - R2: 1.0001 -- iter: 1152/1168\n",
      "Training Step: 836  | total loss: \u001b[1m\u001b[32m0.02092\u001b[0m\u001b[0m | time: 1.163s\n",
      "| SGD | epoch: 044 | loss: 0.02092 - R2: 0.9992 | val_loss: 0.02776 - val_acc: 1.0010 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 837  | total loss: \u001b[1m\u001b[32m0.02092\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 045 | loss: 0.02092 - R2: 0.9992 -- iter: 0064/1168\n",
      "Training Step: 838  | total loss: \u001b[1m\u001b[32m0.01992\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 045 | loss: 0.01992 - R2: 0.9991 -- iter: 0128/1168\n",
      "Training Step: 839  | total loss: \u001b[1m\u001b[32m0.02138\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 045 | loss: 0.02138 - R2: 0.9996 -- iter: 0192/1168\n",
      "Training Step: 840  | total loss: \u001b[1m\u001b[32m0.02138\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 045 | loss: 0.02138 - R2: 0.9996 -- iter: 0256/1168\n",
      "Training Step: 841  | total loss: \u001b[1m\u001b[32m0.02146\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 045 | loss: 0.02146 - R2: 0.9996 -- iter: 0320/1168\n",
      "Training Step: 842  | total loss: \u001b[1m\u001b[32m0.02106\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 045 | loss: 0.02106 - R2: 0.9994 -- iter: 0384/1168\n",
      "Training Step: 843  | total loss: \u001b[1m\u001b[32m0.02119\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 045 | loss: 0.02119 - R2: 1.0002 -- iter: 0448/1168\n",
      "Training Step: 844  | total loss: \u001b[1m\u001b[32m0.02032\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 045 | loss: 0.02032 - R2: 1.0002 -- iter: 0512/1168\n",
      "Training Step: 845  | total loss: \u001b[1m\u001b[32m0.01974\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 045 | loss: 0.01974 - R2: 1.0000 -- iter: 0576/1168\n",
      "Training Step: 846  | total loss: \u001b[1m\u001b[32m0.01874\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 045 | loss: 0.01874 - R2: 0.9998 -- iter: 0640/1168\n",
      "Training Step: 847  | total loss: \u001b[1m\u001b[32m0.05100\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 045 | loss: 0.05100 - R2: 1.0008 -- iter: 0704/1168\n",
      "Training Step: 848  | total loss: \u001b[1m\u001b[32m0.05100\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 045 | loss: 0.05100 - R2: 1.0008 -- iter: 0768/1168\n",
      "Training Step: 849  | total loss: \u001b[1m\u001b[32m0.04819\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 045 | loss: 0.04819 - R2: 1.0000 -- iter: 0832/1168\n",
      "Training Step: 850  | total loss: \u001b[1m\u001b[32m0.04248\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 045 | loss: 0.04248 - R2: 0.9997 -- iter: 0896/1168\n",
      "Training Step: 851  | total loss: \u001b[1m\u001b[32m0.04065\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 045 | loss: 0.04065 - R2: 0.9992 -- iter: 0960/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 852  | total loss: \u001b[1m\u001b[32m0.03841\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 045 | loss: 0.03841 - R2: 0.9992 -- iter: 1024/1168\n",
      "Training Step: 853  | total loss: \u001b[1m\u001b[32m0.03841\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 045 | loss: 0.03841 - R2: 0.9992 -- iter: 1088/1168\n",
      "Training Step: 854  | total loss: \u001b[1m\u001b[32m0.03645\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 045 | loss: 0.03645 - R2: 0.9992 -- iter: 1152/1168\n",
      "Training Step: 855  | total loss: \u001b[1m\u001b[32m0.03473\u001b[0m\u001b[0m | time: 1.096s\n",
      "| SGD | epoch: 045 | loss: 0.03473 - R2: 0.9991 | val_loss: 0.02804 - val_acc: 1.0013 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 856  | total loss: \u001b[1m\u001b[32m0.03074\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 046 | loss: 0.03074 - R2: 0.9993 -- iter: 0064/1168\n",
      "Training Step: 857  | total loss: \u001b[1m\u001b[32m0.03074\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 046 | loss: 0.03074 - R2: 0.9993 -- iter: 0128/1168\n",
      "Training Step: 858  | total loss: \u001b[1m\u001b[32m0.02854\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 046 | loss: 0.02854 - R2: 0.9997 -- iter: 0192/1168\n",
      "Training Step: 859  | total loss: \u001b[1m\u001b[32m0.02854\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 046 | loss: 0.02854 - R2: 0.9997 -- iter: 0256/1168\n",
      "Training Step: 860  | total loss: \u001b[1m\u001b[32m0.02750\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 046 | loss: 0.02750 - R2: 0.9998 -- iter: 0320/1168\n",
      "Training Step: 861  | total loss: \u001b[1m\u001b[32m0.02632\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 046 | loss: 0.02632 - R2: 1.0003 -- iter: 0384/1168\n",
      "Training Step: 862  | total loss: \u001b[1m\u001b[32m0.02569\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 046 | loss: 0.02569 - R2: 1.0000 -- iter: 0448/1168\n",
      "Training Step: 863  | total loss: \u001b[1m\u001b[32m0.02463\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 046 | loss: 0.02463 - R2: 0.9997 -- iter: 0512/1168\n",
      "Training Step: 864  | total loss: \u001b[1m\u001b[32m0.02463\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 046 | loss: 0.02463 - R2: 1.0004 -- iter: 0576/1168\n",
      "Training Step: 865  | total loss: \u001b[1m\u001b[32m0.02523\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 046 | loss: 0.02523 - R2: 1.0004 -- iter: 0640/1168\n",
      "Training Step: 866  | total loss: \u001b[1m\u001b[32m0.02591\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 046 | loss: 0.02591 - R2: 1.0004 -- iter: 0704/1168\n",
      "Training Step: 867  | total loss: \u001b[1m\u001b[32m0.02481\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 046 | loss: 0.02481 - R2: 1.0002 -- iter: 0768/1168\n",
      "Training Step: 868  | total loss: \u001b[1m\u001b[32m0.02265\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 046 | loss: 0.02265 - R2: 0.9998 -- iter: 0832/1168\n",
      "Training Step: 869  | total loss: \u001b[1m\u001b[32m0.02265\u001b[0m\u001b[0m | time: 0.174s\n",
      "| SGD | epoch: 046 | loss: 0.02265 - R2: 0.9998 -- iter: 0896/1168\n",
      "Training Step: 870  | total loss: \u001b[1m\u001b[32m0.02198\u001b[0m\u001b[0m | time: 0.179s\n",
      "| SGD | epoch: 046 | loss: 0.02198 - R2: 1.0005 -- iter: 0960/1168\n",
      "Training Step: 871  | total loss: \u001b[1m\u001b[32m0.02151\u001b[0m\u001b[0m | time: 0.183s\n",
      "| SGD | epoch: 046 | loss: 0.02151 - R2: 1.0004 -- iter: 1024/1168\n",
      "Training Step: 872  | total loss: \u001b[1m\u001b[32m0.02151\u001b[0m\u001b[0m | time: 0.185s\n",
      "| SGD | epoch: 046 | loss: 0.02151 - R2: 1.0004 -- iter: 1088/1168\n",
      "Training Step: 873  | total loss: \u001b[1m\u001b[32m0.02081\u001b[0m\u001b[0m | time: 0.189s\n",
      "| SGD | epoch: 046 | loss: 0.02081 - R2: 0.9997 -- iter: 1152/1168\n",
      "Training Step: 874  | total loss: \u001b[1m\u001b[32m0.01998\u001b[0m\u001b[0m | time: 1.201s\n",
      "| SGD | epoch: 046 | loss: 0.01998 - R2: 0.9996 | val_loss: 0.02815 - val_acc: 1.0003 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 875  | total loss: \u001b[1m\u001b[32m0.01998\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 047 | loss: 0.01998 - R2: 0.9999 -- iter: 0064/1168\n",
      "Training Step: 876  | total loss: \u001b[1m\u001b[32m0.01920\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 047 | loss: 0.01920 - R2: 0.9999 -- iter: 0128/1168\n",
      "Training Step: 877  | total loss: \u001b[1m\u001b[32m0.01795\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 047 | loss: 0.01795 - R2: 0.9999 -- iter: 0192/1168\n",
      "Training Step: 878  | total loss: \u001b[1m\u001b[32m0.01795\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 047 | loss: 0.01795 - R2: 0.9999 -- iter: 0256/1168\n",
      "Training Step: 879  | total loss: \u001b[1m\u001b[32m0.01784\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 047 | loss: 0.01784 - R2: 0.9997 -- iter: 0320/1168\n",
      "Training Step: 880  | total loss: \u001b[1m\u001b[32m0.01770\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 047 | loss: 0.01770 - R2: 0.9995 -- iter: 0384/1168\n",
      "Training Step: 881  | total loss: \u001b[1m\u001b[32m0.01736\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 047 | loss: 0.01736 - R2: 0.9994 -- iter: 0448/1168\n",
      "Training Step: 882  | total loss: \u001b[1m\u001b[32m0.01922\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 047 | loss: 0.01922 - R2: 0.9994 -- iter: 0512/1168\n",
      "Training Step: 883  | total loss: \u001b[1m\u001b[32m0.01975\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 047 | loss: 0.01975 - R2: 0.9998 -- iter: 0576/1168\n",
      "Training Step: 884  | total loss: \u001b[1m\u001b[32m0.02104\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 047 | loss: 0.02104 - R2: 0.9994 -- iter: 0640/1168\n",
      "Training Step: 885  | total loss: \u001b[1m\u001b[32m0.02091\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 047 | loss: 0.02091 - R2: 0.9995 -- iter: 0704/1168\n",
      "Training Step: 886  | total loss: \u001b[1m\u001b[32m0.02036\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 047 | loss: 0.02036 - R2: 0.9992 -- iter: 0768/1168\n",
      "Training Step: 887  | total loss: \u001b[1m\u001b[32m0.02036\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 047 | loss: 0.02036 - R2: 0.9992 -- iter: 0832/1168\n",
      "Training Step: 888  | total loss: \u001b[1m\u001b[32m0.02029\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 047 | loss: 0.02029 - R2: 0.9995 -- iter: 0896/1168\n",
      "Training Step: 889  | total loss: \u001b[1m\u001b[32m0.02209\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 047 | loss: 0.02209 - R2: 0.9998 -- iter: 0960/1168\n",
      "Training Step: 890  | total loss: \u001b[1m\u001b[32m0.02165\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 047 | loss: 0.02165 - R2: 0.9995 -- iter: 1024/1168\n",
      "Training Step: 891  | total loss: \u001b[1m\u001b[32m0.02165\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 047 | loss: 0.02165 - R2: 0.9995 -- iter: 1088/1168\n",
      "Training Step: 892  | total loss: \u001b[1m\u001b[32m0.02249\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 047 | loss: 0.02249 - R2: 0.9999 -- iter: 1152/1168\n",
      "Training Step: 893  | total loss: \u001b[1m\u001b[32m0.02079\u001b[0m\u001b[0m | time: 1.135s\n",
      "| SGD | epoch: 047 | loss: 0.02079 - R2: 0.9995 | val_loss: 0.02832 - val_acc: 1.0012 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 894  | total loss: \u001b[1m\u001b[32m0.02079\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 048 | loss: 0.02079 - R2: 0.9995 -- iter: 0064/1168\n",
      "Training Step: 895  | total loss: \u001b[1m\u001b[32m0.02007\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 048 | loss: 0.02007 - R2: 0.9995 -- iter: 0128/1168\n",
      "Training Step: 896  | total loss: \u001b[1m\u001b[32m0.01932\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 048 | loss: 0.01932 - R2: 1.0001 -- iter: 0192/1168\n",
      "Training Step: 897  | total loss: \u001b[1m\u001b[32m0.01932\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 048 | loss: 0.01932 - R2: 1.0001 -- iter: 0256/1168\n",
      "Training Step: 898  | total loss: \u001b[1m\u001b[32m0.01842\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 048 | loss: 0.01842 - R2: 1.0003 -- iter: 0320/1168\n",
      "Training Step: 899  | total loss: \u001b[1m\u001b[32m0.01780\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 048 | loss: 0.01780 - R2: 0.9999 -- iter: 0384/1168\n",
      "Training Step: 900  | total loss: \u001b[1m\u001b[32m0.01780\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 048 | loss: 0.01780 - R2: 0.9996 -- iter: 0448/1168\n",
      "Training Step: 901  | total loss: \u001b[1m\u001b[32m0.01704\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 048 | loss: 0.01704 - R2: 0.9994 -- iter: 0512/1168\n",
      "Training Step: 902  | total loss: \u001b[1m\u001b[32m0.01779\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 048 | loss: 0.01779 - R2: 0.9995 -- iter: 0576/1168\n",
      "Training Step: 903  | total loss: \u001b[1m\u001b[32m0.01790\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 048 | loss: 0.01790 - R2: 0.9996 -- iter: 0640/1168\n",
      "Training Step: 904  | total loss: \u001b[1m\u001b[32m0.01728\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 048 | loss: 0.01728 - R2: 0.9993 -- iter: 0704/1168\n",
      "Training Step: 905  | total loss: \u001b[1m\u001b[32m0.02005\u001b[0m\u001b[0m | time: 0.165s\n",
      "| SGD | epoch: 048 | loss: 0.02005 - R2: 1.0000 -- iter: 0768/1168\n",
      "Training Step: 906  | total loss: \u001b[1m\u001b[32m0.01970\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 048 | loss: 0.01970 - R2: 0.9998 -- iter: 0832/1168\n",
      "Training Step: 907  | total loss: \u001b[1m\u001b[32m0.04526\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 048 | loss: 0.04526 - R2: 0.9994 -- iter: 0896/1168\n",
      "Training Step: 908  | total loss: \u001b[1m\u001b[32m0.04306\u001b[0m\u001b[0m | time: 0.192s\n",
      "| SGD | epoch: 048 | loss: 0.04306 - R2: 0.9995 -- iter: 0960/1168\n",
      "Training Step: 909  | total loss: \u001b[1m\u001b[32m0.04127\u001b[0m\u001b[0m | time: 0.196s\n",
      "| SGD | epoch: 048 | loss: 0.04127 - R2: 0.9996 -- iter: 1024/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 910  | total loss: \u001b[1m\u001b[32m0.04127\u001b[0m\u001b[0m | time: 0.198s\n",
      "| SGD | epoch: 048 | loss: 0.04127 - R2: 0.9996 -- iter: 1088/1168\n",
      "Training Step: 911  | total loss: \u001b[1m\u001b[32m0.03840\u001b[0m\u001b[0m | time: 0.201s\n",
      "| SGD | epoch: 048 | loss: 0.03840 - R2: 0.9993 -- iter: 1152/1168\n",
      "Training Step: 912  | total loss: \u001b[1m\u001b[32m0.03603\u001b[0m\u001b[0m | time: 1.206s\n",
      "| SGD | epoch: 048 | loss: 0.03603 - R2: 0.9998 | val_loss: 0.02823 - val_acc: 1.0007 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 913  | total loss: \u001b[1m\u001b[32m0.03282\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 049 | loss: 0.03282 - R2: 0.9997 -- iter: 0064/1168\n",
      "Training Step: 914  | total loss: \u001b[1m\u001b[32m0.03053\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 049 | loss: 0.03053 - R2: 0.9998 -- iter: 0128/1168\n",
      "Training Step: 915  | total loss: \u001b[1m\u001b[32m0.02974\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 049 | loss: 0.02974 - R2: 0.9999 -- iter: 0192/1168\n",
      "Training Step: 916  | total loss: \u001b[1m\u001b[32m0.02783\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 049 | loss: 0.02783 - R2: 0.9999 -- iter: 0256/1168\n",
      "Training Step: 917  | total loss: \u001b[1m\u001b[32m0.02783\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 049 | loss: 0.02783 - R2: 0.9999 -- iter: 0320/1168\n",
      "Training Step: 918  | total loss: \u001b[1m\u001b[32m0.02716\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 049 | loss: 0.02716 - R2: 0.9997 -- iter: 0384/1168\n",
      "Training Step: 919  | total loss: \u001b[1m\u001b[32m0.02641\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 049 | loss: 0.02641 - R2: 1.0000 -- iter: 0448/1168\n",
      "Training Step: 920  | total loss: \u001b[1m\u001b[32m0.02508\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 049 | loss: 0.02508 - R2: 1.0000 -- iter: 0512/1168\n",
      "Training Step: 921  | total loss: \u001b[1m\u001b[32m0.02382\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 049 | loss: 0.02382 - R2: 1.0000 -- iter: 0576/1168\n",
      "Training Step: 922  | total loss: \u001b[1m\u001b[32m0.02380\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 049 | loss: 0.02380 - R2: 0.9998 -- iter: 0640/1168\n",
      "Training Step: 923  | total loss: \u001b[1m\u001b[32m0.02257\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 049 | loss: 0.02257 - R2: 0.9998 -- iter: 0704/1168\n",
      "Training Step: 924  | total loss: \u001b[1m\u001b[32m0.02217\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 049 | loss: 0.02217 - R2: 0.9999 -- iter: 0768/1168\n",
      "Training Step: 925  | total loss: \u001b[1m\u001b[32m0.02217\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 049 | loss: 0.02217 - R2: 0.9999 -- iter: 0832/1168\n",
      "Training Step: 926  | total loss: \u001b[1m\u001b[32m0.02228\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 049 | loss: 0.02228 - R2: 1.0001 -- iter: 0896/1168\n",
      "Training Step: 927  | total loss: \u001b[1m\u001b[32m0.04034\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 049 | loss: 0.04034 - R2: 1.0004 -- iter: 0960/1168\n",
      "Training Step: 928  | total loss: \u001b[1m\u001b[32m0.04034\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 049 | loss: 0.04034 - R2: 1.0004 -- iter: 1024/1168\n",
      "Training Step: 929  | total loss: \u001b[1m\u001b[32m0.03729\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 049 | loss: 0.03729 - R2: 1.0003 -- iter: 1088/1168\n",
      "Training Step: 930  | total loss: \u001b[1m\u001b[32m0.03586\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 049 | loss: 0.03586 - R2: 1.0003 -- iter: 1152/1168\n",
      "Training Step: 931  | total loss: \u001b[1m\u001b[32m0.03403\u001b[0m\u001b[0m | time: 1.118s\n",
      "| SGD | epoch: 049 | loss: 0.03403 - R2: 0.9996 | val_loss: 0.02813 - val_acc: 1.0005 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 932  | total loss: \u001b[1m\u001b[32m0.03223\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 050 | loss: 0.03223 - R2: 0.9995 -- iter: 0064/1168\n",
      "Training Step: 933  | total loss: \u001b[1m\u001b[32m0.03113\u001b[0m\u001b[0m | time: 0.008s\n",
      "| SGD | epoch: 050 | loss: 0.03113 - R2: 0.9994 -- iter: 0128/1168\n",
      "Training Step: 934  | total loss: \u001b[1m\u001b[32m0.02817\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 050 | loss: 0.02817 - R2: 0.9992 -- iter: 0192/1168\n",
      "Training Step: 935  | total loss: \u001b[1m\u001b[32m0.02706\u001b[0m\u001b[0m | time: 0.019s\n",
      "| SGD | epoch: 050 | loss: 0.02706 - R2: 0.9996 -- iter: 0256/1168\n",
      "Training Step: 936  | total loss: \u001b[1m\u001b[32m0.02679\u001b[0m\u001b[0m | time: 0.023s\n",
      "| SGD | epoch: 050 | loss: 0.02679 - R2: 0.9998 -- iter: 0320/1168\n",
      "Training Step: 937  | total loss: \u001b[1m\u001b[32m0.02627\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 050 | loss: 0.02627 - R2: 1.0000 -- iter: 0384/1168\n",
      "Training Step: 938  | total loss: \u001b[1m\u001b[32m0.02627\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 050 | loss: 0.02627 - R2: 1.0000 -- iter: 0448/1168\n",
      "Training Step: 939  | total loss: \u001b[1m\u001b[32m0.02531\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 050 | loss: 0.02531 - R2: 1.0001 -- iter: 0512/1168\n",
      "Training Step: 940  | total loss: \u001b[1m\u001b[32m0.02370\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 050 | loss: 0.02370 - R2: 0.9992 -- iter: 0576/1168\n",
      "Training Step: 941  | total loss: \u001b[1m\u001b[32m0.02214\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 050 | loss: 0.02214 - R2: 0.9986 -- iter: 0640/1168\n",
      "Training Step: 942  | total loss: \u001b[1m\u001b[32m0.02193\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 050 | loss: 0.02193 - R2: 0.9988 -- iter: 0704/1168\n",
      "Training Step: 943  | total loss: \u001b[1m\u001b[32m0.02168\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 050 | loss: 0.02168 - R2: 0.9994 -- iter: 0768/1168\n",
      "Training Step: 944  | total loss: \u001b[1m\u001b[32m0.02269\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 050 | loss: 0.02269 - R2: 1.0001 -- iter: 0832/1168\n",
      "Training Step: 945  | total loss: \u001b[1m\u001b[32m0.02242\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 050 | loss: 0.02242 - R2: 1.0002 -- iter: 0896/1168\n",
      "Training Step: 946  | total loss: \u001b[1m\u001b[32m0.02226\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 050 | loss: 0.02226 - R2: 1.0004 -- iter: 0960/1168\n",
      "Training Step: 947  | total loss: \u001b[1m\u001b[32m0.04469\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 050 | loss: 0.04469 - R2: 1.0004 -- iter: 1024/1168\n",
      "Training Step: 948  | total loss: \u001b[1m\u001b[32m0.04120\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 050 | loss: 0.04120 - R2: 1.0001 -- iter: 1088/1168\n",
      "Training Step: 949  | total loss: \u001b[1m\u001b[32m0.03838\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 050 | loss: 0.03838 - R2: 0.9997 -- iter: 1152/1168\n",
      "Training Step: 950  | total loss: \u001b[1m\u001b[32m0.03838\u001b[0m\u001b[0m | time: 1.065s\n",
      "| SGD | epoch: 050 | loss: 0.03838 - R2: 0.9998 | val_loss: 0.02832 - val_acc: 1.0003 -- iter: 1168/1168\n",
      "--\n",
      "---------------------------------\n",
      "Run id: BVTEGZ\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "INFO:tensorflow:Summary name StandardError/ (raw) is illegal; using StandardError/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 1168\n",
      "Validation samples: 292\n",
      "--\n",
      "Training Step: 1  | total loss: \u001b[1m\u001b[32m129.94875\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 001 | loss: 129.94875 - R2: 0.0000 -- iter: 0064/1168\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m140.51649\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 001 | loss: 140.51649 - R2: 0.0000 -- iter: 0128/1168\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m140.51649\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 001 | loss: 140.51649 - R2: 0.0000 -- iter: 0192/1168\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m143.84822\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 001 | loss: 143.84822 - R2: 0.0000 -- iter: 0256/1168\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m143.84822\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 001 | loss: 143.84822 - R2: 0.0000 -- iter: 0320/1168\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m144.61311\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 001 | loss: 144.61311 - R2: 0.0000 -- iter: 0384/1168\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m145.24802\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 001 | loss: 145.24802 - R2: 0.0000 -- iter: 0448/1168\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m145.45828\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 001 | loss: 145.45828 - R2: 0.0000 -- iter: 0512/1168\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m145.11298\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 001 | loss: 145.11298 - R2: 0.0000 -- iter: 0576/1168\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m144.62993\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 001 | loss: 144.62993 - R2: 0.0000 -- iter: 0640/1168\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m143.64993\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 001 | loss: 143.64993 - R2: 0.0000 -- iter: 0704/1168\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m143.89824\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 001 | loss: 143.89824 - R2: 0.0000 -- iter: 0768/1168\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m143.89824\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 001 | loss: 143.89824 - R2: 0.0000 -- iter: 0832/1168\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m143.34207\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 001 | loss: 143.34207 - R2: 0.0000 -- iter: 0896/1168\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m143.85011\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 001 | loss: 143.85011 - R2: 0.0000 -- iter: 0960/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m143.41374\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 001 | loss: 143.41374 - R2: 0.0000 -- iter: 1024/1168\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m142.94255\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 001 | loss: 142.94255 - R2: 0.0000 -- iter: 1088/1168\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m142.94255\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 001 | loss: 142.94255 - R2: 0.0000 -- iter: 1152/1168\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m141.60745\u001b[0m\u001b[0m | time: 1.104s\n",
      "| SGD | epoch: 001 | loss: 141.60745 - R2: 0.0000 | val_loss: 142.42757 - val_acc: 0.0001 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m141.60745\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 002 | loss: 141.60745 - R2: 0.0000 -- iter: 0064/1168\n",
      "Training Step: 21  | total loss: \u001b[1m\u001b[32m141.06168\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 002 | loss: 141.06168 - R2: 0.0001 -- iter: 0128/1168\n",
      "Training Step: 22  | total loss: \u001b[1m\u001b[32m141.00168\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 002 | loss: 141.00168 - R2: 0.0001 -- iter: 0192/1168\n",
      "Training Step: 23  | total loss: \u001b[1m\u001b[32m141.25485\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 002 | loss: 141.25485 - R2: 0.0001 -- iter: 0256/1168\n",
      "Training Step: 24  | total loss: \u001b[1m\u001b[32m141.40512\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 002 | loss: 141.40512 - R2: 0.0001 -- iter: 0320/1168\n",
      "Training Step: 25  | total loss: \u001b[1m\u001b[32m142.16776\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 002 | loss: 142.16776 - R2: 0.0001 -- iter: 0384/1168\n",
      "Training Step: 26  | total loss: \u001b[1m\u001b[32m141.93617\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 002 | loss: 141.93617 - R2: 0.0001 -- iter: 0448/1168\n",
      "Training Step: 27  | total loss: \u001b[1m\u001b[32m141.91515\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 002 | loss: 141.91515 - R2: 0.0001 -- iter: 0512/1168\n",
      "Training Step: 28  | total loss: \u001b[1m\u001b[32m141.75336\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 002 | loss: 141.75336 - R2: 0.0001 -- iter: 0576/1168\n",
      "Training Step: 29  | total loss: \u001b[1m\u001b[32m141.09799\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 002 | loss: 141.09799 - R2: 0.0001 -- iter: 0640/1168\n",
      "Training Step: 30  | total loss: \u001b[1m\u001b[32m141.41104\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 002 | loss: 141.41104 - R2: 0.0001 -- iter: 0704/1168\n",
      "Training Step: 31  | total loss: \u001b[1m\u001b[32m141.53748\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 002 | loss: 141.53748 - R2: 0.0001 -- iter: 0768/1168\n",
      "Training Step: 32  | total loss: \u001b[1m\u001b[32m141.61891\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 002 | loss: 141.61891 - R2: 0.0001 -- iter: 0832/1168\n",
      "Training Step: 33  | total loss: \u001b[1m\u001b[32m141.48505\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 002 | loss: 141.48505 - R2: 0.0001 -- iter: 0896/1168\n",
      "Training Step: 34  | total loss: \u001b[1m\u001b[32m141.48055\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 002 | loss: 141.48055 - R2: 0.0001 -- iter: 0960/1168\n",
      "Training Step: 35  | total loss: \u001b[1m\u001b[32m141.54468\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 002 | loss: 141.54468 - R2: 0.0002 -- iter: 1024/1168\n",
      "Training Step: 36  | total loss: \u001b[1m\u001b[32m141.36584\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 002 | loss: 141.36584 - R2: 0.0002 -- iter: 1088/1168\n",
      "Training Step: 37  | total loss: \u001b[1m\u001b[32m141.44939\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 002 | loss: 141.44939 - R2: 0.0002 -- iter: 1152/1168\n",
      "Training Step: 38  | total loss: \u001b[1m\u001b[32m141.44939\u001b[0m\u001b[0m | time: 1.127s\n",
      "| SGD | epoch: 002 | loss: 141.44939 - R2: 0.0002 | val_loss: 140.16550 - val_acc: 0.0003 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 39  | total loss: \u001b[1m\u001b[32m140.78168\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 003 | loss: 140.78168 - R2: 0.0002 -- iter: 0064/1168\n",
      "Training Step: 40  | total loss: \u001b[1m\u001b[32m140.05722\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 003 | loss: 140.05722 - R2: 0.0002 -- iter: 0128/1168\n",
      "Training Step: 41  | total loss: \u001b[1m\u001b[32m139.45908\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 003 | loss: 139.45908 - R2: 0.0002 -- iter: 0192/1168\n",
      "Training Step: 42  | total loss: \u001b[1m\u001b[32m139.63162\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 003 | loss: 139.63162 - R2: 0.0002 -- iter: 0256/1168\n",
      "Training Step: 43  | total loss: \u001b[1m\u001b[32m139.52736\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 003 | loss: 139.52736 - R2: 0.0002 -- iter: 0320/1168\n",
      "Training Step: 44  | total loss: \u001b[1m\u001b[32m139.58237\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 003 | loss: 139.58237 - R2: 0.0003 -- iter: 0384/1168\n",
      "Training Step: 45  | total loss: \u001b[1m\u001b[32m139.90790\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 003 | loss: 139.90790 - R2: 0.0003 -- iter: 0448/1168\n",
      "Training Step: 46  | total loss: \u001b[1m\u001b[32m139.77333\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 003 | loss: 139.77333 - R2: 0.0003 -- iter: 0512/1168\n",
      "Training Step: 47  | total loss: \u001b[1m\u001b[32m139.40337\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 003 | loss: 139.40337 - R2: 0.0003 -- iter: 0576/1168\n",
      "Training Step: 48  | total loss: \u001b[1m\u001b[32m139.43175\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 003 | loss: 139.43175 - R2: 0.0003 -- iter: 0640/1168\n",
      "Training Step: 49  | total loss: \u001b[1m\u001b[32m139.14127\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 003 | loss: 139.14127 - R2: 0.0003 -- iter: 0704/1168\n",
      "Training Step: 50  | total loss: \u001b[1m\u001b[32m139.17111\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 003 | loss: 139.17111 - R2: 0.0003 -- iter: 0768/1168\n",
      "Training Step: 51  | total loss: \u001b[1m\u001b[32m139.17111\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 003 | loss: 139.17111 - R2: 0.0003 -- iter: 0832/1168\n",
      "Training Step: 52  | total loss: \u001b[1m\u001b[32m138.91829\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 003 | loss: 138.91829 - R2: 0.0004 -- iter: 0896/1168\n",
      "Training Step: 53  | total loss: \u001b[1m\u001b[32m138.95700\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 003 | loss: 138.95700 - R2: 0.0004 -- iter: 0960/1168\n",
      "Training Step: 54  | total loss: \u001b[1m\u001b[32m138.95700\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 003 | loss: 138.95700 - R2: 0.0004 -- iter: 1024/1168\n",
      "Training Step: 55  | total loss: \u001b[1m\u001b[32m139.03448\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 003 | loss: 139.03448 - R2: 0.0004 -- iter: 1088/1168\n",
      "Training Step: 56  | total loss: \u001b[1m\u001b[32m138.74893\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 003 | loss: 138.74893 - R2: 0.0004 -- iter: 1152/1168\n",
      "Training Step: 57  | total loss: \u001b[1m\u001b[32m138.74893\u001b[0m\u001b[0m | time: 1.143s\n",
      "| SGD | epoch: 003 | loss: 138.74893 - R2: 0.0004 | val_loss: 137.92125 - val_acc: 0.0006 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 58  | total loss: \u001b[1m\u001b[32m138.80000\u001b[0m\u001b[0m | time: 0.007s\n",
      "| SGD | epoch: 004 | loss: 138.80000 - R2: 0.0004 -- iter: 0064/1168\n",
      "Training Step: 59  | total loss: \u001b[1m\u001b[32m138.36606\u001b[0m\u001b[0m | time: 0.012s\n",
      "| SGD | epoch: 004 | loss: 138.36606 - R2: 0.0005 -- iter: 0128/1168\n",
      "Training Step: 60  | total loss: \u001b[1m\u001b[32m138.36606\u001b[0m\u001b[0m | time: 0.016s\n",
      "| SGD | epoch: 004 | loss: 138.36606 - R2: 0.0005 -- iter: 0192/1168\n",
      "Training Step: 61  | total loss: \u001b[1m\u001b[32m138.04918\u001b[0m\u001b[0m | time: 0.018s\n",
      "| SGD | epoch: 004 | loss: 138.04918 - R2: 0.0005 -- iter: 0256/1168\n",
      "Training Step: 62  | total loss: \u001b[1m\u001b[32m137.96387\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 004 | loss: 137.96387 - R2: 0.0005 -- iter: 0320/1168\n",
      "Training Step: 63  | total loss: \u001b[1m\u001b[32m137.67624\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 004 | loss: 137.67624 - R2: 0.0005 -- iter: 0384/1168\n",
      "Training Step: 64  | total loss: \u001b[1m\u001b[32m137.67624\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 004 | loss: 137.67624 - R2: 0.0005 -- iter: 0448/1168\n",
      "Training Step: 65  | total loss: \u001b[1m\u001b[32m137.54993\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 004 | loss: 137.54993 - R2: 0.0006 -- iter: 0512/1168\n",
      "Training Step: 66  | total loss: \u001b[1m\u001b[32m137.41487\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 004 | loss: 137.41487 - R2: 0.0006 -- iter: 0576/1168\n",
      "Training Step: 67  | total loss: \u001b[1m\u001b[32m137.63095\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 004 | loss: 137.63095 - R2: 0.0006 -- iter: 0640/1168\n",
      "Training Step: 68  | total loss: \u001b[1m\u001b[32m137.52419\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 004 | loss: 137.52419 - R2: 0.0006 -- iter: 0704/1168\n",
      "Training Step: 69  | total loss: \u001b[1m\u001b[32m137.43219\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 004 | loss: 137.43219 - R2: 0.0006 -- iter: 0768/1168\n",
      "Training Step: 70  | total loss: \u001b[1m\u001b[32m137.37202\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 004 | loss: 137.37202 - R2: 0.0007 -- iter: 0832/1168\n",
      "Training Step: 71  | total loss: \u001b[1m\u001b[32m137.42149\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 004 | loss: 137.42149 - R2: 0.0007 -- iter: 0896/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 72  | total loss: \u001b[1m\u001b[32m137.40627\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 004 | loss: 137.40627 - R2: 0.0007 -- iter: 0960/1168\n",
      "Training Step: 73  | total loss: \u001b[1m\u001b[32m137.08157\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 004 | loss: 137.08157 - R2: 0.0007 -- iter: 1024/1168\n",
      "Training Step: 74  | total loss: \u001b[1m\u001b[32m137.08157\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 004 | loss: 137.08157 - R2: 0.0007 -- iter: 1088/1168\n",
      "Training Step: 75  | total loss: \u001b[1m\u001b[32m137.01056\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 004 | loss: 137.01056 - R2: 0.0008 -- iter: 1152/1168\n",
      "Training Step: 76  | total loss: \u001b[1m\u001b[32m136.75594\u001b[0m\u001b[0m | time: 1.062s\n",
      "| SGD | epoch: 004 | loss: 136.75594 - R2: 0.0008 | val_loss: 135.69459 - val_acc: 0.0010 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 77  | total loss: \u001b[1m\u001b[32m136.45366\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 005 | loss: 136.45366 - R2: 0.0008 -- iter: 0064/1168\n",
      "Training Step: 78  | total loss: \u001b[1m\u001b[32m136.33934\u001b[0m\u001b[0m | time: 0.020s\n",
      "| SGD | epoch: 005 | loss: 136.33934 - R2: 0.0008 -- iter: 0128/1168\n",
      "Training Step: 79  | total loss: \u001b[1m\u001b[32m136.59175\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 005 | loss: 136.59175 - R2: 0.0008 -- iter: 0192/1168\n",
      "Training Step: 80  | total loss: \u001b[1m\u001b[32m136.59175\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 005 | loss: 136.59175 - R2: 0.0009 -- iter: 0256/1168\n",
      "Training Step: 81  | total loss: \u001b[1m\u001b[32m136.80386\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 005 | loss: 136.80386 - R2: 0.0009 -- iter: 0320/1168\n",
      "Training Step: 82  | total loss: \u001b[1m\u001b[32m136.42421\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 005 | loss: 136.42421 - R2: 0.0009 -- iter: 0384/1168\n",
      "Training Step: 83  | total loss: \u001b[1m\u001b[32m136.29338\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 005 | loss: 136.29338 - R2: 0.0009 -- iter: 0448/1168\n",
      "Training Step: 84  | total loss: \u001b[1m\u001b[32m136.29338\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 005 | loss: 136.29338 - R2: 0.0010 -- iter: 0512/1168\n",
      "Training Step: 85  | total loss: \u001b[1m\u001b[32m136.10657\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 005 | loss: 136.10657 - R2: 0.0010 -- iter: 0576/1168\n",
      "Training Step: 86  | total loss: \u001b[1m\u001b[32m135.99475\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 005 | loss: 135.99475 - R2: 0.0010 -- iter: 0640/1168\n",
      "Training Step: 87  | total loss: \u001b[1m\u001b[32m135.99475\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 005 | loss: 135.99475 - R2: 0.0010 -- iter: 0704/1168\n",
      "Training Step: 88  | total loss: \u001b[1m\u001b[32m135.74359\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 005 | loss: 135.74359 - R2: 0.0011 -- iter: 0768/1168\n",
      "Training Step: 89  | total loss: \u001b[1m\u001b[32m135.77843\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 005 | loss: 135.77843 - R2: 0.0011 -- iter: 0832/1168\n",
      "Training Step: 90  | total loss: \u001b[1m\u001b[32m135.77843\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 005 | loss: 135.77843 - R2: 0.0011 -- iter: 0896/1168\n",
      "Training Step: 91  | total loss: \u001b[1m\u001b[32m135.49510\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 005 | loss: 135.49510 - R2: 0.0011 -- iter: 0960/1168\n",
      "Training Step: 92  | total loss: \u001b[1m\u001b[32m135.30887\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 005 | loss: 135.30887 - R2: 0.0012 -- iter: 1024/1168\n",
      "Training Step: 93  | total loss: \u001b[1m\u001b[32m135.16125\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 005 | loss: 135.16125 - R2: 0.0012 -- iter: 1088/1168\n",
      "Training Step: 94  | total loss: \u001b[1m\u001b[32m134.93408\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 005 | loss: 134.93408 - R2: 0.0012 -- iter: 1152/1168\n",
      "Training Step: 95  | total loss: \u001b[1m\u001b[32m134.77855\u001b[0m\u001b[0m | time: 1.073s\n",
      "| SGD | epoch: 005 | loss: 134.77855 - R2: 0.0012 | val_loss: 133.48537 - val_acc: 0.0016 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 96  | total loss: \u001b[1m\u001b[32m134.77855\u001b[0m\u001b[0m | time: 0.006s\n",
      "| SGD | epoch: 006 | loss: 134.77855 - R2: 0.0013 -- iter: 0064/1168\n",
      "Training Step: 97  | total loss: \u001b[1m\u001b[32m134.42804\u001b[0m\u001b[0m | time: 0.009s\n",
      "| SGD | epoch: 006 | loss: 134.42804 - R2: 0.0013 -- iter: 0128/1168\n",
      "Training Step: 98  | total loss: \u001b[1m\u001b[32m134.13365\u001b[0m\u001b[0m | time: 0.011s\n",
      "| SGD | epoch: 006 | loss: 134.13365 - R2: 0.0013 -- iter: 0192/1168\n",
      "Training Step: 99  | total loss: \u001b[1m\u001b[32m134.08046\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 006 | loss: 134.08046 - R2: 0.0014 -- iter: 0256/1168\n",
      "Training Step: 100  | total loss: \u001b[1m\u001b[32m134.01755\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 006 | loss: 134.01755 - R2: 0.0014 -- iter: 0320/1168\n",
      "Training Step: 101  | total loss: \u001b[1m\u001b[32m134.01755\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 006 | loss: 134.01755 - R2: 0.0014 -- iter: 0384/1168\n",
      "Training Step: 102  | total loss: \u001b[1m\u001b[32m133.62225\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 006 | loss: 133.62225 - R2: 0.0015 -- iter: 0448/1168\n",
      "Training Step: 103  | total loss: \u001b[1m\u001b[32m133.76805\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 006 | loss: 133.76805 - R2: 0.0015 -- iter: 0512/1168\n",
      "Training Step: 104  | total loss: \u001b[1m\u001b[32m133.61794\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 006 | loss: 133.61794 - R2: 0.0015 -- iter: 0576/1168\n",
      "Training Step: 105  | total loss: \u001b[1m\u001b[32m133.85399\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 006 | loss: 133.85399 - R2: 0.0016 -- iter: 0640/1168\n",
      "Training Step: 106  | total loss: \u001b[1m\u001b[32m133.85399\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 006 | loss: 133.85399 - R2: 0.0016 -- iter: 0704/1168\n",
      "Training Step: 107  | total loss: \u001b[1m\u001b[32m133.73019\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 006 | loss: 133.73019 - R2: 0.0016 -- iter: 0768/1168\n",
      "Training Step: 108  | total loss: \u001b[1m\u001b[32m133.45673\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 006 | loss: 133.45673 - R2: 0.0016 -- iter: 0832/1168\n",
      "Training Step: 109  | total loss: \u001b[1m\u001b[32m133.27652\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 006 | loss: 133.27652 - R2: 0.0017 -- iter: 0896/1168\n",
      "Training Step: 110  | total loss: \u001b[1m\u001b[32m133.27652\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 006 | loss: 133.27652 - R2: 0.0017 -- iter: 0960/1168\n",
      "Training Step: 111  | total loss: \u001b[1m\u001b[32m133.01274\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 006 | loss: 133.01274 - R2: 0.0018 -- iter: 1024/1168\n",
      "Training Step: 112  | total loss: \u001b[1m\u001b[32m132.85396\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 006 | loss: 132.85396 - R2: 0.0018 -- iter: 1088/1168\n",
      "Training Step: 113  | total loss: \u001b[1m\u001b[32m132.70586\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 006 | loss: 132.70586 - R2: 0.0018 -- iter: 1152/1168\n",
      "Training Step: 114  | total loss: \u001b[1m\u001b[32m132.69997\u001b[0m\u001b[0m | time: 1.077s\n",
      "| SGD | epoch: 006 | loss: 132.69997 - R2: 0.0019 | val_loss: 131.29337 - val_acc: 0.0023 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 115  | total loss: \u001b[1m\u001b[32m132.69664\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 007 | loss: 132.69664 - R2: 0.0019 -- iter: 0064/1168\n",
      "Training Step: 116  | total loss: \u001b[1m\u001b[32m132.51266\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 007 | loss: 132.51266 - R2: 0.0019 -- iter: 0128/1168\n",
      "Training Step: 117  | total loss: \u001b[1m\u001b[32m132.22038\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 007 | loss: 132.22038 - R2: 0.0020 -- iter: 0192/1168\n",
      "Training Step: 118  | total loss: \u001b[1m\u001b[32m132.08792\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 007 | loss: 132.08792 - R2: 0.0020 -- iter: 0256/1168\n",
      "Training Step: 119  | total loss: \u001b[1m\u001b[32m131.83348\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 007 | loss: 131.83348 - R2: 0.0020 -- iter: 0320/1168\n",
      "Training Step: 120  | total loss: \u001b[1m\u001b[32m131.64661\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 007 | loss: 131.64661 - R2: 0.0021 -- iter: 0384/1168\n",
      "Training Step: 121  | total loss: \u001b[1m\u001b[32m131.64661\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 007 | loss: 131.64661 - R2: 0.0021 -- iter: 0448/1168\n",
      "Training Step: 122  | total loss: \u001b[1m\u001b[32m131.47012\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 007 | loss: 131.47012 - R2: 0.0022 -- iter: 0512/1168\n",
      "Training Step: 123  | total loss: \u001b[1m\u001b[32m131.74275\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 007 | loss: 131.74275 - R2: 0.0022 -- iter: 0576/1168\n",
      "Training Step: 124  | total loss: \u001b[1m\u001b[32m131.51872\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 007 | loss: 131.51872 - R2: 0.0022 -- iter: 0640/1168\n",
      "Training Step: 125  | total loss: \u001b[1m\u001b[32m131.51872\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 007 | loss: 131.51872 - R2: 0.0023 -- iter: 0704/1168\n",
      "Training Step: 126  | total loss: \u001b[1m\u001b[32m131.38399\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 007 | loss: 131.38399 - R2: 0.0023 -- iter: 0768/1168\n",
      "Training Step: 127  | total loss: \u001b[1m\u001b[32m131.28680\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 007 | loss: 131.28680 - R2: 0.0023 -- iter: 0832/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 128  | total loss: \u001b[1m\u001b[32m131.24088\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 007 | loss: 131.24088 - R2: 0.0024 -- iter: 0896/1168\n",
      "Training Step: 129  | total loss: \u001b[1m\u001b[32m130.96761\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 007 | loss: 130.96761 - R2: 0.0024 -- iter: 0960/1168\n",
      "Training Step: 130  | total loss: \u001b[1m\u001b[32m130.82938\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 007 | loss: 130.82938 - R2: 0.0025 -- iter: 1024/1168\n",
      "Training Step: 131  | total loss: \u001b[1m\u001b[32m130.59085\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 007 | loss: 130.59085 - R2: 0.0025 -- iter: 1088/1168\n",
      "Training Step: 132  | total loss: \u001b[1m\u001b[32m130.59085\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 007 | loss: 130.59085 - R2: 0.0026 -- iter: 1152/1168\n",
      "Training Step: 133  | total loss: \u001b[1m\u001b[32m130.33284\u001b[0m\u001b[0m | time: 1.135s\n",
      "| SGD | epoch: 007 | loss: 130.33284 - R2: 0.0026 | val_loss: 129.11835 - val_acc: 0.0031 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 134  | total loss: \u001b[1m\u001b[32m130.27161\u001b[0m\u001b[0m | time: 0.007s\n",
      "| SGD | epoch: 008 | loss: 130.27161 - R2: 0.0026 -- iter: 0064/1168\n",
      "Training Step: 135  | total loss: \u001b[1m\u001b[32m130.24586\u001b[0m\u001b[0m | time: 0.012s\n",
      "| SGD | epoch: 008 | loss: 130.24586 - R2: 0.0027 -- iter: 0128/1168\n",
      "Training Step: 136  | total loss: \u001b[1m\u001b[32m130.20714\u001b[0m\u001b[0m | time: 0.019s\n",
      "| SGD | epoch: 008 | loss: 130.20714 - R2: 0.0027 -- iter: 0192/1168\n",
      "Training Step: 137  | total loss: \u001b[1m\u001b[32m130.20714\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 008 | loss: 130.20714 - R2: 0.0028 -- iter: 0256/1168\n",
      "Training Step: 138  | total loss: \u001b[1m\u001b[32m129.67822\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 008 | loss: 129.67822 - R2: 0.0028 -- iter: 0320/1168\n",
      "Training Step: 139  | total loss: \u001b[1m\u001b[32m129.67822\u001b[0m\u001b[0m | time: 0.026s\n",
      "| SGD | epoch: 008 | loss: 129.67822 - R2: 0.0029 -- iter: 0384/1168\n",
      "Training Step: 140  | total loss: \u001b[1m\u001b[32m129.31619\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 008 | loss: 129.31619 - R2: 0.0029 -- iter: 0448/1168\n",
      "Training Step: 141  | total loss: \u001b[1m\u001b[32m128.97910\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 008 | loss: 128.97910 - R2: 0.0030 -- iter: 0512/1168\n",
      "Training Step: 142  | total loss: \u001b[1m\u001b[32m128.73489\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 008 | loss: 128.73489 - R2: 0.0030 -- iter: 0576/1168\n",
      "Training Step: 143  | total loss: \u001b[1m\u001b[32m128.79565\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 008 | loss: 128.79565 - R2: 0.0031 -- iter: 0640/1168\n",
      "Training Step: 144  | total loss: \u001b[1m\u001b[32m128.79565\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 008 | loss: 128.79565 - R2: 0.0031 -- iter: 0704/1168\n",
      "Training Step: 145  | total loss: \u001b[1m\u001b[32m128.88681\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 008 | loss: 128.88681 - R2: 0.0031 -- iter: 0768/1168\n",
      "Training Step: 146  | total loss: \u001b[1m\u001b[32m128.67126\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 008 | loss: 128.67126 - R2: 0.0032 -- iter: 0832/1168\n",
      "Training Step: 147  | total loss: \u001b[1m\u001b[32m128.41174\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 008 | loss: 128.41174 - R2: 0.0032 -- iter: 0896/1168\n",
      "Training Step: 148  | total loss: \u001b[1m\u001b[32m128.38176\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 008 | loss: 128.38176 - R2: 0.0033 -- iter: 0960/1168\n",
      "Training Step: 149  | total loss: \u001b[1m\u001b[32m128.60237\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 008 | loss: 128.60237 - R2: 0.0033 -- iter: 1024/1168\n",
      "Training Step: 150  | total loss: \u001b[1m\u001b[32m128.60237\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 008 | loss: 128.60237 - R2: 0.0034 -- iter: 1088/1168\n",
      "Training Step: 151  | total loss: \u001b[1m\u001b[32m128.47650\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 008 | loss: 128.47650 - R2: 0.0034 -- iter: 1152/1168\n",
      "Training Step: 152  | total loss: \u001b[1m\u001b[32m128.26039\u001b[0m\u001b[0m | time: 1.105s\n",
      "| SGD | epoch: 008 | loss: 128.26039 - R2: 0.0035 | val_loss: 126.96008 - val_acc: 0.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 153  | total loss: \u001b[1m\u001b[32m128.06088\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 009 | loss: 128.06088 - R2: 0.0035 -- iter: 0064/1168\n",
      "Training Step: 154  | total loss: \u001b[1m\u001b[32m128.11253\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 009 | loss: 128.11253 - R2: 0.0036 -- iter: 0128/1168\n",
      "Training Step: 155  | total loss: \u001b[1m\u001b[32m127.97124\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 009 | loss: 127.97124 - R2: 0.0036 -- iter: 0192/1168\n",
      "Training Step: 156  | total loss: \u001b[1m\u001b[32m128.01045\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 009 | loss: 128.01045 - R2: 0.0037 -- iter: 0256/1168\n",
      "Training Step: 157  | total loss: \u001b[1m\u001b[32m127.73967\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 009 | loss: 127.73967 - R2: 0.0037 -- iter: 0320/1168\n",
      "Training Step: 158  | total loss: \u001b[1m\u001b[32m127.41372\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 009 | loss: 127.41372 - R2: 0.0038 -- iter: 0384/1168\n",
      "Training Step: 159  | total loss: \u001b[1m\u001b[32m127.16859\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 009 | loss: 127.16859 - R2: 0.0038 -- iter: 0448/1168\n",
      "Training Step: 160  | total loss: \u001b[1m\u001b[32m127.15107\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 009 | loss: 127.15107 - R2: 0.0039 -- iter: 0512/1168\n",
      "Training Step: 161  | total loss: \u001b[1m\u001b[32m127.12398\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 009 | loss: 127.12398 - R2: 0.0039 -- iter: 0576/1168\n",
      "Training Step: 162  | total loss: \u001b[1m\u001b[32m126.95230\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 009 | loss: 126.95230 - R2: 0.0040 -- iter: 0640/1168\n",
      "Training Step: 163  | total loss: \u001b[1m\u001b[32m126.74358\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 009 | loss: 126.74358 - R2: 0.0040 -- iter: 0704/1168\n",
      "Training Step: 164  | total loss: \u001b[1m\u001b[32m126.66582\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 009 | loss: 126.66582 - R2: 0.0041 -- iter: 0768/1168\n",
      "Training Step: 165  | total loss: \u001b[1m\u001b[32m126.64355\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 009 | loss: 126.64355 - R2: 0.0041 -- iter: 0832/1168\n",
      "Training Step: 166  | total loss: \u001b[1m\u001b[32m126.64355\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 009 | loss: 126.64355 - R2: 0.0042 -- iter: 0896/1168\n",
      "Training Step: 167  | total loss: \u001b[1m\u001b[32m126.50177\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 009 | loss: 126.50177 - R2: 0.0042 -- iter: 0960/1168\n",
      "Training Step: 168  | total loss: \u001b[1m\u001b[32m126.50177\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 009 | loss: 126.50177 - R2: 0.0043 -- iter: 1024/1168\n",
      "Training Step: 169  | total loss: \u001b[1m\u001b[32m126.33932\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 009 | loss: 126.33932 - R2: 0.0044 -- iter: 1088/1168\n",
      "Training Step: 170  | total loss: \u001b[1m\u001b[32m126.49438\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 009 | loss: 126.49438 - R2: 0.0044 -- iter: 1152/1168\n",
      "Training Step: 171  | total loss: \u001b[1m\u001b[32m126.49438\u001b[0m\u001b[0m | time: 1.130s\n",
      "| SGD | epoch: 009 | loss: 126.49438 - R2: 0.0045 | val_loss: 124.81823 - val_acc: 0.0051 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 172  | total loss: \u001b[1m\u001b[32m126.22158\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 010 | loss: 126.22158 - R2: 0.0045 -- iter: 0064/1168\n",
      "Training Step: 173  | total loss: \u001b[1m\u001b[32m126.07656\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 010 | loss: 126.07656 - R2: 0.0046 -- iter: 0128/1168\n",
      "Training Step: 174  | total loss: \u001b[1m\u001b[32m125.95303\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 010 | loss: 125.95303 - R2: 0.0046 -- iter: 0192/1168\n",
      "Training Step: 175  | total loss: \u001b[1m\u001b[32m125.76914\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 010 | loss: 125.76914 - R2: 0.0047 -- iter: 0256/1168\n",
      "Training Step: 176  | total loss: \u001b[1m\u001b[32m125.66021\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 010 | loss: 125.66021 - R2: 0.0047 -- iter: 0320/1168\n",
      "Training Step: 177  | total loss: \u001b[1m\u001b[32m125.66021\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 010 | loss: 125.66021 - R2: 0.0048 -- iter: 0384/1168\n",
      "Training Step: 178  | total loss: \u001b[1m\u001b[32m125.54517\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 010 | loss: 125.54517 - R2: 0.0049 -- iter: 0448/1168\n",
      "Training Step: 179  | total loss: \u001b[1m\u001b[32m124.78973\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 010 | loss: 124.78973 - R2: 0.0049 -- iter: 0512/1168\n",
      "Training Step: 180  | total loss: \u001b[1m\u001b[32m124.78973\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 010 | loss: 124.78973 - R2: 0.0050 -- iter: 0576/1168\n",
      "Training Step: 181  | total loss: \u001b[1m\u001b[32m124.43635\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 010 | loss: 124.43635 - R2: 0.0051 -- iter: 0640/1168\n",
      "Training Step: 182  | total loss: \u001b[1m\u001b[32m124.26273\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 010 | loss: 124.26273 - R2: 0.0051 -- iter: 0704/1168\n",
      "Training Step: 183  | total loss: \u001b[1m\u001b[32m124.33011\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 010 | loss: 124.33011 - R2: 0.0052 -- iter: 0768/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 184  | total loss: \u001b[1m\u001b[32m124.42446\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 010 | loss: 124.42446 - R2: 0.0052 -- iter: 0832/1168\n",
      "Training Step: 185  | total loss: \u001b[1m\u001b[32m124.18766\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 010 | loss: 124.18766 - R2: 0.0053 -- iter: 0896/1168\n",
      "Training Step: 186  | total loss: \u001b[1m\u001b[32m123.97144\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 010 | loss: 123.97144 - R2: 0.0054 -- iter: 0960/1168\n",
      "Training Step: 187  | total loss: \u001b[1m\u001b[32m123.97144\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 010 | loss: 123.97144 - R2: 0.0054 -- iter: 1024/1168\n",
      "Training Step: 188  | total loss: \u001b[1m\u001b[32m123.84539\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 010 | loss: 123.84539 - R2: 0.0055 -- iter: 1088/1168\n",
      "Training Step: 189  | total loss: \u001b[1m\u001b[32m123.71983\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 010 | loss: 123.71983 - R2: 0.0055 -- iter: 1152/1168\n",
      "Training Step: 190  | total loss: \u001b[1m\u001b[32m123.56448\u001b[0m\u001b[0m | time: 1.151s\n",
      "| SGD | epoch: 010 | loss: 123.56448 - R2: 0.0056 | val_loss: 122.69242 - val_acc: 0.0063 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 191  | total loss: \u001b[1m\u001b[32m123.56448\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 011 | loss: 123.56448 - R2: 0.0057 -- iter: 0064/1168\n",
      "Training Step: 192  | total loss: \u001b[1m\u001b[32m123.28405\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 011 | loss: 123.28405 - R2: 0.0057 -- iter: 0128/1168\n",
      "Training Step: 193  | total loss: \u001b[1m\u001b[32m123.28511\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 011 | loss: 123.28511 - R2: 0.0058 -- iter: 0192/1168\n",
      "Training Step: 194  | total loss: \u001b[1m\u001b[32m123.28511\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 011 | loss: 123.28511 - R2: 0.0059 -- iter: 0256/1168\n",
      "Training Step: 195  | total loss: \u001b[1m\u001b[32m123.21482\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 011 | loss: 123.21482 - R2: 0.0059 -- iter: 0320/1168\n",
      "Training Step: 196  | total loss: \u001b[1m\u001b[32m123.08519\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 011 | loss: 123.08519 - R2: 0.0060 -- iter: 0384/1168\n",
      "Training Step: 197  | total loss: \u001b[1m\u001b[32m123.03436\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 011 | loss: 123.03436 - R2: 0.0060 -- iter: 0448/1168\n",
      "Training Step: 198  | total loss: \u001b[1m\u001b[32m122.82786\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 011 | loss: 122.82786 - R2: 0.0061 -- iter: 0512/1168\n",
      "Training Step: 199  | total loss: \u001b[1m\u001b[32m122.89895\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 011 | loss: 122.89895 - R2: 0.0062 -- iter: 0576/1168\n",
      "Training Step: 200  | total loss: \u001b[1m\u001b[32m122.95174\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 011 | loss: 122.95174 - R2: 0.0062 -- iter: 0640/1168\n",
      "Training Step: 201  | total loss: \u001b[1m\u001b[32m122.96527\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 011 | loss: 122.96527 - R2: 0.0063 -- iter: 0704/1168\n",
      "Training Step: 202  | total loss: \u001b[1m\u001b[32m122.96527\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 011 | loss: 122.96527 - R2: 0.0063 -- iter: 0768/1168\n",
      "Training Step: 203  | total loss: \u001b[1m\u001b[32m122.44777\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 011 | loss: 122.44777 - R2: 0.0064 -- iter: 0832/1168\n",
      "Training Step: 204  | total loss: \u001b[1m\u001b[32m122.44777\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 011 | loss: 122.44777 - R2: 0.0065 -- iter: 0896/1168\n",
      "Training Step: 205  | total loss: \u001b[1m\u001b[32m122.33492\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 011 | loss: 122.33492 - R2: 0.0066 -- iter: 0960/1168\n",
      "Training Step: 206  | total loss: \u001b[1m\u001b[32m122.33492\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 011 | loss: 122.33492 - R2: 0.0066 -- iter: 1024/1168\n",
      "Training Step: 207  | total loss: \u001b[1m\u001b[32m122.25786\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 011 | loss: 122.25786 - R2: 0.0067 -- iter: 1088/1168\n",
      "Training Step: 208  | total loss: \u001b[1m\u001b[32m122.12028\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 011 | loss: 122.12028 - R2: 0.0068 -- iter: 1152/1168\n",
      "Training Step: 209  | total loss: \u001b[1m\u001b[32m122.07288\u001b[0m\u001b[0m | time: 1.162s\n",
      "| SGD | epoch: 011 | loss: 122.07288 - R2: 0.0068 | val_loss: 120.58236 - val_acc: 0.0076 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 210  | total loss: \u001b[1m\u001b[32m121.86359\u001b[0m\u001b[0m | time: 0.007s\n",
      "| SGD | epoch: 012 | loss: 121.86359 - R2: 0.0069 -- iter: 0064/1168\n",
      "Training Step: 211  | total loss: \u001b[1m\u001b[32m121.50694\u001b[0m\u001b[0m | time: 0.012s\n",
      "| SGD | epoch: 012 | loss: 121.50694 - R2: 0.0070 -- iter: 0128/1168\n",
      "Training Step: 212  | total loss: \u001b[1m\u001b[32m121.50694\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 012 | loss: 121.50694 - R2: 0.0070 -- iter: 0192/1168\n",
      "Training Step: 213  | total loss: \u001b[1m\u001b[32m121.29259\u001b[0m\u001b[0m | time: 0.023s\n",
      "| SGD | epoch: 012 | loss: 121.29259 - R2: 0.0071 -- iter: 0256/1168\n",
      "Training Step: 214  | total loss: \u001b[1m\u001b[32m121.12848\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 012 | loss: 121.12848 - R2: 0.0072 -- iter: 0320/1168\n",
      "Training Step: 215  | total loss: \u001b[1m\u001b[32m121.12848\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 012 | loss: 121.12848 - R2: 0.0073 -- iter: 0384/1168\n",
      "Training Step: 216  | total loss: \u001b[1m\u001b[32m120.94482\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 012 | loss: 120.94482 - R2: 0.0073 -- iter: 0448/1168\n",
      "Training Step: 217  | total loss: \u001b[1m\u001b[32m120.84460\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 012 | loss: 120.84460 - R2: 0.0074 -- iter: 0512/1168\n",
      "Training Step: 218  | total loss: \u001b[1m\u001b[32m120.65990\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 012 | loss: 120.65990 - R2: 0.0075 -- iter: 0576/1168\n",
      "Training Step: 219  | total loss: \u001b[1m\u001b[32m120.85796\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 012 | loss: 120.85796 - R2: 0.0076 -- iter: 0640/1168\n",
      "Training Step: 220  | total loss: \u001b[1m\u001b[32m121.02503\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 012 | loss: 121.02503 - R2: 0.0076 -- iter: 0704/1168\n",
      "Training Step: 221  | total loss: \u001b[1m\u001b[32m121.02503\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 012 | loss: 121.02503 - R2: 0.0077 -- iter: 0768/1168\n",
      "Training Step: 222  | total loss: \u001b[1m\u001b[32m120.86240\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 012 | loss: 120.86240 - R2: 0.0077 -- iter: 0832/1168\n",
      "Training Step: 223  | total loss: \u001b[1m\u001b[32m120.61314\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 012 | loss: 120.61314 - R2: 0.0078 -- iter: 0896/1168\n",
      "Training Step: 224  | total loss: \u001b[1m\u001b[32m120.34987\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 012 | loss: 120.34987 - R2: 0.0079 -- iter: 0960/1168\n",
      "Training Step: 225  | total loss: \u001b[1m\u001b[32m120.30471\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 012 | loss: 120.30471 - R2: 0.0080 -- iter: 1024/1168\n",
      "Training Step: 226  | total loss: \u001b[1m\u001b[32m120.30471\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 012 | loss: 120.30471 - R2: 0.0080 -- iter: 1088/1168\n",
      "Training Step: 227  | total loss: \u001b[1m\u001b[32m120.22594\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 012 | loss: 120.22594 - R2: 0.0081 -- iter: 1152/1168\n",
      "Training Step: 228  | total loss: \u001b[1m\u001b[32m120.10877\u001b[0m\u001b[0m | time: 1.097s\n",
      "| SGD | epoch: 012 | loss: 120.10877 - R2: 0.0082 | val_loss: 118.48738 - val_acc: 0.0091 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 229  | total loss: \u001b[1m\u001b[32m119.84055\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 013 | loss: 119.84055 - R2: 0.0083 -- iter: 0064/1168\n",
      "Training Step: 230  | total loss: \u001b[1m\u001b[32m119.84055\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 013 | loss: 119.84055 - R2: 0.0083 -- iter: 0128/1168\n",
      "Training Step: 231  | total loss: \u001b[1m\u001b[32m119.58199\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 013 | loss: 119.58199 - R2: 0.0084 -- iter: 0192/1168\n",
      "Training Step: 232  | total loss: \u001b[1m\u001b[32m119.57352\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 013 | loss: 119.57352 - R2: 0.0085 -- iter: 0256/1168\n",
      "Training Step: 233  | total loss: \u001b[1m\u001b[32m119.50549\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 013 | loss: 119.50549 - R2: 0.0086 -- iter: 0320/1168\n",
      "Training Step: 234  | total loss: \u001b[1m\u001b[32m119.35345\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 013 | loss: 119.35345 - R2: 0.0087 -- iter: 0384/1168\n",
      "Training Step: 235  | total loss: \u001b[1m\u001b[32m119.35345\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 013 | loss: 119.35345 - R2: 0.0087 -- iter: 0448/1168\n",
      "Training Step: 236  | total loss: \u001b[1m\u001b[32m119.01892\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 013 | loss: 119.01892 - R2: 0.0088 -- iter: 0512/1168\n",
      "Training Step: 237  | total loss: \u001b[1m\u001b[32m118.75388\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 013 | loss: 118.75388 - R2: 0.0089 -- iter: 0576/1168\n",
      "Training Step: 238  | total loss: \u001b[1m\u001b[32m118.75388\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 013 | loss: 118.75388 - R2: 0.0090 -- iter: 0640/1168\n",
      "Training Step: 239  | total loss: \u001b[1m\u001b[32m118.70711\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 013 | loss: 118.70711 - R2: 0.0091 -- iter: 0704/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 240  | total loss: \u001b[1m\u001b[32m118.65435\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 013 | loss: 118.65435 - R2: 0.0091 -- iter: 0768/1168\n",
      "Training Step: 241  | total loss: \u001b[1m\u001b[32m118.65435\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 013 | loss: 118.65435 - R2: 0.0092 -- iter: 0832/1168\n",
      "Training Step: 242  | total loss: \u001b[1m\u001b[32m118.47023\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 013 | loss: 118.47023 - R2: 0.0093 -- iter: 0896/1168\n",
      "Training Step: 243  | total loss: \u001b[1m\u001b[32m118.18983\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 013 | loss: 118.18983 - R2: 0.0094 -- iter: 0960/1168\n",
      "Training Step: 244  | total loss: \u001b[1m\u001b[32m118.08816\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 013 | loss: 118.08816 - R2: 0.0095 -- iter: 1024/1168\n",
      "Training Step: 245  | total loss: \u001b[1m\u001b[32m117.74529\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 013 | loss: 117.74529 - R2: 0.0095 -- iter: 1088/1168\n",
      "Training Step: 246  | total loss: \u001b[1m\u001b[32m117.74529\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 013 | loss: 117.74529 - R2: 0.0096 -- iter: 1152/1168\n",
      "Training Step: 247  | total loss: \u001b[1m\u001b[32m117.64948\u001b[0m\u001b[0m | time: 1.163s\n",
      "| SGD | epoch: 013 | loss: 117.64948 - R2: 0.0097 | val_loss: 116.40694 - val_acc: 0.0106 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 248  | total loss: \u001b[1m\u001b[32m117.39659\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 014 | loss: 117.39659 - R2: 0.0098 -- iter: 0064/1168\n",
      "Training Step: 249  | total loss: \u001b[1m\u001b[32m117.43106\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 014 | loss: 117.43106 - R2: 0.0099 -- iter: 0128/1168\n",
      "Training Step: 250  | total loss: \u001b[1m\u001b[32m116.92500\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 014 | loss: 116.92500 - R2: 0.0100 -- iter: 0192/1168\n",
      "Training Step: 251  | total loss: \u001b[1m\u001b[32m116.89245\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 014 | loss: 116.89245 - R2: 0.0101 -- iter: 0256/1168\n",
      "Training Step: 252  | total loss: \u001b[1m\u001b[32m116.89245\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 014 | loss: 116.89245 - R2: 0.0102 -- iter: 0320/1168\n",
      "Training Step: 253  | total loss: \u001b[1m\u001b[32m116.76580\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 014 | loss: 116.76580 - R2: 0.0102 -- iter: 0384/1168\n",
      "Training Step: 254  | total loss: \u001b[1m\u001b[32m116.67365\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 014 | loss: 116.67365 - R2: 0.0103 -- iter: 0448/1168\n",
      "Training Step: 255  | total loss: \u001b[1m\u001b[32m116.53262\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 014 | loss: 116.53262 - R2: 0.0104 -- iter: 0512/1168\n",
      "Training Step: 256  | total loss: \u001b[1m\u001b[32m116.53262\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 014 | loss: 116.53262 - R2: 0.0105 -- iter: 0576/1168\n",
      "Training Step: 257  | total loss: \u001b[1m\u001b[32m116.43327\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 014 | loss: 116.43327 - R2: 0.0106 -- iter: 0640/1168\n",
      "Training Step: 258  | total loss: \u001b[1m\u001b[32m116.30003\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 014 | loss: 116.30003 - R2: 0.0107 -- iter: 0704/1168\n",
      "Training Step: 259  | total loss: \u001b[1m\u001b[32m116.14891\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 014 | loss: 116.14891 - R2: 0.0108 -- iter: 0768/1168\n",
      "Training Step: 260  | total loss: \u001b[1m\u001b[32m116.51563\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 014 | loss: 116.51563 - R2: 0.0108 -- iter: 0832/1168\n",
      "Training Step: 261  | total loss: \u001b[1m\u001b[32m116.83459\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 014 | loss: 116.83459 - R2: 0.0109 -- iter: 0896/1168\n",
      "Training Step: 262  | total loss: \u001b[1m\u001b[32m116.47974\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 014 | loss: 116.47974 - R2: 0.0109 -- iter: 0960/1168\n",
      "Training Step: 263  | total loss: \u001b[1m\u001b[32m116.47974\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 014 | loss: 116.47974 - R2: 0.0110 -- iter: 1024/1168\n",
      "Training Step: 264  | total loss: \u001b[1m\u001b[32m116.17088\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 014 | loss: 116.17088 - R2: 0.0111 -- iter: 1088/1168\n",
      "Training Step: 265  | total loss: \u001b[1m\u001b[32m116.12229\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 014 | loss: 116.12229 - R2: 0.0112 -- iter: 1152/1168\n",
      "Training Step: 266  | total loss: \u001b[1m\u001b[32m116.05674\u001b[0m\u001b[0m | time: 1.122s\n",
      "| SGD | epoch: 014 | loss: 116.05674 - R2: 0.0113 | val_loss: 114.34045 - val_acc: 0.0124 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 267  | total loss: \u001b[1m\u001b[32m115.83604\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 015 | loss: 115.83604 - R2: 0.0114 -- iter: 0064/1168\n",
      "Training Step: 268  | total loss: \u001b[1m\u001b[32m115.83604\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 015 | loss: 115.83604 - R2: 0.0115 -- iter: 0128/1168\n",
      "Training Step: 269  | total loss: \u001b[1m\u001b[32m115.53350\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 015 | loss: 115.53350 - R2: 0.0116 -- iter: 0192/1168\n",
      "Training Step: 270  | total loss: \u001b[1m\u001b[32m115.41439\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 015 | loss: 115.41439 - R2: 0.0117 -- iter: 0256/1168\n",
      "Training Step: 271  | total loss: \u001b[1m\u001b[32m115.07352\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 015 | loss: 115.07352 - R2: 0.0118 -- iter: 0320/1168\n",
      "Training Step: 272  | total loss: \u001b[1m\u001b[32m115.04386\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 015 | loss: 115.04386 - R2: 0.0119 -- iter: 0384/1168\n",
      "Training Step: 273  | total loss: \u001b[1m\u001b[32m114.86205\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 015 | loss: 114.86205 - R2: 0.0120 -- iter: 0448/1168\n",
      "Training Step: 274  | total loss: \u001b[1m\u001b[32m114.75135\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 015 | loss: 114.75135 - R2: 0.0121 -- iter: 0512/1168\n",
      "Training Step: 275  | total loss: \u001b[1m\u001b[32m114.75135\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 015 | loss: 114.75135 - R2: 0.0122 -- iter: 0576/1168\n",
      "Training Step: 276  | total loss: \u001b[1m\u001b[32m114.62972\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 015 | loss: 114.62972 - R2: 0.0123 -- iter: 0640/1168\n",
      "Training Step: 277  | total loss: \u001b[1m\u001b[32m114.55238\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 015 | loss: 114.55238 - R2: 0.0123 -- iter: 0704/1168\n",
      "Training Step: 278  | total loss: \u001b[1m\u001b[32m114.37246\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 015 | loss: 114.37246 - R2: 0.0124 -- iter: 0768/1168\n",
      "Training Step: 279  | total loss: \u001b[1m\u001b[32m114.27528\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 015 | loss: 114.27528 - R2: 0.0125 -- iter: 0832/1168\n",
      "Training Step: 280  | total loss: \u001b[1m\u001b[32m114.27528\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 015 | loss: 114.27528 - R2: 0.0126 -- iter: 0896/1168\n",
      "Training Step: 281  | total loss: \u001b[1m\u001b[32m113.99979\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 015 | loss: 113.99979 - R2: 0.0127 -- iter: 0960/1168\n",
      "Training Step: 282  | total loss: \u001b[1m\u001b[32m113.99979\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 015 | loss: 113.99979 - R2: 0.0128 -- iter: 1024/1168\n",
      "Training Step: 283  | total loss: \u001b[1m\u001b[32m113.93245\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 015 | loss: 113.93245 - R2: 0.0129 -- iter: 1088/1168\n",
      "Training Step: 284  | total loss: \u001b[1m\u001b[32m113.72968\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 015 | loss: 113.72968 - R2: 0.0130 -- iter: 1152/1168\n",
      "Training Step: 285  | total loss: \u001b[1m\u001b[32m113.52902\u001b[0m\u001b[0m | time: 1.121s\n",
      "| SGD | epoch: 015 | loss: 113.52902 - R2: 0.0131 | val_loss: 112.28705 - val_acc: 0.0142 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 286  | total loss: \u001b[1m\u001b[32m113.48766\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 016 | loss: 113.48766 - R2: 0.0132 -- iter: 0064/1168\n",
      "Training Step: 287  | total loss: \u001b[1m\u001b[32m113.45136\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 016 | loss: 113.45136 - R2: 0.0133 -- iter: 0128/1168\n",
      "Training Step: 288  | total loss: \u001b[1m\u001b[32m113.27453\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 016 | loss: 113.27453 - R2: 0.0134 -- iter: 0192/1168\n",
      "Training Step: 289  | total loss: \u001b[1m\u001b[32m112.93908\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 016 | loss: 112.93908 - R2: 0.0135 -- iter: 0256/1168\n",
      "Training Step: 290  | total loss: \u001b[1m\u001b[32m112.74235\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 016 | loss: 112.74235 - R2: 0.0136 -- iter: 0320/1168\n",
      "Training Step: 291  | total loss: \u001b[1m\u001b[32m112.72246\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 016 | loss: 112.72246 - R2: 0.0137 -- iter: 0384/1168\n",
      "Training Step: 292  | total loss: \u001b[1m\u001b[32m112.52860\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 016 | loss: 112.52860 - R2: 0.0138 -- iter: 0448/1168\n",
      "Training Step: 293  | total loss: \u001b[1m\u001b[32m112.52860\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 016 | loss: 112.52860 - R2: 0.0139 -- iter: 0512/1168\n",
      "Training Step: 294  | total loss: \u001b[1m\u001b[32m112.33764\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 016 | loss: 112.33764 - R2: 0.0140 -- iter: 0576/1168\n",
      "Training Step: 295  | total loss: \u001b[1m\u001b[32m112.02618\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 016 | loss: 112.02618 - R2: 0.0141 -- iter: 0640/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 296  | total loss: \u001b[1m\u001b[32m112.02618\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 016 | loss: 112.02618 - R2: 0.0143 -- iter: 0704/1168\n",
      "Training Step: 297  | total loss: \u001b[1m\u001b[32m111.96439\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 016 | loss: 111.96439 - R2: 0.0143 -- iter: 0768/1168\n",
      "Training Step: 298  | total loss: \u001b[1m\u001b[32m111.96439\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 016 | loss: 111.96439 - R2: 0.0144 -- iter: 0832/1168\n",
      "Training Step: 299  | total loss: \u001b[1m\u001b[32m112.17057\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 016 | loss: 112.17057 - R2: 0.0145 -- iter: 0896/1168\n",
      "Training Step: 300  | total loss: \u001b[1m\u001b[32m112.17057\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 016 | loss: 112.17057 - R2: 0.0146 -- iter: 0960/1168\n",
      "Training Step: 301  | total loss: \u001b[1m\u001b[32m112.30653\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 016 | loss: 112.30653 - R2: 0.0147 -- iter: 1024/1168\n",
      "Training Step: 302  | total loss: \u001b[1m\u001b[32m112.30653\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 016 | loss: 112.30653 - R2: 0.0148 -- iter: 1088/1168\n",
      "Training Step: 303  | total loss: \u001b[1m\u001b[32m112.06725\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 016 | loss: 112.06725 - R2: 0.0149 -- iter: 1152/1168\n",
      "Training Step: 304  | total loss: \u001b[1m\u001b[32m112.06725\u001b[0m\u001b[0m | time: 1.159s\n",
      "| SGD | epoch: 016 | loss: 112.06725 - R2: 0.0150 | val_loss: 110.24583 - val_acc: 0.0162 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 305  | total loss: \u001b[1m\u001b[32m111.68858\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 017 | loss: 111.68858 - R2: 0.0151 -- iter: 0064/1168\n",
      "Training Step: 306  | total loss: \u001b[1m\u001b[32m111.68858\u001b[0m\u001b[0m | time: 0.006s\n",
      "| SGD | epoch: 017 | loss: 111.68858 - R2: 0.0152 -- iter: 0128/1168\n",
      "Training Step: 307  | total loss: \u001b[1m\u001b[32m111.59740\u001b[0m\u001b[0m | time: 0.008s\n",
      "| SGD | epoch: 017 | loss: 111.59740 - R2: 0.0153 -- iter: 0192/1168\n",
      "Training Step: 308  | total loss: \u001b[1m\u001b[32m111.48728\u001b[0m\u001b[0m | time: 0.011s\n",
      "| SGD | epoch: 017 | loss: 111.48728 - R2: 0.0154 -- iter: 0256/1168\n",
      "Training Step: 309  | total loss: \u001b[1m\u001b[32m111.25980\u001b[0m\u001b[0m | time: 0.013s\n",
      "| SGD | epoch: 017 | loss: 111.25980 - R2: 0.0155 -- iter: 0320/1168\n",
      "Training Step: 310  | total loss: \u001b[1m\u001b[32m111.22398\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 017 | loss: 111.22398 - R2: 0.0156 -- iter: 0384/1168\n",
      "Training Step: 311  | total loss: \u001b[1m\u001b[32m111.00152\u001b[0m\u001b[0m | time: 0.018s\n",
      "| SGD | epoch: 017 | loss: 111.00152 - R2: 0.0157 -- iter: 0448/1168\n",
      "Training Step: 312  | total loss: \u001b[1m\u001b[32m110.91773\u001b[0m\u001b[0m | time: 0.020s\n",
      "| SGD | epoch: 017 | loss: 110.91773 - R2: 0.0158 -- iter: 0512/1168\n",
      "Training Step: 313  | total loss: \u001b[1m\u001b[32m110.98627\u001b[0m\u001b[0m | time: 0.023s\n",
      "| SGD | epoch: 017 | loss: 110.98627 - R2: 0.0160 -- iter: 0576/1168\n",
      "Training Step: 314  | total loss: \u001b[1m\u001b[32m110.86324\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 017 | loss: 110.86324 - R2: 0.0160 -- iter: 0640/1168\n",
      "Training Step: 315  | total loss: \u001b[1m\u001b[32m110.65489\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 017 | loss: 110.65489 - R2: 0.0162 -- iter: 0704/1168\n",
      "Training Step: 316  | total loss: \u001b[1m\u001b[32m110.44494\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 017 | loss: 110.44494 - R2: 0.0163 -- iter: 0768/1168\n",
      "Training Step: 317  | total loss: \u001b[1m\u001b[32m110.44494\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 017 | loss: 110.44494 - R2: 0.0164 -- iter: 0832/1168\n",
      "Training Step: 318  | total loss: \u001b[1m\u001b[32m110.13435\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 017 | loss: 110.13435 - R2: 0.0165 -- iter: 0896/1168\n",
      "Training Step: 319  | total loss: \u001b[1m\u001b[32m110.20119\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 017 | loss: 110.20119 - R2: 0.0166 -- iter: 0960/1168\n",
      "Training Step: 320  | total loss: \u001b[1m\u001b[32m110.36121\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 017 | loss: 110.36121 - R2: 0.0167 -- iter: 1024/1168\n",
      "Training Step: 321  | total loss: \u001b[1m\u001b[32m110.36121\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 017 | loss: 110.36121 - R2: 0.0168 -- iter: 1088/1168\n",
      "Training Step: 322  | total loss: \u001b[1m\u001b[32m110.27219\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 017 | loss: 110.27219 - R2: 0.0169 -- iter: 1152/1168\n",
      "Training Step: 323  | total loss: \u001b[1m\u001b[32m110.03256\u001b[0m\u001b[0m | time: 1.051s\n",
      "| SGD | epoch: 017 | loss: 110.03256 - R2: 0.0170 | val_loss: 108.21563 - val_acc: 0.0183 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 324  | total loss: \u001b[1m\u001b[32m109.59388\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 018 | loss: 109.59388 - R2: 0.0171 -- iter: 0064/1168\n",
      "Training Step: 325  | total loss: \u001b[1m\u001b[32m109.59388\u001b[0m\u001b[0m | time: 0.018s\n",
      "| SGD | epoch: 018 | loss: 109.59388 - R2: 0.0173 -- iter: 0128/1168\n",
      "Training Step: 326  | total loss: \u001b[1m\u001b[32m109.32877\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 018 | loss: 109.32877 - R2: 0.0174 -- iter: 0192/1168\n",
      "Training Step: 327  | total loss: \u001b[1m\u001b[32m109.32877\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 018 | loss: 109.32877 - R2: 0.0175 -- iter: 0256/1168\n",
      "Training Step: 328  | total loss: \u001b[1m\u001b[32m109.03136\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 018 | loss: 109.03136 - R2: 0.0176 -- iter: 0320/1168\n",
      "Training Step: 329  | total loss: \u001b[1m\u001b[32m109.03243\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 018 | loss: 109.03243 - R2: 0.0177 -- iter: 0384/1168\n",
      "Training Step: 330  | total loss: \u001b[1m\u001b[32m109.03243\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 018 | loss: 109.03243 - R2: 0.0178 -- iter: 0448/1168\n",
      "Training Step: 331  | total loss: \u001b[1m\u001b[32m108.64158\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 018 | loss: 108.64158 - R2: 0.0179 -- iter: 0512/1168\n",
      "Training Step: 332  | total loss: \u001b[1m\u001b[32m108.64158\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 018 | loss: 108.64158 - R2: 0.0181 -- iter: 0576/1168\n",
      "Training Step: 333  | total loss: \u001b[1m\u001b[32m108.59724\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 018 | loss: 108.59724 - R2: 0.0182 -- iter: 0640/1168\n",
      "Training Step: 334  | total loss: \u001b[1m\u001b[32m108.51974\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 018 | loss: 108.51974 - R2: 0.0183 -- iter: 0704/1168\n",
      "Training Step: 335  | total loss: \u001b[1m\u001b[32m108.36189\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 018 | loss: 108.36189 - R2: 0.0184 -- iter: 0768/1168\n",
      "Training Step: 336  | total loss: \u001b[1m\u001b[32m108.26318\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 018 | loss: 108.26318 - R2: 0.0185 -- iter: 0832/1168\n",
      "Training Step: 337  | total loss: \u001b[1m\u001b[32m108.17583\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 018 | loss: 108.17583 - R2: 0.0186 -- iter: 0896/1168\n",
      "Training Step: 338  | total loss: \u001b[1m\u001b[32m107.95309\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 018 | loss: 107.95309 - R2: 0.0187 -- iter: 0960/1168\n",
      "Training Step: 339  | total loss: \u001b[1m\u001b[32m107.95309\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 018 | loss: 107.95309 - R2: 0.0189 -- iter: 1024/1168\n",
      "Training Step: 340  | total loss: \u001b[1m\u001b[32m107.46782\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 018 | loss: 107.46782 - R2: 0.0191 -- iter: 1088/1168\n",
      "Training Step: 341  | total loss: \u001b[1m\u001b[32m107.02062\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 018 | loss: 107.02062 - R2: 0.0192 -- iter: 1152/1168\n",
      "Training Step: 342  | total loss: \u001b[1m\u001b[32m106.63738\u001b[0m\u001b[0m | time: 1.065s\n",
      "| SGD | epoch: 018 | loss: 106.63738 - R2: 0.0194 | val_loss: 106.19519 - val_acc: 0.0206 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 343  | total loss: \u001b[1m\u001b[32m106.63738\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 019 | loss: 106.63738 - R2: 0.0195 -- iter: 0064/1168\n",
      "Training Step: 344  | total loss: \u001b[1m\u001b[32m106.66120\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 019 | loss: 106.66120 - R2: 0.0196 -- iter: 0128/1168\n",
      "Training Step: 345  | total loss: \u001b[1m\u001b[32m106.64838\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 019 | loss: 106.64838 - R2: 0.0197 -- iter: 0192/1168\n",
      "Training Step: 346  | total loss: \u001b[1m\u001b[32m106.43838\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 019 | loss: 106.43838 - R2: 0.0198 -- iter: 0256/1168\n",
      "Training Step: 347  | total loss: \u001b[1m\u001b[32m106.58650\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 019 | loss: 106.58650 - R2: 0.0199 -- iter: 0320/1168\n",
      "Training Step: 348  | total loss: \u001b[1m\u001b[32m106.52029\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 019 | loss: 106.52029 - R2: 0.0200 -- iter: 0384/1168\n",
      "Training Step: 349  | total loss: \u001b[1m\u001b[32m106.44453\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 019 | loss: 106.44453 - R2: 0.0201 -- iter: 0448/1168\n",
      "Training Step: 350  | total loss: \u001b[1m\u001b[32m106.32762\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 019 | loss: 106.32762 - R2: 0.0203 -- iter: 0512/1168\n",
      "Training Step: 351  | total loss: \u001b[1m\u001b[32m106.14994\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 019 | loss: 106.14994 - R2: 0.0204 -- iter: 0576/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 352  | total loss: \u001b[1m\u001b[32m106.09842\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 019 | loss: 106.09842 - R2: 0.0205 -- iter: 0640/1168\n",
      "Training Step: 353  | total loss: \u001b[1m\u001b[32m106.09842\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 019 | loss: 106.09842 - R2: 0.0206 -- iter: 0704/1168\n",
      "Training Step: 354  | total loss: \u001b[1m\u001b[32m105.97124\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 019 | loss: 105.97124 - R2: 0.0207 -- iter: 0768/1168\n",
      "Training Step: 355  | total loss: \u001b[1m\u001b[32m106.06054\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 019 | loss: 106.06054 - R2: 0.0208 -- iter: 0832/1168\n",
      "Training Step: 356  | total loss: \u001b[1m\u001b[32m105.87783\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 019 | loss: 105.87783 - R2: 0.0209 -- iter: 0896/1168\n",
      "Training Step: 357  | total loss: \u001b[1m\u001b[32m105.87783\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 019 | loss: 105.87783 - R2: 0.0211 -- iter: 0960/1168\n",
      "Training Step: 358  | total loss: \u001b[1m\u001b[32m105.71082\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 019 | loss: 105.71082 - R2: 0.0212 -- iter: 1024/1168\n",
      "Training Step: 359  | total loss: \u001b[1m\u001b[32m105.53007\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 019 | loss: 105.53007 - R2: 0.0213 -- iter: 1088/1168\n",
      "Training Step: 360  | total loss: \u001b[1m\u001b[32m105.32684\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 019 | loss: 105.32684 - R2: 0.0215 -- iter: 1152/1168\n",
      "Training Step: 361  | total loss: \u001b[1m\u001b[32m105.32684\u001b[0m\u001b[0m | time: 1.090s\n",
      "| SGD | epoch: 019 | loss: 105.32684 - R2: 0.0216 | val_loss: 104.18320 - val_acc: 0.0230 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 362  | total loss: \u001b[1m\u001b[32m105.19588\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 020 | loss: 105.19588 - R2: 0.0217 -- iter: 0064/1168\n",
      "Training Step: 363  | total loss: \u001b[1m\u001b[32m105.14355\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 020 | loss: 105.14355 - R2: 0.0218 -- iter: 0128/1168\n",
      "Training Step: 364  | total loss: \u001b[1m\u001b[32m105.00116\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 020 | loss: 105.00116 - R2: 0.0220 -- iter: 0192/1168\n",
      "Training Step: 365  | total loss: \u001b[1m\u001b[32m104.86346\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 020 | loss: 104.86346 - R2: 0.0221 -- iter: 0256/1168\n",
      "Training Step: 366  | total loss: \u001b[1m\u001b[32m104.66035\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 020 | loss: 104.66035 - R2: 0.0223 -- iter: 0320/1168\n",
      "Training Step: 367  | total loss: \u001b[1m\u001b[32m104.66035\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 020 | loss: 104.66035 - R2: 0.0224 -- iter: 0384/1168\n",
      "Training Step: 368  | total loss: \u001b[1m\u001b[32m104.57658\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 020 | loss: 104.57658 - R2: 0.0225 -- iter: 0448/1168\n",
      "Training Step: 369  | total loss: \u001b[1m\u001b[32m104.56924\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 020 | loss: 104.56924 - R2: 0.0226 -- iter: 0512/1168\n",
      "Training Step: 370  | total loss: \u001b[1m\u001b[32m104.40327\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 020 | loss: 104.40327 - R2: 0.0227 -- iter: 0576/1168\n",
      "Training Step: 371  | total loss: \u001b[1m\u001b[32m104.40327\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 020 | loss: 104.40327 - R2: 0.0228 -- iter: 0640/1168\n",
      "Training Step: 372  | total loss: \u001b[1m\u001b[32m104.08236\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 020 | loss: 104.08236 - R2: 0.0230 -- iter: 0704/1168\n",
      "Training Step: 373  | total loss: \u001b[1m\u001b[32m104.08236\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 020 | loss: 104.08236 - R2: 0.0231 -- iter: 0768/1168\n",
      "Training Step: 374  | total loss: \u001b[1m\u001b[32m103.92978\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 020 | loss: 103.92978 - R2: 0.0233 -- iter: 0832/1168\n",
      "Training Step: 375  | total loss: \u001b[1m\u001b[32m103.86779\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 020 | loss: 103.86779 - R2: 0.0234 -- iter: 0896/1168\n",
      "Training Step: 376  | total loss: \u001b[1m\u001b[32m103.74343\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 020 | loss: 103.74343 - R2: 0.0235 -- iter: 0960/1168\n",
      "Training Step: 377  | total loss: \u001b[1m\u001b[32m103.61433\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 020 | loss: 103.61433 - R2: 0.0237 -- iter: 1024/1168\n",
      "Training Step: 378  | total loss: \u001b[1m\u001b[32m103.50920\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 020 | loss: 103.50920 - R2: 0.0238 -- iter: 1088/1168\n",
      "Training Step: 379  | total loss: \u001b[1m\u001b[32m103.13921\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 020 | loss: 103.13921 - R2: 0.0239 -- iter: 1152/1168\n",
      "Training Step: 380  | total loss: \u001b[1m\u001b[32m102.79579\u001b[0m\u001b[0m | time: 1.110s\n",
      "| SGD | epoch: 020 | loss: 102.79579 - R2: 0.0241 | val_loss: 102.17774 - val_acc: 0.0255 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 381  | total loss: \u001b[1m\u001b[32m102.79579\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 021 | loss: 102.79579 - R2: 0.0243 -- iter: 0064/1168\n",
      "Training Step: 382  | total loss: \u001b[1m\u001b[32m102.91380\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 021 | loss: 102.91380 - R2: 0.0244 -- iter: 0128/1168\n",
      "Training Step: 383  | total loss: \u001b[1m\u001b[32m102.70389\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 021 | loss: 102.70389 - R2: 0.0245 -- iter: 0192/1168\n",
      "Training Step: 384  | total loss: \u001b[1m\u001b[32m102.58932\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 021 | loss: 102.58932 - R2: 0.0247 -- iter: 0256/1168\n",
      "Training Step: 385  | total loss: \u001b[1m\u001b[32m102.52778\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 021 | loss: 102.52778 - R2: 0.0248 -- iter: 0320/1168\n",
      "Training Step: 386  | total loss: \u001b[1m\u001b[32m102.24468\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 021 | loss: 102.24468 - R2: 0.0249 -- iter: 0384/1168\n",
      "Training Step: 387  | total loss: \u001b[1m\u001b[32m102.24468\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 021 | loss: 102.24468 - R2: 0.0251 -- iter: 0448/1168\n",
      "Training Step: 388  | total loss: \u001b[1m\u001b[32m102.29317\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 021 | loss: 102.29317 - R2: 0.0252 -- iter: 0512/1168\n",
      "Training Step: 389  | total loss: \u001b[1m\u001b[32m102.29317\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 021 | loss: 102.29317 - R2: 0.0253 -- iter: 0576/1168\n",
      "Training Step: 390  | total loss: \u001b[1m\u001b[32m102.13539\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 021 | loss: 102.13539 - R2: 0.0255 -- iter: 0640/1168\n",
      "Training Step: 391  | total loss: \u001b[1m\u001b[32m101.85983\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 021 | loss: 101.85983 - R2: 0.0256 -- iter: 0704/1168\n",
      "Training Step: 392  | total loss: \u001b[1m\u001b[32m101.85983\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 021 | loss: 101.85983 - R2: 0.0257 -- iter: 0768/1168\n",
      "Training Step: 393  | total loss: \u001b[1m\u001b[32m101.71109\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 021 | loss: 101.71109 - R2: 0.0259 -- iter: 0832/1168\n",
      "Training Step: 394  | total loss: \u001b[1m\u001b[32m101.51507\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 021 | loss: 101.51507 - R2: 0.0260 -- iter: 0896/1168\n",
      "Training Step: 395  | total loss: \u001b[1m\u001b[32m101.40397\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 021 | loss: 101.40397 - R2: 0.0262 -- iter: 0960/1168\n",
      "Training Step: 396  | total loss: \u001b[1m\u001b[32m101.40397\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 021 | loss: 101.40397 - R2: 0.0263 -- iter: 1024/1168\n",
      "Training Step: 397  | total loss: \u001b[1m\u001b[32m101.33404\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 021 | loss: 101.33404 - R2: 0.0264 -- iter: 1088/1168\n",
      "Training Step: 398  | total loss: \u001b[1m\u001b[32m101.10851\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 021 | loss: 101.10851 - R2: 0.0266 -- iter: 1152/1168\n",
      "Training Step: 399  | total loss: \u001b[1m\u001b[32m101.10851\u001b[0m\u001b[0m | time: 1.158s\n",
      "| SGD | epoch: 021 | loss: 101.10851 - R2: 0.0267 | val_loss: 100.17696 - val_acc: 0.0282 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 400  | total loss: \u001b[1m\u001b[32m101.19549\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 022 | loss: 101.19549 - R2: 0.0268 -- iter: 0064/1168\n",
      "Training Step: 401  | total loss: \u001b[1m\u001b[32m101.19549\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 022 | loss: 101.19549 - R2: 0.0269 -- iter: 0128/1168\n",
      "Training Step: 402  | total loss: \u001b[1m\u001b[32m101.18737\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 022 | loss: 101.18737 - R2: 0.0271 -- iter: 0192/1168\n",
      "Training Step: 403  | total loss: \u001b[1m\u001b[32m101.18737\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 022 | loss: 101.18737 - R2: 0.0272 -- iter: 0256/1168\n",
      "Training Step: 404  | total loss: \u001b[1m\u001b[32m101.11765\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 022 | loss: 101.11765 - R2: 0.0273 -- iter: 0320/1168\n",
      "Training Step: 405  | total loss: \u001b[1m\u001b[32m100.95561\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 022 | loss: 100.95561 - R2: 0.0275 -- iter: 0384/1168\n",
      "Training Step: 406  | total loss: \u001b[1m\u001b[32m100.45747\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 022 | loss: 100.45747 - R2: 0.0276 -- iter: 0448/1168\n",
      "Training Step: 407  | total loss: \u001b[1m\u001b[32m100.53463\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 022 | loss: 100.53463 - R2: 0.0278 -- iter: 0512/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 408  | total loss: \u001b[1m\u001b[32m100.53463\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 022 | loss: 100.53463 - R2: 0.0280 -- iter: 0576/1168\n",
      "Training Step: 409  | total loss: \u001b[1m\u001b[32m100.42049\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 022 | loss: 100.42049 - R2: 0.0281 -- iter: 0640/1168\n",
      "Training Step: 410  | total loss: \u001b[1m\u001b[32m100.19537\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 022 | loss: 100.19537 - R2: 0.0283 -- iter: 0704/1168\n",
      "Training Step: 411  | total loss: \u001b[1m\u001b[32m100.15783\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 022 | loss: 100.15783 - R2: 0.0284 -- iter: 0768/1168\n",
      "Training Step: 412  | total loss: \u001b[1m\u001b[32m100.03988\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 022 | loss: 100.03988 - R2: 0.0286 -- iter: 0832/1168\n",
      "Training Step: 413  | total loss: \u001b[1m\u001b[32m100.05118\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 022 | loss: 100.05118 - R2: 0.0287 -- iter: 0896/1168\n",
      "Training Step: 414  | total loss: \u001b[1m\u001b[32m99.87553\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 022 | loss: 99.87553 - R2: 0.0288 -- iter: 0960/1168\n",
      "Training Step: 415  | total loss: \u001b[1m\u001b[32m99.85028\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 022 | loss: 99.85028 - R2: 0.0290 -- iter: 1024/1168\n",
      "Training Step: 416  | total loss: \u001b[1m\u001b[32m99.75721\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 022 | loss: 99.75721 - R2: 0.0291 -- iter: 1088/1168\n",
      "Training Step: 417  | total loss: \u001b[1m\u001b[32m99.74792\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 022 | loss: 99.74792 - R2: 0.0292 -- iter: 1152/1168\n",
      "Training Step: 418  | total loss: \u001b[1m\u001b[32m99.51424\u001b[0m\u001b[0m | time: 1.126s\n",
      "| SGD | epoch: 022 | loss: 99.51424 - R2: 0.0294 | val_loss: 98.17863 - val_acc: 0.0311 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 419  | total loss: \u001b[1m\u001b[32m99.28252\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 023 | loss: 99.28252 - R2: 0.0296 -- iter: 0064/1168\n",
      "Training Step: 420  | total loss: \u001b[1m\u001b[32m99.32034\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 023 | loss: 99.32034 - R2: 0.0297 -- iter: 0128/1168\n",
      "Training Step: 421  | total loss: \u001b[1m\u001b[32m99.32034\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 023 | loss: 99.32034 - R2: 0.0299 -- iter: 0192/1168\n",
      "Training Step: 422  | total loss: \u001b[1m\u001b[32m99.15709\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 023 | loss: 99.15709 - R2: 0.0300 -- iter: 0256/1168\n",
      "Training Step: 423  | total loss: \u001b[1m\u001b[32m99.08003\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 023 | loss: 99.08003 - R2: 0.0302 -- iter: 0320/1168\n",
      "Training Step: 424  | total loss: \u001b[1m\u001b[32m98.67744\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 023 | loss: 98.67744 - R2: 0.0304 -- iter: 0384/1168\n",
      "Training Step: 425  | total loss: \u001b[1m\u001b[32m98.67744\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 023 | loss: 98.67744 - R2: 0.0305 -- iter: 0448/1168\n",
      "Training Step: 426  | total loss: \u001b[1m\u001b[32m98.51608\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 023 | loss: 98.51608 - R2: 0.0307 -- iter: 0512/1168\n",
      "Training Step: 427  | total loss: \u001b[1m\u001b[32m98.41842\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 023 | loss: 98.41842 - R2: 0.0308 -- iter: 0576/1168\n",
      "Training Step: 428  | total loss: \u001b[1m\u001b[32m98.48728\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 023 | loss: 98.48728 - R2: 0.0310 -- iter: 0640/1168\n",
      "Training Step: 429  | total loss: \u001b[1m\u001b[32m98.48728\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 023 | loss: 98.48728 - R2: 0.0311 -- iter: 0704/1168\n",
      "Training Step: 430  | total loss: \u001b[1m\u001b[32m98.40417\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 023 | loss: 98.40417 - R2: 0.0312 -- iter: 0768/1168\n",
      "Training Step: 431  | total loss: \u001b[1m\u001b[32m98.62110\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 023 | loss: 98.62110 - R2: 0.0313 -- iter: 0832/1168\n",
      "Training Step: 432  | total loss: \u001b[1m\u001b[32m98.32348\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 023 | loss: 98.32348 - R2: 0.0315 -- iter: 0896/1168\n",
      "Training Step: 433  | total loss: \u001b[1m\u001b[32m97.87121\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 023 | loss: 97.87121 - R2: 0.0317 -- iter: 0960/1168\n",
      "Training Step: 434  | total loss: \u001b[1m\u001b[32m97.74494\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 023 | loss: 97.74494 - R2: 0.0319 -- iter: 1024/1168\n",
      "Training Step: 435  | total loss: \u001b[1m\u001b[32m97.50296\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 023 | loss: 97.50296 - R2: 0.0320 -- iter: 1088/1168\n",
      "Training Step: 436  | total loss: \u001b[1m\u001b[32m97.50296\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 023 | loss: 97.50296 - R2: 0.0322 -- iter: 1152/1168\n",
      "Training Step: 437  | total loss: \u001b[1m\u001b[32m97.19392\u001b[0m\u001b[0m | time: 1.179s\n",
      "| SGD | epoch: 023 | loss: 97.19392 - R2: 0.0324 | val_loss: 96.18032 - val_acc: 0.0342 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 438  | total loss: \u001b[1m\u001b[32m97.07792\u001b[0m\u001b[0m | time: 0.010s\n",
      "| SGD | epoch: 024 | loss: 97.07792 - R2: 0.0326 -- iter: 0064/1168\n",
      "Training Step: 439  | total loss: \u001b[1m\u001b[32m97.34898\u001b[0m\u001b[0m | time: 0.016s\n",
      "| SGD | epoch: 024 | loss: 97.34898 - R2: 0.0328 -- iter: 0128/1168\n",
      "Training Step: 440  | total loss: \u001b[1m\u001b[32m97.34898\u001b[0m\u001b[0m | time: 0.019s\n",
      "| SGD | epoch: 024 | loss: 97.34898 - R2: 0.0328 -- iter: 0192/1168\n",
      "Training Step: 441  | total loss: \u001b[1m\u001b[32m97.58219\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 024 | loss: 97.58219 - R2: 0.0329 -- iter: 0256/1168\n",
      "Training Step: 442  | total loss: \u001b[1m\u001b[32m97.37166\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 024 | loss: 97.37166 - R2: 0.0331 -- iter: 0320/1168\n",
      "Training Step: 443  | total loss: \u001b[1m\u001b[32m97.15752\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 024 | loss: 97.15752 - R2: 0.0333 -- iter: 0384/1168\n",
      "Training Step: 444  | total loss: \u001b[1m\u001b[32m97.04076\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 024 | loss: 97.04076 - R2: 0.0334 -- iter: 0448/1168\n",
      "Training Step: 445  | total loss: \u001b[1m\u001b[32m96.77761\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 024 | loss: 96.77761 - R2: 0.0336 -- iter: 0512/1168\n",
      "Training Step: 446  | total loss: \u001b[1m\u001b[32m96.62457\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 024 | loss: 96.62457 - R2: 0.0338 -- iter: 0576/1168\n",
      "Training Step: 447  | total loss: \u001b[1m\u001b[32m96.58700\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 024 | loss: 96.58700 - R2: 0.0339 -- iter: 0640/1168\n",
      "Training Step: 448  | total loss: \u001b[1m\u001b[32m96.58700\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 024 | loss: 96.58700 - R2: 0.0341 -- iter: 0704/1168\n",
      "Training Step: 449  | total loss: \u001b[1m\u001b[32m96.60217\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 024 | loss: 96.60217 - R2: 0.0342 -- iter: 0768/1168\n",
      "Training Step: 450  | total loss: \u001b[1m\u001b[32m96.40005\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 024 | loss: 96.40005 - R2: 0.0344 -- iter: 0832/1168\n",
      "Training Step: 451  | total loss: \u001b[1m\u001b[32m96.33733\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 024 | loss: 96.33733 - R2: 0.0346 -- iter: 0896/1168\n",
      "Training Step: 452  | total loss: \u001b[1m\u001b[32m96.26174\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 024 | loss: 96.26174 - R2: 0.0347 -- iter: 0960/1168\n",
      "Training Step: 453  | total loss: \u001b[1m\u001b[32m96.06602\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 024 | loss: 96.06602 - R2: 0.0349 -- iter: 1024/1168\n",
      "Training Step: 454  | total loss: \u001b[1m\u001b[32m95.91601\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 024 | loss: 95.91601 - R2: 0.0350 -- iter: 1088/1168\n",
      "Training Step: 455  | total loss: \u001b[1m\u001b[32m95.91601\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 024 | loss: 95.91601 - R2: 0.0352 -- iter: 1152/1168\n",
      "Training Step: 456  | total loss: \u001b[1m\u001b[32m95.80990\u001b[0m\u001b[0m | time: 1.097s\n",
      "| SGD | epoch: 024 | loss: 95.80990 - R2: 0.0354 | val_loss: 94.17927 - val_acc: 0.0374 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 457  | total loss: \u001b[1m\u001b[32m95.80990\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 025 | loss: 95.80990 - R2: 0.0355 -- iter: 0064/1168\n",
      "Training Step: 458  | total loss: \u001b[1m\u001b[32m95.67371\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 025 | loss: 95.67371 - R2: 0.0357 -- iter: 0128/1168\n",
      "Training Step: 459  | total loss: \u001b[1m\u001b[32m95.39615\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 025 | loss: 95.39615 - R2: 0.0359 -- iter: 0192/1168\n",
      "Training Step: 460  | total loss: \u001b[1m\u001b[32m95.16417\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 025 | loss: 95.16417 - R2: 0.0361 -- iter: 0256/1168\n",
      "Training Step: 461  | total loss: \u001b[1m\u001b[32m94.94484\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 025 | loss: 94.94484 - R2: 0.0364 -- iter: 0320/1168\n",
      "Training Step: 462  | total loss: \u001b[1m\u001b[32m94.60608\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 025 | loss: 94.60608 - R2: 0.0365 -- iter: 0384/1168\n",
      "Training Step: 463  | total loss: \u001b[1m\u001b[32m94.60608\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 025 | loss: 94.60608 - R2: 0.0367 -- iter: 0448/1168\n",
      "Training Step: 464  | total loss: \u001b[1m\u001b[32m94.38819\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 025 | loss: 94.38819 - R2: 0.0370 -- iter: 0512/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 465  | total loss: \u001b[1m\u001b[32m94.05252\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 025 | loss: 94.05252 - R2: 0.0372 -- iter: 0576/1168\n",
      "Training Step: 466  | total loss: \u001b[1m\u001b[32m94.05252\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 025 | loss: 94.05252 - R2: 0.0373 -- iter: 0640/1168\n",
      "Training Step: 467  | total loss: \u001b[1m\u001b[32m93.81573\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 025 | loss: 93.81573 - R2: 0.0376 -- iter: 0704/1168\n",
      "Training Step: 468  | total loss: \u001b[1m\u001b[32m93.67560\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 025 | loss: 93.67560 - R2: 0.0377 -- iter: 0768/1168\n",
      "Training Step: 469  | total loss: \u001b[1m\u001b[32m93.57709\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 025 | loss: 93.57709 - R2: 0.0379 -- iter: 0832/1168\n",
      "Training Step: 470  | total loss: \u001b[1m\u001b[32m93.54059\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 025 | loss: 93.54059 - R2: 0.0381 -- iter: 0896/1168\n",
      "Training Step: 471  | total loss: \u001b[1m\u001b[32m93.50279\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 025 | loss: 93.50279 - R2: 0.0382 -- iter: 0960/1168\n",
      "Training Step: 472  | total loss: \u001b[1m\u001b[32m93.28606\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 025 | loss: 93.28606 - R2: 0.0384 -- iter: 1024/1168\n",
      "Training Step: 473  | total loss: \u001b[1m\u001b[32m93.28606\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 025 | loss: 93.28606 - R2: 0.0386 -- iter: 1088/1168\n",
      "Training Step: 474  | total loss: \u001b[1m\u001b[32m93.22536\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 025 | loss: 93.22536 - R2: 0.0387 -- iter: 1152/1168\n",
      "Training Step: 475  | total loss: \u001b[1m\u001b[32m93.13793\u001b[0m\u001b[0m | time: 1.129s\n",
      "| SGD | epoch: 025 | loss: 93.13793 - R2: 0.0389 | val_loss: 92.17248 - val_acc: 0.0408 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 476  | total loss: \u001b[1m\u001b[32m93.04386\u001b[0m\u001b[0m | time: 0.008s\n",
      "| SGD | epoch: 026 | loss: 93.04386 - R2: 0.0391 -- iter: 0064/1168\n",
      "Training Step: 477  | total loss: \u001b[1m\u001b[32m93.04386\u001b[0m\u001b[0m | time: 0.012s\n",
      "| SGD | epoch: 026 | loss: 93.04386 - R2: 0.0392 -- iter: 0128/1168\n",
      "Training Step: 478  | total loss: \u001b[1m\u001b[32m92.80694\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 026 | loss: 92.80694 - R2: 0.0395 -- iter: 0192/1168\n",
      "Training Step: 479  | total loss: \u001b[1m\u001b[32m92.78680\u001b[0m\u001b[0m | time: 0.018s\n",
      "| SGD | epoch: 026 | loss: 92.78680 - R2: 0.0397 -- iter: 0256/1168\n",
      "Training Step: 480  | total loss: \u001b[1m\u001b[32m92.78680\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 026 | loss: 92.78680 - R2: 0.0398 -- iter: 0320/1168\n",
      "Training Step: 481  | total loss: \u001b[1m\u001b[32m92.90618\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 026 | loss: 92.90618 - R2: 0.0399 -- iter: 0384/1168\n",
      "Training Step: 482  | total loss: \u001b[1m\u001b[32m92.81226\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 026 | loss: 92.81226 - R2: 0.0400 -- iter: 0448/1168\n",
      "Training Step: 483  | total loss: \u001b[1m\u001b[32m92.65144\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 026 | loss: 92.65144 - R2: 0.0402 -- iter: 0512/1168\n",
      "Training Step: 484  | total loss: \u001b[1m\u001b[32m92.77953\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 026 | loss: 92.77953 - R2: 0.0403 -- iter: 0576/1168\n",
      "Training Step: 485  | total loss: \u001b[1m\u001b[32m92.38465\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 026 | loss: 92.38465 - R2: 0.0406 -- iter: 0640/1168\n",
      "Training Step: 486  | total loss: \u001b[1m\u001b[32m92.32060\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 026 | loss: 92.32060 - R2: 0.0408 -- iter: 0704/1168\n",
      "Training Step: 487  | total loss: \u001b[1m\u001b[32m92.11876\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 026 | loss: 92.11876 - R2: 0.0409 -- iter: 0768/1168\n",
      "Training Step: 488  | total loss: \u001b[1m\u001b[32m91.95322\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 026 | loss: 91.95322 - R2: 0.0412 -- iter: 0832/1168\n",
      "Training Step: 489  | total loss: \u001b[1m\u001b[32m91.72636\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 026 | loss: 91.72636 - R2: 0.0414 -- iter: 0896/1168\n",
      "Training Step: 490  | total loss: \u001b[1m\u001b[32m91.72636\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 026 | loss: 91.72636 - R2: 0.0416 -- iter: 0960/1168\n",
      "Training Step: 491  | total loss: \u001b[1m\u001b[32m91.68970\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 026 | loss: 91.68970 - R2: 0.0418 -- iter: 1024/1168\n",
      "Training Step: 492  | total loss: \u001b[1m\u001b[32m91.55460\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 026 | loss: 91.55460 - R2: 0.0420 -- iter: 1088/1168\n",
      "Training Step: 493  | total loss: \u001b[1m\u001b[32m91.42519\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 026 | loss: 91.42519 - R2: 0.0422 -- iter: 1152/1168\n",
      "Training Step: 494  | total loss: \u001b[1m\u001b[32m91.25075\u001b[0m\u001b[0m | time: 1.074s\n",
      "| SGD | epoch: 026 | loss: 91.25075 - R2: 0.0424 | val_loss: 90.15682 - val_acc: 0.0444 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 495  | total loss: \u001b[1m\u001b[32m90.99655\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 027 | loss: 90.99655 - R2: 0.0426 -- iter: 0064/1168\n",
      "Training Step: 496  | total loss: \u001b[1m\u001b[32m90.93150\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 027 | loss: 90.93150 - R2: 0.0428 -- iter: 0128/1168\n",
      "Training Step: 497  | total loss: \u001b[1m\u001b[32m90.92425\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 027 | loss: 90.92425 - R2: 0.0430 -- iter: 0192/1168\n",
      "Training Step: 498  | total loss: \u001b[1m\u001b[32m90.81065\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 027 | loss: 90.81065 - R2: 0.0431 -- iter: 0256/1168\n",
      "Training Step: 499  | total loss: \u001b[1m\u001b[32m90.62740\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 027 | loss: 90.62740 - R2: 0.0433 -- iter: 0320/1168\n",
      "Training Step: 500  | total loss: \u001b[1m\u001b[32m90.62740\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 027 | loss: 90.62740 - R2: 0.0435 -- iter: 0384/1168\n",
      "Training Step: 501  | total loss: \u001b[1m\u001b[32m90.45179\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 027 | loss: 90.45179 - R2: 0.0438 -- iter: 0448/1168\n",
      "Training Step: 502  | total loss: \u001b[1m\u001b[32m90.55521\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 027 | loss: 90.55521 - R2: 0.0439 -- iter: 0512/1168\n",
      "Training Step: 503  | total loss: \u001b[1m\u001b[32m90.42739\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 027 | loss: 90.42739 - R2: 0.0440 -- iter: 0576/1168\n",
      "Training Step: 504  | total loss: \u001b[1m\u001b[32m90.19550\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 027 | loss: 90.19550 - R2: 0.0442 -- iter: 0640/1168\n",
      "Training Step: 505  | total loss: \u001b[1m\u001b[32m90.19550\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 027 | loss: 90.19550 - R2: 0.0445 -- iter: 0704/1168\n",
      "Training Step: 506  | total loss: \u001b[1m\u001b[32m89.90992\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 027 | loss: 89.90992 - R2: 0.0447 -- iter: 0768/1168\n",
      "Training Step: 507  | total loss: \u001b[1m\u001b[32m89.66937\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 027 | loss: 89.66937 - R2: 0.0449 -- iter: 0832/1168\n",
      "Training Step: 508  | total loss: \u001b[1m\u001b[32m89.66183\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 027 | loss: 89.66183 - R2: 0.0451 -- iter: 0896/1168\n",
      "Training Step: 509  | total loss: \u001b[1m\u001b[32m89.59846\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 027 | loss: 89.59846 - R2: 0.0453 -- iter: 0960/1168\n",
      "Training Step: 510  | total loss: \u001b[1m\u001b[32m89.57439\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 027 | loss: 89.57439 - R2: 0.0455 -- iter: 1024/1168\n",
      "Training Step: 511  | total loss: \u001b[1m\u001b[32m89.49631\u001b[0m\u001b[0m | time: 0.178s\n",
      "| SGD | epoch: 027 | loss: 89.49631 - R2: 0.0456 -- iter: 1088/1168\n",
      "Training Step: 512  | total loss: \u001b[1m\u001b[32m89.49631\u001b[0m\u001b[0m | time: 0.181s\n",
      "| SGD | epoch: 027 | loss: 89.49631 - R2: 0.0458 -- iter: 1152/1168\n",
      "Training Step: 513  | total loss: \u001b[1m\u001b[32m89.40319\u001b[0m\u001b[0m | time: 1.187s\n",
      "| SGD | epoch: 027 | loss: 89.40319 - R2: 0.0461 | val_loss: 88.12875 - val_acc: 0.0483 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 514  | total loss: \u001b[1m\u001b[32m89.40319\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 028 | loss: 89.40319 - R2: 0.0462 -- iter: 0064/1168\n",
      "Training Step: 515  | total loss: \u001b[1m\u001b[32m89.22039\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 028 | loss: 89.22039 - R2: 0.0464 -- iter: 0128/1168\n",
      "Training Step: 516  | total loss: \u001b[1m\u001b[32m88.96156\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 028 | loss: 88.96156 - R2: 0.0467 -- iter: 0192/1168\n",
      "Training Step: 517  | total loss: \u001b[1m\u001b[32m88.83470\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 028 | loss: 88.83470 - R2: 0.0469 -- iter: 0256/1168\n",
      "Training Step: 518  | total loss: \u001b[1m\u001b[32m88.83470\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 028 | loss: 88.83470 - R2: 0.0471 -- iter: 0320/1168\n",
      "Training Step: 519  | total loss: \u001b[1m\u001b[32m88.66945\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 028 | loss: 88.66945 - R2: 0.0473 -- iter: 0384/1168\n",
      "Training Step: 520  | total loss: \u001b[1m\u001b[32m88.82551\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 028 | loss: 88.82551 - R2: 0.0474 -- iter: 0448/1168\n",
      "Training Step: 521  | total loss: \u001b[1m\u001b[32m88.71466\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 028 | loss: 88.71466 - R2: 0.0476 -- iter: 0512/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 522  | total loss: \u001b[1m\u001b[32m88.71466\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 028 | loss: 88.71466 - R2: 0.0478 -- iter: 0576/1168\n",
      "Training Step: 523  | total loss: \u001b[1m\u001b[32m88.51614\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 028 | loss: 88.51614 - R2: 0.0480 -- iter: 0640/1168\n",
      "Training Step: 524  | total loss: \u001b[1m\u001b[32m88.34012\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 028 | loss: 88.34012 - R2: 0.0482 -- iter: 0704/1168\n",
      "Training Step: 525  | total loss: \u001b[1m\u001b[32m88.25346\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 028 | loss: 88.25346 - R2: 0.0484 -- iter: 0768/1168\n",
      "Training Step: 526  | total loss: \u001b[1m\u001b[32m87.96184\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 028 | loss: 87.96184 - R2: 0.0487 -- iter: 0832/1168\n",
      "Training Step: 527  | total loss: \u001b[1m\u001b[32m87.94492\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 028 | loss: 87.94492 - R2: 0.0489 -- iter: 0896/1168\n",
      "Training Step: 528  | total loss: \u001b[1m\u001b[32m87.87538\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 028 | loss: 87.87538 - R2: 0.0491 -- iter: 0960/1168\n",
      "Training Step: 529  | total loss: \u001b[1m\u001b[32m87.78944\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 028 | loss: 87.78944 - R2: 0.0493 -- iter: 1024/1168\n",
      "Training Step: 530  | total loss: \u001b[1m\u001b[32m87.41827\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 028 | loss: 87.41827 - R2: 0.0496 -- iter: 1088/1168\n",
      "Training Step: 531  | total loss: \u001b[1m\u001b[32m87.55452\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 028 | loss: 87.55452 - R2: 0.0497 -- iter: 1152/1168\n",
      "Training Step: 532  | total loss: \u001b[1m\u001b[32m87.19859\u001b[0m\u001b[0m | time: 1.124s\n",
      "| SGD | epoch: 028 | loss: 87.19859 - R2: 0.0500 | val_loss: 86.08411 - val_acc: 0.0524 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 533  | total loss: \u001b[1m\u001b[32m87.13231\u001b[0m\u001b[0m | time: 0.011s\n",
      "| SGD | epoch: 029 | loss: 87.13231 - R2: 0.0502 -- iter: 0064/1168\n",
      "Training Step: 534  | total loss: \u001b[1m\u001b[32m86.86360\u001b[0m\u001b[0m | time: 0.016s\n",
      "| SGD | epoch: 029 | loss: 86.86360 - R2: 0.0504 -- iter: 0128/1168\n",
      "Training Step: 535  | total loss: \u001b[1m\u001b[32m86.64021\u001b[0m\u001b[0m | time: 0.020s\n",
      "| SGD | epoch: 029 | loss: 86.64021 - R2: 0.0507 -- iter: 0192/1168\n",
      "Training Step: 536  | total loss: \u001b[1m\u001b[32m86.64021\u001b[0m\u001b[0m | time: 0.023s\n",
      "| SGD | epoch: 029 | loss: 86.64021 - R2: 0.0510 -- iter: 0256/1168\n",
      "Training Step: 537  | total loss: \u001b[1m\u001b[32m86.51409\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 029 | loss: 86.51409 - R2: 0.0512 -- iter: 0320/1168\n",
      "Training Step: 538  | total loss: \u001b[1m\u001b[32m86.42407\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 029 | loss: 86.42407 - R2: 0.0514 -- iter: 0384/1168\n",
      "Training Step: 539  | total loss: \u001b[1m\u001b[32m86.42006\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 029 | loss: 86.42006 - R2: 0.0516 -- iter: 0448/1168\n",
      "Training Step: 540  | total loss: \u001b[1m\u001b[32m86.40216\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 029 | loss: 86.40216 - R2: 0.0518 -- iter: 0512/1168\n",
      "Training Step: 541  | total loss: \u001b[1m\u001b[32m86.32405\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 029 | loss: 86.32405 - R2: 0.0519 -- iter: 0576/1168\n",
      "Training Step: 542  | total loss: \u001b[1m\u001b[32m86.32405\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 029 | loss: 86.32405 - R2: 0.0521 -- iter: 0640/1168\n",
      "Training Step: 543  | total loss: \u001b[1m\u001b[32m86.17191\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 029 | loss: 86.17191 - R2: 0.0524 -- iter: 0704/1168\n",
      "Training Step: 544  | total loss: \u001b[1m\u001b[32m86.17091\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 029 | loss: 86.17091 - R2: 0.0526 -- iter: 0768/1168\n",
      "Training Step: 545  | total loss: \u001b[1m\u001b[32m86.01262\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 029 | loss: 86.01262 - R2: 0.0528 -- iter: 0832/1168\n",
      "Training Step: 546  | total loss: \u001b[1m\u001b[32m86.01262\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 029 | loss: 86.01262 - R2: 0.0530 -- iter: 0896/1168\n",
      "Training Step: 547  | total loss: \u001b[1m\u001b[32m85.62777\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 029 | loss: 85.62777 - R2: 0.0532 -- iter: 0960/1168\n",
      "Training Step: 548  | total loss: \u001b[1m\u001b[32m85.46533\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 029 | loss: 85.46533 - R2: 0.0535 -- iter: 1024/1168\n",
      "Training Step: 549  | total loss: \u001b[1m\u001b[32m85.33614\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 029 | loss: 85.33614 - R2: 0.0538 -- iter: 1088/1168\n",
      "Training Step: 550  | total loss: \u001b[1m\u001b[32m85.33614\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 029 | loss: 85.33614 - R2: 0.0540 -- iter: 1152/1168\n",
      "Training Step: 551  | total loss: \u001b[1m\u001b[32m85.38818\u001b[0m\u001b[0m | time: 1.067s\n",
      "| SGD | epoch: 029 | loss: 85.38818 - R2: 0.0541 | val_loss: 84.02030 - val_acc: 0.0567 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 552  | total loss: \u001b[1m\u001b[32m85.11267\u001b[0m\u001b[0m | time: 0.026s\n",
      "| SGD | epoch: 030 | loss: 85.11267 - R2: 0.0544 -- iter: 0064/1168\n",
      "Training Step: 553  | total loss: \u001b[1m\u001b[32m85.11267\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 030 | loss: 85.11267 - R2: 0.0546 -- iter: 0128/1168\n",
      "Training Step: 554  | total loss: \u001b[1m\u001b[32m84.75964\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 030 | loss: 84.75964 - R2: 0.0549 -- iter: 0192/1168\n",
      "Training Step: 555  | total loss: \u001b[1m\u001b[32m84.63080\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 030 | loss: 84.63080 - R2: 0.0552 -- iter: 0256/1168\n",
      "Training Step: 556  | total loss: \u001b[1m\u001b[32m84.48478\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 030 | loss: 84.48478 - R2: 0.0554 -- iter: 0320/1168\n",
      "Training Step: 557  | total loss: \u001b[1m\u001b[32m84.48019\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 030 | loss: 84.48019 - R2: 0.0557 -- iter: 0384/1168\n",
      "Training Step: 558  | total loss: \u001b[1m\u001b[32m84.65294\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 030 | loss: 84.65294 - R2: 0.0558 -- iter: 0448/1168\n",
      "Training Step: 559  | total loss: \u001b[1m\u001b[32m84.34658\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 030 | loss: 84.34658 - R2: 0.0559 -- iter: 0512/1168\n",
      "Training Step: 560  | total loss: \u001b[1m\u001b[32m84.34658\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 030 | loss: 84.34658 - R2: 0.0563 -- iter: 0576/1168\n",
      "Training Step: 561  | total loss: \u001b[1m\u001b[32m84.01801\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 030 | loss: 84.01801 - R2: 0.0566 -- iter: 0640/1168\n",
      "Training Step: 562  | total loss: \u001b[1m\u001b[32m83.76917\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 030 | loss: 83.76917 - R2: 0.0568 -- iter: 0704/1168\n",
      "Training Step: 563  | total loss: \u001b[1m\u001b[32m83.76073\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 030 | loss: 83.76073 - R2: 0.0571 -- iter: 0768/1168\n",
      "Training Step: 564  | total loss: \u001b[1m\u001b[32m83.76073\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 030 | loss: 83.76073 - R2: 0.0573 -- iter: 0832/1168\n",
      "Training Step: 565  | total loss: \u001b[1m\u001b[32m83.46531\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 030 | loss: 83.46531 - R2: 0.0575 -- iter: 0896/1168\n",
      "Training Step: 566  | total loss: \u001b[1m\u001b[32m83.46531\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 030 | loss: 83.46531 - R2: 0.0578 -- iter: 0960/1168\n",
      "Training Step: 567  | total loss: \u001b[1m\u001b[32m83.34640\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 030 | loss: 83.34640 - R2: 0.0580 -- iter: 1024/1168\n",
      "Training Step: 568  | total loss: \u001b[1m\u001b[32m83.29998\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 030 | loss: 83.29998 - R2: 0.0582 -- iter: 1088/1168\n",
      "Training Step: 569  | total loss: \u001b[1m\u001b[32m83.25443\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 030 | loss: 83.25443 - R2: 0.0584 -- iter: 1152/1168\n",
      "Training Step: 570  | total loss: \u001b[1m\u001b[32m82.99645\u001b[0m\u001b[0m | time: 1.107s\n",
      "| SGD | epoch: 030 | loss: 82.99645 - R2: 0.0586 | val_loss: 81.93223 - val_acc: 0.0614 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 571  | total loss: \u001b[1m\u001b[32m82.99645\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 031 | loss: 82.99645 - R2: 0.0590 -- iter: 0064/1168\n",
      "Training Step: 572  | total loss: \u001b[1m\u001b[32m82.73465\u001b[0m\u001b[0m | time: 0.007s\n",
      "| SGD | epoch: 031 | loss: 82.73465 - R2: 0.0593 -- iter: 0128/1168\n",
      "Training Step: 573  | total loss: \u001b[1m\u001b[32m82.42907\u001b[0m\u001b[0m | time: 0.011s\n",
      "| SGD | epoch: 031 | loss: 82.42907 - R2: 0.0595 -- iter: 0192/1168\n",
      "Training Step: 574  | total loss: \u001b[1m\u001b[32m82.41389\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 031 | loss: 82.41389 - R2: 0.0598 -- iter: 0256/1168\n",
      "Training Step: 575  | total loss: \u001b[1m\u001b[32m82.31345\u001b[0m\u001b[0m | time: 0.020s\n",
      "| SGD | epoch: 031 | loss: 82.31345 - R2: 0.0600 -- iter: 0320/1168\n",
      "Training Step: 576  | total loss: \u001b[1m\u001b[32m82.36872\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 031 | loss: 82.36872 - R2: 0.0603 -- iter: 0384/1168\n",
      "Training Step: 577  | total loss: \u001b[1m\u001b[32m82.26283\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 031 | loss: 82.26283 - R2: 0.0604 -- iter: 0448/1168\n",
      "Training Step: 578  | total loss: \u001b[1m\u001b[32m82.26283\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 031 | loss: 82.26283 - R2: 0.0607 -- iter: 0512/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 579  | total loss: \u001b[1m\u001b[32m82.21805\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 031 | loss: 82.21805 - R2: 0.0609 -- iter: 0576/1168\n",
      "Training Step: 580  | total loss: \u001b[1m\u001b[32m81.45316\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 031 | loss: 81.45316 - R2: 0.0613 -- iter: 0640/1168\n",
      "Training Step: 581  | total loss: \u001b[1m\u001b[32m81.37074\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 031 | loss: 81.37074 - R2: 0.0617 -- iter: 0704/1168\n",
      "Training Step: 582  | total loss: \u001b[1m\u001b[32m81.37074\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 031 | loss: 81.37074 - R2: 0.0620 -- iter: 0768/1168\n",
      "Training Step: 583  | total loss: \u001b[1m\u001b[32m81.22315\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 031 | loss: 81.22315 - R2: 0.0622 -- iter: 0832/1168\n",
      "Training Step: 584  | total loss: \u001b[1m\u001b[32m81.19643\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 031 | loss: 81.19643 - R2: 0.0624 -- iter: 0896/1168\n",
      "Training Step: 585  | total loss: \u001b[1m\u001b[32m81.11991\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 031 | loss: 81.11991 - R2: 0.0627 -- iter: 0960/1168\n",
      "Training Step: 586  | total loss: \u001b[1m\u001b[32m81.09498\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 031 | loss: 81.09498 - R2: 0.0629 -- iter: 1024/1168\n",
      "Training Step: 587  | total loss: \u001b[1m\u001b[32m81.05322\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 031 | loss: 81.05322 - R2: 0.0631 -- iter: 1088/1168\n",
      "Training Step: 588  | total loss: \u001b[1m\u001b[32m80.80947\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 031 | loss: 80.80947 - R2: 0.0633 -- iter: 1152/1168\n",
      "Training Step: 589  | total loss: \u001b[1m\u001b[32m80.71745\u001b[0m\u001b[0m | time: 1.062s\n",
      "| SGD | epoch: 031 | loss: 80.71745 - R2: 0.0636 | val_loss: 79.81642 - val_acc: 0.0663 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 590  | total loss: \u001b[1m\u001b[32m80.79663\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 032 | loss: 80.79663 - R2: 0.0639 -- iter: 0064/1168\n",
      "Training Step: 591  | total loss: \u001b[1m\u001b[32m80.79663\u001b[0m\u001b[0m | time: 0.008s\n",
      "| SGD | epoch: 032 | loss: 80.79663 - R2: 0.0640 -- iter: 0128/1168\n",
      "Training Step: 592  | total loss: \u001b[1m\u001b[32m80.65041\u001b[0m\u001b[0m | time: 0.012s\n",
      "| SGD | epoch: 032 | loss: 80.65041 - R2: 0.0643 -- iter: 0192/1168\n",
      "Training Step: 593  | total loss: \u001b[1m\u001b[32m80.48795\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 032 | loss: 80.48795 - R2: 0.0645 -- iter: 0256/1168\n",
      "Training Step: 594  | total loss: \u001b[1m\u001b[32m80.51645\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 032 | loss: 80.51645 - R2: 0.0648 -- iter: 0320/1168\n",
      "Training Step: 595  | total loss: \u001b[1m\u001b[32m80.51645\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 032 | loss: 80.51645 - R2: 0.0649 -- iter: 0384/1168\n",
      "Training Step: 596  | total loss: \u001b[1m\u001b[32m80.40820\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 032 | loss: 80.40820 - R2: 0.0655 -- iter: 0448/1168\n",
      "Training Step: 597  | total loss: \u001b[1m\u001b[32m80.26599\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 032 | loss: 80.26599 - R2: 0.0655 -- iter: 0512/1168\n",
      "Training Step: 598  | total loss: \u001b[1m\u001b[32m80.20969\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 032 | loss: 80.20969 - R2: 0.0657 -- iter: 0576/1168\n",
      "Training Step: 599  | total loss: \u001b[1m\u001b[32m80.04021\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 032 | loss: 80.04021 - R2: 0.0666 -- iter: 0640/1168\n",
      "Training Step: 600  | total loss: \u001b[1m\u001b[32m79.09163\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 032 | loss: 79.09163 - R2: 0.0666 -- iter: 0704/1168\n",
      "Training Step: 601  | total loss: \u001b[1m\u001b[32m78.99905\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 032 | loss: 78.99905 - R2: 0.0671 -- iter: 0768/1168\n",
      "Training Step: 602  | total loss: \u001b[1m\u001b[32m78.84581\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 032 | loss: 78.84581 - R2: 0.0673 -- iter: 0832/1168\n",
      "Training Step: 603  | total loss: \u001b[1m\u001b[32m78.82755\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 032 | loss: 78.82755 - R2: 0.0676 -- iter: 0896/1168\n",
      "Training Step: 604  | total loss: \u001b[1m\u001b[32m78.68124\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 032 | loss: 78.68124 - R2: 0.0678 -- iter: 0960/1168\n",
      "Training Step: 605  | total loss: \u001b[1m\u001b[32m78.59832\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 032 | loss: 78.59832 - R2: 0.0681 -- iter: 1024/1168\n",
      "Training Step: 606  | total loss: \u001b[1m\u001b[32m78.59048\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 032 | loss: 78.59048 - R2: 0.0684 -- iter: 1088/1168\n",
      "Training Step: 607  | total loss: \u001b[1m\u001b[32m78.56368\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 032 | loss: 78.56368 - R2: 0.0686 -- iter: 1152/1168\n",
      "Training Step: 608  | total loss: \u001b[1m\u001b[32m78.56368\u001b[0m\u001b[0m | time: 1.090s\n",
      "| SGD | epoch: 032 | loss: 78.56368 - R2: 0.0688 | val_loss: 77.66958 - val_acc: 0.0716 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 609  | total loss: \u001b[1m\u001b[32m78.45945\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 033 | loss: 78.45945 - R2: 0.0691 -- iter: 0064/1168\n",
      "Training Step: 610  | total loss: \u001b[1m\u001b[32m78.46397\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 033 | loss: 78.46397 - R2: 0.0692 -- iter: 0128/1168\n",
      "Training Step: 611  | total loss: \u001b[1m\u001b[32m78.46397\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 033 | loss: 78.46397 - R2: 0.0695 -- iter: 0192/1168\n",
      "Training Step: 612  | total loss: \u001b[1m\u001b[32m78.29378\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 033 | loss: 78.29378 - R2: 0.0698 -- iter: 0256/1168\n",
      "Training Step: 613  | total loss: \u001b[1m\u001b[32m78.21913\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 033 | loss: 78.21913 - R2: 0.0700 -- iter: 0320/1168\n",
      "Training Step: 614  | total loss: \u001b[1m\u001b[32m78.03445\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 033 | loss: 78.03445 - R2: 0.0703 -- iter: 0384/1168\n",
      "Training Step: 615  | total loss: \u001b[1m\u001b[32m77.78669\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 033 | loss: 77.78669 - R2: 0.0706 -- iter: 0448/1168\n",
      "Training Step: 616  | total loss: \u001b[1m\u001b[32m77.67207\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 033 | loss: 77.67207 - R2: 0.0710 -- iter: 0512/1168\n",
      "Training Step: 617  | total loss: \u001b[1m\u001b[32m77.46546\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 033 | loss: 77.46546 - R2: 0.0712 -- iter: 0576/1168\n",
      "Training Step: 618  | total loss: \u001b[1m\u001b[32m77.46546\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 033 | loss: 77.46546 - R2: 0.0716 -- iter: 0640/1168\n",
      "Training Step: 619  | total loss: \u001b[1m\u001b[32m77.70450\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 033 | loss: 77.70450 - R2: 0.0718 -- iter: 0704/1168\n",
      "Training Step: 620  | total loss: \u001b[1m\u001b[32m77.91107\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 033 | loss: 77.91107 - R2: 0.0719 -- iter: 0768/1168\n",
      "Training Step: 621  | total loss: \u001b[1m\u001b[32m77.91107\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 033 | loss: 77.91107 - R2: 0.0719 -- iter: 0832/1168\n",
      "Training Step: 622  | total loss: \u001b[1m\u001b[32m77.80882\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 033 | loss: 77.80882 - R2: 0.0722 -- iter: 0896/1168\n",
      "Training Step: 623  | total loss: \u001b[1m\u001b[32m77.62753\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 033 | loss: 77.62753 - R2: 0.0725 -- iter: 0960/1168\n",
      "Training Step: 624  | total loss: \u001b[1m\u001b[32m77.37855\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 033 | loss: 77.37855 - R2: 0.0729 -- iter: 1024/1168\n",
      "Training Step: 625  | total loss: \u001b[1m\u001b[32m77.23278\u001b[0m\u001b[0m | time: 0.165s\n",
      "| SGD | epoch: 033 | loss: 77.23278 - R2: 0.0732 -- iter: 1088/1168\n",
      "Training Step: 626  | total loss: \u001b[1m\u001b[32m77.23278\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 033 | loss: 77.23278 - R2: 0.0735 -- iter: 1152/1168\n",
      "Training Step: 627  | total loss: \u001b[1m\u001b[32m76.96420\u001b[0m\u001b[0m | time: 1.172s\n",
      "| SGD | epoch: 033 | loss: 76.96420 - R2: 0.0739 | val_loss: 75.48898 - val_acc: 0.0772 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 628  | total loss: \u001b[1m\u001b[32m76.61900\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 034 | loss: 76.61900 - R2: 0.0742 -- iter: 0064/1168\n",
      "Training Step: 629  | total loss: \u001b[1m\u001b[32m76.61900\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 034 | loss: 76.61900 - R2: 0.0745 -- iter: 0128/1168\n",
      "Training Step: 630  | total loss: \u001b[1m\u001b[32m76.41307\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 034 | loss: 76.41307 - R2: 0.0747 -- iter: 0192/1168\n",
      "Training Step: 631  | total loss: \u001b[1m\u001b[32m76.07403\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 034 | loss: 76.07403 - R2: 0.0751 -- iter: 0256/1168\n",
      "Training Step: 632  | total loss: \u001b[1m\u001b[32m75.96747\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 034 | loss: 75.96747 - R2: 0.0756 -- iter: 0320/1168\n",
      "Training Step: 633  | total loss: \u001b[1m\u001b[32m75.96747\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 034 | loss: 75.96747 - R2: 0.0759 -- iter: 0384/1168\n",
      "Training Step: 634  | total loss: \u001b[1m\u001b[32m75.90152\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 034 | loss: 75.90152 - R2: 0.0761 -- iter: 0448/1168\n",
      "Training Step: 635  | total loss: \u001b[1m\u001b[32m75.67195\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 034 | loss: 75.67195 - R2: 0.0765 -- iter: 0512/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 636  | total loss: \u001b[1m\u001b[32m75.69452\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 034 | loss: 75.69452 - R2: 0.0768 -- iter: 0576/1168\n",
      "Training Step: 637  | total loss: \u001b[1m\u001b[32m75.55344\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 034 | loss: 75.55344 - R2: 0.0770 -- iter: 0640/1168\n",
      "Training Step: 638  | total loss: \u001b[1m\u001b[32m75.55344\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 034 | loss: 75.55344 - R2: 0.0773 -- iter: 0704/1168\n",
      "Training Step: 639  | total loss: \u001b[1m\u001b[32m75.42263\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 034 | loss: 75.42263 - R2: 0.0776 -- iter: 0768/1168\n",
      "Training Step: 640  | total loss: \u001b[1m\u001b[32m75.43690\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 034 | loss: 75.43690 - R2: 0.0778 -- iter: 0832/1168\n",
      "Training Step: 641  | total loss: \u001b[1m\u001b[32m75.42479\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 034 | loss: 75.42479 - R2: 0.0781 -- iter: 0896/1168\n",
      "Training Step: 642  | total loss: \u001b[1m\u001b[32m75.37476\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 034 | loss: 75.37476 - R2: 0.0783 -- iter: 0960/1168\n",
      "Training Step: 643  | total loss: \u001b[1m\u001b[32m75.25698\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 034 | loss: 75.25698 - R2: 0.0785 -- iter: 1024/1168\n",
      "Training Step: 644  | total loss: \u001b[1m\u001b[32m74.92223\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 034 | loss: 74.92223 - R2: 0.0789 -- iter: 1088/1168\n",
      "Training Step: 645  | total loss: \u001b[1m\u001b[32m74.84353\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 034 | loss: 74.84353 - R2: 0.0793 -- iter: 1152/1168\n",
      "Training Step: 646  | total loss: \u001b[1m\u001b[32m74.63899\u001b[0m\u001b[0m | time: 1.138s\n",
      "| SGD | epoch: 034 | loss: 74.63899 - R2: 0.0796 | val_loss: 73.27141 - val_acc: 0.0833 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 647  | total loss: \u001b[1m\u001b[32m74.48824\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 035 | loss: 74.48824 - R2: 0.0800 -- iter: 0064/1168\n",
      "Training Step: 648  | total loss: \u001b[1m\u001b[32m74.48824\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 035 | loss: 74.48824 - R2: 0.0804 -- iter: 0128/1168\n",
      "Training Step: 649  | total loss: \u001b[1m\u001b[32m74.42652\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 035 | loss: 74.42652 - R2: 0.0806 -- iter: 0192/1168\n",
      "Training Step: 650  | total loss: \u001b[1m\u001b[32m74.13120\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 035 | loss: 74.13120 - R2: 0.0810 -- iter: 0256/1168\n",
      "Training Step: 651  | total loss: \u001b[1m\u001b[32m74.09476\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 035 | loss: 74.09476 - R2: 0.0813 -- iter: 0320/1168\n",
      "Training Step: 652  | total loss: \u001b[1m\u001b[32m74.09476\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 035 | loss: 74.09476 - R2: 0.0816 -- iter: 0384/1168\n",
      "Training Step: 653  | total loss: \u001b[1m\u001b[32m73.95685\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 035 | loss: 73.95685 - R2: 0.0819 -- iter: 0448/1168\n",
      "Training Step: 654  | total loss: \u001b[1m\u001b[32m73.75496\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 035 | loss: 73.75496 - R2: 0.0823 -- iter: 0512/1168\n",
      "Training Step: 655  | total loss: \u001b[1m\u001b[32m73.67490\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 035 | loss: 73.67490 - R2: 0.0826 -- iter: 0576/1168\n",
      "Training Step: 656  | total loss: \u001b[1m\u001b[32m73.55166\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 035 | loss: 73.55166 - R2: 0.0830 -- iter: 0640/1168\n",
      "Training Step: 657  | total loss: \u001b[1m\u001b[32m73.52129\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 035 | loss: 73.52129 - R2: 0.0832 -- iter: 0704/1168\n",
      "Training Step: 658  | total loss: \u001b[1m\u001b[32m73.06334\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 035 | loss: 73.06334 - R2: 0.0837 -- iter: 0768/1168\n",
      "Training Step: 659  | total loss: \u001b[1m\u001b[32m73.06178\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 035 | loss: 73.06178 - R2: 0.0841 -- iter: 0832/1168\n",
      "Training Step: 660  | total loss: \u001b[1m\u001b[32m73.04827\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 035 | loss: 73.04827 - R2: 0.0843 -- iter: 0896/1168\n",
      "Training Step: 661  | total loss: \u001b[1m\u001b[32m72.87419\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 035 | loss: 72.87419 - R2: 0.0845 -- iter: 0960/1168\n",
      "Training Step: 662  | total loss: \u001b[1m\u001b[32m72.87419\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 035 | loss: 72.87419 - R2: 0.0849 -- iter: 1024/1168\n",
      "Training Step: 663  | total loss: \u001b[1m\u001b[32m72.72664\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 035 | loss: 72.72664 - R2: 0.0853 -- iter: 1088/1168\n",
      "Training Step: 664  | total loss: \u001b[1m\u001b[32m72.40916\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 035 | loss: 72.40916 - R2: 0.0855 -- iter: 1152/1168\n",
      "Training Step: 665  | total loss: \u001b[1m\u001b[32m72.20042\u001b[0m\u001b[0m | time: 1.137s\n",
      "| SGD | epoch: 035 | loss: 72.20042 - R2: 0.0860 | val_loss: 71.01500 - val_acc: 0.0898 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 666  | total loss: \u001b[1m\u001b[32m72.06255\u001b[0m\u001b[0m | time: 0.010s\n",
      "| SGD | epoch: 036 | loss: 72.06255 - R2: 0.0864 -- iter: 0064/1168\n",
      "Training Step: 667  | total loss: \u001b[1m\u001b[32m71.96536\u001b[0m\u001b[0m | time: 0.014s\n",
      "| SGD | epoch: 036 | loss: 71.96536 - R2: 0.0868 -- iter: 0128/1168\n",
      "Training Step: 668  | total loss: \u001b[1m\u001b[32m71.78732\u001b[0m\u001b[0m | time: 0.019s\n",
      "| SGD | epoch: 036 | loss: 71.78732 - R2: 0.0871 -- iter: 0192/1168\n",
      "Training Step: 669  | total loss: \u001b[1m\u001b[32m71.78732\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 036 | loss: 71.78732 - R2: 0.0875 -- iter: 0256/1168\n",
      "Training Step: 670  | total loss: \u001b[1m\u001b[32m71.73871\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 036 | loss: 71.73871 - R2: 0.0878 -- iter: 0320/1168\n",
      "Training Step: 671  | total loss: \u001b[1m\u001b[32m71.57767\u001b[0m\u001b[0m | time: 0.026s\n",
      "| SGD | epoch: 036 | loss: 71.57767 - R2: 0.0882 -- iter: 0384/1168\n",
      "Training Step: 672  | total loss: \u001b[1m\u001b[32m71.26672\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 036 | loss: 71.26672 - R2: 0.0885 -- iter: 0448/1168\n",
      "Training Step: 673  | total loss: \u001b[1m\u001b[32m71.22626\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 036 | loss: 71.22626 - R2: 0.0889 -- iter: 0512/1168\n",
      "Training Step: 674  | total loss: \u001b[1m\u001b[32m70.96552\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 036 | loss: 70.96552 - R2: 0.0892 -- iter: 0576/1168\n",
      "Training Step: 675  | total loss: \u001b[1m\u001b[32m70.86839\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 036 | loss: 70.86839 - R2: 0.0897 -- iter: 0640/1168\n",
      "Training Step: 676  | total loss: \u001b[1m\u001b[32m70.72978\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 036 | loss: 70.72978 - R2: 0.0900 -- iter: 0704/1168\n",
      "Training Step: 677  | total loss: \u001b[1m\u001b[32m70.72978\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 036 | loss: 70.72978 - R2: 0.0904 -- iter: 0768/1168\n",
      "Training Step: 678  | total loss: \u001b[1m\u001b[32m70.62937\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 036 | loss: 70.62937 - R2: 0.0907 -- iter: 0832/1168\n",
      "Training Step: 679  | total loss: \u001b[1m\u001b[32m70.62937\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 036 | loss: 70.62937 - R2: 0.0910 -- iter: 0896/1168\n",
      "Training Step: 680  | total loss: \u001b[1m\u001b[32m70.39331\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 036 | loss: 70.39331 - R2: 0.0915 -- iter: 0960/1168\n",
      "Training Step: 681  | total loss: \u001b[1m\u001b[32m70.12785\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 036 | loss: 70.12785 - R2: 0.0919 -- iter: 1024/1168\n",
      "Training Step: 682  | total loss: \u001b[1m\u001b[32m70.12785\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 036 | loss: 70.12785 - R2: 0.0922 -- iter: 1088/1168\n",
      "Training Step: 683  | total loss: \u001b[1m\u001b[32m70.04990\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 036 | loss: 70.04990 - R2: 0.0926 -- iter: 1152/1168\n",
      "Training Step: 684  | total loss: \u001b[1m\u001b[32m70.09039\u001b[0m\u001b[0m | time: 1.074s\n",
      "| SGD | epoch: 036 | loss: 70.09039 - R2: 0.0928 | val_loss: 68.71651 - val_acc: 0.0968 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 685  | total loss: \u001b[1m\u001b[32m69.89406\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 037 | loss: 69.89406 - R2: 0.0930 -- iter: 0064/1168\n",
      "Training Step: 686  | total loss: \u001b[1m\u001b[32m69.89406\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 037 | loss: 69.89406 - R2: 0.0934 -- iter: 0128/1168\n",
      "Training Step: 687  | total loss: \u001b[1m\u001b[32m69.43426\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 037 | loss: 69.43426 - R2: 0.0940 -- iter: 0192/1168\n",
      "Training Step: 688  | total loss: \u001b[1m\u001b[32m69.36346\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 037 | loss: 69.36346 - R2: 0.0944 -- iter: 0256/1168\n",
      "Training Step: 689  | total loss: \u001b[1m\u001b[32m69.36346\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 037 | loss: 69.36346 - R2: 0.0947 -- iter: 0320/1168\n",
      "Training Step: 690  | total loss: \u001b[1m\u001b[32m68.98170\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 037 | loss: 68.98170 - R2: 0.0951 -- iter: 0384/1168\n",
      "Training Step: 691  | total loss: \u001b[1m\u001b[32m68.98170\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 037 | loss: 68.98170 - R2: 0.0956 -- iter: 0448/1168\n",
      "Training Step: 692  | total loss: \u001b[1m\u001b[32m68.93039\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 037 | loss: 68.93039 - R2: 0.0959 -- iter: 0512/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 693  | total loss: \u001b[1m\u001b[32m68.68979\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 037 | loss: 68.68979 - R2: 0.0964 -- iter: 0576/1168\n",
      "Training Step: 694  | total loss: \u001b[1m\u001b[32m68.63064\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 037 | loss: 68.63064 - R2: 0.0967 -- iter: 0640/1168\n",
      "Training Step: 695  | total loss: \u001b[1m\u001b[32m68.58636\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 037 | loss: 68.58636 - R2: 0.0970 -- iter: 0704/1168\n",
      "Training Step: 696  | total loss: \u001b[1m\u001b[32m68.41179\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 037 | loss: 68.41179 - R2: 0.0975 -- iter: 0768/1168\n",
      "Training Step: 697  | total loss: \u001b[1m\u001b[32m68.41179\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 037 | loss: 68.41179 - R2: 0.0977 -- iter: 0832/1168\n",
      "Training Step: 698  | total loss: \u001b[1m\u001b[32m68.15466\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 037 | loss: 68.15466 - R2: 0.0981 -- iter: 0896/1168\n",
      "Training Step: 699  | total loss: \u001b[1m\u001b[32m68.15466\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 037 | loss: 68.15466 - R2: 0.0985 -- iter: 0960/1168\n",
      "Training Step: 700  | total loss: \u001b[1m\u001b[32m67.69955\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 037 | loss: 67.69955 - R2: 0.0990 -- iter: 1024/1168\n",
      "Training Step: 701  | total loss: \u001b[1m\u001b[32m67.76172\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 037 | loss: 67.76172 - R2: 0.0995 -- iter: 1088/1168\n",
      "Training Step: 702  | total loss: \u001b[1m\u001b[32m67.63621\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 037 | loss: 67.63621 - R2: 0.0997 -- iter: 1152/1168\n",
      "Training Step: 703  | total loss: \u001b[1m\u001b[32m67.63113\u001b[0m\u001b[0m | time: 1.174s\n",
      "| SGD | epoch: 037 | loss: 67.63113 - R2: 0.1001 | val_loss: 66.37714 - val_acc: 0.1043 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 704  | total loss: \u001b[1m\u001b[32m67.62714\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 038 | loss: 67.62714 - R2: 0.1004 -- iter: 0064/1168\n",
      "Training Step: 705  | total loss: \u001b[1m\u001b[32m67.62714\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 038 | loss: 67.62714 - R2: 0.1007 -- iter: 0128/1168\n",
      "Training Step: 706  | total loss: \u001b[1m\u001b[32m67.45870\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 038 | loss: 67.45870 - R2: 0.1011 -- iter: 0192/1168\n",
      "Training Step: 707  | total loss: \u001b[1m\u001b[32m67.17532\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 038 | loss: 67.17532 - R2: 0.1017 -- iter: 0256/1168\n",
      "Training Step: 708  | total loss: \u001b[1m\u001b[32m66.79753\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 038 | loss: 66.79753 - R2: 0.1021 -- iter: 0320/1168\n",
      "Training Step: 709  | total loss: \u001b[1m\u001b[32m66.65937\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 038 | loss: 66.65937 - R2: 0.1026 -- iter: 0384/1168\n",
      "Training Step: 710  | total loss: \u001b[1m\u001b[32m66.54588\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 038 | loss: 66.54588 - R2: 0.1030 -- iter: 0448/1168\n",
      "Training Step: 711  | total loss: \u001b[1m\u001b[32m66.41956\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 038 | loss: 66.41956 - R2: 0.1034 -- iter: 0512/1168\n",
      "Training Step: 712  | total loss: \u001b[1m\u001b[32m66.22089\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 038 | loss: 66.22089 - R2: 0.1038 -- iter: 0576/1168\n",
      "Training Step: 713  | total loss: \u001b[1m\u001b[32m66.20998\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 038 | loss: 66.20998 - R2: 0.1043 -- iter: 0640/1168\n",
      "Training Step: 714  | total loss: \u001b[1m\u001b[32m66.20998\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 038 | loss: 66.20998 - R2: 0.1046 -- iter: 0704/1168\n",
      "Training Step: 715  | total loss: \u001b[1m\u001b[32m66.04713\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 038 | loss: 66.04713 - R2: 0.1051 -- iter: 0768/1168\n",
      "Training Step: 716  | total loss: \u001b[1m\u001b[32m66.04713\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 038 | loss: 66.04713 - R2: 0.1053 -- iter: 0832/1168\n",
      "Training Step: 717  | total loss: \u001b[1m\u001b[32m65.76890\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 038 | loss: 65.76890 - R2: 0.1059 -- iter: 0896/1168\n",
      "Training Step: 718  | total loss: \u001b[1m\u001b[32m65.62997\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 038 | loss: 65.62997 - R2: 0.1062 -- iter: 0960/1168\n",
      "Training Step: 719  | total loss: \u001b[1m\u001b[32m65.35934\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 038 | loss: 65.35934 - R2: 0.1066 -- iter: 1024/1168\n",
      "Training Step: 720  | total loss: \u001b[1m\u001b[32m65.35934\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 038 | loss: 65.35934 - R2: 0.1072 -- iter: 1088/1168\n",
      "Training Step: 721  | total loss: \u001b[1m\u001b[32m65.10317\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 038 | loss: 65.10317 - R2: 0.1078 -- iter: 1152/1168\n",
      "Training Step: 722  | total loss: \u001b[1m\u001b[32m65.04794\u001b[0m\u001b[0m | time: 1.127s\n",
      "| SGD | epoch: 038 | loss: 65.04794 - R2: 0.1081 | val_loss: 63.99504 - val_acc: 0.1124 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 723  | total loss: \u001b[1m\u001b[32m65.00426\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 039 | loss: 65.00426 - R2: 0.1084 -- iter: 0064/1168\n",
      "Training Step: 724  | total loss: \u001b[1m\u001b[32m65.00426\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 039 | loss: 65.00426 - R2: 0.1087 -- iter: 0128/1168\n",
      "Training Step: 725  | total loss: \u001b[1m\u001b[32m64.95394\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 039 | loss: 64.95394 - R2: 0.1091 -- iter: 0192/1168\n",
      "Training Step: 726  | total loss: \u001b[1m\u001b[32m64.76136\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 039 | loss: 64.76136 - R2: 0.1094 -- iter: 0256/1168\n",
      "Training Step: 727  | total loss: \u001b[1m\u001b[32m64.76136\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 039 | loss: 64.76136 - R2: 0.1099 -- iter: 0320/1168\n",
      "Training Step: 728  | total loss: \u001b[1m\u001b[32m64.50793\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 039 | loss: 64.50793 - R2: 0.1104 -- iter: 0384/1168\n",
      "Training Step: 729  | total loss: \u001b[1m\u001b[32m64.50793\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 039 | loss: 64.50793 - R2: 0.1108 -- iter: 0448/1168\n",
      "Training Step: 730  | total loss: \u001b[1m\u001b[32m64.25450\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 039 | loss: 64.25450 - R2: 0.1113 -- iter: 0512/1168\n",
      "Training Step: 731  | total loss: \u001b[1m\u001b[32m64.20957\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 039 | loss: 64.20957 - R2: 0.1117 -- iter: 0576/1168\n",
      "Training Step: 732  | total loss: \u001b[1m\u001b[32m63.78570\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 039 | loss: 63.78570 - R2: 0.1122 -- iter: 0640/1168\n",
      "Training Step: 733  | total loss: \u001b[1m\u001b[32m63.66421\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 039 | loss: 63.66421 - R2: 0.1128 -- iter: 0704/1168\n",
      "Training Step: 734  | total loss: \u001b[1m\u001b[32m63.66421\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 039 | loss: 63.66421 - R2: 0.1132 -- iter: 0768/1168\n",
      "Training Step: 735  | total loss: \u001b[1m\u001b[32m63.45337\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 039 | loss: 63.45337 - R2: 0.1138 -- iter: 0832/1168\n",
      "Training Step: 736  | total loss: \u001b[1m\u001b[32m63.38354\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 039 | loss: 63.38354 - R2: 0.1141 -- iter: 0896/1168\n",
      "Training Step: 737  | total loss: \u001b[1m\u001b[32m63.22918\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 039 | loss: 63.22918 - R2: 0.1145 -- iter: 0960/1168\n",
      "Training Step: 738  | total loss: \u001b[1m\u001b[32m63.13452\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 039 | loss: 63.13452 - R2: 0.1149 -- iter: 1024/1168\n",
      "Training Step: 739  | total loss: \u001b[1m\u001b[32m63.13452\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 039 | loss: 63.13452 - R2: 0.1153 -- iter: 1088/1168\n",
      "Training Step: 740  | total loss: \u001b[1m\u001b[32m63.05039\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 039 | loss: 63.05039 - R2: 0.1157 -- iter: 1152/1168\n",
      "Training Step: 741  | total loss: \u001b[1m\u001b[32m62.95159\u001b[0m\u001b[0m | time: 1.159s\n",
      "| SGD | epoch: 039 | loss: 62.95159 - R2: 0.1161 | val_loss: 61.56956 - val_acc: 0.1211 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 742  | total loss: \u001b[1m\u001b[32m62.78514\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 040 | loss: 62.78514 - R2: 0.1164 -- iter: 0064/1168\n",
      "Training Step: 743  | total loss: \u001b[1m\u001b[32m62.68083\u001b[0m\u001b[0m | time: 0.007s\n",
      "| SGD | epoch: 040 | loss: 62.68083 - R2: 0.1169 -- iter: 0128/1168\n",
      "Training Step: 744  | total loss: \u001b[1m\u001b[32m62.49552\u001b[0m\u001b[0m | time: 0.010s\n",
      "| SGD | epoch: 040 | loss: 62.49552 - R2: 0.1174 -- iter: 0192/1168\n",
      "Training Step: 745  | total loss: \u001b[1m\u001b[32m62.49552\u001b[0m\u001b[0m | time: 0.012s\n",
      "| SGD | epoch: 040 | loss: 62.49552 - R2: 0.1179 -- iter: 0256/1168\n",
      "Training Step: 746  | total loss: \u001b[1m\u001b[32m62.39868\u001b[0m\u001b[0m | time: 0.014s\n",
      "| SGD | epoch: 040 | loss: 62.39868 - R2: 0.1183 -- iter: 0320/1168\n",
      "Training Step: 747  | total loss: \u001b[1m\u001b[32m62.20910\u001b[0m\u001b[0m | time: 0.017s\n",
      "| SGD | epoch: 040 | loss: 62.20910 - R2: 0.1186 -- iter: 0384/1168\n",
      "Training Step: 748  | total loss: \u001b[1m\u001b[32m62.20910\u001b[0m\u001b[0m | time: 0.019s\n",
      "| SGD | epoch: 040 | loss: 62.20910 - R2: 0.1192 -- iter: 0448/1168\n",
      "Training Step: 749  | total loss: \u001b[1m\u001b[32m61.91052\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 040 | loss: 61.91052 - R2: 0.1196 -- iter: 0512/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 750  | total loss: \u001b[1m\u001b[32m61.89136\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 040 | loss: 61.89136 - R2: 0.1202 -- iter: 0576/1168\n",
      "Training Step: 751  | total loss: \u001b[1m\u001b[32m61.87102\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 040 | loss: 61.87102 - R2: 0.1205 -- iter: 0640/1168\n",
      "Training Step: 752  | total loss: \u001b[1m\u001b[32m61.68518\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 040 | loss: 61.68518 - R2: 0.1208 -- iter: 0704/1168\n",
      "Training Step: 753  | total loss: \u001b[1m\u001b[32m61.64745\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 040 | loss: 61.64745 - R2: 0.1214 -- iter: 0768/1168\n",
      "Training Step: 754  | total loss: \u001b[1m\u001b[32m61.64745\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 040 | loss: 61.64745 - R2: 0.1217 -- iter: 0832/1168\n",
      "Training Step: 755  | total loss: \u001b[1m\u001b[32m61.27486\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 040 | loss: 61.27486 - R2: 0.1224 -- iter: 0896/1168\n",
      "Training Step: 756  | total loss: \u001b[1m\u001b[32m61.12571\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 040 | loss: 61.12571 - R2: 0.1228 -- iter: 0960/1168\n",
      "Training Step: 757  | total loss: \u001b[1m\u001b[32m61.10917\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 040 | loss: 61.10917 - R2: 0.1233 -- iter: 1024/1168\n",
      "Training Step: 758  | total loss: \u001b[1m\u001b[32m60.95415\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 040 | loss: 60.95415 - R2: 0.1237 -- iter: 1088/1168\n",
      "Training Step: 759  | total loss: \u001b[1m\u001b[32m60.58279\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 040 | loss: 60.58279 - R2: 0.1242 -- iter: 1152/1168\n",
      "Training Step: 760  | total loss: \u001b[1m\u001b[32m60.23540\u001b[0m\u001b[0m | time: 1.075s\n",
      "| SGD | epoch: 040 | loss: 60.23540 - R2: 0.1251 | val_loss: 59.10471 - val_acc: 0.1305 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 761  | total loss: \u001b[1m\u001b[32m60.10074\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 041 | loss: 60.10074 - R2: 0.1259 -- iter: 0064/1168\n",
      "Training Step: 762  | total loss: \u001b[1m\u001b[32m59.97251\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 041 | loss: 59.97251 - R2: 0.1264 -- iter: 0128/1168\n",
      "Training Step: 763  | total loss: \u001b[1m\u001b[32m59.80458\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 041 | loss: 59.80458 - R2: 0.1269 -- iter: 0192/1168\n",
      "Training Step: 764  | total loss: \u001b[1m\u001b[32m59.80458\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 041 | loss: 59.80458 - R2: 0.1274 -- iter: 0256/1168\n",
      "Training Step: 765  | total loss: \u001b[1m\u001b[32m59.41919\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 041 | loss: 59.41919 - R2: 0.1280 -- iter: 0320/1168\n",
      "Training Step: 766  | total loss: \u001b[1m\u001b[32m59.41919\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 041 | loss: 59.41919 - R2: 0.1286 -- iter: 0384/1168\n",
      "Training Step: 767  | total loss: \u001b[1m\u001b[32m59.13967\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 041 | loss: 59.13967 - R2: 0.1291 -- iter: 0448/1168\n",
      "Training Step: 768  | total loss: \u001b[1m\u001b[32m59.03649\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 041 | loss: 59.03649 - R2: 0.1296 -- iter: 0512/1168\n",
      "Training Step: 769  | total loss: \u001b[1m\u001b[32m59.03649\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 041 | loss: 59.03649 - R2: 0.1301 -- iter: 0576/1168\n",
      "Training Step: 770  | total loss: \u001b[1m\u001b[32m58.93242\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 041 | loss: 58.93242 - R2: 0.1306 -- iter: 0640/1168\n",
      "Training Step: 771  | total loss: \u001b[1m\u001b[32m58.73886\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 041 | loss: 58.73886 - R2: 0.1311 -- iter: 0704/1168\n",
      "Training Step: 772  | total loss: \u001b[1m\u001b[32m58.65272\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 041 | loss: 58.65272 - R2: 0.1316 -- iter: 0768/1168\n",
      "Training Step: 773  | total loss: \u001b[1m\u001b[32m58.58191\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 041 | loss: 58.58191 - R2: 0.1320 -- iter: 0832/1168\n",
      "Training Step: 774  | total loss: \u001b[1m\u001b[32m58.48603\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 041 | loss: 58.48603 - R2: 0.1325 -- iter: 0896/1168\n",
      "Training Step: 775  | total loss: \u001b[1m\u001b[32m58.26165\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 041 | loss: 58.26165 - R2: 0.1332 -- iter: 0960/1168\n",
      "Training Step: 776  | total loss: \u001b[1m\u001b[32m58.27615\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 041 | loss: 58.27615 - R2: 0.1335 -- iter: 1024/1168\n",
      "Training Step: 777  | total loss: \u001b[1m\u001b[32m58.14067\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 041 | loss: 58.14067 - R2: 0.1340 -- iter: 1088/1168\n",
      "Training Step: 778  | total loss: \u001b[1m\u001b[32m58.09162\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 041 | loss: 58.09162 - R2: 0.1344 -- iter: 1152/1168\n",
      "Training Step: 779  | total loss: \u001b[1m\u001b[32m57.95248\u001b[0m\u001b[0m | time: 1.109s\n",
      "| SGD | epoch: 041 | loss: 57.95248 - R2: 0.1350 | val_loss: 56.59868 - val_acc: 0.1406 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 780  | total loss: \u001b[1m\u001b[32m57.51283\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 042 | loss: 57.51283 - R2: 0.1360 -- iter: 0064/1168\n",
      "Training Step: 781  | total loss: \u001b[1m\u001b[32m57.10375\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 042 | loss: 57.10375 - R2: 0.1370 -- iter: 0128/1168\n",
      "Training Step: 782  | total loss: \u001b[1m\u001b[32m56.98997\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 042 | loss: 56.98997 - R2: 0.1375 -- iter: 0192/1168\n",
      "Training Step: 783  | total loss: \u001b[1m\u001b[32m56.92396\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 042 | loss: 56.92396 - R2: 0.1379 -- iter: 0256/1168\n",
      "Training Step: 784  | total loss: \u001b[1m\u001b[32m56.83948\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 042 | loss: 56.83948 - R2: 0.1383 -- iter: 0320/1168\n",
      "Training Step: 785  | total loss: \u001b[1m\u001b[32m56.83948\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 042 | loss: 56.83948 - R2: 0.1387 -- iter: 0384/1168\n",
      "Training Step: 786  | total loss: \u001b[1m\u001b[32m56.83402\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 042 | loss: 56.83402 - R2: 0.1390 -- iter: 0448/1168\n",
      "Training Step: 787  | total loss: \u001b[1m\u001b[32m56.77134\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 042 | loss: 56.77134 - R2: 0.1395 -- iter: 0512/1168\n",
      "Training Step: 788  | total loss: \u001b[1m\u001b[32m56.62228\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 042 | loss: 56.62228 - R2: 0.1400 -- iter: 0576/1168\n",
      "Training Step: 789  | total loss: \u001b[1m\u001b[32m56.56606\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 042 | loss: 56.56606 - R2: 0.1405 -- iter: 0640/1168\n",
      "Training Step: 790  | total loss: \u001b[1m\u001b[32m56.38866\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 042 | loss: 56.38866 - R2: 0.1411 -- iter: 0704/1168\n",
      "Training Step: 791  | total loss: \u001b[1m\u001b[32m56.12267\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 042 | loss: 56.12267 - R2: 0.1417 -- iter: 0768/1168\n",
      "Training Step: 792  | total loss: \u001b[1m\u001b[32m56.12267\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 042 | loss: 56.12267 - R2: 0.1422 -- iter: 0832/1168\n",
      "Training Step: 793  | total loss: \u001b[1m\u001b[32m55.90911\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 042 | loss: 55.90911 - R2: 0.1429 -- iter: 0896/1168\n",
      "Training Step: 794  | total loss: \u001b[1m\u001b[32m55.76406\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 042 | loss: 55.76406 - R2: 0.1434 -- iter: 0960/1168\n",
      "Training Step: 795  | total loss: \u001b[1m\u001b[32m55.60051\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 042 | loss: 55.60051 - R2: 0.1441 -- iter: 1024/1168\n",
      "Training Step: 796  | total loss: \u001b[1m\u001b[32m55.53555\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 042 | loss: 55.53555 - R2: 0.1445 -- iter: 1088/1168\n",
      "Training Step: 797  | total loss: \u001b[1m\u001b[32m55.44035\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 042 | loss: 55.44035 - R2: 0.1450 -- iter: 1152/1168\n",
      "Training Step: 798  | total loss: \u001b[1m\u001b[32m55.35785\u001b[0m\u001b[0m | time: 1.172s\n",
      "| SGD | epoch: 042 | loss: 55.35785 - R2: 0.1455 | val_loss: 54.05475 - val_acc: 0.1514 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 799  | total loss: \u001b[1m\u001b[32m55.40159\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 043 | loss: 55.40159 - R2: 0.1458 -- iter: 0064/1168\n",
      "Training Step: 800  | total loss: \u001b[1m\u001b[32m55.47377\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 043 | loss: 55.47377 - R2: 0.1460 -- iter: 0128/1168\n",
      "Training Step: 801  | total loss: \u001b[1m\u001b[32m55.52458\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 043 | loss: 55.52458 - R2: 0.1463 -- iter: 0192/1168\n",
      "Training Step: 802  | total loss: \u001b[1m\u001b[32m55.18727\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 043 | loss: 55.18727 - R2: 0.1471 -- iter: 0256/1168\n",
      "Training Step: 803  | total loss: \u001b[1m\u001b[32m55.18727\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 043 | loss: 55.18727 - R2: 0.1476 -- iter: 0320/1168\n",
      "Training Step: 804  | total loss: \u001b[1m\u001b[32m55.07726\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 043 | loss: 55.07726 - R2: 0.1481 -- iter: 0384/1168\n",
      "Training Step: 805  | total loss: \u001b[1m\u001b[32m54.80893\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 043 | loss: 54.80893 - R2: 0.1486 -- iter: 0448/1168\n",
      "Training Step: 806  | total loss: \u001b[1m\u001b[32m54.80893\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 043 | loss: 54.80893 - R2: 0.1493 -- iter: 0512/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 807  | total loss: \u001b[1m\u001b[32m54.47004\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 043 | loss: 54.47004 - R2: 0.1502 -- iter: 0576/1168\n",
      "Training Step: 808  | total loss: \u001b[1m\u001b[32m54.32015\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 043 | loss: 54.32015 - R2: 0.1506 -- iter: 0640/1168\n",
      "Training Step: 809  | total loss: \u001b[1m\u001b[32m54.16050\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 043 | loss: 54.16050 - R2: 0.1512 -- iter: 0704/1168\n",
      "Training Step: 810  | total loss: \u001b[1m\u001b[32m54.07117\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 043 | loss: 54.07117 - R2: 0.1518 -- iter: 0768/1168\n",
      "Training Step: 811  | total loss: \u001b[1m\u001b[32m54.07117\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 043 | loss: 54.07117 - R2: 0.1523 -- iter: 0832/1168\n",
      "Training Step: 812  | total loss: \u001b[1m\u001b[32m53.90161\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 043 | loss: 53.90161 - R2: 0.1530 -- iter: 0896/1168\n",
      "Training Step: 813  | total loss: \u001b[1m\u001b[32m53.33619\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 043 | loss: 53.33619 - R2: 0.1541 -- iter: 0960/1168\n",
      "Training Step: 814  | total loss: \u001b[1m\u001b[32m53.28459\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 043 | loss: 53.28459 - R2: 0.1548 -- iter: 1024/1168\n",
      "Training Step: 815  | total loss: \u001b[1m\u001b[32m53.28459\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 043 | loss: 53.28459 - R2: 0.1552 -- iter: 1088/1168\n",
      "Training Step: 816  | total loss: \u001b[1m\u001b[32m52.91257\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 043 | loss: 52.91257 - R2: 0.1559 -- iter: 1152/1168\n",
      "Training Step: 817  | total loss: \u001b[1m\u001b[32m52.81026\u001b[0m\u001b[0m | time: 1.178s\n",
      "| SGD | epoch: 043 | loss: 52.81026 - R2: 0.1566 | val_loss: 51.47795 - val_acc: 0.1631 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 818  | total loss: \u001b[1m\u001b[32m52.66799\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 044 | loss: 52.66799 - R2: 0.1572 -- iter: 0064/1168\n",
      "Training Step: 819  | total loss: \u001b[1m\u001b[32m52.59914\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 044 | loss: 52.59914 - R2: 0.1578 -- iter: 0128/1168\n",
      "Training Step: 820  | total loss: \u001b[1m\u001b[32m52.52304\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 044 | loss: 52.52304 - R2: 0.1583 -- iter: 0192/1168\n",
      "Training Step: 821  | total loss: \u001b[1m\u001b[32m52.42093\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 044 | loss: 52.42093 - R2: 0.1588 -- iter: 0256/1168\n",
      "Training Step: 822  | total loss: \u001b[1m\u001b[32m52.42093\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 044 | loss: 52.42093 - R2: 0.1593 -- iter: 0320/1168\n",
      "Training Step: 823  | total loss: \u001b[1m\u001b[32m52.37694\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 044 | loss: 52.37694 - R2: 0.1598 -- iter: 0384/1168\n",
      "Training Step: 824  | total loss: \u001b[1m\u001b[32m51.99086\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 044 | loss: 51.99086 - R2: 0.1605 -- iter: 0448/1168\n",
      "Training Step: 825  | total loss: \u001b[1m\u001b[32m51.99086\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 044 | loss: 51.99086 - R2: 0.1612 -- iter: 0512/1168\n",
      "Training Step: 826  | total loss: \u001b[1m\u001b[32m51.66497\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 044 | loss: 51.66497 - R2: 0.1618 -- iter: 0576/1168\n",
      "Training Step: 827  | total loss: \u001b[1m\u001b[32m51.63870\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 044 | loss: 51.63870 - R2: 0.1626 -- iter: 0640/1168\n",
      "Training Step: 828  | total loss: \u001b[1m\u001b[32m51.49274\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 044 | loss: 51.49274 - R2: 0.1630 -- iter: 0704/1168\n",
      "Training Step: 829  | total loss: \u001b[1m\u001b[32m51.37432\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 044 | loss: 51.37432 - R2: 0.1637 -- iter: 0768/1168\n",
      "Training Step: 830  | total loss: \u001b[1m\u001b[32m51.34894\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 044 | loss: 51.34894 - R2: 0.1643 -- iter: 0832/1168\n",
      "Training Step: 831  | total loss: \u001b[1m\u001b[32m51.34894\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 044 | loss: 51.34894 - R2: 0.1647 -- iter: 0896/1168\n",
      "Training Step: 832  | total loss: \u001b[1m\u001b[32m50.96420\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 044 | loss: 50.96420 - R2: 0.1656 -- iter: 0960/1168\n",
      "Training Step: 833  | total loss: \u001b[1m\u001b[32m50.96420\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 044 | loss: 50.96420 - R2: 0.1663 -- iter: 1024/1168\n",
      "Training Step: 834  | total loss: \u001b[1m\u001b[32m50.64685\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 044 | loss: 50.64685 - R2: 0.1667 -- iter: 1088/1168\n",
      "Training Step: 835  | total loss: \u001b[1m\u001b[32m50.43872\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 044 | loss: 50.43872 - R2: 0.1677 -- iter: 1152/1168\n",
      "Training Step: 836  | total loss: \u001b[1m\u001b[32m50.25110\u001b[0m\u001b[0m | time: 1.141s\n",
      "| SGD | epoch: 044 | loss: 50.25110 - R2: 0.1684 | val_loss: 48.86976 - val_acc: 0.1758 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 837  | total loss: \u001b[1m\u001b[32m50.13023\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 045 | loss: 50.13023 - R2: 0.1692 -- iter: 0064/1168\n",
      "Training Step: 838  | total loss: \u001b[1m\u001b[32m49.97778\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 045 | loss: 49.97778 - R2: 0.1698 -- iter: 0128/1168\n",
      "Training Step: 839  | total loss: \u001b[1m\u001b[32m49.87223\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 045 | loss: 49.87223 - R2: 0.1705 -- iter: 0192/1168\n",
      "Training Step: 840  | total loss: \u001b[1m\u001b[32m49.76305\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 045 | loss: 49.76305 - R2: 0.1711 -- iter: 0256/1168\n",
      "Training Step: 841  | total loss: \u001b[1m\u001b[32m49.67505\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 045 | loss: 49.67505 - R2: 0.1718 -- iter: 0320/1168\n",
      "Training Step: 842  | total loss: \u001b[1m\u001b[32m49.50423\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 045 | loss: 49.50423 - R2: 0.1723 -- iter: 0384/1168\n",
      "Training Step: 843  | total loss: \u001b[1m\u001b[32m49.29927\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 045 | loss: 49.29927 - R2: 0.1731 -- iter: 0448/1168\n",
      "Training Step: 844  | total loss: \u001b[1m\u001b[32m49.19420\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 045 | loss: 49.19420 - R2: 0.1739 -- iter: 0512/1168\n",
      "Training Step: 845  | total loss: \u001b[1m\u001b[32m49.07953\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 045 | loss: 49.07953 - R2: 0.1745 -- iter: 0576/1168\n",
      "Training Step: 846  | total loss: \u001b[1m\u001b[32m48.91271\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 045 | loss: 48.91271 - R2: 0.1752 -- iter: 0640/1168\n",
      "Training Step: 847  | total loss: \u001b[1m\u001b[32m48.71249\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 045 | loss: 48.71249 - R2: 0.1759 -- iter: 0704/1168\n",
      "Training Step: 848  | total loss: \u001b[1m\u001b[32m48.71249\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 045 | loss: 48.71249 - R2: 0.1768 -- iter: 0768/1168\n",
      "Training Step: 849  | total loss: \u001b[1m\u001b[32m48.46018\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 045 | loss: 48.46018 - R2: 0.1777 -- iter: 0832/1168\n",
      "Training Step: 850  | total loss: \u001b[1m\u001b[32m48.01095\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 045 | loss: 48.01095 - R2: 0.1789 -- iter: 0896/1168\n",
      "Training Step: 851  | total loss: \u001b[1m\u001b[32m47.94600\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 045 | loss: 47.94600 - R2: 0.1795 -- iter: 0960/1168\n",
      "Training Step: 852  | total loss: \u001b[1m\u001b[32m47.94600\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 045 | loss: 47.94600 - R2: 0.1800 -- iter: 1024/1168\n",
      "Training Step: 853  | total loss: \u001b[1m\u001b[32m47.81315\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 045 | loss: 47.81315 - R2: 0.1807 -- iter: 1088/1168\n",
      "Training Step: 854  | total loss: \u001b[1m\u001b[32m47.67259\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 045 | loss: 47.67259 - R2: 0.1815 -- iter: 1152/1168\n",
      "Training Step: 855  | total loss: \u001b[1m\u001b[32m47.40164\u001b[0m\u001b[0m | time: 1.147s\n",
      "| SGD | epoch: 045 | loss: 47.40164 - R2: 0.1821 | val_loss: 46.23330 - val_acc: 0.1894 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 856  | total loss: \u001b[1m\u001b[32m47.44212\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 046 | loss: 47.44212 - R2: 0.1829 -- iter: 0064/1168\n",
      "Training Step: 857  | total loss: \u001b[1m\u001b[32m47.44212\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 046 | loss: 47.44212 - R2: 0.1832 -- iter: 0128/1168\n",
      "Training Step: 858  | total loss: \u001b[1m\u001b[32m47.20543\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 046 | loss: 47.20543 - R2: 0.1838 -- iter: 0192/1168\n",
      "Training Step: 859  | total loss: \u001b[1m\u001b[32m47.19725\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 046 | loss: 47.19725 - R2: 0.1845 -- iter: 0256/1168\n",
      "Training Step: 860  | total loss: \u001b[1m\u001b[32m47.17531\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 046 | loss: 47.17531 - R2: 0.1850 -- iter: 0320/1168\n",
      "Training Step: 861  | total loss: \u001b[1m\u001b[32m47.05285\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 046 | loss: 47.05285 - R2: 0.1855 -- iter: 0384/1168\n",
      "Training Step: 862  | total loss: \u001b[1m\u001b[32m46.97784\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 046 | loss: 46.97784 - R2: 0.1861 -- iter: 0448/1168\n",
      "Training Step: 863  | total loss: \u001b[1m\u001b[32m46.97784\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 046 | loss: 46.97784 - R2: 0.1867 -- iter: 0512/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 864  | total loss: \u001b[1m\u001b[32m46.76671\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 046 | loss: 46.76671 - R2: 0.1876 -- iter: 0576/1168\n",
      "Training Step: 865  | total loss: \u001b[1m\u001b[32m46.68961\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 046 | loss: 46.68961 - R2: 0.1882 -- iter: 0640/1168\n",
      "Training Step: 866  | total loss: \u001b[1m\u001b[32m46.29147\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 046 | loss: 46.29147 - R2: 0.1891 -- iter: 0704/1168\n",
      "Training Step: 867  | total loss: \u001b[1m\u001b[32m46.19859\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 046 | loss: 46.19859 - R2: 0.1899 -- iter: 0768/1168\n",
      "Training Step: 868  | total loss: \u001b[1m\u001b[32m46.19859\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 046 | loss: 46.19859 - R2: 0.1906 -- iter: 0832/1168\n",
      "Training Step: 869  | total loss: \u001b[1m\u001b[32m45.95046\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 046 | loss: 45.95046 - R2: 0.1916 -- iter: 0896/1168\n",
      "Training Step: 870  | total loss: \u001b[1m\u001b[32m45.72201\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 046 | loss: 45.72201 - R2: 0.1922 -- iter: 0960/1168\n",
      "Training Step: 871  | total loss: \u001b[1m\u001b[32m45.57296\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 046 | loss: 45.57296 - R2: 0.1930 -- iter: 1024/1168\n",
      "Training Step: 872  | total loss: \u001b[1m\u001b[32m45.57296\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 046 | loss: 45.57296 - R2: 0.1937 -- iter: 1088/1168\n",
      "Training Step: 873  | total loss: \u001b[1m\u001b[32m45.42791\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 046 | loss: 45.42791 - R2: 0.1944 -- iter: 1152/1168\n",
      "Training Step: 874  | total loss: \u001b[1m\u001b[32m45.42791\u001b[0m\u001b[0m | time: 1.132s\n",
      "| SGD | epoch: 046 | loss: 45.42791 - R2: 0.1949 | val_loss: 43.57502 - val_acc: 0.2040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 875  | total loss: \u001b[1m\u001b[32m45.21300\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 047 | loss: 45.21300 - R2: 0.1959 -- iter: 0064/1168\n",
      "Training Step: 876  | total loss: \u001b[1m\u001b[32m44.83134\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 047 | loss: 44.83134 - R2: 0.1968 -- iter: 0128/1168\n",
      "Training Step: 877  | total loss: \u001b[1m\u001b[32m44.83134\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 047 | loss: 44.83134 - R2: 0.1977 -- iter: 0192/1168\n",
      "Training Step: 878  | total loss: \u001b[1m\u001b[32m44.62440\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 047 | loss: 44.62440 - R2: 0.1986 -- iter: 0256/1168\n",
      "Training Step: 879  | total loss: \u001b[1m\u001b[32m44.14454\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 047 | loss: 44.14454 - R2: 0.1996 -- iter: 0320/1168\n",
      "Training Step: 880  | total loss: \u001b[1m\u001b[32m43.88240\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 047 | loss: 43.88240 - R2: 0.2007 -- iter: 0384/1168\n",
      "Training Step: 881  | total loss: \u001b[1m\u001b[32m43.79520\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 047 | loss: 43.79520 - R2: 0.2018 -- iter: 0448/1168\n",
      "Training Step: 882  | total loss: \u001b[1m\u001b[32m43.79520\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 047 | loss: 43.79520 - R2: 0.2025 -- iter: 0512/1168\n",
      "Training Step: 883  | total loss: \u001b[1m\u001b[32m43.73341\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 047 | loss: 43.73341 - R2: 0.2031 -- iter: 0576/1168\n",
      "Training Step: 884  | total loss: \u001b[1m\u001b[32m43.34155\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 047 | loss: 43.34155 - R2: 0.2040 -- iter: 0640/1168\n",
      "Training Step: 885  | total loss: \u001b[1m\u001b[32m43.34155\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 047 | loss: 43.34155 - R2: 0.2049 -- iter: 0704/1168\n",
      "Training Step: 886  | total loss: \u001b[1m\u001b[32m43.11712\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 047 | loss: 43.11712 - R2: 0.2057 -- iter: 0768/1168\n",
      "Training Step: 887  | total loss: \u001b[1m\u001b[32m43.05874\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 047 | loss: 43.05874 - R2: 0.2064 -- iter: 0832/1168\n",
      "Training Step: 888  | total loss: \u001b[1m\u001b[32m42.95621\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 047 | loss: 42.95621 - R2: 0.2070 -- iter: 0896/1168\n",
      "Training Step: 889  | total loss: \u001b[1m\u001b[32m42.95621\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 047 | loss: 42.95621 - R2: 0.2077 -- iter: 0960/1168\n",
      "Training Step: 890  | total loss: \u001b[1m\u001b[32m42.80747\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 047 | loss: 42.80747 - R2: 0.2086 -- iter: 1024/1168\n",
      "Training Step: 891  | total loss: \u001b[1m\u001b[32m42.60153\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 047 | loss: 42.60153 - R2: 0.2092 -- iter: 1088/1168\n",
      "Training Step: 892  | total loss: \u001b[1m\u001b[32m42.43859\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 047 | loss: 42.43859 - R2: 0.2100 -- iter: 1152/1168\n",
      "Training Step: 893  | total loss: \u001b[1m\u001b[32m42.35771\u001b[0m\u001b[0m | time: 1.135s\n",
      "| SGD | epoch: 047 | loss: 42.35771 - R2: 0.2109 | val_loss: 40.90483 - val_acc: 0.2198 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 894  | total loss: \u001b[1m\u001b[32m42.19355\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 048 | loss: 42.19355 - R2: 0.2115 -- iter: 0064/1168\n",
      "Training Step: 895  | total loss: \u001b[1m\u001b[32m42.19355\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 048 | loss: 42.19355 - R2: 0.2125 -- iter: 0128/1168\n",
      "Training Step: 896  | total loss: \u001b[1m\u001b[32m41.91373\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 048 | loss: 41.91373 - R2: 0.2132 -- iter: 0192/1168\n",
      "Training Step: 897  | total loss: \u001b[1m\u001b[32m41.75450\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 048 | loss: 41.75450 - R2: 0.2141 -- iter: 0256/1168\n",
      "Training Step: 898  | total loss: \u001b[1m\u001b[32m41.53588\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 048 | loss: 41.53588 - R2: 0.2150 -- iter: 0320/1168\n",
      "Training Step: 899  | total loss: \u001b[1m\u001b[32m41.41667\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 048 | loss: 41.41667 - R2: 0.2161 -- iter: 0384/1168\n",
      "Training Step: 900  | total loss: \u001b[1m\u001b[32m41.29475\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 048 | loss: 41.29475 - R2: 0.2169 -- iter: 0448/1168\n",
      "Training Step: 901  | total loss: \u001b[1m\u001b[32m41.19109\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 048 | loss: 41.19109 - R2: 0.2177 -- iter: 0512/1168\n",
      "Training Step: 902  | total loss: \u001b[1m\u001b[32m41.06768\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 048 | loss: 41.06768 - R2: 0.2185 -- iter: 0576/1168\n",
      "Training Step: 903  | total loss: \u001b[1m\u001b[32m41.06768\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 048 | loss: 41.06768 - R2: 0.2193 -- iter: 0640/1168\n",
      "Training Step: 904  | total loss: \u001b[1m\u001b[32m40.99040\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 048 | loss: 40.99040 - R2: 0.2199 -- iter: 0704/1168\n",
      "Training Step: 905  | total loss: \u001b[1m\u001b[32m40.67570\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 048 | loss: 40.67570 - R2: 0.2213 -- iter: 0768/1168\n",
      "Training Step: 906  | total loss: \u001b[1m\u001b[32m40.55817\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 048 | loss: 40.55817 - R2: 0.2220 -- iter: 0832/1168\n",
      "Training Step: 907  | total loss: \u001b[1m\u001b[32m40.40789\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 048 | loss: 40.40789 - R2: 0.2229 -- iter: 0896/1168\n",
      "Training Step: 908  | total loss: \u001b[1m\u001b[32m40.21565\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 048 | loss: 40.21565 - R2: 0.2250 -- iter: 0960/1168\n",
      "Training Step: 909  | total loss: \u001b[1m\u001b[32m39.77588\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 048 | loss: 39.77588 - R2: 0.2250 -- iter: 1024/1168\n",
      "Training Step: 910  | total loss: \u001b[1m\u001b[32m39.53861\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 048 | loss: 39.53861 - R2: 0.2262 -- iter: 1088/1168\n",
      "Training Step: 911  | total loss: \u001b[1m\u001b[32m39.53861\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 048 | loss: 39.53861 - R2: 0.2273 -- iter: 1152/1168\n",
      "Training Step: 912  | total loss: \u001b[1m\u001b[32m39.24532\u001b[0m\u001b[0m | time: 1.174s\n",
      "| SGD | epoch: 048 | loss: 39.24532 - R2: 0.2282 | val_loss: 38.22467 - val_acc: 0.2367 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 913  | total loss: \u001b[1m\u001b[32m39.24532\u001b[0m\u001b[0m | time: 0.018s\n",
      "| SGD | epoch: 049 | loss: 39.24532 - R2: 0.2292 -- iter: 0064/1168\n",
      "Training Step: 914  | total loss: \u001b[1m\u001b[32m39.28218\u001b[0m\u001b[0m | time: 0.020s\n",
      "| SGD | epoch: 049 | loss: 39.28218 - R2: 0.2295 -- iter: 0128/1168\n",
      "Training Step: 915  | total loss: \u001b[1m\u001b[32m39.12261\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 049 | loss: 39.12261 - R2: 0.2305 -- iter: 0192/1168\n",
      "Training Step: 916  | total loss: \u001b[1m\u001b[32m39.07991\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 049 | loss: 39.07991 - R2: 0.2311 -- iter: 0256/1168\n",
      "Training Step: 917  | total loss: \u001b[1m\u001b[32m38.91809\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 049 | loss: 38.91809 - R2: 0.2321 -- iter: 0320/1168\n",
      "Training Step: 918  | total loss: \u001b[1m\u001b[32m38.79037\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 049 | loss: 38.79037 - R2: 0.2329 -- iter: 0384/1168\n",
      "Training Step: 919  | total loss: \u001b[1m\u001b[32m38.74298\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 049 | loss: 38.74298 - R2: 0.2336 -- iter: 0448/1168\n",
      "Training Step: 920  | total loss: \u001b[1m\u001b[32m38.61962\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 049 | loss: 38.61962 - R2: 0.2344 -- iter: 0512/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 921  | total loss: \u001b[1m\u001b[32m38.49412\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 049 | loss: 38.49412 - R2: 0.2352 -- iter: 0576/1168\n",
      "Training Step: 922  | total loss: \u001b[1m\u001b[32m38.22150\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 049 | loss: 38.22150 - R2: 0.2361 -- iter: 0640/1168\n",
      "Training Step: 923  | total loss: \u001b[1m\u001b[32m38.09934\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 049 | loss: 38.09934 - R2: 0.2371 -- iter: 0704/1168\n",
      "Training Step: 924  | total loss: \u001b[1m\u001b[32m38.09934\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 049 | loss: 38.09934 - R2: 0.2379 -- iter: 0768/1168\n",
      "Training Step: 925  | total loss: \u001b[1m\u001b[32m38.03635\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 049 | loss: 38.03635 - R2: 0.2386 -- iter: 0832/1168\n",
      "Training Step: 926  | total loss: \u001b[1m\u001b[32m37.91795\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 049 | loss: 37.91795 - R2: 0.2394 -- iter: 0896/1168\n",
      "Training Step: 927  | total loss: \u001b[1m\u001b[32m37.74556\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 049 | loss: 37.74556 - R2: 0.2403 -- iter: 0960/1168\n",
      "Training Step: 928  | total loss: \u001b[1m\u001b[32m37.63604\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 049 | loss: 37.63604 - R2: 0.2409 -- iter: 1024/1168\n",
      "Training Step: 929  | total loss: \u001b[1m\u001b[32m37.40961\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 049 | loss: 37.40961 - R2: 0.2417 -- iter: 1088/1168\n",
      "Training Step: 930  | total loss: \u001b[1m\u001b[32m37.24574\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 049 | loss: 37.24574 - R2: 0.2430 -- iter: 1152/1168\n",
      "Training Step: 931  | total loss: \u001b[1m\u001b[32m37.24574\u001b[0m\u001b[0m | time: 1.128s\n",
      "| SGD | epoch: 049 | loss: 37.24574 - R2: 0.2440 | val_loss: 35.54290 - val_acc: 0.2549 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 932  | total loss: \u001b[1m\u001b[32m37.04929\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 050 | loss: 37.04929 - R2: 0.2451 -- iter: 0064/1168\n",
      "Training Step: 933  | total loss: \u001b[1m\u001b[32m36.77940\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 050 | loss: 36.77940 - R2: 0.2462 -- iter: 0128/1168\n",
      "Training Step: 934  | total loss: \u001b[1m\u001b[32m36.69594\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 050 | loss: 36.69594 - R2: 0.2471 -- iter: 0192/1168\n",
      "Training Step: 935  | total loss: \u001b[1m\u001b[32m36.69594\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 050 | loss: 36.69594 - R2: 0.2478 -- iter: 0256/1168\n",
      "Training Step: 936  | total loss: \u001b[1m\u001b[32m36.46240\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 050 | loss: 36.46240 - R2: 0.2491 -- iter: 0320/1168\n",
      "Training Step: 937  | total loss: \u001b[1m\u001b[32m36.31225\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 050 | loss: 36.31225 - R2: 0.2501 -- iter: 0384/1168\n",
      "Training Step: 938  | total loss: \u001b[1m\u001b[32m35.88628\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 050 | loss: 35.88628 - R2: 0.2510 -- iter: 0448/1168\n",
      "Training Step: 939  | total loss: \u001b[1m\u001b[32m35.70768\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 050 | loss: 35.70768 - R2: 0.2526 -- iter: 0512/1168\n",
      "Training Step: 940  | total loss: \u001b[1m\u001b[32m35.53241\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 050 | loss: 35.53241 - R2: 0.2550 -- iter: 0576/1168\n",
      "Training Step: 941  | total loss: \u001b[1m\u001b[32m35.44155\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 050 | loss: 35.44155 - R2: 0.2550 -- iter: 0640/1168\n",
      "Training Step: 942  | total loss: \u001b[1m\u001b[32m35.25794\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 050 | loss: 35.25794 - R2: 0.2558 -- iter: 0704/1168\n",
      "Training Step: 943  | total loss: \u001b[1m\u001b[32m35.10678\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 050 | loss: 35.10678 - R2: 0.2570 -- iter: 0768/1168\n",
      "Training Step: 944  | total loss: \u001b[1m\u001b[32m34.96112\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 050 | loss: 34.96112 - R2: 0.2580 -- iter: 0832/1168\n",
      "Training Step: 945  | total loss: \u001b[1m\u001b[32m34.95112\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 050 | loss: 34.95112 - R2: 0.2590 -- iter: 0896/1168\n",
      "Training Step: 946  | total loss: \u001b[1m\u001b[32m34.77019\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 050 | loss: 34.77019 - R2: 0.2596 -- iter: 0960/1168\n",
      "Training Step: 947  | total loss: \u001b[1m\u001b[32m34.77019\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 050 | loss: 34.77019 - R2: 0.2607 -- iter: 1024/1168\n",
      "Training Step: 948  | total loss: \u001b[1m\u001b[32m34.72440\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 050 | loss: 34.72440 - R2: 0.2614 -- iter: 1088/1168\n",
      "Training Step: 949  | total loss: \u001b[1m\u001b[32m34.64381\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 050 | loss: 34.64381 - R2: 0.2621 -- iter: 1152/1168\n",
      "Training Step: 950  | total loss: \u001b[1m\u001b[32m34.49857\u001b[0m\u001b[0m | time: 1.174s\n",
      "| SGD | epoch: 050 | loss: 34.49857 - R2: 0.2632 | val_loss: 32.86961 - val_acc: 0.2745 -- iter: 1168/1168\n",
      "--\n",
      "---------------------------------\n",
      "Run id: ZELVRL\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "INFO:tensorflow:Summary name StandardError/ (raw) is illegal; using StandardError/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 1168\n",
      "Validation samples: 292\n",
      "--\n",
      "Training Step: 1  | total loss: \u001b[1m\u001b[32m129.37994\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 001 | loss: 129.37994 - R2: 0.0000 -- iter: 0064/1168\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m129.37994\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 001 | loss: 129.37994 - R2: 0.0000 -- iter: 0128/1168\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m141.59843\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 001 | loss: 141.59843 - R2: 0.0000 -- iter: 0192/1168\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m141.41562\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 001 | loss: 141.41562 - R2: 0.0001 -- iter: 0256/1168\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m142.60971\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 001 | loss: 142.60971 - R2: 0.0002 -- iter: 0320/1168\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m140.09573\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 001 | loss: 140.09573 - R2: 0.0002 -- iter: 0384/1168\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m140.09573\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 001 | loss: 140.09573 - R2: 0.0003 -- iter: 0448/1168\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m138.64584\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 001 | loss: 138.64584 - R2: 0.0005 -- iter: 0512/1168\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m137.02283\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 001 | loss: 137.02283 - R2: 0.0009 -- iter: 0576/1168\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m135.88448\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 001 | loss: 135.88448 - R2: 0.0011 -- iter: 0640/1168\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m135.88448\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 001 | loss: 135.88448 - R2: 0.0011 -- iter: 0704/1168\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m134.51865\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 001 | loss: 134.51865 - R2: 0.0017 -- iter: 0768/1168\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m131.90312\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 001 | loss: 131.90312 - R2: 0.0020 -- iter: 0832/1168\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m131.42145\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 001 | loss: 131.42145 - R2: 0.0024 -- iter: 0896/1168\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m129.87279\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 001 | loss: 129.87279 - R2: 0.0028 -- iter: 0960/1168\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m129.87279\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 001 | loss: 129.87279 - R2: 0.0028 -- iter: 1024/1168\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m127.68390\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 001 | loss: 127.68390 - R2: 0.0036 -- iter: 1088/1168\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m127.68390\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 001 | loss: 127.68390 - R2: 0.0036 -- iter: 1152/1168\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m127.35135\u001b[0m\u001b[0m | time: 1.163s\n",
      "| SGD | epoch: 001 | loss: 127.35135 - R2: 0.0045 | val_loss: 121.85868 - val_acc: 0.0063 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m127.25018\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 002 | loss: 127.25018 - R2: 0.0050 -- iter: 0064/1168\n",
      "Training Step: 21  | total loss: \u001b[1m\u001b[32m125.10187\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 002 | loss: 125.10187 - R2: 0.0056 -- iter: 0128/1168\n",
      "Training Step: 22  | total loss: \u001b[1m\u001b[32m123.83160\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 002 | loss: 123.83160 - R2: 0.0062 -- iter: 0192/1168\n",
      "Training Step: 23  | total loss: \u001b[1m\u001b[32m123.83160\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 002 | loss: 123.83160 - R2: 0.0062 -- iter: 0256/1168\n",
      "Training Step: 24  | total loss: \u001b[1m\u001b[32m122.55241\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 002 | loss: 122.55241 - R2: 0.0068 -- iter: 0320/1168\n",
      "Training Step: 25  | total loss: \u001b[1m\u001b[32m122.08939\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 002 | loss: 122.08939 - R2: 0.0074 -- iter: 0384/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 26  | total loss: \u001b[1m\u001b[32m121.03569\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 002 | loss: 121.03569 - R2: 0.0081 -- iter: 0448/1168\n",
      "Training Step: 27  | total loss: \u001b[1m\u001b[32m119.63425\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 002 | loss: 119.63425 - R2: 0.0088 -- iter: 0512/1168\n",
      "Training Step: 28  | total loss: \u001b[1m\u001b[32m118.68800\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 002 | loss: 118.68800 - R2: 0.0103 -- iter: 0576/1168\n",
      "Training Step: 29  | total loss: \u001b[1m\u001b[32m117.38301\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 002 | loss: 117.38301 - R2: 0.0103 -- iter: 0640/1168\n",
      "Training Step: 30  | total loss: \u001b[1m\u001b[32m116.43993\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 002 | loss: 116.43993 - R2: 0.0111 -- iter: 0704/1168\n",
      "Training Step: 31  | total loss: \u001b[1m\u001b[32m114.81978\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 002 | loss: 114.81978 - R2: 0.0127 -- iter: 0768/1168\n",
      "Training Step: 32  | total loss: \u001b[1m\u001b[32m114.81978\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 002 | loss: 114.81978 - R2: 0.0127 -- iter: 0832/1168\n",
      "Training Step: 33  | total loss: \u001b[1m\u001b[32m113.76881\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 002 | loss: 113.76881 - R2: 0.0136 -- iter: 0896/1168\n",
      "Training Step: 34  | total loss: \u001b[1m\u001b[32m112.84376\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 002 | loss: 112.84376 - R2: 0.0145 -- iter: 0960/1168\n",
      "Training Step: 35  | total loss: \u001b[1m\u001b[32m111.98397\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 002 | loss: 111.98397 - R2: 0.0154 -- iter: 1024/1168\n",
      "Training Step: 36  | total loss: \u001b[1m\u001b[32m110.56876\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 002 | loss: 110.56876 - R2: 0.0164 -- iter: 1088/1168\n",
      "Training Step: 37  | total loss: \u001b[1m\u001b[32m109.55483\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 002 | loss: 109.55483 - R2: 0.0174 -- iter: 1152/1168\n",
      "Training Step: 38  | total loss: \u001b[1m\u001b[32m108.56287\u001b[0m\u001b[0m | time: 1.116s\n",
      "| SGD | epoch: 002 | loss: 108.56287 - R2: 0.0185 | val_loss: 101.54960 - val_acc: 0.0255 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 39  | total loss: \u001b[1m\u001b[32m107.51900\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 003 | loss: 107.51900 - R2: 0.0195 -- iter: 0064/1168\n",
      "Training Step: 40  | total loss: \u001b[1m\u001b[32m106.60316\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 003 | loss: 106.60316 - R2: 0.0206 -- iter: 0128/1168\n",
      "Training Step: 41  | total loss: \u001b[1m\u001b[32m105.68307\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 003 | loss: 105.68307 - R2: 0.0217 -- iter: 0192/1168\n",
      "Training Step: 42  | total loss: \u001b[1m\u001b[32m104.82825\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 003 | loss: 104.82825 - R2: 0.0228 -- iter: 0256/1168\n",
      "Training Step: 43  | total loss: \u001b[1m\u001b[32m103.98214\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 003 | loss: 103.98214 - R2: 0.0240 -- iter: 0320/1168\n",
      "Training Step: 44  | total loss: \u001b[1m\u001b[32m102.94112\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 003 | loss: 102.94112 - R2: 0.0252 -- iter: 0384/1168\n",
      "Training Step: 45  | total loss: \u001b[1m\u001b[32m101.84858\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 003 | loss: 101.84858 - R2: 0.0265 -- iter: 0448/1168\n",
      "Training Step: 46  | total loss: \u001b[1m\u001b[32m100.93826\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 003 | loss: 100.93826 - R2: 0.0278 -- iter: 0512/1168\n",
      "Training Step: 47  | total loss: \u001b[1m\u001b[32m99.92792\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 003 | loss: 99.92792 - R2: 0.0291 -- iter: 0576/1168\n",
      "Training Step: 48  | total loss: \u001b[1m\u001b[32m99.10562\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 003 | loss: 99.10562 - R2: 0.0305 -- iter: 0640/1168\n",
      "Training Step: 49  | total loss: \u001b[1m\u001b[32m98.21774\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 003 | loss: 98.21774 - R2: 0.0318 -- iter: 0704/1168\n",
      "Training Step: 50  | total loss: \u001b[1m\u001b[32m96.52402\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 003 | loss: 96.52402 - R2: 0.0347 -- iter: 0768/1168\n",
      "Training Step: 51  | total loss: \u001b[1m\u001b[32m96.52402\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 003 | loss: 96.52402 - R2: 0.0347 -- iter: 0832/1168\n",
      "Training Step: 52  | total loss: \u001b[1m\u001b[32m95.36610\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 003 | loss: 95.36610 - R2: 0.0362 -- iter: 0896/1168\n",
      "Training Step: 53  | total loss: \u001b[1m\u001b[32m94.40244\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 003 | loss: 94.40244 - R2: 0.0378 -- iter: 0960/1168\n",
      "Training Step: 54  | total loss: \u001b[1m\u001b[32m93.59702\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 003 | loss: 93.59702 - R2: 0.0410 -- iter: 1024/1168\n",
      "Training Step: 55  | total loss: \u001b[1m\u001b[32m91.72785\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 003 | loss: 91.72785 - R2: 0.0426 -- iter: 1088/1168\n",
      "Training Step: 56  | total loss: \u001b[1m\u001b[32m91.72785\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 003 | loss: 91.72785 - R2: 0.0426 -- iter: 1152/1168\n",
      "Training Step: 57  | total loss: \u001b[1m\u001b[32m90.82488\u001b[0m\u001b[0m | time: 1.116s\n",
      "| SGD | epoch: 003 | loss: 90.82488 - R2: 0.0443 | val_loss: 82.02071 - val_acc: 0.0600 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 58  | total loss: \u001b[1m\u001b[32m88.83771\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 004 | loss: 88.83771 - R2: 0.0479 -- iter: 0064/1168\n",
      "Training Step: 59  | total loss: \u001b[1m\u001b[32m88.83771\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 004 | loss: 88.83771 - R2: 0.0479 -- iter: 0128/1168\n",
      "Training Step: 60  | total loss: \u001b[1m\u001b[32m88.27793\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 004 | loss: 88.27793 - R2: 0.0496 -- iter: 0192/1168\n",
      "Training Step: 61  | total loss: \u001b[1m\u001b[32m87.65907\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 004 | loss: 87.65907 - R2: 0.0513 -- iter: 0256/1168\n",
      "Training Step: 62  | total loss: \u001b[1m\u001b[32m86.45821\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 004 | loss: 86.45821 - R2: 0.0534 -- iter: 0320/1168\n",
      "Training Step: 63  | total loss: \u001b[1m\u001b[32m85.54530\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 004 | loss: 85.54530 - R2: 0.0554 -- iter: 0384/1168\n",
      "Training Step: 64  | total loss: \u001b[1m\u001b[32m84.58878\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 004 | loss: 84.58878 - R2: 0.0595 -- iter: 0448/1168\n",
      "Training Step: 65  | total loss: \u001b[1m\u001b[32m82.54264\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 004 | loss: 82.54264 - R2: 0.0619 -- iter: 0512/1168\n",
      "Training Step: 66  | total loss: \u001b[1m\u001b[32m81.64169\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 004 | loss: 81.64169 - R2: 0.0641 -- iter: 0576/1168\n",
      "Training Step: 67  | total loss: \u001b[1m\u001b[32m81.64169\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 004 | loss: 81.64169 - R2: 0.0641 -- iter: 0640/1168\n",
      "Training Step: 68  | total loss: \u001b[1m\u001b[32m80.66827\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 004 | loss: 80.66827 - R2: 0.0664 -- iter: 0704/1168\n",
      "Training Step: 69  | total loss: \u001b[1m\u001b[32m79.76722\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 004 | loss: 79.76722 - R2: 0.0687 -- iter: 0768/1168\n",
      "Training Step: 70  | total loss: \u001b[1m\u001b[32m78.66914\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 004 | loss: 78.66914 - R2: 0.0713 -- iter: 0832/1168\n",
      "Training Step: 71  | total loss: \u001b[1m\u001b[32m77.51681\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 004 | loss: 77.51681 - R2: 0.0739 -- iter: 0896/1168\n",
      "Training Step: 72  | total loss: \u001b[1m\u001b[32m76.63731\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 004 | loss: 76.63731 - R2: 0.0765 -- iter: 0960/1168\n",
      "Training Step: 73  | total loss: \u001b[1m\u001b[32m75.64235\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 004 | loss: 75.64235 - R2: 0.0820 -- iter: 1024/1168\n",
      "Training Step: 74  | total loss: \u001b[1m\u001b[32m74.55431\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 004 | loss: 74.55431 - R2: 0.0820 -- iter: 1088/1168\n",
      "Training Step: 75  | total loss: \u001b[1m\u001b[32m72.19997\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 004 | loss: 72.19997 - R2: 0.0851 -- iter: 1152/1168\n",
      "Training Step: 76  | total loss: \u001b[1m\u001b[32m72.19997\u001b[0m\u001b[0m | time: 1.111s\n",
      "| SGD | epoch: 004 | loss: 72.19997 - R2: 0.0882 | val_loss: 60.89304 - val_acc: 0.1222 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 77  | total loss: \u001b[1m\u001b[32m71.17395\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 005 | loss: 71.17395 - R2: 0.0913 -- iter: 0064/1168\n",
      "Training Step: 78  | total loss: \u001b[1m\u001b[32m70.25094\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 005 | loss: 70.25094 - R2: 0.0943 -- iter: 0128/1168\n",
      "Training Step: 79  | total loss: \u001b[1m\u001b[32m69.28929\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 005 | loss: 69.28929 - R2: 0.0975 -- iter: 0192/1168\n",
      "Training Step: 80  | total loss: \u001b[1m\u001b[32m67.54467\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 005 | loss: 67.54467 - R2: 0.1039 -- iter: 0256/1168\n",
      "Training Step: 81  | total loss: \u001b[1m\u001b[32m66.58155\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 005 | loss: 66.58155 - R2: 0.1074 -- iter: 0320/1168\n",
      "Training Step: 82  | total loss: \u001b[1m\u001b[32m65.40740\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 005 | loss: 65.40740 - R2: 0.1113 -- iter: 0384/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 83  | total loss: \u001b[1m\u001b[32m65.40740\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 005 | loss: 65.40740 - R2: 0.1113 -- iter: 0448/1168\n",
      "Training Step: 84  | total loss: \u001b[1m\u001b[32m64.42296\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 005 | loss: 64.42296 - R2: 0.1151 -- iter: 0512/1168\n",
      "Training Step: 85  | total loss: \u001b[1m\u001b[32m62.25517\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 005 | loss: 62.25517 - R2: 0.1234 -- iter: 0576/1168\n",
      "Training Step: 86  | total loss: \u001b[1m\u001b[32m61.03180\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 005 | loss: 61.03180 - R2: 0.1280 -- iter: 0640/1168\n",
      "Training Step: 87  | total loss: \u001b[1m\u001b[32m59.78436\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 005 | loss: 59.78436 - R2: 0.1328 -- iter: 0704/1168\n",
      "Training Step: 88  | total loss: \u001b[1m\u001b[32m58.63967\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 005 | loss: 58.63967 - R2: 0.1376 -- iter: 0768/1168\n",
      "Training Step: 89  | total loss: \u001b[1m\u001b[32m58.63967\u001b[0m\u001b[0m | time: 0.180s\n",
      "| SGD | epoch: 005 | loss: 58.63967 - R2: 0.1376 -- iter: 0832/1168\n",
      "Training Step: 90  | total loss: \u001b[1m\u001b[32m56.24133\u001b[0m\u001b[0m | time: 0.187s\n",
      "| SGD | epoch: 005 | loss: 56.24133 - R2: 0.1479 -- iter: 0896/1168\n",
      "Training Step: 91  | total loss: \u001b[1m\u001b[32m56.24133\u001b[0m\u001b[0m | time: 0.189s\n",
      "| SGD | epoch: 005 | loss: 56.24133 - R2: 0.1479 -- iter: 0960/1168\n",
      "Training Step: 92  | total loss: \u001b[1m\u001b[32m54.97902\u001b[0m\u001b[0m | time: 0.201s\n",
      "| SGD | epoch: 005 | loss: 54.97902 - R2: 0.1535 -- iter: 1024/1168\n",
      "Training Step: 93  | total loss: \u001b[1m\u001b[32m52.43485\u001b[0m\u001b[0m | time: 0.212s\n",
      "| SGD | epoch: 005 | loss: 52.43485 - R2: 0.1653 -- iter: 1088/1168\n",
      "Training Step: 94  | total loss: \u001b[1m\u001b[32m51.00496\u001b[0m\u001b[0m | time: 0.225s\n",
      "| SGD | epoch: 005 | loss: 51.00496 - R2: 0.1721 -- iter: 1152/1168\n",
      "Training Step: 95  | total loss: \u001b[1m\u001b[32m51.00496\u001b[0m\u001b[0m | time: 1.232s\n",
      "| SGD | epoch: 005 | loss: 51.00496 - R2: 0.1721 | val_loss: 36.44977 - val_acc: 0.2471 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 96  | total loss: \u001b[1m\u001b[32m48.57609\u001b[0m\u001b[0m | time: 0.017s\n",
      "| SGD | epoch: 006 | loss: 48.57609 - R2: 0.1848 -- iter: 0064/1168\n",
      "Training Step: 97  | total loss: \u001b[1m\u001b[32m47.26962\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 006 | loss: 47.26962 - R2: 0.1918 -- iter: 0128/1168\n",
      "Training Step: 98  | total loss: \u001b[1m\u001b[32m47.26962\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 006 | loss: 47.26962 - R2: 0.1918 -- iter: 0192/1168\n",
      "Training Step: 99  | total loss: \u001b[1m\u001b[32m44.71284\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 006 | loss: 44.71284 - R2: 0.2064 -- iter: 0256/1168\n",
      "Training Step: 100  | total loss: \u001b[1m\u001b[32m43.47181\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 006 | loss: 43.47181 - R2: 0.2140 -- iter: 0320/1168\n",
      "Training Step: 101  | total loss: \u001b[1m\u001b[32m42.14325\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 006 | loss: 42.14325 - R2: 0.2222 -- iter: 0384/1168\n",
      "Training Step: 102  | total loss: \u001b[1m\u001b[32m40.91901\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 006 | loss: 40.91901 - R2: 0.2302 -- iter: 0448/1168\n",
      "Training Step: 103  | total loss: \u001b[1m\u001b[32m39.56860\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 006 | loss: 39.56860 - R2: 0.2392 -- iter: 0512/1168\n",
      "Training Step: 104  | total loss: \u001b[1m\u001b[32m38.28143\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 006 | loss: 38.28143 - R2: 0.2481 -- iter: 0576/1168\n",
      "Training Step: 105  | total loss: \u001b[1m\u001b[32m38.28143\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 006 | loss: 38.28143 - R2: 0.2481 -- iter: 0640/1168\n",
      "Training Step: 106  | total loss: \u001b[1m\u001b[32m35.54068\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 006 | loss: 35.54068 - R2: 0.2679 -- iter: 0704/1168\n",
      "Training Step: 107  | total loss: \u001b[1m\u001b[32m35.54068\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 006 | loss: 35.54068 - R2: 0.2679 -- iter: 0768/1168\n",
      "Training Step: 108  | total loss: \u001b[1m\u001b[32m34.23076\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 006 | loss: 34.23076 - R2: 0.2780 -- iter: 0832/1168\n",
      "Training Step: 109  | total loss: \u001b[1m\u001b[32m32.92206\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 006 | loss: 32.92206 - R2: 0.2885 -- iter: 0896/1168\n",
      "Training Step: 110  | total loss: \u001b[1m\u001b[32m30.33410\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 006 | loss: 30.33410 - R2: 0.3107 -- iter: 0960/1168\n",
      "Training Step: 111  | total loss: \u001b[1m\u001b[32m30.33410\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 006 | loss: 30.33410 - R2: 0.3107 -- iter: 1024/1168\n",
      "Training Step: 112  | total loss: \u001b[1m\u001b[32m27.80912\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 006 | loss: 27.80912 - R2: 0.3342 -- iter: 1088/1168\n",
      "Training Step: 113  | total loss: \u001b[1m\u001b[32m26.49108\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 006 | loss: 26.49108 - R2: 0.3473 -- iter: 1152/1168\n",
      "Training Step: 114  | total loss: \u001b[1m\u001b[32m25.24471\u001b[0m\u001b[0m | time: 1.104s\n",
      "| SGD | epoch: 006 | loss: 25.24471 - R2: 0.3602 | val_loss: 12.54847 - val_acc: 0.4983 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 115  | total loss: \u001b[1m\u001b[32m25.24471\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 007 | loss: 25.24471 - R2: 0.3602 -- iter: 0064/1168\n",
      "Training Step: 116  | total loss: \u001b[1m\u001b[32m22.77362\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 007 | loss: 22.77362 - R2: 0.3878 -- iter: 0128/1168\n",
      "Training Step: 117  | total loss: \u001b[1m\u001b[32m21.55208\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 007 | loss: 21.55208 - R2: 0.4024 -- iter: 0192/1168\n",
      "Training Step: 118  | total loss: \u001b[1m\u001b[32m20.36826\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 007 | loss: 20.36826 - R2: 0.4174 -- iter: 0256/1168\n",
      "Training Step: 119  | total loss: \u001b[1m\u001b[32m20.36826\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 007 | loss: 20.36826 - R2: 0.4174 -- iter: 0320/1168\n",
      "Training Step: 120  | total loss: \u001b[1m\u001b[32m18.11138\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 007 | loss: 18.11138 - R2: 0.4483 -- iter: 0384/1168\n",
      "Training Step: 121  | total loss: \u001b[1m\u001b[32m18.11138\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 007 | loss: 18.11138 - R2: 0.4483 -- iter: 0448/1168\n",
      "Training Step: 122  | total loss: \u001b[1m\u001b[32m17.00352\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 007 | loss: 17.00352 - R2: 0.4822 -- iter: 0512/1168\n",
      "Training Step: 123  | total loss: \u001b[1m\u001b[32m14.78947\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 007 | loss: 14.78947 - R2: 0.5009 -- iter: 0576/1168\n",
      "Training Step: 124  | total loss: \u001b[1m\u001b[32m14.78947\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 007 | loss: 14.78947 - R2: 0.5009 -- iter: 0640/1168\n",
      "Training Step: 125  | total loss: \u001b[1m\u001b[32m12.72992\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 007 | loss: 12.72992 - R2: 0.5389 -- iter: 0704/1168\n",
      "Training Step: 126  | total loss: \u001b[1m\u001b[32m12.72992\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 007 | loss: 12.72992 - R2: 0.5389 -- iter: 0768/1168\n",
      "Training Step: 127  | total loss: \u001b[1m\u001b[32m11.74383\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 007 | loss: 11.74383 - R2: 0.5593 -- iter: 0832/1168\n",
      "Training Step: 128  | total loss: \u001b[1m\u001b[32m9.91645\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 007 | loss: 9.91645 - R2: 0.6011 -- iter: 0896/1168\n",
      "Training Step: 129  | total loss: \u001b[1m\u001b[32m9.91645\u001b[0m\u001b[0m | time: 0.172s\n",
      "| SGD | epoch: 007 | loss: 9.91645 - R2: 0.6011 -- iter: 0960/1168\n",
      "Training Step: 130  | total loss: \u001b[1m\u001b[32m8.24037\u001b[0m\u001b[0m | time: 0.176s\n",
      "| SGD | epoch: 007 | loss: 8.24037 - R2: 0.6474 -- iter: 1024/1168\n",
      "Training Step: 131  | total loss: \u001b[1m\u001b[32m7.50070\u001b[0m\u001b[0m | time: 0.181s\n",
      "| SGD | epoch: 007 | loss: 7.50070 - R2: 0.6692 -- iter: 1088/1168\n",
      "Training Step: 132  | total loss: \u001b[1m\u001b[32m7.50070\u001b[0m\u001b[0m | time: 0.188s\n",
      "| SGD | epoch: 007 | loss: 7.50070 - R2: 0.6692 -- iter: 1152/1168\n",
      "Training Step: 133  | total loss: \u001b[1m\u001b[32m6.13385\u001b[0m\u001b[0m | time: 1.195s\n",
      "| SGD | epoch: 007 | loss: 6.13385 - R2: 0.7185 | val_loss: 0.15705 - val_acc: 0.9670 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 134  | total loss: \u001b[1m\u001b[32m6.13385\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 008 | loss: 6.13385 - R2: 0.7185 -- iter: 0064/1168\n",
      "Training Step: 135  | total loss: \u001b[1m\u001b[32m4.99269\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 008 | loss: 4.99269 - R2: 0.7671 -- iter: 0128/1168\n",
      "Training Step: 136  | total loss: \u001b[1m\u001b[32m4.50661\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 008 | loss: 4.50661 - R2: 0.7890 -- iter: 0192/1168\n",
      "Training Step: 137  | total loss: \u001b[1m\u001b[32m4.50661\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 008 | loss: 4.50661 - R2: 0.7890 -- iter: 0256/1168\n",
      "Training Step: 138  | total loss: \u001b[1m\u001b[32m3.67314\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 008 | loss: 3.67314 - R2: 0.8271 -- iter: 0320/1168\n",
      "Training Step: 139  | total loss: \u001b[1m\u001b[32m3.67314\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 008 | loss: 3.67314 - R2: 0.8271 -- iter: 0384/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 140  | total loss: \u001b[1m\u001b[32m2.98776\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 008 | loss: 2.98776 - R2: 0.8588 -- iter: 0448/1168\n",
      "Training Step: 141  | total loss: \u001b[1m\u001b[32m2.69816\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 008 | loss: 2.69816 - R2: 0.8734 -- iter: 0512/1168\n",
      "Training Step: 142  | total loss: \u001b[1m\u001b[32m2.69816\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 008 | loss: 2.69816 - R2: 0.8734 -- iter: 0576/1168\n",
      "Training Step: 143  | total loss: \u001b[1m\u001b[32m2.43573\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 008 | loss: 2.43573 - R2: 0.8859 -- iter: 0640/1168\n",
      "Training Step: 144  | total loss: \u001b[1m\u001b[32m2.19875\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 008 | loss: 2.19875 - R2: 0.8972 -- iter: 0704/1168\n",
      "Training Step: 145  | total loss: \u001b[1m\u001b[32m1.79294\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 008 | loss: 1.79294 - R2: 0.9169 -- iter: 0768/1168\n",
      "Training Step: 146  | total loss: \u001b[1m\u001b[32m1.79294\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 008 | loss: 1.79294 - R2: 0.9256 -- iter: 0832/1168\n",
      "Training Step: 147  | total loss: \u001b[1m\u001b[32m1.47368\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 008 | loss: 1.47368 - R2: 0.9322 -- iter: 0896/1168\n",
      "Training Step: 148  | total loss: \u001b[1m\u001b[32m1.33560\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 008 | loss: 1.33560 - R2: 0.9387 -- iter: 0960/1168\n",
      "Training Step: 149  | total loss: \u001b[1m\u001b[32m1.20636\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 008 | loss: 1.20636 - R2: 0.9448 -- iter: 1024/1168\n",
      "Training Step: 150  | total loss: \u001b[1m\u001b[32m1.09012\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 008 | loss: 1.09012 - R2: 0.9502 -- iter: 1088/1168\n",
      "Training Step: 151  | total loss: \u001b[1m\u001b[32m1.09012\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 008 | loss: 1.09012 - R2: 0.9502 -- iter: 1152/1168\n",
      "Training Step: 152  | total loss: \u001b[1m\u001b[32m0.98662\u001b[0m\u001b[0m | time: 1.173s\n",
      "| SGD | epoch: 008 | loss: 0.98662 - R2: 0.9548 | val_loss: 0.05322 - val_acc: 1.0025 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 153  | total loss: \u001b[1m\u001b[32m0.80826\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 009 | loss: 0.80826 - R2: 0.9647 -- iter: 0064/1168\n",
      "Training Step: 154  | total loss: \u001b[1m\u001b[32m0.73142\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 009 | loss: 0.73142 - R2: 0.9683 -- iter: 0128/1168\n",
      "Training Step: 155  | total loss: \u001b[1m\u001b[32m0.66111\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 009 | loss: 0.66111 - R2: 0.9711 -- iter: 0192/1168\n",
      "Training Step: 156  | total loss: \u001b[1m\u001b[32m0.60041\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 009 | loss: 0.60041 - R2: 0.9736 -- iter: 0256/1168\n",
      "Training Step: 157  | total loss: \u001b[1m\u001b[32m0.60041\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 009 | loss: 0.60041 - R2: 0.9736 -- iter: 0320/1168\n",
      "Training Step: 158  | total loss: \u001b[1m\u001b[32m0.49216\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 009 | loss: 0.49216 - R2: 0.9787 -- iter: 0384/1168\n",
      "Training Step: 159  | total loss: \u001b[1m\u001b[32m0.44630\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 009 | loss: 0.44630 - R2: 0.9819 -- iter: 0448/1168\n",
      "Training Step: 160  | total loss: \u001b[1m\u001b[32m0.44630\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 009 | loss: 0.44630 - R2: 0.9819 -- iter: 0512/1168\n",
      "Training Step: 161  | total loss: \u001b[1m\u001b[32m0.40420\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 009 | loss: 0.40420 - R2: 0.9843 -- iter: 0576/1168\n",
      "Training Step: 162  | total loss: \u001b[1m\u001b[32m0.36747\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 009 | loss: 0.36747 - R2: 0.9856 -- iter: 0640/1168\n",
      "Training Step: 163  | total loss: \u001b[1m\u001b[32m0.30472\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 009 | loss: 0.30472 - R2: 0.9884 -- iter: 0704/1168\n",
      "Training Step: 164  | total loss: \u001b[1m\u001b[32m0.27671\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 009 | loss: 0.27671 - R2: 0.9893 -- iter: 0768/1168\n",
      "Training Step: 165  | total loss: \u001b[1m\u001b[32m0.25184\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 009 | loss: 0.25184 - R2: 0.9903 -- iter: 0832/1168\n",
      "Training Step: 166  | total loss: \u001b[1m\u001b[32m0.25184\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 009 | loss: 0.25184 - R2: 0.9907 -- iter: 0896/1168\n",
      "Training Step: 167  | total loss: \u001b[1m\u001b[32m0.22908\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 009 | loss: 0.22908 - R2: 0.9907 -- iter: 0960/1168\n",
      "Training Step: 168  | total loss: \u001b[1m\u001b[32m0.20821\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 009 | loss: 0.20821 - R2: 0.9912 -- iter: 1024/1168\n",
      "Training Step: 169  | total loss: \u001b[1m\u001b[32m0.19034\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 009 | loss: 0.19034 - R2: 0.9921 -- iter: 1088/1168\n",
      "Training Step: 170  | total loss: \u001b[1m\u001b[32m0.15863\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 009 | loss: 0.15863 - R2: 0.9936 -- iter: 1152/1168\n",
      "Training Step: 171  | total loss: \u001b[1m\u001b[32m0.15863\u001b[0m\u001b[0m | time: 1.152s\n",
      "| SGD | epoch: 009 | loss: 0.15863 - R2: 0.9936 | val_loss: 0.03326 - val_acc: 1.0019 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 172  | total loss: \u001b[1m\u001b[32m0.14555\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 010 | loss: 0.14555 - R2: 0.9946 -- iter: 0064/1168\n",
      "Training Step: 173  | total loss: \u001b[1m\u001b[32m0.12218\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 010 | loss: 0.12218 - R2: 0.9955 -- iter: 0128/1168\n",
      "Training Step: 174  | total loss: \u001b[1m\u001b[32m0.11211\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 010 | loss: 0.11211 - R2: 0.9959 -- iter: 0192/1168\n",
      "Training Step: 175  | total loss: \u001b[1m\u001b[32m0.10312\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 010 | loss: 0.10312 - R2: 0.9962 -- iter: 0256/1168\n",
      "Training Step: 176  | total loss: \u001b[1m\u001b[32m0.09457\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 010 | loss: 0.09457 - R2: 0.9968 -- iter: 0320/1168\n",
      "Training Step: 177  | total loss: \u001b[1m\u001b[32m0.08965\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 010 | loss: 0.08965 - R2: 0.9974 -- iter: 0384/1168\n",
      "Training Step: 178  | total loss: \u001b[1m\u001b[32m0.08277\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 010 | loss: 0.08277 - R2: 0.9973 -- iter: 0448/1168\n",
      "Training Step: 179  | total loss: \u001b[1m\u001b[32m0.07699\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 010 | loss: 0.07699 - R2: 0.9966 -- iter: 0512/1168\n",
      "Training Step: 180  | total loss: \u001b[1m\u001b[32m0.07699\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 010 | loss: 0.07699 - R2: 0.9966 -- iter: 0576/1168\n",
      "Training Step: 181  | total loss: \u001b[1m\u001b[32m0.06629\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 010 | loss: 0.06629 - R2: 0.9980 -- iter: 0640/1168\n",
      "Training Step: 182  | total loss: \u001b[1m\u001b[32m0.06147\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 010 | loss: 0.06147 - R2: 0.9979 -- iter: 0704/1168\n",
      "Training Step: 183  | total loss: \u001b[1m\u001b[32m0.05803\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 010 | loss: 0.05803 - R2: 0.9987 -- iter: 0768/1168\n",
      "Training Step: 184  | total loss: \u001b[1m\u001b[32m0.05584\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 010 | loss: 0.05584 - R2: 0.9994 -- iter: 0832/1168\n",
      "Training Step: 185  | total loss: \u001b[1m\u001b[32m0.05197\u001b[0m\u001b[0m | time: 0.181s\n",
      "| SGD | epoch: 010 | loss: 0.05197 - R2: 0.9989 -- iter: 0896/1168\n",
      "Training Step: 186  | total loss: \u001b[1m\u001b[32m0.04790\u001b[0m\u001b[0m | time: 0.186s\n",
      "| SGD | epoch: 010 | loss: 0.04790 - R2: 0.9989 -- iter: 0960/1168\n",
      "Training Step: 187  | total loss: \u001b[1m\u001b[32m0.04790\u001b[0m\u001b[0m | time: 0.191s\n",
      "| SGD | epoch: 010 | loss: 0.04790 - R2: 0.9994 -- iter: 1024/1168\n",
      "Training Step: 188  | total loss: \u001b[1m\u001b[32m0.07090\u001b[0m\u001b[0m | time: 0.194s\n",
      "| SGD | epoch: 010 | loss: 0.07090 - R2: 0.9996 -- iter: 1088/1168\n",
      "Training Step: 189  | total loss: \u001b[1m\u001b[32m0.07090\u001b[0m\u001b[0m | time: 0.197s\n",
      "| SGD | epoch: 010 | loss: 0.07090 - R2: 0.9996 -- iter: 1152/1168\n",
      "Training Step: 190  | total loss: \u001b[1m\u001b[32m0.06256\u001b[0m\u001b[0m | time: 1.204s\n",
      "| SGD | epoch: 010 | loss: 0.06256 - R2: 0.9993 | val_loss: 0.02816 - val_acc: 1.0009 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 191  | total loss: \u001b[1m\u001b[32m0.05820\u001b[0m\u001b[0m | time: 0.019s\n",
      "| SGD | epoch: 011 | loss: 0.05820 - R2: 0.9991 -- iter: 0064/1168\n",
      "Training Step: 192  | total loss: \u001b[1m\u001b[32m0.05820\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 011 | loss: 0.05820 - R2: 0.9991 -- iter: 0128/1168\n",
      "Training Step: 193  | total loss: \u001b[1m\u001b[32m0.05380\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 011 | loss: 0.05380 - R2: 0.9991 -- iter: 0192/1168\n",
      "Training Step: 194  | total loss: \u001b[1m\u001b[32m0.05110\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 011 | loss: 0.05110 - R2: 0.9991 -- iter: 0256/1168\n",
      "Training Step: 195  | total loss: \u001b[1m\u001b[32m0.05006\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 011 | loss: 0.05006 - R2: 0.9991 -- iter: 0320/1168\n",
      "Training Step: 196  | total loss: \u001b[1m\u001b[32m0.04456\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 011 | loss: 0.04456 - R2: 0.9995 -- iter: 0384/1168\n",
      "Training Step: 197  | total loss: \u001b[1m\u001b[32m0.04284\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 011 | loss: 0.04284 - R2: 0.9998 -- iter: 0448/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 198  | total loss: \u001b[1m\u001b[32m0.04284\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 011 | loss: 0.04284 - R2: 0.9998 -- iter: 0512/1168\n",
      "Training Step: 199  | total loss: \u001b[1m\u001b[32m0.04082\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 011 | loss: 0.04082 - R2: 0.9998 -- iter: 0576/1168\n",
      "Training Step: 200  | total loss: \u001b[1m\u001b[32m0.03919\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 011 | loss: 0.03919 - R2: 0.9992 -- iter: 0640/1168\n",
      "Training Step: 201  | total loss: \u001b[1m\u001b[32m0.03653\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 011 | loss: 0.03653 - R2: 0.9999 -- iter: 0704/1168\n",
      "Training Step: 202  | total loss: \u001b[1m\u001b[32m0.03483\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 011 | loss: 0.03483 - R2: 0.9997 -- iter: 0768/1168\n",
      "Training Step: 203  | total loss: \u001b[1m\u001b[32m0.03281\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 011 | loss: 0.03281 - R2: 0.9997 -- iter: 0832/1168\n",
      "Training Step: 204  | total loss: \u001b[1m\u001b[32m0.03281\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 011 | loss: 0.03281 - R2: 0.9997 -- iter: 0896/1168\n",
      "Training Step: 205  | total loss: \u001b[1m\u001b[32m0.03142\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 011 | loss: 0.03142 - R2: 0.9997 -- iter: 0960/1168\n",
      "Training Step: 206  | total loss: \u001b[1m\u001b[32m0.02966\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 011 | loss: 0.02966 - R2: 0.9996 -- iter: 1024/1168\n",
      "Training Step: 207  | total loss: \u001b[1m\u001b[32m0.02901\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 011 | loss: 0.02901 - R2: 0.9996 -- iter: 1088/1168\n",
      "Training Step: 208  | total loss: \u001b[1m\u001b[32m0.05712\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 011 | loss: 0.05712 - R2: 1.0004 -- iter: 1152/1168\n",
      "Training Step: 209  | total loss: \u001b[1m\u001b[32m0.05275\u001b[0m\u001b[0m | time: 1.130s\n",
      "| SGD | epoch: 011 | loss: 0.05275 - R2: 1.0001 | val_loss: 0.02976 - val_acc: 1.0025 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 210  | total loss: \u001b[1m\u001b[32m0.05148\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 012 | loss: 0.05148 - R2: 0.9995 -- iter: 0064/1168\n",
      "Training Step: 211  | total loss: \u001b[1m\u001b[32m0.04824\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 012 | loss: 0.04824 - R2: 0.9999 -- iter: 0128/1168\n",
      "Training Step: 212  | total loss: \u001b[1m\u001b[32m0.04571\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 012 | loss: 0.04571 - R2: 0.9996 -- iter: 0192/1168\n",
      "Training Step: 213  | total loss: \u001b[1m\u001b[32m0.04143\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 012 | loss: 0.04143 - R2: 0.9991 -- iter: 0256/1168\n",
      "Training Step: 214  | total loss: \u001b[1m\u001b[32m0.03906\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 012 | loss: 0.03906 - R2: 0.9994 -- iter: 0320/1168\n",
      "Training Step: 215  | total loss: \u001b[1m\u001b[32m0.03715\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 012 | loss: 0.03715 - R2: 0.9990 -- iter: 0384/1168\n",
      "Training Step: 216  | total loss: \u001b[1m\u001b[32m0.03715\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 012 | loss: 0.03715 - R2: 0.9990 -- iter: 0448/1168\n",
      "Training Step: 217  | total loss: \u001b[1m\u001b[32m0.03634\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 012 | loss: 0.03634 - R2: 0.9996 -- iter: 0512/1168\n",
      "Training Step: 218  | total loss: \u001b[1m\u001b[32m0.03483\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 012 | loss: 0.03483 - R2: 0.9996 -- iter: 0576/1168\n",
      "Training Step: 219  | total loss: \u001b[1m\u001b[32m0.03577\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 012 | loss: 0.03577 - R2: 1.0002 -- iter: 0640/1168\n",
      "Training Step: 220  | total loss: \u001b[1m\u001b[32m0.03577\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 012 | loss: 0.03577 - R2: 1.0002 -- iter: 0704/1168\n",
      "Training Step: 221  | total loss: \u001b[1m\u001b[32m0.03393\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 012 | loss: 0.03393 - R2: 1.0003 -- iter: 0768/1168\n",
      "Training Step: 222  | total loss: \u001b[1m\u001b[32m0.03215\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 012 | loss: 0.03215 - R2: 0.9999 -- iter: 0832/1168\n",
      "Training Step: 223  | total loss: \u001b[1m\u001b[32m0.03215\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 012 | loss: 0.03215 - R2: 0.9999 -- iter: 0896/1168\n",
      "Training Step: 224  | total loss: \u001b[1m\u001b[32m0.03115\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 012 | loss: 0.03115 - R2: 1.0001 -- iter: 0960/1168\n",
      "Training Step: 225  | total loss: \u001b[1m\u001b[32m0.02900\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 012 | loss: 0.02900 - R2: 1.0002 -- iter: 1024/1168\n",
      "Training Step: 226  | total loss: \u001b[1m\u001b[32m0.02760\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 012 | loss: 0.02760 - R2: 1.0002 -- iter: 1088/1168\n",
      "Training Step: 227  | total loss: \u001b[1m\u001b[32m0.04657\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 012 | loss: 0.04657 - R2: 0.9991 -- iter: 1152/1168\n",
      "Training Step: 228  | total loss: \u001b[1m\u001b[32m0.04391\u001b[0m\u001b[0m | time: 1.120s\n",
      "| SGD | epoch: 012 | loss: 0.04391 - R2: 0.9991 | val_loss: 0.02858 - val_acc: 1.0033 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 229  | total loss: \u001b[1m\u001b[32m0.04391\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 013 | loss: 0.04391 - R2: 0.9995 -- iter: 0064/1168\n",
      "Training Step: 230  | total loss: \u001b[1m\u001b[32m0.04038\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 013 | loss: 0.04038 - R2: 1.0001 -- iter: 0128/1168\n",
      "Training Step: 231  | total loss: \u001b[1m\u001b[32m0.03857\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 013 | loss: 0.03857 - R2: 0.9994 -- iter: 0192/1168\n",
      "Training Step: 232  | total loss: \u001b[1m\u001b[32m0.03626\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 013 | loss: 0.03626 - R2: 0.9994 -- iter: 0256/1168\n",
      "Training Step: 233  | total loss: \u001b[1m\u001b[32m0.03626\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 013 | loss: 0.03626 - R2: 0.9994 -- iter: 0320/1168\n",
      "Training Step: 234  | total loss: \u001b[1m\u001b[32m0.03553\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 013 | loss: 0.03553 - R2: 0.9997 -- iter: 0384/1168\n",
      "Training Step: 235  | total loss: \u001b[1m\u001b[32m0.03213\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 013 | loss: 0.03213 - R2: 0.9999 -- iter: 0448/1168\n",
      "Training Step: 236  | total loss: \u001b[1m\u001b[32m0.03213\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 013 | loss: 0.03213 - R2: 0.9999 -- iter: 0512/1168\n",
      "Training Step: 237  | total loss: \u001b[1m\u001b[32m0.03146\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 013 | loss: 0.03146 - R2: 1.0002 -- iter: 0576/1168\n",
      "Training Step: 238  | total loss: \u001b[1m\u001b[32m0.03045\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 013 | loss: 0.03045 - R2: 1.0001 -- iter: 0640/1168\n",
      "Training Step: 239  | total loss: \u001b[1m\u001b[32m0.02840\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 013 | loss: 0.02840 - R2: 0.9990 -- iter: 0704/1168\n",
      "Training Step: 240  | total loss: \u001b[1m\u001b[32m0.02697\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 013 | loss: 0.02697 - R2: 0.9986 -- iter: 0768/1168\n",
      "Training Step: 241  | total loss: \u001b[1m\u001b[32m0.02674\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 013 | loss: 0.02674 - R2: 0.9994 -- iter: 0832/1168\n",
      "Training Step: 242  | total loss: \u001b[1m\u001b[32m0.02538\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 013 | loss: 0.02538 - R2: 0.9994 -- iter: 0896/1168\n",
      "Training Step: 243  | total loss: \u001b[1m\u001b[32m0.02498\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 013 | loss: 0.02498 - R2: 1.0002 -- iter: 0960/1168\n",
      "Training Step: 244  | total loss: \u001b[1m\u001b[32m0.02498\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 013 | loss: 0.02498 - R2: 1.0002 -- iter: 1024/1168\n",
      "Training Step: 245  | total loss: \u001b[1m\u001b[32m0.02432\u001b[0m\u001b[0m | time: 0.185s\n",
      "| SGD | epoch: 013 | loss: 0.02432 - R2: 1.0005 -- iter: 1088/1168\n",
      "Training Step: 246  | total loss: \u001b[1m\u001b[32m0.02307\u001b[0m\u001b[0m | time: 0.195s\n",
      "| SGD | epoch: 013 | loss: 0.02307 - R2: 0.9997 -- iter: 1152/1168\n",
      "Training Step: 247  | total loss: \u001b[1m\u001b[32m0.04729\u001b[0m\u001b[0m | time: 1.203s\n",
      "| SGD | epoch: 013 | loss: 0.04729 - R2: 1.0004 | val_loss: 0.02748 - val_acc: 1.0005 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 248  | total loss: \u001b[1m\u001b[32m0.04624\u001b[0m\u001b[0m | time: 0.014s\n",
      "| SGD | epoch: 014 | loss: 0.04624 - R2: 1.0006 -- iter: 0064/1168\n",
      "Training Step: 249  | total loss: \u001b[1m\u001b[32m0.04364\u001b[0m\u001b[0m | time: 0.018s\n",
      "| SGD | epoch: 014 | loss: 0.04364 - R2: 1.0000 -- iter: 0128/1168\n",
      "Training Step: 250  | total loss: \u001b[1m\u001b[32m0.04139\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 014 | loss: 0.04139 - R2: 0.9996 -- iter: 0192/1168\n",
      "Training Step: 251  | total loss: \u001b[1m\u001b[32m0.03867\u001b[0m\u001b[0m | time: 0.026s\n",
      "| SGD | epoch: 014 | loss: 0.03867 - R2: 0.9996 -- iter: 0256/1168\n",
      "Training Step: 252  | total loss: \u001b[1m\u001b[32m0.03712\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 014 | loss: 0.03712 - R2: 0.9997 -- iter: 0320/1168\n",
      "Training Step: 253  | total loss: \u001b[1m\u001b[32m0.03479\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 014 | loss: 0.03479 - R2: 0.9996 -- iter: 0384/1168\n",
      "Training Step: 254  | total loss: \u001b[1m\u001b[32m0.03298\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 014 | loss: 0.03298 - R2: 0.9997 -- iter: 0448/1168\n",
      "Training Step: 255  | total loss: \u001b[1m\u001b[32m0.03098\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 014 | loss: 0.03098 - R2: 0.9995 -- iter: 0512/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 256  | total loss: \u001b[1m\u001b[32m0.02952\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 014 | loss: 0.02952 - R2: 0.9996 -- iter: 0576/1168\n",
      "Training Step: 257  | total loss: \u001b[1m\u001b[32m0.02845\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 014 | loss: 0.02845 - R2: 0.9998 -- iter: 0640/1168\n",
      "Training Step: 258  | total loss: \u001b[1m\u001b[32m0.02696\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 014 | loss: 0.02696 - R2: 0.9999 -- iter: 0704/1168\n",
      "Training Step: 259  | total loss: \u001b[1m\u001b[32m0.02663\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 014 | loss: 0.02663 - R2: 0.9998 -- iter: 0768/1168\n",
      "Training Step: 260  | total loss: \u001b[1m\u001b[32m0.02584\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 014 | loss: 0.02584 - R2: 0.9998 -- iter: 0832/1168\n",
      "Training Step: 261  | total loss: \u001b[1m\u001b[32m0.02501\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 014 | loss: 0.02501 - R2: 0.9998 -- iter: 0896/1168\n",
      "Training Step: 262  | total loss: \u001b[1m\u001b[32m0.02501\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 014 | loss: 0.02501 - R2: 0.9998 -- iter: 0960/1168\n",
      "Training Step: 263  | total loss: \u001b[1m\u001b[32m0.02476\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 014 | loss: 0.02476 - R2: 1.0001 -- iter: 1024/1168\n",
      "Training Step: 264  | total loss: \u001b[1m\u001b[32m0.02385\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 014 | loss: 0.02385 - R2: 0.9997 -- iter: 1088/1168\n",
      "Training Step: 265  | total loss: \u001b[1m\u001b[32m0.02385\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 014 | loss: 0.02385 - R2: 0.9997 -- iter: 1152/1168\n",
      "Training Step: 266  | total loss: \u001b[1m\u001b[32m0.02334\u001b[0m\u001b[0m | time: 1.161s\n",
      "| SGD | epoch: 014 | loss: 0.02334 - R2: 0.9996 | val_loss: 0.02749 - val_acc: 1.0020 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 267  | total loss: \u001b[1m\u001b[32m0.02376\u001b[0m\u001b[0m | time: 0.013s\n",
      "| SGD | epoch: 015 | loss: 0.02376 - R2: 1.0003 -- iter: 0064/1168\n",
      "Training Step: 268  | total loss: \u001b[1m\u001b[32m0.05185\u001b[0m\u001b[0m | time: 0.016s\n",
      "| SGD | epoch: 015 | loss: 0.05185 - R2: 0.9980 -- iter: 0128/1168\n",
      "Training Step: 269  | total loss: \u001b[1m\u001b[32m0.04878\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 015 | loss: 0.04878 - R2: 0.9988 -- iter: 0192/1168\n",
      "Training Step: 270  | total loss: \u001b[1m\u001b[32m0.04604\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 015 | loss: 0.04604 - R2: 0.9992 -- iter: 0256/1168\n",
      "Training Step: 271  | total loss: \u001b[1m\u001b[32m0.04340\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 015 | loss: 0.04340 - R2: 0.9996 -- iter: 0320/1168\n",
      "Training Step: 272  | total loss: \u001b[1m\u001b[32m0.04224\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 015 | loss: 0.04224 - R2: 1.0001 -- iter: 0384/1168\n",
      "Training Step: 273  | total loss: \u001b[1m\u001b[32m0.04107\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 015 | loss: 0.04107 - R2: 1.0002 -- iter: 0448/1168\n",
      "Training Step: 274  | total loss: \u001b[1m\u001b[32m0.03856\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 015 | loss: 0.03856 - R2: 0.9998 -- iter: 0512/1168\n",
      "Training Step: 275  | total loss: \u001b[1m\u001b[32m0.03590\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 015 | loss: 0.03590 - R2: 1.0002 -- iter: 0576/1168\n",
      "Training Step: 276  | total loss: \u001b[1m\u001b[32m0.03542\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 015 | loss: 0.03542 - R2: 0.9997 -- iter: 0640/1168\n",
      "Training Step: 277  | total loss: \u001b[1m\u001b[32m0.03375\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 015 | loss: 0.03375 - R2: 0.9997 -- iter: 0704/1168\n",
      "Training Step: 278  | total loss: \u001b[1m\u001b[32m0.03076\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 015 | loss: 0.03076 - R2: 0.9994 -- iter: 0768/1168\n",
      "Training Step: 279  | total loss: \u001b[1m\u001b[32m0.02933\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 015 | loss: 0.02933 - R2: 0.9999 -- iter: 0832/1168\n",
      "Training Step: 280  | total loss: \u001b[1m\u001b[32m0.02933\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 015 | loss: 0.02933 - R2: 0.9999 -- iter: 0896/1168\n",
      "Training Step: 281  | total loss: \u001b[1m\u001b[32m0.02770\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 015 | loss: 0.02770 - R2: 1.0001 -- iter: 0960/1168\n",
      "Training Step: 282  | total loss: \u001b[1m\u001b[32m0.02699\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 015 | loss: 0.02699 - R2: 1.0002 -- iter: 1024/1168\n",
      "Training Step: 283  | total loss: \u001b[1m\u001b[32m0.02699\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 015 | loss: 0.02699 - R2: 1.0000 -- iter: 1088/1168\n",
      "Training Step: 284  | total loss: \u001b[1m\u001b[32m0.02493\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 015 | loss: 0.02493 - R2: 1.0001 -- iter: 1152/1168\n",
      "Training Step: 285  | total loss: \u001b[1m\u001b[32m0.02493\u001b[0m\u001b[0m | time: 1.141s\n",
      "| SGD | epoch: 015 | loss: 0.02493 - R2: 1.0001 | val_loss: 0.02770 - val_acc: 1.0019 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 286  | total loss: \u001b[1m\u001b[32m0.02280\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 016 | loss: 0.02280 - R2: 0.9999 -- iter: 0064/1168\n",
      "Training Step: 287  | total loss: \u001b[1m\u001b[32m0.02210\u001b[0m\u001b[0m | time: 0.019s\n",
      "| SGD | epoch: 016 | loss: 0.02210 - R2: 0.9999 -- iter: 0128/1168\n",
      "Training Step: 288  | total loss: \u001b[1m\u001b[32m0.02436\u001b[0m\u001b[0m | time: 0.023s\n",
      "| SGD | epoch: 016 | loss: 0.02436 - R2: 1.0006 -- iter: 0192/1168\n",
      "Training Step: 289  | total loss: \u001b[1m\u001b[32m0.02456\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 016 | loss: 0.02456 - R2: 1.0000 -- iter: 0256/1168\n",
      "Training Step: 290  | total loss: \u001b[1m\u001b[32m0.02456\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 016 | loss: 0.02456 - R2: 1.0002 -- iter: 0320/1168\n",
      "Training Step: 291  | total loss: \u001b[1m\u001b[32m0.02395\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 016 | loss: 0.02395 - R2: 1.0003 -- iter: 0384/1168\n",
      "Training Step: 292  | total loss: \u001b[1m\u001b[32m0.02352\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 016 | loss: 0.02352 - R2: 0.9998 -- iter: 0448/1168\n",
      "Training Step: 293  | total loss: \u001b[1m\u001b[32m0.02268\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 016 | loss: 0.02268 - R2: 0.9999 -- iter: 0512/1168\n",
      "Training Step: 294  | total loss: \u001b[1m\u001b[32m0.02268\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 016 | loss: 0.02268 - R2: 0.9999 -- iter: 0576/1168\n",
      "Training Step: 295  | total loss: \u001b[1m\u001b[32m0.02160\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 016 | loss: 0.02160 - R2: 0.9994 -- iter: 0640/1168\n",
      "Training Step: 296  | total loss: \u001b[1m\u001b[32m0.02153\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 016 | loss: 0.02153 - R2: 1.0000 -- iter: 0704/1168\n",
      "Training Step: 297  | total loss: \u001b[1m\u001b[32m0.02077\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 016 | loss: 0.02077 - R2: 1.0000 -- iter: 0768/1168\n",
      "Training Step: 298  | total loss: \u001b[1m\u001b[32m0.02077\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 016 | loss: 0.02077 - R2: 1.0000 -- iter: 0832/1168\n",
      "Training Step: 299  | total loss: \u001b[1m\u001b[32m0.01948\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 016 | loss: 0.01948 - R2: 0.9995 -- iter: 0896/1168\n",
      "Training Step: 300  | total loss: \u001b[1m\u001b[32m0.01948\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 016 | loss: 0.01948 - R2: 0.9995 -- iter: 0960/1168\n",
      "Training Step: 301  | total loss: \u001b[1m\u001b[32m0.01799\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 016 | loss: 0.01799 - R2: 0.9994 -- iter: 1024/1168\n",
      "Training Step: 302  | total loss: \u001b[1m\u001b[32m0.01804\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 016 | loss: 0.01804 - R2: 0.9999 -- iter: 1088/1168\n",
      "Training Step: 303  | total loss: \u001b[1m\u001b[32m0.01813\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 016 | loss: 0.01813 - R2: 1.0002 -- iter: 1152/1168\n",
      "Training Step: 304  | total loss: \u001b[1m\u001b[32m0.01751\u001b[0m\u001b[0m | time: 1.108s\n",
      "| SGD | epoch: 016 | loss: 0.01751 - R2: 0.9999 | val_loss: 0.02811 - val_acc: 1.0032 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 305  | total loss: \u001b[1m\u001b[32m0.01783\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 017 | loss: 0.01783 - R2: 0.9999 -- iter: 0064/1168\n",
      "Training Step: 306  | total loss: \u001b[1m\u001b[32m0.01783\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 017 | loss: 0.01783 - R2: 0.9999 -- iter: 0128/1168\n",
      "Training Step: 307  | total loss: \u001b[1m\u001b[32m0.01696\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 017 | loss: 0.01696 - R2: 0.9997 -- iter: 0192/1168\n",
      "Training Step: 308  | total loss: \u001b[1m\u001b[32m0.01653\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 017 | loss: 0.01653 - R2: 1.0000 -- iter: 0256/1168\n",
      "Training Step: 309  | total loss: \u001b[1m\u001b[32m0.01653\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 017 | loss: 0.01653 - R2: 1.0000 -- iter: 0320/1168\n",
      "Training Step: 310  | total loss: \u001b[1m\u001b[32m0.01654\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 017 | loss: 0.01654 - R2: 1.0005 -- iter: 0384/1168\n",
      "Training Step: 311  | total loss: \u001b[1m\u001b[32m0.01636\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 017 | loss: 0.01636 - R2: 1.0003 -- iter: 0448/1168\n",
      "Training Step: 312  | total loss: \u001b[1m\u001b[32m0.01614\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 017 | loss: 0.01614 - R2: 0.9999 -- iter: 0512/1168\n",
      "Training Step: 313  | total loss: \u001b[1m\u001b[32m0.01564\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 017 | loss: 0.01564 - R2: 1.0000 -- iter: 0576/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 314  | total loss: \u001b[1m\u001b[32m0.01647\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 017 | loss: 0.01647 - R2: 0.9997 -- iter: 0640/1168\n",
      "Training Step: 315  | total loss: \u001b[1m\u001b[32m0.01647\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 017 | loss: 0.01647 - R2: 0.9997 -- iter: 0704/1168\n",
      "Training Step: 316  | total loss: \u001b[1m\u001b[32m0.01760\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 017 | loss: 0.01760 - R2: 1.0000 -- iter: 0768/1168\n",
      "Training Step: 317  | total loss: \u001b[1m\u001b[32m0.01834\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 017 | loss: 0.01834 - R2: 1.0003 -- iter: 0832/1168\n",
      "Training Step: 318  | total loss: \u001b[1m\u001b[32m0.02067\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 017 | loss: 0.02067 - R2: 1.0003 -- iter: 0896/1168\n",
      "Training Step: 319  | total loss: \u001b[1m\u001b[32m0.02059\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 017 | loss: 0.02059 - R2: 0.9994 -- iter: 0960/1168\n",
      "Training Step: 320  | total loss: \u001b[1m\u001b[32m0.02011\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 017 | loss: 0.02011 - R2: 0.9992 -- iter: 1024/1168\n",
      "Training Step: 321  | total loss: \u001b[1m\u001b[32m0.01922\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 017 | loss: 0.01922 - R2: 0.9997 -- iter: 1088/1168\n",
      "Training Step: 322  | total loss: \u001b[1m\u001b[32m0.01868\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 017 | loss: 0.01868 - R2: 0.9995 -- iter: 1152/1168\n",
      "Training Step: 323  | total loss: \u001b[1m\u001b[32m0.01868\u001b[0m\u001b[0m | time: 1.155s\n",
      "| SGD | epoch: 017 | loss: 0.01868 - R2: 0.9995 | val_loss: 0.02812 - val_acc: 1.0010 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 324  | total loss: \u001b[1m\u001b[32m0.02073\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 018 | loss: 0.02073 - R2: 1.0004 -- iter: 0064/1168\n",
      "Training Step: 325  | total loss: \u001b[1m\u001b[32m0.01917\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 018 | loss: 0.01917 - R2: 0.9993 -- iter: 0128/1168\n",
      "Training Step: 326  | total loss: \u001b[1m\u001b[32m0.01917\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 018 | loss: 0.01917 - R2: 0.9993 -- iter: 0192/1168\n",
      "Training Step: 327  | total loss: \u001b[1m\u001b[32m0.01895\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 018 | loss: 0.01895 - R2: 0.9995 -- iter: 0256/1168\n",
      "Training Step: 328  | total loss: \u001b[1m\u001b[32m0.01867\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 018 | loss: 0.01867 - R2: 0.9997 -- iter: 0320/1168\n",
      "Training Step: 329  | total loss: \u001b[1m\u001b[32m0.01867\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 018 | loss: 0.01867 - R2: 0.9997 -- iter: 0384/1168\n",
      "Training Step: 330  | total loss: \u001b[1m\u001b[32m0.01744\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 018 | loss: 0.01744 - R2: 0.9998 -- iter: 0448/1168\n",
      "Training Step: 331  | total loss: \u001b[1m\u001b[32m0.01728\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 018 | loss: 0.01728 - R2: 0.9997 -- iter: 0512/1168\n",
      "Training Step: 332  | total loss: \u001b[1m\u001b[32m0.01977\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 018 | loss: 0.01977 - R2: 1.0003 -- iter: 0576/1168\n",
      "Training Step: 333  | total loss: \u001b[1m\u001b[32m0.01991\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 018 | loss: 0.01991 - R2: 1.0006 -- iter: 0640/1168\n",
      "Training Step: 334  | total loss: \u001b[1m\u001b[32m0.01991\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 018 | loss: 0.01991 - R2: 1.0006 -- iter: 0704/1168\n",
      "Training Step: 335  | total loss: \u001b[1m\u001b[32m0.01975\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 018 | loss: 0.01975 - R2: 1.0006 -- iter: 0768/1168\n",
      "Training Step: 336  | total loss: \u001b[1m\u001b[32m0.01843\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 018 | loss: 0.01843 - R2: 1.0001 -- iter: 0832/1168\n",
      "Training Step: 337  | total loss: \u001b[1m\u001b[32m0.01936\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 018 | loss: 0.01936 - R2: 1.0001 -- iter: 0896/1168\n",
      "Training Step: 338  | total loss: \u001b[1m\u001b[32m0.01936\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 018 | loss: 0.01936 - R2: 1.0001 -- iter: 0960/1168\n",
      "Training Step: 339  | total loss: \u001b[1m\u001b[32m0.01911\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 018 | loss: 0.01911 - R2: 0.9999 -- iter: 1024/1168\n",
      "Training Step: 340  | total loss: \u001b[1m\u001b[32m0.01922\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 018 | loss: 0.01922 - R2: 1.0004 -- iter: 1088/1168\n",
      "Training Step: 341  | total loss: \u001b[1m\u001b[32m0.01901\u001b[0m\u001b[0m | time: 0.182s\n",
      "| SGD | epoch: 018 | loss: 0.01901 - R2: 1.0007 -- iter: 1152/1168\n",
      "Training Step: 342  | total loss: \u001b[1m\u001b[32m0.01877\u001b[0m\u001b[0m | time: 1.188s\n",
      "| SGD | epoch: 018 | loss: 0.01877 - R2: 0.9999 | val_loss: 0.02828 - val_acc: 1.0019 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 343  | total loss: \u001b[1m\u001b[32m0.01807\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 019 | loss: 0.01807 - R2: 1.0000 -- iter: 0064/1168\n",
      "Training Step: 344  | total loss: \u001b[1m\u001b[32m0.01991\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 019 | loss: 0.01991 - R2: 0.9999 -- iter: 0128/1168\n",
      "Training Step: 345  | total loss: \u001b[1m\u001b[32m0.01994\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 019 | loss: 0.01994 - R2: 0.9998 -- iter: 0192/1168\n",
      "Training Step: 346  | total loss: \u001b[1m\u001b[32m0.02042\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 019 | loss: 0.02042 - R2: 0.9997 -- iter: 0256/1168\n",
      "Training Step: 347  | total loss: \u001b[1m\u001b[32m0.04766\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 019 | loss: 0.04766 - R2: 0.9991 -- iter: 0320/1168\n",
      "Training Step: 348  | total loss: \u001b[1m\u001b[32m0.04766\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 019 | loss: 0.04766 - R2: 0.9996 -- iter: 0384/1168\n",
      "Training Step: 349  | total loss: \u001b[1m\u001b[32m0.04303\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 019 | loss: 0.04303 - R2: 0.9991 -- iter: 0448/1168\n",
      "Training Step: 350  | total loss: \u001b[1m\u001b[32m0.04339\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 019 | loss: 0.04339 - R2: 0.9991 -- iter: 0512/1168\n",
      "Training Step: 351  | total loss: \u001b[1m\u001b[32m0.04339\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 019 | loss: 0.04339 - R2: 0.9991 -- iter: 0576/1168\n",
      "Training Step: 352  | total loss: \u001b[1m\u001b[32m0.03817\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 019 | loss: 0.03817 - R2: 0.9994 -- iter: 0640/1168\n",
      "Training Step: 353  | total loss: \u001b[1m\u001b[32m0.03817\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 019 | loss: 0.03817 - R2: 0.9994 -- iter: 0704/1168\n",
      "Training Step: 354  | total loss: \u001b[1m\u001b[32m0.03286\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 019 | loss: 0.03286 - R2: 0.9991 -- iter: 0768/1168\n",
      "Training Step: 355  | total loss: \u001b[1m\u001b[32m0.03203\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 019 | loss: 0.03203 - R2: 0.9990 -- iter: 0832/1168\n",
      "Training Step: 356  | total loss: \u001b[1m\u001b[32m0.03203\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 019 | loss: 0.03203 - R2: 0.9990 -- iter: 0896/1168\n",
      "Training Step: 357  | total loss: \u001b[1m\u001b[32m0.03057\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 019 | loss: 0.03057 - R2: 0.9996 -- iter: 0960/1168\n",
      "Training Step: 358  | total loss: \u001b[1m\u001b[32m0.02787\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 019 | loss: 0.02787 - R2: 0.9997 -- iter: 1024/1168\n",
      "Training Step: 359  | total loss: \u001b[1m\u001b[32m0.02591\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 019 | loss: 0.02591 - R2: 0.9993 -- iter: 1088/1168\n",
      "Training Step: 360  | total loss: \u001b[1m\u001b[32m0.02591\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 019 | loss: 0.02591 - R2: 0.9993 -- iter: 1152/1168\n",
      "Training Step: 361  | total loss: \u001b[1m\u001b[32m0.02396\u001b[0m\u001b[0m | time: 1.129s\n",
      "| SGD | epoch: 019 | loss: 0.02396 - R2: 0.9994 | val_loss: 0.02854 - val_acc: 1.0058 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 362  | total loss: \u001b[1m\u001b[32m0.02432\u001b[0m\u001b[0m | time: 0.016s\n",
      "| SGD | epoch: 020 | loss: 0.02432 - R2: 0.9994 -- iter: 0064/1168\n",
      "Training Step: 363  | total loss: \u001b[1m\u001b[32m0.02365\u001b[0m\u001b[0m | time: 0.020s\n",
      "| SGD | epoch: 020 | loss: 0.02365 - R2: 1.0001 -- iter: 0128/1168\n",
      "Training Step: 364  | total loss: \u001b[1m\u001b[32m0.02290\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 020 | loss: 0.02290 - R2: 1.0004 -- iter: 0192/1168\n",
      "Training Step: 365  | total loss: \u001b[1m\u001b[32m0.02175\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 020 | loss: 0.02175 - R2: 1.0004 -- iter: 0256/1168\n",
      "Training Step: 366  | total loss: \u001b[1m\u001b[32m0.02207\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 020 | loss: 0.02207 - R2: 1.0002 -- iter: 0320/1168\n",
      "Training Step: 367  | total loss: \u001b[1m\u001b[32m0.02207\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 020 | loss: 0.02207 - R2: 1.0002 -- iter: 0384/1168\n",
      "Training Step: 368  | total loss: \u001b[1m\u001b[32m0.02126\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 020 | loss: 0.02126 - R2: 1.0001 -- iter: 0448/1168\n",
      "Training Step: 369  | total loss: \u001b[1m\u001b[32m0.02024\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 020 | loss: 0.02024 - R2: 0.9999 -- iter: 0512/1168\n",
      "Training Step: 370  | total loss: \u001b[1m\u001b[32m0.02024\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 020 | loss: 0.02024 - R2: 0.9999 -- iter: 0576/1168\n",
      "Training Step: 371  | total loss: \u001b[1m\u001b[32m0.01990\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 020 | loss: 0.01990 - R2: 0.9999 -- iter: 0640/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 372  | total loss: \u001b[1m\u001b[32m0.02078\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 020 | loss: 0.02078 - R2: 1.0001 -- iter: 0704/1168\n",
      "Training Step: 373  | total loss: \u001b[1m\u001b[32m0.02230\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 020 | loss: 0.02230 - R2: 1.0005 -- iter: 0768/1168\n",
      "Training Step: 374  | total loss: \u001b[1m\u001b[32m0.02146\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 020 | loss: 0.02146 - R2: 1.0002 -- iter: 0832/1168\n",
      "Training Step: 375  | total loss: \u001b[1m\u001b[32m0.02228\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 020 | loss: 0.02228 - R2: 0.9999 -- iter: 0896/1168\n",
      "Training Step: 376  | total loss: \u001b[1m\u001b[32m0.02269\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 020 | loss: 0.02269 - R2: 0.9998 -- iter: 0960/1168\n",
      "Training Step: 377  | total loss: \u001b[1m\u001b[32m0.02269\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 020 | loss: 0.02269 - R2: 0.9998 -- iter: 1024/1168\n",
      "Training Step: 378  | total loss: \u001b[1m\u001b[32m0.02189\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 020 | loss: 0.02189 - R2: 0.9995 -- iter: 1088/1168\n",
      "Training Step: 379  | total loss: \u001b[1m\u001b[32m0.02215\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 020 | loss: 0.02215 - R2: 1.0000 -- iter: 1152/1168\n",
      "Training Step: 380  | total loss: \u001b[1m\u001b[32m0.02079\u001b[0m\u001b[0m | time: 1.131s\n",
      "| SGD | epoch: 020 | loss: 0.02079 - R2: 0.9991 | val_loss: 0.02889 - val_acc: 1.0074 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 381  | total loss: \u001b[1m\u001b[32m0.01923\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 021 | loss: 0.01923 - R2: 0.9996 -- iter: 0064/1168\n",
      "Training Step: 382  | total loss: \u001b[1m\u001b[32m0.01894\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 021 | loss: 0.01894 - R2: 0.9996 -- iter: 0128/1168\n",
      "Training Step: 383  | total loss: \u001b[1m\u001b[32m0.01869\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 021 | loss: 0.01869 - R2: 0.9997 -- iter: 0192/1168\n",
      "Training Step: 384  | total loss: \u001b[1m\u001b[32m0.01817\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 021 | loss: 0.01817 - R2: 0.9996 -- iter: 0256/1168\n",
      "Training Step: 385  | total loss: \u001b[1m\u001b[32m0.01762\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 021 | loss: 0.01762 - R2: 0.9997 -- iter: 0320/1168\n",
      "Training Step: 386  | total loss: \u001b[1m\u001b[32m0.01912\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 021 | loss: 0.01912 - R2: 0.9998 -- iter: 0384/1168\n",
      "Training Step: 387  | total loss: \u001b[1m\u001b[32m0.05439\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 021 | loss: 0.05439 - R2: 0.9993 -- iter: 0448/1168\n",
      "Training Step: 388  | total loss: \u001b[1m\u001b[32m0.05439\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 021 | loss: 0.05439 - R2: 0.9993 -- iter: 0512/1168\n",
      "Training Step: 389  | total loss: \u001b[1m\u001b[32m0.05075\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 021 | loss: 0.05075 - R2: 0.9993 -- iter: 0576/1168\n",
      "Training Step: 390  | total loss: \u001b[1m\u001b[32m0.04726\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 021 | loss: 0.04726 - R2: 0.9997 -- iter: 0640/1168\n",
      "Training Step: 391  | total loss: \u001b[1m\u001b[32m0.04355\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 021 | loss: 0.04355 - R2: 0.9995 -- iter: 0704/1168\n",
      "Training Step: 392  | total loss: \u001b[1m\u001b[32m0.04342\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 021 | loss: 0.04342 - R2: 1.0003 -- iter: 0768/1168\n",
      "Training Step: 393  | total loss: \u001b[1m\u001b[32m0.04062\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 021 | loss: 0.04062 - R2: 0.9997 -- iter: 0832/1168\n",
      "Training Step: 394  | total loss: \u001b[1m\u001b[32m0.03807\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 021 | loss: 0.03807 - R2: 0.9999 -- iter: 0896/1168\n",
      "Training Step: 395  | total loss: \u001b[1m\u001b[32m0.03326\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 021 | loss: 0.03326 - R2: 0.9999 -- iter: 0960/1168\n",
      "Training Step: 396  | total loss: \u001b[1m\u001b[32m0.03326\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 021 | loss: 0.03326 - R2: 0.9999 -- iter: 1024/1168\n",
      "Training Step: 397  | total loss: \u001b[1m\u001b[32m0.03168\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 021 | loss: 0.03168 - R2: 0.9994 -- iter: 1088/1168\n",
      "Training Step: 398  | total loss: \u001b[1m\u001b[32m0.03006\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 021 | loss: 0.03006 - R2: 0.9993 -- iter: 1152/1168\n",
      "Training Step: 399  | total loss: \u001b[1m\u001b[32m0.03138\u001b[0m\u001b[0m | time: 1.147s\n",
      "| SGD | epoch: 021 | loss: 0.03138 - R2: 0.9997 | val_loss: 0.02699 - val_acc: 1.0027 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 400  | total loss: \u001b[1m\u001b[32m0.02721\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 022 | loss: 0.02721 - R2: 1.0002 -- iter: 0064/1168\n",
      "Training Step: 401  | total loss: \u001b[1m\u001b[32m0.02721\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 022 | loss: 0.02721 - R2: 1.0002 -- iter: 0128/1168\n",
      "Training Step: 402  | total loss: \u001b[1m\u001b[32m0.02678\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 022 | loss: 0.02678 - R2: 0.9999 -- iter: 0192/1168\n",
      "Training Step: 403  | total loss: \u001b[1m\u001b[32m0.02556\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 022 | loss: 0.02556 - R2: 0.9997 -- iter: 0256/1168\n",
      "Training Step: 404  | total loss: \u001b[1m\u001b[32m0.02511\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 022 | loss: 0.02511 - R2: 1.0005 -- iter: 0320/1168\n",
      "Training Step: 405  | total loss: \u001b[1m\u001b[32m0.02458\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 022 | loss: 0.02458 - R2: 1.0001 -- iter: 0384/1168\n",
      "Training Step: 406  | total loss: \u001b[1m\u001b[32m0.02395\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 022 | loss: 0.02395 - R2: 1.0003 -- iter: 0448/1168\n",
      "Training Step: 407  | total loss: \u001b[1m\u001b[32m0.02348\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 022 | loss: 0.02348 - R2: 0.9999 -- iter: 0512/1168\n",
      "Training Step: 408  | total loss: \u001b[1m\u001b[32m0.02276\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 022 | loss: 0.02276 - R2: 0.9997 -- iter: 0576/1168\n",
      "Training Step: 409  | total loss: \u001b[1m\u001b[32m0.02212\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 022 | loss: 0.02212 - R2: 0.9999 -- iter: 0640/1168\n",
      "Training Step: 410  | total loss: \u001b[1m\u001b[32m0.02100\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 022 | loss: 0.02100 - R2: 0.9997 -- iter: 0704/1168\n",
      "Training Step: 411  | total loss: \u001b[1m\u001b[32m0.04810\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 022 | loss: 0.04810 - R2: 1.0043 -- iter: 0768/1168\n",
      "Training Step: 412  | total loss: \u001b[1m\u001b[32m0.04810\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 022 | loss: 0.04810 - R2: 1.0043 -- iter: 0832/1168\n",
      "Training Step: 413  | total loss: \u001b[1m\u001b[32m0.04395\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 022 | loss: 0.04395 - R2: 1.0007 -- iter: 0896/1168\n",
      "Training Step: 414  | total loss: \u001b[1m\u001b[32m0.04395\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 022 | loss: 0.04395 - R2: 1.0007 -- iter: 0960/1168\n",
      "Training Step: 415  | total loss: \u001b[1m\u001b[32m0.03935\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 022 | loss: 0.03935 - R2: 0.9993 -- iter: 1024/1168\n",
      "Training Step: 416  | total loss: \u001b[1m\u001b[32m0.03672\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 022 | loss: 0.03672 - R2: 0.9994 -- iter: 1088/1168\n",
      "Training Step: 417  | total loss: \u001b[1m\u001b[32m0.03422\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 022 | loss: 0.03422 - R2: 0.9990 -- iter: 1152/1168\n",
      "Training Step: 418  | total loss: \u001b[1m\u001b[32m0.03407\u001b[0m\u001b[0m | time: 1.159s\n",
      "| SGD | epoch: 022 | loss: 0.03407 - R2: 0.9995 | val_loss: 0.02660 - val_acc: 1.0020 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 419  | total loss: \u001b[1m\u001b[32m0.03158\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 023 | loss: 0.03158 - R2: 0.9996 -- iter: 0064/1168\n",
      "Training Step: 420  | total loss: \u001b[1m\u001b[32m0.03158\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 023 | loss: 0.03158 - R2: 0.9996 -- iter: 0128/1168\n",
      "Training Step: 421  | total loss: \u001b[1m\u001b[32m0.02919\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 023 | loss: 0.02919 - R2: 0.9997 -- iter: 0192/1168\n",
      "Training Step: 422  | total loss: \u001b[1m\u001b[32m0.02873\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 023 | loss: 0.02873 - R2: 0.9998 -- iter: 0256/1168\n",
      "Training Step: 423  | total loss: \u001b[1m\u001b[32m0.02705\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 023 | loss: 0.02705 - R2: 1.0000 -- iter: 0320/1168\n",
      "Training Step: 424  | total loss: \u001b[1m\u001b[32m0.02716\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 023 | loss: 0.02716 - R2: 0.9994 -- iter: 0384/1168\n",
      "Training Step: 425  | total loss: \u001b[1m\u001b[32m0.02754\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 023 | loss: 0.02754 - R2: 0.9998 -- iter: 0448/1168\n",
      "Training Step: 426  | total loss: \u001b[1m\u001b[32m0.02732\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 023 | loss: 0.02732 - R2: 0.9998 -- iter: 0512/1168\n",
      "Training Step: 427  | total loss: \u001b[1m\u001b[32m0.02659\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 023 | loss: 0.02659 - R2: 0.9996 -- iter: 0576/1168\n",
      "Training Step: 428  | total loss: \u001b[1m\u001b[32m0.04860\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 023 | loss: 0.04860 - R2: 1.0001 -- iter: 0640/1168\n",
      "Training Step: 429  | total loss: \u001b[1m\u001b[32m0.04546\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 023 | loss: 0.04546 - R2: 0.9991 -- iter: 0704/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 430  | total loss: \u001b[1m\u001b[32m0.04278\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 023 | loss: 0.04278 - R2: 0.9991 -- iter: 0768/1168\n",
      "Training Step: 431  | total loss: \u001b[1m\u001b[32m0.03748\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 023 | loss: 0.03748 - R2: 0.9994 -- iter: 0832/1168\n",
      "Training Step: 432  | total loss: \u001b[1m\u001b[32m0.03588\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 023 | loss: 0.03588 - R2: 0.9995 -- iter: 0896/1168\n",
      "Training Step: 433  | total loss: \u001b[1m\u001b[32m0.03588\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 023 | loss: 0.03588 - R2: 0.9995 -- iter: 0960/1168\n",
      "Training Step: 434  | total loss: \u001b[1m\u001b[32m0.03394\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 023 | loss: 0.03394 - R2: 0.9995 -- iter: 1024/1168\n",
      "Training Step: 435  | total loss: \u001b[1m\u001b[32m0.03268\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 023 | loss: 0.03268 - R2: 0.9996 -- iter: 1088/1168\n",
      "Training Step: 436  | total loss: \u001b[1m\u001b[32m0.03104\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 023 | loss: 0.03104 - R2: 0.9996 -- iter: 1152/1168\n",
      "Training Step: 437  | total loss: \u001b[1m\u001b[32m0.03095\u001b[0m\u001b[0m | time: 1.113s\n",
      "| SGD | epoch: 023 | loss: 0.03095 - R2: 0.9993 | val_loss: 0.02758 - val_acc: 1.0041 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 438  | total loss: \u001b[1m\u001b[32m0.03095\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 024 | loss: 0.03095 - R2: 0.9993 -- iter: 0064/1168\n",
      "Training Step: 439  | total loss: \u001b[1m\u001b[32m0.02880\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 024 | loss: 0.02880 - R2: 0.9993 -- iter: 0128/1168\n",
      "Training Step: 440  | total loss: \u001b[1m\u001b[32m0.02880\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 024 | loss: 0.02880 - R2: 0.9993 -- iter: 0192/1168\n",
      "Training Step: 441  | total loss: \u001b[1m\u001b[32m0.02773\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 024 | loss: 0.02773 - R2: 0.9993 -- iter: 0256/1168\n",
      "Training Step: 442  | total loss: \u001b[1m\u001b[32m0.02623\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 024 | loss: 0.02623 - R2: 0.9999 -- iter: 0320/1168\n",
      "Training Step: 443  | total loss: \u001b[1m\u001b[32m0.02684\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 024 | loss: 0.02684 - R2: 1.0000 -- iter: 0384/1168\n",
      "Training Step: 444  | total loss: \u001b[1m\u001b[32m0.02524\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 024 | loss: 0.02524 - R2: 1.0001 -- iter: 0448/1168\n",
      "Training Step: 445  | total loss: \u001b[1m\u001b[32m0.02432\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 024 | loss: 0.02432 - R2: 1.0000 -- iter: 0512/1168\n",
      "Training Step: 446  | total loss: \u001b[1m\u001b[32m0.02490\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 024 | loss: 0.02490 - R2: 1.0000 -- iter: 0576/1168\n",
      "Training Step: 447  | total loss: \u001b[1m\u001b[32m0.02396\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 024 | loss: 0.02396 - R2: 0.9999 -- iter: 0640/1168\n",
      "Training Step: 448  | total loss: \u001b[1m\u001b[32m0.02286\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 024 | loss: 0.02286 - R2: 1.0004 -- iter: 0704/1168\n",
      "Training Step: 449  | total loss: \u001b[1m\u001b[32m0.02207\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 024 | loss: 0.02207 - R2: 0.9996 -- iter: 0768/1168\n",
      "Training Step: 450  | total loss: \u001b[1m\u001b[32m0.02207\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 024 | loss: 0.02207 - R2: 0.9996 -- iter: 0832/1168\n",
      "Training Step: 451  | total loss: \u001b[1m\u001b[32m0.02133\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 024 | loss: 0.02133 - R2: 0.9994 -- iter: 0896/1168\n",
      "Training Step: 452  | total loss: \u001b[1m\u001b[32m0.02117\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 024 | loss: 0.02117 - R2: 0.9995 -- iter: 0960/1168\n",
      "Training Step: 453  | total loss: \u001b[1m\u001b[32m0.02168\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 024 | loss: 0.02168 - R2: 0.9997 -- iter: 1024/1168\n",
      "Training Step: 454  | total loss: \u001b[1m\u001b[32m0.02168\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 024 | loss: 0.02168 - R2: 0.9997 -- iter: 1088/1168\n",
      "Training Step: 455  | total loss: \u001b[1m\u001b[32m0.02169\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 024 | loss: 0.02169 - R2: 0.9996 -- iter: 1152/1168\n",
      "Training Step: 456  | total loss: \u001b[1m\u001b[32m0.02071\u001b[0m\u001b[0m | time: 1.125s\n",
      "| SGD | epoch: 024 | loss: 0.02071 - R2: 1.0001 | val_loss: 0.02664 - val_acc: 1.0022 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 457  | total loss: \u001b[1m\u001b[32m0.02071\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 025 | loss: 0.02071 - R2: 1.0001 -- iter: 0064/1168\n",
      "Training Step: 458  | total loss: \u001b[1m\u001b[32m0.02106\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 025 | loss: 0.02106 - R2: 1.0002 -- iter: 0128/1168\n",
      "Training Step: 459  | total loss: \u001b[1m\u001b[32m0.02031\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 025 | loss: 0.02031 - R2: 1.0006 -- iter: 0192/1168\n",
      "Training Step: 460  | total loss: \u001b[1m\u001b[32m0.01863\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 025 | loss: 0.01863 - R2: 1.0004 -- iter: 0256/1168\n",
      "Training Step: 461  | total loss: \u001b[1m\u001b[32m0.01701\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 025 | loss: 0.01701 - R2: 1.0003 -- iter: 0320/1168\n",
      "Training Step: 462  | total loss: \u001b[1m\u001b[32m0.01770\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 025 | loss: 0.01770 - R2: 0.9999 -- iter: 0384/1168\n",
      "Training Step: 463  | total loss: \u001b[1m\u001b[32m0.01819\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 025 | loss: 0.01819 - R2: 1.0003 -- iter: 0448/1168\n",
      "Training Step: 464  | total loss: \u001b[1m\u001b[32m0.01838\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 025 | loss: 0.01838 - R2: 1.0001 -- iter: 0512/1168\n",
      "Training Step: 465  | total loss: \u001b[1m\u001b[32m0.01846\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 025 | loss: 0.01846 - R2: 0.9996 -- iter: 0576/1168\n",
      "Training Step: 466  | total loss: \u001b[1m\u001b[32m0.01926\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 025 | loss: 0.01926 - R2: 0.9994 -- iter: 0640/1168\n",
      "Training Step: 467  | total loss: \u001b[1m\u001b[32m0.01891\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 025 | loss: 0.01891 - R2: 0.9991 -- iter: 0704/1168\n",
      "Training Step: 468  | total loss: \u001b[1m\u001b[32m0.03834\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 025 | loss: 0.03834 - R2: 1.0002 -- iter: 0768/1168\n",
      "Training Step: 469  | total loss: \u001b[1m\u001b[32m0.03345\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 025 | loss: 0.03345 - R2: 0.9996 -- iter: 0832/1168\n",
      "Training Step: 470  | total loss: \u001b[1m\u001b[32m0.03250\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 025 | loss: 0.03250 - R2: 0.9999 -- iter: 0896/1168\n",
      "Training Step: 471  | total loss: \u001b[1m\u001b[32m0.03050\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 025 | loss: 0.03050 - R2: 0.9996 -- iter: 0960/1168\n",
      "Training Step: 472  | total loss: \u001b[1m\u001b[32m0.03050\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 025 | loss: 0.03050 - R2: 0.9996 -- iter: 1024/1168\n",
      "Training Step: 473  | total loss: \u001b[1m\u001b[32m0.02901\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 025 | loss: 0.02901 - R2: 0.9996 -- iter: 1088/1168\n",
      "Training Step: 474  | total loss: \u001b[1m\u001b[32m0.02857\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 025 | loss: 0.02857 - R2: 0.9997 -- iter: 1152/1168\n",
      "Training Step: 475  | total loss: \u001b[1m\u001b[32m0.02758\u001b[0m\u001b[0m | time: 1.168s\n",
      "| SGD | epoch: 025 | loss: 0.02758 - R2: 0.9992 | val_loss: 0.02929 - val_acc: 1.0051 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 476  | total loss: \u001b[1m\u001b[32m0.02778\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 026 | loss: 0.02778 - R2: 0.9994 -- iter: 0064/1168\n",
      "Training Step: 477  | total loss: \u001b[1m\u001b[32m0.02762\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 026 | loss: 0.02762 - R2: 0.9994 -- iter: 0128/1168\n",
      "Training Step: 478  | total loss: \u001b[1m\u001b[32m0.02718\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 026 | loss: 0.02718 - R2: 0.9997 -- iter: 0192/1168\n",
      "Training Step: 479  | total loss: \u001b[1m\u001b[32m0.03228\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 026 | loss: 0.03228 - R2: 0.9998 -- iter: 0256/1168\n",
      "Training Step: 480  | total loss: \u001b[1m\u001b[32m0.03228\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 026 | loss: 0.03228 - R2: 1.0012 -- iter: 0320/1168\n",
      "Training Step: 481  | total loss: \u001b[1m\u001b[32m0.03496\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 026 | loss: 0.03496 - R2: 1.0007 -- iter: 0384/1168\n",
      "Training Step: 482  | total loss: \u001b[1m\u001b[32m0.03496\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 026 | loss: 0.03496 - R2: 1.0007 -- iter: 0448/1168\n",
      "Training Step: 483  | total loss: \u001b[1m\u001b[32m0.03303\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 026 | loss: 0.03303 - R2: 1.0003 -- iter: 0512/1168\n",
      "Training Step: 484  | total loss: \u001b[1m\u001b[32m0.03158\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 026 | loss: 0.03158 - R2: 0.9993 -- iter: 0576/1168\n",
      "Training Step: 485  | total loss: \u001b[1m\u001b[32m0.02988\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 026 | loss: 0.02988 - R2: 0.9993 -- iter: 0640/1168\n",
      "Training Step: 486  | total loss: \u001b[1m\u001b[32m0.02986\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 026 | loss: 0.02986 - R2: 0.9996 -- iter: 0704/1168\n",
      "Training Step: 487  | total loss: \u001b[1m\u001b[32m0.02855\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 026 | loss: 0.02855 - R2: 0.9994 -- iter: 0768/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 488  | total loss: \u001b[1m\u001b[32m0.02738\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 026 | loss: 0.02738 - R2: 0.9994 -- iter: 0832/1168\n",
      "Training Step: 489  | total loss: \u001b[1m\u001b[32m0.02636\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 026 | loss: 0.02636 - R2: 0.9996 -- iter: 0896/1168\n",
      "Training Step: 490  | total loss: \u001b[1m\u001b[32m0.02411\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 026 | loss: 0.02411 - R2: 0.9997 -- iter: 0960/1168\n",
      "Training Step: 491  | total loss: \u001b[1m\u001b[32m0.02411\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 026 | loss: 0.02411 - R2: 0.9997 -- iter: 1024/1168\n",
      "Training Step: 492  | total loss: \u001b[1m\u001b[32m0.02372\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 026 | loss: 0.02372 - R2: 0.9998 -- iter: 1088/1168\n",
      "Training Step: 493  | total loss: \u001b[1m\u001b[32m0.02269\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 026 | loss: 0.02269 - R2: 0.9998 -- iter: 1152/1168\n",
      "Training Step: 494  | total loss: \u001b[1m\u001b[32m0.02250\u001b[0m\u001b[0m | time: 1.178s\n",
      "| SGD | epoch: 026 | loss: 0.02250 - R2: 0.9988 | val_loss: 0.02961 - val_acc: 1.0057 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 495  | total loss: \u001b[1m\u001b[32m0.02295\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 027 | loss: 0.02295 - R2: 0.9990 -- iter: 0064/1168\n",
      "Training Step: 496  | total loss: \u001b[1m\u001b[32m0.02255\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 027 | loss: 0.02255 - R2: 0.9998 -- iter: 0128/1168\n",
      "Training Step: 497  | total loss: \u001b[1m\u001b[32m0.02169\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 027 | loss: 0.02169 - R2: 0.9994 -- iter: 0192/1168\n",
      "Training Step: 498  | total loss: \u001b[1m\u001b[32m0.02169\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 027 | loss: 0.02169 - R2: 0.9994 -- iter: 0256/1168\n",
      "Training Step: 499  | total loss: \u001b[1m\u001b[32m0.02172\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 027 | loss: 0.02172 - R2: 1.0000 -- iter: 0320/1168\n",
      "Training Step: 500  | total loss: \u001b[1m\u001b[32m0.02081\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 027 | loss: 0.02081 - R2: 0.9998 -- iter: 0384/1168\n",
      "Training Step: 501  | total loss: \u001b[1m\u001b[32m0.01995\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 027 | loss: 0.01995 - R2: 1.0000 -- iter: 0448/1168\n",
      "Training Step: 502  | total loss: \u001b[1m\u001b[32m0.01908\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 027 | loss: 0.01908 - R2: 1.0000 -- iter: 0512/1168\n",
      "Training Step: 503  | total loss: \u001b[1m\u001b[32m0.01908\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 027 | loss: 0.01908 - R2: 1.0000 -- iter: 0576/1168\n",
      "Training Step: 504  | total loss: \u001b[1m\u001b[32m0.02022\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 027 | loss: 0.02022 - R2: 1.0003 -- iter: 0640/1168\n",
      "Training Step: 505  | total loss: \u001b[1m\u001b[32m0.02022\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 027 | loss: 0.02022 - R2: 1.0003 -- iter: 0704/1168\n",
      "Training Step: 506  | total loss: \u001b[1m\u001b[32m0.02115\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 027 | loss: 0.02115 - R2: 1.0002 -- iter: 0768/1168\n",
      "Training Step: 507  | total loss: \u001b[1m\u001b[32m0.02168\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 027 | loss: 0.02168 - R2: 0.9995 -- iter: 0832/1168\n",
      "Training Step: 508  | total loss: \u001b[1m\u001b[32m0.02116\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 027 | loss: 0.02116 - R2: 0.9998 -- iter: 0896/1168\n",
      "Training Step: 509  | total loss: \u001b[1m\u001b[32m0.02095\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 027 | loss: 0.02095 - R2: 0.9997 -- iter: 0960/1168\n",
      "Training Step: 510  | total loss: \u001b[1m\u001b[32m0.02026\u001b[0m\u001b[0m | time: 0.171s\n",
      "| SGD | epoch: 027 | loss: 0.02026 - R2: 0.9997 -- iter: 1024/1168\n",
      "Training Step: 511  | total loss: \u001b[1m\u001b[32m0.01995\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 027 | loss: 0.01995 - R2: 0.9999 -- iter: 1088/1168\n",
      "Training Step: 512  | total loss: \u001b[1m\u001b[32m0.01995\u001b[0m\u001b[0m | time: 0.178s\n",
      "| SGD | epoch: 027 | loss: 0.01995 - R2: 0.9999 -- iter: 1152/1168\n",
      "Training Step: 513  | total loss: \u001b[1m\u001b[32m0.01870\u001b[0m\u001b[0m | time: 1.207s\n",
      "| SGD | epoch: 027 | loss: 0.01870 - R2: 0.9999 | val_loss: 0.02819 - val_acc: 1.0035 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 514  | total loss: \u001b[1m\u001b[32m0.01870\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 028 | loss: 0.01870 - R2: 0.9999 -- iter: 0064/1168\n",
      "Training Step: 515  | total loss: \u001b[1m\u001b[32m0.01804\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 028 | loss: 0.01804 - R2: 0.9995 -- iter: 0128/1168\n",
      "Training Step: 516  | total loss: \u001b[1m\u001b[32m0.01824\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 028 | loss: 0.01824 - R2: 0.9990 -- iter: 0192/1168\n",
      "Training Step: 517  | total loss: \u001b[1m\u001b[32m0.01824\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 028 | loss: 0.01824 - R2: 0.9990 -- iter: 0256/1168\n",
      "Training Step: 518  | total loss: \u001b[1m\u001b[32m0.01852\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 028 | loss: 0.01852 - R2: 0.9998 -- iter: 0320/1168\n",
      "Training Step: 519  | total loss: \u001b[1m\u001b[32m0.01852\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 028 | loss: 0.01852 - R2: 0.9998 -- iter: 0384/1168\n",
      "Training Step: 520  | total loss: \u001b[1m\u001b[32m0.01970\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 028 | loss: 0.01970 - R2: 1.0010 -- iter: 0448/1168\n",
      "Training Step: 521  | total loss: \u001b[1m\u001b[32m0.01909\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 028 | loss: 0.01909 - R2: 1.0005 -- iter: 0512/1168\n",
      "Training Step: 522  | total loss: \u001b[1m\u001b[32m0.01909\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 028 | loss: 0.01909 - R2: 1.0005 -- iter: 0576/1168\n",
      "Training Step: 523  | total loss: \u001b[1m\u001b[32m0.01912\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 028 | loss: 0.01912 - R2: 1.0005 -- iter: 0640/1168\n",
      "Training Step: 524  | total loss: \u001b[1m\u001b[32m0.01938\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 028 | loss: 0.01938 - R2: 1.0004 -- iter: 0704/1168\n",
      "Training Step: 525  | total loss: \u001b[1m\u001b[32m0.01871\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 028 | loss: 0.01871 - R2: 0.9998 -- iter: 0768/1168\n",
      "Training Step: 526  | total loss: \u001b[1m\u001b[32m0.02243\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 028 | loss: 0.02243 - R2: 1.0005 -- iter: 0832/1168\n",
      "Training Step: 527  | total loss: \u001b[1m\u001b[32m0.02243\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 028 | loss: 0.02243 - R2: 1.0005 -- iter: 0896/1168\n",
      "Training Step: 528  | total loss: \u001b[1m\u001b[32m0.06056\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 028 | loss: 0.06056 - R2: 0.9999 -- iter: 0960/1168\n",
      "Training Step: 529  | total loss: \u001b[1m\u001b[32m0.05656\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 028 | loss: 0.05656 - R2: 0.9993 -- iter: 1024/1168\n",
      "Training Step: 530  | total loss: \u001b[1m\u001b[32m0.05249\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 028 | loss: 0.05249 - R2: 0.9991 -- iter: 1088/1168\n",
      "Training Step: 531  | total loss: \u001b[1m\u001b[32m0.04627\u001b[0m\u001b[0m | time: 0.178s\n",
      "| SGD | epoch: 028 | loss: 0.04627 - R2: 0.9992 -- iter: 1152/1168\n",
      "Training Step: 532  | total loss: \u001b[1m\u001b[32m0.04627\u001b[0m\u001b[0m | time: 1.190s\n",
      "| SGD | epoch: 028 | loss: 0.04627 - R2: 0.9992 | val_loss: 0.02906 - val_acc: 1.0021 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 533  | total loss: \u001b[1m\u001b[32m0.04531\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 029 | loss: 0.04531 - R2: 0.9996 -- iter: 0064/1168\n",
      "Training Step: 534  | total loss: \u001b[1m\u001b[32m0.04217\u001b[0m\u001b[0m | time: 0.006s\n",
      "| SGD | epoch: 029 | loss: 0.04217 - R2: 1.0000 -- iter: 0128/1168\n",
      "Training Step: 535  | total loss: \u001b[1m\u001b[32m0.04013\u001b[0m\u001b[0m | time: 0.014s\n",
      "| SGD | epoch: 029 | loss: 0.04013 - R2: 0.9994 -- iter: 0192/1168\n",
      "Training Step: 536  | total loss: \u001b[1m\u001b[32m0.03819\u001b[0m\u001b[0m | time: 0.018s\n",
      "| SGD | epoch: 029 | loss: 0.03819 - R2: 0.9992 -- iter: 0256/1168\n",
      "Training Step: 537  | total loss: \u001b[1m\u001b[32m0.03650\u001b[0m\u001b[0m | time: 0.020s\n",
      "| SGD | epoch: 029 | loss: 0.03650 - R2: 0.9991 -- iter: 0320/1168\n",
      "Training Step: 538  | total loss: \u001b[1m\u001b[32m0.03586\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 029 | loss: 0.03586 - R2: 0.9997 -- iter: 0384/1168\n",
      "Training Step: 539  | total loss: \u001b[1m\u001b[32m0.03380\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 029 | loss: 0.03380 - R2: 0.9998 -- iter: 0448/1168\n",
      "Training Step: 540  | total loss: \u001b[1m\u001b[32m0.03119\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 029 | loss: 0.03119 - R2: 1.0000 -- iter: 0512/1168\n",
      "Training Step: 541  | total loss: \u001b[1m\u001b[32m0.02871\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 029 | loss: 0.02871 - R2: 1.0002 -- iter: 0576/1168\n",
      "Training Step: 542  | total loss: \u001b[1m\u001b[32m0.02841\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 029 | loss: 0.02841 - R2: 1.0002 -- iter: 0640/1168\n",
      "Training Step: 543  | total loss: \u001b[1m\u001b[32m0.02830\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 029 | loss: 0.02830 - R2: 0.9999 -- iter: 0704/1168\n",
      "Training Step: 544  | total loss: \u001b[1m\u001b[32m0.02731\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 029 | loss: 0.02731 - R2: 0.9999 -- iter: 0768/1168\n",
      "Training Step: 545  | total loss: \u001b[1m\u001b[32m0.02631\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 029 | loss: 0.02631 - R2: 0.9997 -- iter: 0832/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 546  | total loss: \u001b[1m\u001b[32m0.02578\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 029 | loss: 0.02578 - R2: 0.9999 -- iter: 0896/1168\n",
      "Training Step: 547  | total loss: \u001b[1m\u001b[32m0.02448\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 029 | loss: 0.02448 - R2: 1.0001 -- iter: 0960/1168\n",
      "Training Step: 548  | total loss: \u001b[1m\u001b[32m0.04440\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 029 | loss: 0.04440 - R2: 1.0002 -- iter: 1024/1168\n",
      "Training Step: 549  | total loss: \u001b[1m\u001b[32m0.04141\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 029 | loss: 0.04141 - R2: 1.0000 -- iter: 1088/1168\n",
      "Training Step: 550  | total loss: \u001b[1m\u001b[32m0.03969\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 029 | loss: 0.03969 - R2: 1.0000 -- iter: 1152/1168\n",
      "Training Step: 551  | total loss: \u001b[1m\u001b[32m0.03874\u001b[0m\u001b[0m | time: 1.089s\n",
      "| SGD | epoch: 029 | loss: 0.03874 - R2: 0.9997 | val_loss: 0.02878 - val_acc: 1.0023 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 552  | total loss: \u001b[1m\u001b[32m0.03449\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 030 | loss: 0.03449 - R2: 0.9998 -- iter: 0064/1168\n",
      "Training Step: 553  | total loss: \u001b[1m\u001b[32m0.03449\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 030 | loss: 0.03449 - R2: 0.9998 -- iter: 0128/1168\n",
      "Training Step: 554  | total loss: \u001b[1m\u001b[32m0.03257\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 030 | loss: 0.03257 - R2: 0.9993 -- iter: 0192/1168\n",
      "Training Step: 555  | total loss: \u001b[1m\u001b[32m0.03176\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 030 | loss: 0.03176 - R2: 0.9995 -- iter: 0256/1168\n",
      "Training Step: 556  | total loss: \u001b[1m\u001b[32m0.03066\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 030 | loss: 0.03066 - R2: 0.9993 -- iter: 0320/1168\n",
      "Training Step: 557  | total loss: \u001b[1m\u001b[32m0.02979\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 030 | loss: 0.02979 - R2: 0.9988 -- iter: 0384/1168\n",
      "Training Step: 558  | total loss: \u001b[1m\u001b[32m0.02869\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 030 | loss: 0.02869 - R2: 0.9994 -- iter: 0448/1168\n",
      "Training Step: 559  | total loss: \u001b[1m\u001b[32m0.02635\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 030 | loss: 0.02635 - R2: 1.0005 -- iter: 0512/1168\n",
      "Training Step: 560  | total loss: \u001b[1m\u001b[32m0.02635\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 030 | loss: 0.02635 - R2: 1.0005 -- iter: 0576/1168\n",
      "Training Step: 561  | total loss: \u001b[1m\u001b[32m0.02446\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 030 | loss: 0.02446 - R2: 1.0008 -- iter: 0640/1168\n",
      "Training Step: 562  | total loss: \u001b[1m\u001b[32m0.02383\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 030 | loss: 0.02383 - R2: 1.0002 -- iter: 0704/1168\n",
      "Training Step: 563  | total loss: \u001b[1m\u001b[32m0.02329\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 030 | loss: 0.02329 - R2: 0.9992 -- iter: 0768/1168\n",
      "Training Step: 564  | total loss: \u001b[1m\u001b[32m0.02273\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 030 | loss: 0.02273 - R2: 0.9992 -- iter: 0832/1168\n",
      "Training Step: 565  | total loss: \u001b[1m\u001b[32m0.02282\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 030 | loss: 0.02282 - R2: 0.9993 -- iter: 0896/1168\n",
      "Training Step: 566  | total loss: \u001b[1m\u001b[32m0.02309\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 030 | loss: 0.02309 - R2: 0.9999 -- iter: 0960/1168\n",
      "Training Step: 567  | total loss: \u001b[1m\u001b[32m0.04161\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 030 | loss: 0.04161 - R2: 0.9995 -- iter: 1024/1168\n",
      "Training Step: 568  | total loss: \u001b[1m\u001b[32m0.03898\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 030 | loss: 0.03898 - R2: 0.9994 -- iter: 1088/1168\n",
      "Training Step: 569  | total loss: \u001b[1m\u001b[32m0.03737\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 030 | loss: 0.03737 - R2: 0.9995 -- iter: 1152/1168\n",
      "Training Step: 570  | total loss: \u001b[1m\u001b[32m0.03737\u001b[0m\u001b[0m | time: 1.154s\n",
      "| SGD | epoch: 030 | loss: 0.03737 - R2: 0.9995 | val_loss: 0.02807 - val_acc: 1.0026 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 571  | total loss: \u001b[1m\u001b[32m0.03379\u001b[0m\u001b[0m | time: 0.016s\n",
      "| SGD | epoch: 031 | loss: 0.03379 - R2: 0.9994 -- iter: 0064/1168\n",
      "Training Step: 572  | total loss: \u001b[1m\u001b[32m0.03245\u001b[0m\u001b[0m | time: 0.020s\n",
      "| SGD | epoch: 031 | loss: 0.03245 - R2: 0.9998 -- iter: 0128/1168\n",
      "Training Step: 573  | total loss: \u001b[1m\u001b[32m0.03107\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 031 | loss: 0.03107 - R2: 0.9992 -- iter: 0192/1168\n",
      "Training Step: 574  | total loss: \u001b[1m\u001b[32m0.03107\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 031 | loss: 0.03107 - R2: 0.9992 -- iter: 0256/1168\n",
      "Training Step: 575  | total loss: \u001b[1m\u001b[32m0.02783\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 031 | loss: 0.02783 - R2: 0.9997 -- iter: 0320/1168\n",
      "Training Step: 576  | total loss: \u001b[1m\u001b[32m0.02783\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 031 | loss: 0.02783 - R2: 0.9997 -- iter: 0384/1168\n",
      "Training Step: 577  | total loss: \u001b[1m\u001b[32m0.02719\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 031 | loss: 0.02719 - R2: 0.9999 -- iter: 0448/1168\n",
      "Training Step: 578  | total loss: \u001b[1m\u001b[32m0.02580\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 031 | loss: 0.02580 - R2: 0.9999 -- iter: 0512/1168\n",
      "Training Step: 579  | total loss: \u001b[1m\u001b[32m0.02580\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 031 | loss: 0.02580 - R2: 0.9999 -- iter: 0576/1168\n",
      "Training Step: 580  | total loss: \u001b[1m\u001b[32m0.02659\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 031 | loss: 0.02659 - R2: 0.9984 -- iter: 0640/1168\n",
      "Training Step: 581  | total loss: \u001b[1m\u001b[32m0.02575\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 031 | loss: 0.02575 - R2: 0.9993 -- iter: 0704/1168\n",
      "Training Step: 582  | total loss: \u001b[1m\u001b[32m0.02575\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 031 | loss: 0.02575 - R2: 0.9993 -- iter: 0768/1168\n",
      "Training Step: 583  | total loss: \u001b[1m\u001b[32m0.02474\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 031 | loss: 0.02474 - R2: 0.9996 -- iter: 0832/1168\n",
      "Training Step: 584  | total loss: \u001b[1m\u001b[32m0.02416\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 031 | loss: 0.02416 - R2: 0.9998 -- iter: 0896/1168\n",
      "Training Step: 585  | total loss: \u001b[1m\u001b[32m0.02556\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 031 | loss: 0.02556 - R2: 1.0010 -- iter: 0960/1168\n",
      "Training Step: 586  | total loss: \u001b[1m\u001b[32m0.02558\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 031 | loss: 0.02558 - R2: 1.0010 -- iter: 1024/1168\n",
      "Training Step: 587  | total loss: \u001b[1m\u001b[32m0.02415\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 031 | loss: 0.02415 - R2: 1.0007 -- iter: 1088/1168\n",
      "Training Step: 588  | total loss: \u001b[1m\u001b[32m0.02415\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 031 | loss: 0.02415 - R2: 1.0007 -- iter: 1152/1168\n",
      "Training Step: 589  | total loss: \u001b[1m\u001b[32m0.02302\u001b[0m\u001b[0m | time: 1.106s\n",
      "| SGD | epoch: 031 | loss: 0.02302 - R2: 1.0002 | val_loss: 0.02873 - val_acc: 1.0025 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 590  | total loss: \u001b[1m\u001b[32m0.02237\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 032 | loss: 0.02237 - R2: 1.0004 -- iter: 0064/1168\n",
      "Training Step: 591  | total loss: \u001b[1m\u001b[32m0.02237\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 032 | loss: 0.02237 - R2: 1.0004 -- iter: 0128/1168\n",
      "Training Step: 592  | total loss: \u001b[1m\u001b[32m0.02263\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 032 | loss: 0.02263 - R2: 1.0002 -- iter: 0192/1168\n",
      "Training Step: 593  | total loss: \u001b[1m\u001b[32m0.02342\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 032 | loss: 0.02342 - R2: 1.0002 -- iter: 0256/1168\n",
      "Training Step: 594  | total loss: \u001b[1m\u001b[32m0.02280\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 032 | loss: 0.02280 - R2: 1.0001 -- iter: 0320/1168\n",
      "Training Step: 595  | total loss: \u001b[1m\u001b[32m0.02218\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 032 | loss: 0.02218 - R2: 1.0001 -- iter: 0384/1168\n",
      "Training Step: 596  | total loss: \u001b[1m\u001b[32m0.02107\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 032 | loss: 0.02107 - R2: 0.9998 -- iter: 0448/1168\n",
      "Training Step: 597  | total loss: \u001b[1m\u001b[32m0.02054\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 032 | loss: 0.02054 - R2: 1.0002 -- iter: 0512/1168\n",
      "Training Step: 598  | total loss: \u001b[1m\u001b[32m0.02102\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 032 | loss: 0.02102 - R2: 0.9997 -- iter: 0576/1168\n",
      "Training Step: 599  | total loss: \u001b[1m\u001b[32m0.02008\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 032 | loss: 0.02008 - R2: 0.9992 -- iter: 0640/1168\n",
      "Training Step: 600  | total loss: \u001b[1m\u001b[32m0.02113\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 032 | loss: 0.02113 - R2: 1.0006 -- iter: 0704/1168\n",
      "Training Step: 601  | total loss: \u001b[1m\u001b[32m0.02116\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 032 | loss: 0.02116 - R2: 1.0012 -- iter: 0768/1168\n",
      "Training Step: 602  | total loss: \u001b[1m\u001b[32m0.02045\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 032 | loss: 0.02045 - R2: 1.0006 -- iter: 0832/1168\n",
      "Training Step: 603  | total loss: \u001b[1m\u001b[32m0.01997\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 032 | loss: 0.01997 - R2: 1.0002 -- iter: 0896/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 604  | total loss: \u001b[1m\u001b[32m0.02182\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 032 | loss: 0.02182 - R2: 1.0001 -- iter: 0960/1168\n",
      "Training Step: 605  | total loss: \u001b[1m\u001b[32m0.02182\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 032 | loss: 0.02182 - R2: 1.0001 -- iter: 1024/1168\n",
      "Training Step: 606  | total loss: \u001b[1m\u001b[32m0.02265\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 032 | loss: 0.02265 - R2: 0.9999 -- iter: 1088/1168\n",
      "Training Step: 607  | total loss: \u001b[1m\u001b[32m0.02147\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 032 | loss: 0.02147 - R2: 0.9999 -- iter: 1152/1168\n",
      "Training Step: 608  | total loss: \u001b[1m\u001b[32m0.02083\u001b[0m\u001b[0m | time: 1.113s\n",
      "| SGD | epoch: 032 | loss: 0.02083 - R2: 0.9996 | val_loss: 0.02911 - val_acc: 1.0034 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 609  | total loss: \u001b[1m\u001b[32m0.02009\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 033 | loss: 0.02009 - R2: 0.9995 -- iter: 0064/1168\n",
      "Training Step: 610  | total loss: \u001b[1m\u001b[32m0.02063\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 033 | loss: 0.02063 - R2: 1.0001 -- iter: 0128/1168\n",
      "Training Step: 611  | total loss: \u001b[1m\u001b[32m0.01964\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 033 | loss: 0.01964 - R2: 0.9999 -- iter: 0192/1168\n",
      "Training Step: 612  | total loss: \u001b[1m\u001b[32m0.01924\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 033 | loss: 0.01924 - R2: 0.9998 -- iter: 0256/1168\n",
      "Training Step: 613  | total loss: \u001b[1m\u001b[32m0.01963\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 033 | loss: 0.01963 - R2: 1.0000 -- iter: 0320/1168\n",
      "Training Step: 614  | total loss: \u001b[1m\u001b[32m0.01963\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 033 | loss: 0.01963 - R2: 1.0000 -- iter: 0384/1168\n",
      "Training Step: 615  | total loss: \u001b[1m\u001b[32m0.01946\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 033 | loss: 0.01946 - R2: 0.9999 -- iter: 0448/1168\n",
      "Training Step: 616  | total loss: \u001b[1m\u001b[32m0.01848\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 033 | loss: 0.01848 - R2: 0.9996 -- iter: 0512/1168\n",
      "Training Step: 617  | total loss: \u001b[1m\u001b[32m0.01837\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 033 | loss: 0.01837 - R2: 0.9996 -- iter: 0576/1168\n",
      "Training Step: 618  | total loss: \u001b[1m\u001b[32m0.01813\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 033 | loss: 0.01813 - R2: 0.9993 -- iter: 0640/1168\n",
      "Training Step: 619  | total loss: \u001b[1m\u001b[32m0.01834\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 033 | loss: 0.01834 - R2: 0.9997 -- iter: 0704/1168\n",
      "Training Step: 620  | total loss: \u001b[1m\u001b[32m0.01772\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 033 | loss: 0.01772 - R2: 0.9993 -- iter: 0768/1168\n",
      "Training Step: 621  | total loss: \u001b[1m\u001b[32m0.01703\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 033 | loss: 0.01703 - R2: 0.9994 -- iter: 0832/1168\n",
      "Training Step: 622  | total loss: \u001b[1m\u001b[32m0.01703\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 033 | loss: 0.01703 - R2: 0.9994 -- iter: 0896/1168\n",
      "Training Step: 623  | total loss: \u001b[1m\u001b[32m0.01736\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 033 | loss: 0.01736 - R2: 0.9997 -- iter: 0960/1168\n",
      "Training Step: 624  | total loss: \u001b[1m\u001b[32m0.01736\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 033 | loss: 0.01736 - R2: 0.9997 -- iter: 1024/1168\n",
      "Training Step: 625  | total loss: \u001b[1m\u001b[32m0.01923\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 033 | loss: 0.01923 - R2: 1.0002 -- iter: 1088/1168\n",
      "Training Step: 626  | total loss: \u001b[1m\u001b[32m0.01920\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 033 | loss: 0.01920 - R2: 1.0000 -- iter: 1152/1168\n",
      "Training Step: 627  | total loss: \u001b[1m\u001b[32m0.05033\u001b[0m\u001b[0m | time: 1.138s\n",
      "| SGD | epoch: 033 | loss: 0.05033 - R2: 1.0004 | val_loss: 0.02642 - val_acc: 0.9991 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 628  | total loss: \u001b[1m\u001b[32m0.04705\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 034 | loss: 0.04705 - R2: 0.9997 -- iter: 0064/1168\n",
      "Training Step: 629  | total loss: \u001b[1m\u001b[32m0.04705\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 034 | loss: 0.04705 - R2: 0.9997 -- iter: 0128/1168\n",
      "Training Step: 630  | total loss: \u001b[1m\u001b[32m0.04528\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 034 | loss: 0.04528 - R2: 0.9998 -- iter: 0192/1168\n",
      "Training Step: 631  | total loss: \u001b[1m\u001b[32m0.04292\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 034 | loss: 0.04292 - R2: 0.9991 -- iter: 0256/1168\n",
      "Training Step: 632  | total loss: \u001b[1m\u001b[32m0.04014\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 034 | loss: 0.04014 - R2: 0.9994 -- iter: 0320/1168\n",
      "Training Step: 633  | total loss: \u001b[1m\u001b[32m0.03839\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 034 | loss: 0.03839 - R2: 0.9995 -- iter: 0384/1168\n",
      "Training Step: 634  | total loss: \u001b[1m\u001b[32m0.03624\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 034 | loss: 0.03624 - R2: 0.9991 -- iter: 0448/1168\n",
      "Training Step: 635  | total loss: \u001b[1m\u001b[32m0.03418\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 034 | loss: 0.03418 - R2: 0.9993 -- iter: 0512/1168\n",
      "Training Step: 636  | total loss: \u001b[1m\u001b[32m0.03224\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 034 | loss: 0.03224 - R2: 0.9994 -- iter: 0576/1168\n",
      "Training Step: 637  | total loss: \u001b[1m\u001b[32m0.02990\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 034 | loss: 0.02990 - R2: 1.0001 -- iter: 0640/1168\n",
      "Training Step: 638  | total loss: \u001b[1m\u001b[32m0.02990\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 034 | loss: 0.02990 - R2: 1.0001 -- iter: 0704/1168\n",
      "Training Step: 639  | total loss: \u001b[1m\u001b[32m0.02847\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 034 | loss: 0.02847 - R2: 1.0000 -- iter: 0768/1168\n",
      "Training Step: 640  | total loss: \u001b[1m\u001b[32m0.02448\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 034 | loss: 0.02448 - R2: 0.9994 -- iter: 0832/1168\n",
      "Training Step: 641  | total loss: \u001b[1m\u001b[32m0.02448\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 034 | loss: 0.02448 - R2: 0.9994 -- iter: 0896/1168\n",
      "Training Step: 642  | total loss: \u001b[1m\u001b[32m0.02594\u001b[0m\u001b[0m | time: 0.174s\n",
      "| SGD | epoch: 034 | loss: 0.02594 - R2: 0.9996 -- iter: 0960/1168\n",
      "Training Step: 643  | total loss: \u001b[1m\u001b[32m0.02480\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 034 | loss: 0.02480 - R2: 0.9999 -- iter: 1024/1168\n",
      "Training Step: 644  | total loss: \u001b[1m\u001b[32m0.02445\u001b[0m\u001b[0m | time: 0.180s\n",
      "| SGD | epoch: 034 | loss: 0.02445 - R2: 0.9998 -- iter: 1088/1168\n",
      "Training Step: 645  | total loss: \u001b[1m\u001b[32m0.02566\u001b[0m\u001b[0m | time: 0.184s\n",
      "| SGD | epoch: 034 | loss: 0.02566 - R2: 1.0008 -- iter: 1152/1168\n",
      "Training Step: 646  | total loss: \u001b[1m\u001b[32m0.02389\u001b[0m\u001b[0m | time: 1.212s\n",
      "| SGD | epoch: 034 | loss: 0.02389 - R2: 1.0001 | val_loss: 0.02684 - val_acc: 1.0017 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 647  | total loss: \u001b[1m\u001b[32m0.04997\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 035 | loss: 0.04997 - R2: 0.9992 -- iter: 0064/1168\n",
      "Training Step: 648  | total loss: \u001b[1m\u001b[32m0.04806\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 035 | loss: 0.04806 - R2: 0.9999 -- iter: 0128/1168\n",
      "Training Step: 649  | total loss: \u001b[1m\u001b[32m0.04806\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 035 | loss: 0.04806 - R2: 0.9999 -- iter: 0192/1168\n",
      "Training Step: 650  | total loss: \u001b[1m\u001b[32m0.04190\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 035 | loss: 0.04190 - R2: 0.9989 -- iter: 0256/1168\n",
      "Training Step: 651  | total loss: \u001b[1m\u001b[32m0.04143\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 035 | loss: 0.04143 - R2: 0.9995 -- iter: 0320/1168\n",
      "Training Step: 652  | total loss: \u001b[1m\u001b[32m0.03872\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 035 | loss: 0.03872 - R2: 0.9991 -- iter: 0384/1168\n",
      "Training Step: 653  | total loss: \u001b[1m\u001b[32m0.03669\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 035 | loss: 0.03669 - R2: 0.9992 -- iter: 0448/1168\n",
      "Training Step: 654  | total loss: \u001b[1m\u001b[32m0.03669\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 035 | loss: 0.03669 - R2: 1.0000 -- iter: 0512/1168\n",
      "Training Step: 655  | total loss: \u001b[1m\u001b[32m0.03478\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 035 | loss: 0.03478 - R2: 1.0000 -- iter: 0576/1168\n",
      "Training Step: 656  | total loss: \u001b[1m\u001b[32m0.03315\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 035 | loss: 0.03315 - R2: 0.9994 -- iter: 0640/1168\n",
      "Training Step: 657  | total loss: \u001b[1m\u001b[32m0.03187\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 035 | loss: 0.03187 - R2: 0.9996 -- iter: 0704/1168\n",
      "Training Step: 658  | total loss: \u001b[1m\u001b[32m0.03045\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 035 | loss: 0.03045 - R2: 0.9997 -- iter: 0768/1168\n",
      "Training Step: 659  | total loss: \u001b[1m\u001b[32m0.02934\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 035 | loss: 0.02934 - R2: 0.9997 -- iter: 0832/1168\n",
      "Training Step: 660  | total loss: \u001b[1m\u001b[32m0.02751\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 035 | loss: 0.02751 - R2: 1.0003 -- iter: 0896/1168\n",
      "Training Step: 661  | total loss: \u001b[1m\u001b[32m0.02560\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 035 | loss: 0.02560 - R2: 1.0004 -- iter: 0960/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 662  | total loss: \u001b[1m\u001b[32m0.02340\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 035 | loss: 0.02340 - R2: 1.0003 -- iter: 1024/1168\n",
      "Training Step: 663  | total loss: \u001b[1m\u001b[32m0.02441\u001b[0m\u001b[0m | time: 0.174s\n",
      "| SGD | epoch: 035 | loss: 0.02441 - R2: 1.0005 -- iter: 1088/1168\n",
      "Training Step: 664  | total loss: \u001b[1m\u001b[32m0.02527\u001b[0m\u001b[0m | time: 0.179s\n",
      "| SGD | epoch: 035 | loss: 0.02527 - R2: 1.0002 -- iter: 1152/1168\n",
      "Training Step: 665  | total loss: \u001b[1m\u001b[32m0.02507\u001b[0m\u001b[0m | time: 1.186s\n",
      "| SGD | epoch: 035 | loss: 0.02507 - R2: 1.0002 | val_loss: 0.02715 - val_acc: 1.0016 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 666  | total loss: \u001b[1m\u001b[32m0.02406\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 036 | loss: 0.02406 - R2: 1.0001 -- iter: 0064/1168\n",
      "Training Step: 667  | total loss: \u001b[1m\u001b[32m0.02406\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 036 | loss: 0.02406 - R2: 1.0001 -- iter: 0128/1168\n",
      "Training Step: 668  | total loss: \u001b[1m\u001b[32m0.04817\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 036 | loss: 0.04817 - R2: 1.0000 -- iter: 0192/1168\n",
      "Training Step: 669  | total loss: \u001b[1m\u001b[32m0.04817\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 036 | loss: 0.04817 - R2: 1.0000 -- iter: 0256/1168\n",
      "Training Step: 670  | total loss: \u001b[1m\u001b[32m0.04571\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 036 | loss: 0.04571 - R2: 1.0001 -- iter: 0320/1168\n",
      "Training Step: 671  | total loss: \u001b[1m\u001b[32m0.04206\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 036 | loss: 0.04206 - R2: 0.9998 -- iter: 0384/1168\n",
      "Training Step: 672  | total loss: \u001b[1m\u001b[32m0.03941\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 036 | loss: 0.03941 - R2: 0.9995 -- iter: 0448/1168\n",
      "Training Step: 673  | total loss: \u001b[1m\u001b[32m0.03813\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 036 | loss: 0.03813 - R2: 0.9996 -- iter: 0512/1168\n",
      "Training Step: 674  | total loss: \u001b[1m\u001b[32m0.03623\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 036 | loss: 0.03623 - R2: 0.9996 -- iter: 0576/1168\n",
      "Training Step: 675  | total loss: \u001b[1m\u001b[32m0.03299\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 036 | loss: 0.03299 - R2: 1.0003 -- iter: 0640/1168\n",
      "Training Step: 676  | total loss: \u001b[1m\u001b[32m0.03299\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 036 | loss: 0.03299 - R2: 1.0003 -- iter: 0704/1168\n",
      "Training Step: 677  | total loss: \u001b[1m\u001b[32m0.03202\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 036 | loss: 0.03202 - R2: 0.9994 -- iter: 0768/1168\n",
      "Training Step: 678  | total loss: \u001b[1m\u001b[32m0.03202\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 036 | loss: 0.03202 - R2: 0.9998 -- iter: 0832/1168\n",
      "Training Step: 679  | total loss: \u001b[1m\u001b[32m0.03057\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 036 | loss: 0.03057 - R2: 0.9997 -- iter: 0896/1168\n",
      "Training Step: 680  | total loss: \u001b[1m\u001b[32m0.02733\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 036 | loss: 0.02733 - R2: 0.9994 -- iter: 0960/1168\n",
      "Training Step: 681  | total loss: \u001b[1m\u001b[32m0.02633\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 036 | loss: 0.02633 - R2: 0.9995 -- iter: 1024/1168\n",
      "Training Step: 682  | total loss: \u001b[1m\u001b[32m0.02546\u001b[0m\u001b[0m | time: 0.172s\n",
      "| SGD | epoch: 036 | loss: 0.02546 - R2: 0.9997 -- iter: 1088/1168\n",
      "Training Step: 683  | total loss: \u001b[1m\u001b[32m0.02452\u001b[0m\u001b[0m | time: 0.176s\n",
      "| SGD | epoch: 036 | loss: 0.02452 - R2: 0.9995 -- iter: 1152/1168\n",
      "Training Step: 684  | total loss: \u001b[1m\u001b[32m0.02464\u001b[0m\u001b[0m | time: 1.199s\n",
      "| SGD | epoch: 036 | loss: 0.02464 - R2: 0.9999 | val_loss: 0.02866 - val_acc: 1.0035 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 685  | total loss: \u001b[1m\u001b[32m0.02464\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 037 | loss: 0.02464 - R2: 0.9999 -- iter: 0064/1168\n",
      "Training Step: 686  | total loss: \u001b[1m\u001b[32m0.02298\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 037 | loss: 0.02298 - R2: 0.9998 -- iter: 0128/1168\n",
      "Training Step: 687  | total loss: \u001b[1m\u001b[32m0.02356\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 037 | loss: 0.02356 - R2: 0.9996 -- iter: 0192/1168\n",
      "Training Step: 688  | total loss: \u001b[1m\u001b[32m0.02213\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 037 | loss: 0.02213 - R2: 0.9996 -- iter: 0256/1168\n",
      "Training Step: 689  | total loss: \u001b[1m\u001b[32m0.02179\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 037 | loss: 0.02179 - R2: 0.9999 -- iter: 0320/1168\n",
      "Training Step: 690  | total loss: \u001b[1m\u001b[32m0.02179\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 037 | loss: 0.02179 - R2: 0.9999 -- iter: 0384/1168\n",
      "Training Step: 691  | total loss: \u001b[1m\u001b[32m0.02058\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 037 | loss: 0.02058 - R2: 1.0001 -- iter: 0448/1168\n",
      "Training Step: 692  | total loss: \u001b[1m\u001b[32m0.02040\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 037 | loss: 0.02040 - R2: 1.0001 -- iter: 0512/1168\n",
      "Training Step: 693  | total loss: \u001b[1m\u001b[32m0.02040\u001b[0m\u001b[0m | time: 0.173s\n",
      "| SGD | epoch: 037 | loss: 0.02040 - R2: 1.0001 -- iter: 0576/1168\n",
      "Training Step: 694  | total loss: \u001b[1m\u001b[32m0.02152\u001b[0m\u001b[0m | time: 0.186s\n",
      "| SGD | epoch: 037 | loss: 0.02152 - R2: 1.0001 -- iter: 0640/1168\n",
      "Training Step: 695  | total loss: \u001b[1m\u001b[32m0.02184\u001b[0m\u001b[0m | time: 0.191s\n",
      "| SGD | epoch: 037 | loss: 0.02184 - R2: 0.9997 -- iter: 0704/1168\n",
      "Training Step: 696  | total loss: \u001b[1m\u001b[32m0.02245\u001b[0m\u001b[0m | time: 0.204s\n",
      "| SGD | epoch: 037 | loss: 0.02245 - R2: 1.0008 -- iter: 0768/1168\n",
      "Training Step: 697  | total loss: \u001b[1m\u001b[32m0.02354\u001b[0m\u001b[0m | time: 0.207s\n",
      "| SGD | epoch: 037 | loss: 0.02354 - R2: 1.0008 -- iter: 0832/1168\n",
      "Training Step: 698  | total loss: \u001b[1m\u001b[32m0.02263\u001b[0m\u001b[0m | time: 0.211s\n",
      "| SGD | epoch: 037 | loss: 0.02263 - R2: 0.9995 -- iter: 0896/1168\n",
      "Training Step: 699  | total loss: \u001b[1m\u001b[32m0.02263\u001b[0m\u001b[0m | time: 0.245s\n",
      "| SGD | epoch: 037 | loss: 0.02263 - R2: 1.0003 -- iter: 0960/1168\n",
      "Training Step: 700  | total loss: \u001b[1m\u001b[32m0.02290\u001b[0m\u001b[0m | time: 0.252s\n",
      "| SGD | epoch: 037 | loss: 0.02290 - R2: 1.0003 -- iter: 1024/1168\n",
      "Training Step: 701  | total loss: \u001b[1m\u001b[32m0.02232\u001b[0m\u001b[0m | time: 0.256s\n",
      "| SGD | epoch: 037 | loss: 0.02232 - R2: 0.9997 -- iter: 1088/1168\n",
      "Training Step: 702  | total loss: \u001b[1m\u001b[32m0.02202\u001b[0m\u001b[0m | time: 0.269s\n",
      "| SGD | epoch: 037 | loss: 0.02202 - R2: 1.0000 -- iter: 1152/1168\n",
      "Training Step: 703  | total loss: \u001b[1m\u001b[32m0.02202\u001b[0m\u001b[0m | time: 1.281s\n",
      "| SGD | epoch: 037 | loss: 0.02202 - R2: 1.0000 | val_loss: 0.02886 - val_acc: 1.0016 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 704  | total loss: \u001b[1m\u001b[32m0.02058\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 038 | loss: 0.02058 - R2: 1.0001 -- iter: 0064/1168\n",
      "Training Step: 705  | total loss: \u001b[1m\u001b[32m0.02035\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 038 | loss: 0.02035 - R2: 0.9999 -- iter: 0128/1168\n",
      "Training Step: 706  | total loss: \u001b[1m\u001b[32m0.01990\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 038 | loss: 0.01990 - R2: 0.9998 -- iter: 0192/1168\n",
      "Training Step: 707  | total loss: \u001b[1m\u001b[32m0.05182\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 038 | loss: 0.05182 - R2: 0.9996 -- iter: 0256/1168\n",
      "Training Step: 708  | total loss: \u001b[1m\u001b[32m0.05182\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 038 | loss: 0.05182 - R2: 0.9996 -- iter: 0320/1168\n",
      "Training Step: 709  | total loss: \u001b[1m\u001b[32m0.04585\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 038 | loss: 0.04585 - R2: 0.9993 -- iter: 0384/1168\n",
      "Training Step: 710  | total loss: \u001b[1m\u001b[32m0.04253\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 038 | loss: 0.04253 - R2: 0.9993 -- iter: 0448/1168\n",
      "Training Step: 711  | total loss: \u001b[1m\u001b[32m0.04012\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 038 | loss: 0.04012 - R2: 0.9995 -- iter: 0512/1168\n",
      "Training Step: 712  | total loss: \u001b[1m\u001b[32m0.04012\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 038 | loss: 0.04012 - R2: 0.9995 -- iter: 0576/1168\n",
      "Training Step: 713  | total loss: \u001b[1m\u001b[32m0.03834\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 038 | loss: 0.03834 - R2: 1.0000 -- iter: 0640/1168\n",
      "Training Step: 714  | total loss: \u001b[1m\u001b[32m0.03760\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 038 | loss: 0.03760 - R2: 1.0007 -- iter: 0704/1168\n",
      "Training Step: 715  | total loss: \u001b[1m\u001b[32m0.03791\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 038 | loss: 0.03791 - R2: 0.9999 -- iter: 0768/1168\n",
      "Training Step: 716  | total loss: \u001b[1m\u001b[32m0.03370\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 038 | loss: 0.03370 - R2: 0.9996 -- iter: 0832/1168\n",
      "Training Step: 717  | total loss: \u001b[1m\u001b[32m0.03265\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 038 | loss: 0.03265 - R2: 0.9996 -- iter: 0896/1168\n",
      "Training Step: 718  | total loss: \u001b[1m\u001b[32m0.03265\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 038 | loss: 0.03265 - R2: 0.9998 -- iter: 0960/1168\n",
      "Training Step: 719  | total loss: \u001b[1m\u001b[32m0.03066\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 038 | loss: 0.03066 - R2: 0.9999 -- iter: 1024/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 720  | total loss: \u001b[1m\u001b[32m0.02774\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 038 | loss: 0.02774 - R2: 1.0000 -- iter: 1088/1168\n",
      "Training Step: 721  | total loss: \u001b[1m\u001b[32m0.02637\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 038 | loss: 0.02637 - R2: 0.9998 -- iter: 1152/1168\n",
      "Training Step: 722  | total loss: \u001b[1m\u001b[32m0.02524\u001b[0m\u001b[0m | time: 1.133s\n",
      "| SGD | epoch: 038 | loss: 0.02524 - R2: 0.9995 | val_loss: 0.02794 - val_acc: 1.0032 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 723  | total loss: \u001b[1m\u001b[32m0.02508\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 039 | loss: 0.02508 - R2: 0.9998 -- iter: 0064/1168\n",
      "Training Step: 724  | total loss: \u001b[1m\u001b[32m0.02426\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 039 | loss: 0.02426 - R2: 0.9993 -- iter: 0128/1168\n",
      "Training Step: 725  | total loss: \u001b[1m\u001b[32m0.02349\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 039 | loss: 0.02349 - R2: 0.9997 -- iter: 0192/1168\n",
      "Training Step: 726  | total loss: \u001b[1m\u001b[32m0.02349\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 039 | loss: 0.02349 - R2: 0.9997 -- iter: 0256/1168\n",
      "Training Step: 727  | total loss: \u001b[1m\u001b[32m0.04651\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 039 | loss: 0.04651 - R2: 1.0010 -- iter: 0320/1168\n",
      "Training Step: 728  | total loss: \u001b[1m\u001b[32m0.04651\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 039 | loss: 0.04651 - R2: 1.0010 -- iter: 0384/1168\n",
      "Training Step: 729  | total loss: \u001b[1m\u001b[32m0.04320\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 039 | loss: 0.04320 - R2: 1.0003 -- iter: 0448/1168\n",
      "Training Step: 730  | total loss: \u001b[1m\u001b[32m0.04010\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 039 | loss: 0.04010 - R2: 0.9994 -- iter: 0512/1168\n",
      "Training Step: 731  | total loss: \u001b[1m\u001b[32m0.03555\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 039 | loss: 0.03555 - R2: 0.9993 -- iter: 0576/1168\n",
      "Training Step: 732  | total loss: \u001b[1m\u001b[32m0.03430\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 039 | loss: 0.03430 - R2: 1.0002 -- iter: 0640/1168\n",
      "Training Step: 733  | total loss: \u001b[1m\u001b[32m0.03273\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 039 | loss: 0.03273 - R2: 0.9997 -- iter: 0704/1168\n",
      "Training Step: 734  | total loss: \u001b[1m\u001b[32m0.03095\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 039 | loss: 0.03095 - R2: 0.9992 -- iter: 0768/1168\n",
      "Training Step: 735  | total loss: \u001b[1m\u001b[32m0.02988\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 039 | loss: 0.02988 - R2: 0.9990 -- iter: 0832/1168\n",
      "Training Step: 736  | total loss: \u001b[1m\u001b[32m0.02866\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 039 | loss: 0.02866 - R2: 0.9992 -- iter: 0896/1168\n",
      "Training Step: 737  | total loss: \u001b[1m\u001b[32m0.03005\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 039 | loss: 0.03005 - R2: 1.0000 -- iter: 0960/1168\n",
      "Training Step: 738  | total loss: \u001b[1m\u001b[32m0.02940\u001b[0m\u001b[0m | time: 0.169s\n",
      "| SGD | epoch: 039 | loss: 0.02940 - R2: 1.0000 -- iter: 1024/1168\n",
      "Training Step: 739  | total loss: \u001b[1m\u001b[32m0.02758\u001b[0m\u001b[0m | time: 0.174s\n",
      "| SGD | epoch: 039 | loss: 0.02758 - R2: 1.0001 -- iter: 1088/1168\n",
      "Training Step: 740  | total loss: \u001b[1m\u001b[32m0.02571\u001b[0m\u001b[0m | time: 0.178s\n",
      "| SGD | epoch: 039 | loss: 0.02571 - R2: 1.0001 -- iter: 1152/1168\n",
      "Training Step: 741  | total loss: \u001b[1m\u001b[32m0.02455\u001b[0m\u001b[0m | time: 1.198s\n",
      "| SGD | epoch: 039 | loss: 0.02455 - R2: 0.9998 | val_loss: 0.02660 - val_acc: 1.0026 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 742  | total loss: \u001b[1m\u001b[32m0.02455\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 040 | loss: 0.02455 - R2: 0.9998 -- iter: 0064/1168\n",
      "Training Step: 743  | total loss: \u001b[1m\u001b[32m0.02467\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 040 | loss: 0.02467 - R2: 1.0000 -- iter: 0128/1168\n",
      "Training Step: 744  | total loss: \u001b[1m\u001b[32m0.02459\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 040 | loss: 0.02459 - R2: 1.0002 -- iter: 0192/1168\n",
      "Training Step: 745  | total loss: \u001b[1m\u001b[32m0.02221\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 040 | loss: 0.02221 - R2: 1.0002 -- iter: 0256/1168\n",
      "Training Step: 746  | total loss: \u001b[1m\u001b[32m0.02221\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 040 | loss: 0.02221 - R2: 1.0002 -- iter: 0320/1168\n",
      "Training Step: 747  | total loss: \u001b[1m\u001b[32m0.02242\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 040 | loss: 0.02242 - R2: 0.9997 -- iter: 0384/1168\n",
      "Training Step: 748  | total loss: \u001b[1m\u001b[32m0.05467\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 040 | loss: 0.05467 - R2: 1.0003 -- iter: 0448/1168\n",
      "Training Step: 749  | total loss: \u001b[1m\u001b[32m0.04887\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 040 | loss: 0.04887 - R2: 1.0000 -- iter: 0512/1168\n",
      "Training Step: 750  | total loss: \u001b[1m\u001b[32m0.04547\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 040 | loss: 0.04547 - R2: 0.9998 -- iter: 0576/1168\n",
      "Training Step: 751  | total loss: \u001b[1m\u001b[32m0.04224\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 040 | loss: 0.04224 - R2: 0.9997 -- iter: 0640/1168\n",
      "Training Step: 752  | total loss: \u001b[1m\u001b[32m0.04153\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 040 | loss: 0.04153 - R2: 0.9999 -- iter: 0704/1168\n",
      "Training Step: 753  | total loss: \u001b[1m\u001b[32m0.03949\u001b[0m\u001b[0m | time: 0.171s\n",
      "| SGD | epoch: 040 | loss: 0.03949 - R2: 0.9991 -- iter: 0768/1168\n",
      "Training Step: 754  | total loss: \u001b[1m\u001b[32m0.03949\u001b[0m\u001b[0m | time: 0.176s\n",
      "| SGD | epoch: 040 | loss: 0.03949 - R2: 0.9990 -- iter: 0832/1168\n",
      "Training Step: 755  | total loss: \u001b[1m\u001b[32m0.03548\u001b[0m\u001b[0m | time: 0.181s\n",
      "| SGD | epoch: 040 | loss: 0.03548 - R2: 0.9989 -- iter: 0896/1168\n",
      "Training Step: 756  | total loss: \u001b[1m\u001b[32m0.03306\u001b[0m\u001b[0m | time: 0.184s\n",
      "| SGD | epoch: 040 | loss: 0.03306 - R2: 0.9990 -- iter: 0960/1168\n",
      "Training Step: 757  | total loss: \u001b[1m\u001b[32m0.03306\u001b[0m\u001b[0m | time: 0.187s\n",
      "| SGD | epoch: 040 | loss: 0.03306 - R2: 0.9990 -- iter: 1024/1168\n",
      "Training Step: 758  | total loss: \u001b[1m\u001b[32m0.03219\u001b[0m\u001b[0m | time: 0.190s\n",
      "| SGD | epoch: 040 | loss: 0.03219 - R2: 0.9994 -- iter: 1088/1168\n",
      "Training Step: 759  | total loss: \u001b[1m\u001b[32m0.02825\u001b[0m\u001b[0m | time: 0.194s\n",
      "| SGD | epoch: 040 | loss: 0.02825 - R2: 0.9990 -- iter: 1152/1168\n",
      "Training Step: 760  | total loss: \u001b[1m\u001b[32m0.02825\u001b[0m\u001b[0m | time: 1.200s\n",
      "| SGD | epoch: 040 | loss: 0.02825 - R2: 0.9990 | val_loss: 0.02913 - val_acc: 1.0074 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 761  | total loss: \u001b[1m\u001b[32m0.02598\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 041 | loss: 0.02598 - R2: 0.9987 -- iter: 0064/1168\n",
      "Training Step: 762  | total loss: \u001b[1m\u001b[32m0.02511\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 041 | loss: 0.02511 - R2: 0.9993 -- iter: 0128/1168\n",
      "Training Step: 763  | total loss: \u001b[1m\u001b[32m0.02352\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 041 | loss: 0.02352 - R2: 0.9993 -- iter: 0192/1168\n",
      "Training Step: 764  | total loss: \u001b[1m\u001b[32m0.02282\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 041 | loss: 0.02282 - R2: 0.9994 -- iter: 0256/1168\n",
      "Training Step: 765  | total loss: \u001b[1m\u001b[32m0.02369\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 041 | loss: 0.02369 - R2: 0.9998 -- iter: 0320/1168\n",
      "Training Step: 766  | total loss: \u001b[1m\u001b[32m0.02369\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 041 | loss: 0.02369 - R2: 0.9998 -- iter: 0384/1168\n",
      "Training Step: 767  | total loss: \u001b[1m\u001b[32m0.04162\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 041 | loss: 0.04162 - R2: 0.9992 -- iter: 0448/1168\n",
      "Training Step: 768  | total loss: \u001b[1m\u001b[32m0.04162\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 041 | loss: 0.04162 - R2: 0.9992 -- iter: 0512/1168\n",
      "Training Step: 769  | total loss: \u001b[1m\u001b[32m0.03940\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 041 | loss: 0.03940 - R2: 0.9991 -- iter: 0576/1168\n",
      "Training Step: 770  | total loss: \u001b[1m\u001b[32m0.03676\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 041 | loss: 0.03676 - R2: 0.9998 -- iter: 0640/1168\n",
      "Training Step: 771  | total loss: \u001b[1m\u001b[32m0.03476\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 041 | loss: 0.03476 - R2: 1.0002 -- iter: 0704/1168\n",
      "Training Step: 772  | total loss: \u001b[1m\u001b[32m0.03285\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 041 | loss: 0.03285 - R2: 0.9998 -- iter: 0768/1168\n",
      "Training Step: 773  | total loss: \u001b[1m\u001b[32m0.03285\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 041 | loss: 0.03285 - R2: 0.9998 -- iter: 0832/1168\n",
      "Training Step: 774  | total loss: \u001b[1m\u001b[32m0.03084\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 041 | loss: 0.03084 - R2: 0.9999 -- iter: 0896/1168\n",
      "Training Step: 775  | total loss: \u001b[1m\u001b[32m0.03001\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 041 | loss: 0.03001 - R2: 1.0008 -- iter: 0960/1168\n",
      "Training Step: 776  | total loss: \u001b[1m\u001b[32m0.02891\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 041 | loss: 0.02891 - R2: 1.0003 -- iter: 1024/1168\n",
      "Training Step: 777  | total loss: \u001b[1m\u001b[32m0.02891\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 041 | loss: 0.02891 - R2: 1.0003 -- iter: 1088/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 778  | total loss: \u001b[1m\u001b[32m0.02721\u001b[0m\u001b[0m | time: 0.178s\n",
      "| SGD | epoch: 041 | loss: 0.02721 - R2: 1.0004 -- iter: 1152/1168\n",
      "Training Step: 779  | total loss: \u001b[1m\u001b[32m0.03240\u001b[0m\u001b[0m | time: 1.188s\n",
      "| SGD | epoch: 041 | loss: 0.03240 - R2: 1.0016 | val_loss: 0.02827 - val_acc: 0.9957 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 780  | total loss: \u001b[1m\u001b[32m0.03240\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 042 | loss: 0.03240 - R2: 1.0016 -- iter: 0064/1168\n",
      "Training Step: 781  | total loss: \u001b[1m\u001b[32m0.03521\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 042 | loss: 0.03521 - R2: 1.0019 -- iter: 0128/1168\n",
      "Training Step: 782  | total loss: \u001b[1m\u001b[32m0.03469\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 042 | loss: 0.03469 - R2: 1.0002 -- iter: 0192/1168\n",
      "Training Step: 783  | total loss: \u001b[1m\u001b[32m0.03307\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 042 | loss: 0.03307 - R2: 0.9997 -- iter: 0256/1168\n",
      "Training Step: 784  | total loss: \u001b[1m\u001b[32m0.03191\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 042 | loss: 0.03191 - R2: 0.9995 -- iter: 0320/1168\n",
      "Training Step: 785  | total loss: \u001b[1m\u001b[32m0.03136\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 042 | loss: 0.03136 - R2: 0.9992 -- iter: 0384/1168\n",
      "Training Step: 786  | total loss: \u001b[1m\u001b[32m0.03017\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 042 | loss: 0.03017 - R2: 0.9991 -- iter: 0448/1168\n",
      "Training Step: 787  | total loss: \u001b[1m\u001b[32m0.02844\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 042 | loss: 0.02844 - R2: 0.9992 -- iter: 0512/1168\n",
      "Training Step: 788  | total loss: \u001b[1m\u001b[32m0.04335\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 042 | loss: 0.04335 - R2: 0.9992 -- iter: 0576/1168\n",
      "Training Step: 789  | total loss: \u001b[1m\u001b[32m0.04124\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 042 | loss: 0.04124 - R2: 0.9994 -- iter: 0640/1168\n",
      "Training Step: 790  | total loss: \u001b[1m\u001b[32m0.03926\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 042 | loss: 0.03926 - R2: 0.9996 -- iter: 0704/1168\n",
      "Training Step: 791  | total loss: \u001b[1m\u001b[32m0.03926\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 042 | loss: 0.03926 - R2: 0.9996 -- iter: 0768/1168\n",
      "Training Step: 792  | total loss: \u001b[1m\u001b[32m0.03539\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 042 | loss: 0.03539 - R2: 0.9995 -- iter: 0832/1168\n",
      "Training Step: 793  | total loss: \u001b[1m\u001b[32m0.03457\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 042 | loss: 0.03457 - R2: 0.9992 -- iter: 0896/1168\n",
      "Training Step: 794  | total loss: \u001b[1m\u001b[32m0.03457\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 042 | loss: 0.03457 - R2: 0.9992 -- iter: 0960/1168\n",
      "Training Step: 795  | total loss: \u001b[1m\u001b[32m0.03232\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 042 | loss: 0.03232 - R2: 0.9996 -- iter: 1024/1168\n",
      "Training Step: 796  | total loss: \u001b[1m\u001b[32m0.03040\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 042 | loss: 0.03040 - R2: 0.9996 -- iter: 1088/1168\n",
      "Training Step: 797  | total loss: \u001b[1m\u001b[32m0.02936\u001b[0m\u001b[0m | time: 0.165s\n",
      "| SGD | epoch: 042 | loss: 0.02936 - R2: 0.9995 -- iter: 1152/1168\n",
      "Training Step: 798  | total loss: \u001b[1m\u001b[32m0.02950\u001b[0m\u001b[0m | time: 1.172s\n",
      "| SGD | epoch: 042 | loss: 0.02950 - R2: 0.9998 | val_loss: 0.02896 - val_acc: 1.0035 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 799  | total loss: \u001b[1m\u001b[32m0.02950\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 043 | loss: 0.02950 - R2: 0.9998 -- iter: 0064/1168\n",
      "Training Step: 800  | total loss: \u001b[1m\u001b[32m0.02888\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 043 | loss: 0.02888 - R2: 0.9990 -- iter: 0128/1168\n",
      "Training Step: 801  | total loss: \u001b[1m\u001b[32m0.02788\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 043 | loss: 0.02788 - R2: 0.9987 -- iter: 0192/1168\n",
      "Training Step: 802  | total loss: \u001b[1m\u001b[32m0.02720\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 043 | loss: 0.02720 - R2: 0.9996 -- iter: 0256/1168\n",
      "Training Step: 803  | total loss: \u001b[1m\u001b[32m0.02629\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 043 | loss: 0.02629 - R2: 1.0000 -- iter: 0320/1168\n",
      "Training Step: 804  | total loss: \u001b[1m\u001b[32m0.02560\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 043 | loss: 0.02560 - R2: 1.0003 -- iter: 0384/1168\n",
      "Training Step: 805  | total loss: \u001b[1m\u001b[32m0.02561\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 043 | loss: 0.02561 - R2: 1.0006 -- iter: 0448/1168\n",
      "Training Step: 806  | total loss: \u001b[1m\u001b[32m0.02468\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 043 | loss: 0.02468 - R2: 0.9999 -- iter: 0512/1168\n",
      "Training Step: 807  | total loss: \u001b[1m\u001b[32m0.04650\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 043 | loss: 0.04650 - R2: 1.0002 -- iter: 0576/1168\n",
      "Training Step: 808  | total loss: \u001b[1m\u001b[32m0.04650\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 043 | loss: 0.04650 - R2: 1.0002 -- iter: 0640/1168\n",
      "Training Step: 809  | total loss: \u001b[1m\u001b[32m0.04440\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 043 | loss: 0.04440 - R2: 1.0000 -- iter: 0704/1168\n",
      "Training Step: 810  | total loss: \u001b[1m\u001b[32m0.04331\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 043 | loss: 0.04331 - R2: 1.0004 -- iter: 0768/1168\n",
      "Training Step: 811  | total loss: \u001b[1m\u001b[32m0.03901\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 043 | loss: 0.03901 - R2: 0.9998 -- iter: 0832/1168\n",
      "Training Step: 812  | total loss: \u001b[1m\u001b[32m0.03901\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 043 | loss: 0.03901 - R2: 0.9998 -- iter: 0896/1168\n",
      "Training Step: 813  | total loss: \u001b[1m\u001b[32m0.03688\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 043 | loss: 0.03688 - R2: 0.9994 -- iter: 0960/1168\n",
      "Training Step: 814  | total loss: \u001b[1m\u001b[32m0.03433\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 043 | loss: 0.03433 - R2: 0.9991 -- iter: 1024/1168\n",
      "Training Step: 815  | total loss: \u001b[1m\u001b[32m0.03241\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 043 | loss: 0.03241 - R2: 0.9993 -- iter: 1088/1168\n",
      "Training Step: 816  | total loss: \u001b[1m\u001b[32m0.03145\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 043 | loss: 0.03145 - R2: 0.9997 -- iter: 1152/1168\n",
      "Training Step: 817  | total loss: \u001b[1m\u001b[32m0.03003\u001b[0m\u001b[0m | time: 1.138s\n",
      "| SGD | epoch: 043 | loss: 0.03003 - R2: 0.9998 | val_loss: 0.02810 - val_acc: 1.0034 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 818  | total loss: \u001b[1m\u001b[32m0.02947\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 044 | loss: 0.02947 - R2: 1.0000 -- iter: 0064/1168\n",
      "Training Step: 819  | total loss: \u001b[1m\u001b[32m0.02781\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 044 | loss: 0.02781 - R2: 1.0003 -- iter: 0128/1168\n",
      "Training Step: 820  | total loss: \u001b[1m\u001b[32m0.02781\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 044 | loss: 0.02781 - R2: 1.0003 -- iter: 0192/1168\n",
      "Training Step: 821  | total loss: \u001b[1m\u001b[32m0.02593\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 044 | loss: 0.02593 - R2: 1.0003 -- iter: 0256/1168\n",
      "Training Step: 822  | total loss: \u001b[1m\u001b[32m0.02489\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 044 | loss: 0.02489 - R2: 1.0002 -- iter: 0320/1168\n",
      "Training Step: 823  | total loss: \u001b[1m\u001b[32m0.02489\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 044 | loss: 0.02489 - R2: 1.0002 -- iter: 0384/1168\n",
      "Training Step: 824  | total loss: \u001b[1m\u001b[32m0.02391\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 044 | loss: 0.02391 - R2: 0.9995 -- iter: 0448/1168\n",
      "Training Step: 825  | total loss: \u001b[1m\u001b[32m0.02304\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 044 | loss: 0.02304 - R2: 0.9993 -- iter: 0512/1168\n",
      "Training Step: 826  | total loss: \u001b[1m\u001b[32m0.02214\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 044 | loss: 0.02214 - R2: 0.9993 -- iter: 0576/1168\n",
      "Training Step: 827  | total loss: \u001b[1m\u001b[32m0.02237\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 044 | loss: 0.02237 - R2: 0.9998 -- iter: 0640/1168\n",
      "Training Step: 828  | total loss: \u001b[1m\u001b[32m0.04557\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 044 | loss: 0.04557 - R2: 0.9998 -- iter: 0704/1168\n",
      "Training Step: 829  | total loss: \u001b[1m\u001b[32m0.04238\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 044 | loss: 0.04238 - R2: 0.9995 -- iter: 0768/1168\n",
      "Training Step: 830  | total loss: \u001b[1m\u001b[32m0.03968\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 044 | loss: 0.03968 - R2: 0.9992 -- iter: 0832/1168\n",
      "Training Step: 831  | total loss: \u001b[1m\u001b[32m0.03794\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 044 | loss: 0.03794 - R2: 0.9995 -- iter: 0896/1168\n",
      "Training Step: 832  | total loss: \u001b[1m\u001b[32m0.03701\u001b[0m\u001b[0m | time: 0.200s\n",
      "| SGD | epoch: 044 | loss: 0.03701 - R2: 0.9993 -- iter: 0960/1168\n",
      "Training Step: 833  | total loss: \u001b[1m\u001b[32m0.03701\u001b[0m\u001b[0m | time: 0.202s\n",
      "| SGD | epoch: 044 | loss: 0.03701 - R2: 0.9993 -- iter: 1024/1168\n",
      "Training Step: 834  | total loss: \u001b[1m\u001b[32m0.03458\u001b[0m\u001b[0m | time: 0.205s\n",
      "| SGD | epoch: 044 | loss: 0.03458 - R2: 0.9995 -- iter: 1088/1168\n",
      "Training Step: 835  | total loss: \u001b[1m\u001b[32m0.03349\u001b[0m\u001b[0m | time: 0.209s\n",
      "| SGD | epoch: 044 | loss: 0.03349 - R2: 0.9995 -- iter: 1152/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 836  | total loss: \u001b[1m\u001b[32m0.03389\u001b[0m\u001b[0m | time: 1.224s\n",
      "| SGD | epoch: 044 | loss: 0.03389 - R2: 1.0000 | val_loss: 0.02818 - val_acc: 1.0028 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 837  | total loss: \u001b[1m\u001b[32m0.03389\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 045 | loss: 0.03389 - R2: 1.0000 -- iter: 0064/1168\n",
      "Training Step: 838  | total loss: \u001b[1m\u001b[32m0.03243\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 045 | loss: 0.03243 - R2: 1.0000 -- iter: 0128/1168\n",
      "Training Step: 839  | total loss: \u001b[1m\u001b[32m0.03125\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 045 | loss: 0.03125 - R2: 0.9997 -- iter: 0192/1168\n",
      "Training Step: 840  | total loss: \u001b[1m\u001b[32m0.02856\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 045 | loss: 0.02856 - R2: 0.9993 -- iter: 0256/1168\n",
      "Training Step: 841  | total loss: \u001b[1m\u001b[32m0.02604\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 045 | loss: 0.02604 - R2: 0.9993 -- iter: 0320/1168\n",
      "Training Step: 842  | total loss: \u001b[1m\u001b[32m0.02457\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 045 | loss: 0.02457 - R2: 0.9997 -- iter: 0384/1168\n",
      "Training Step: 843  | total loss: \u001b[1m\u001b[32m0.02416\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 045 | loss: 0.02416 - R2: 1.0002 -- iter: 0448/1168\n",
      "Training Step: 844  | total loss: \u001b[1m\u001b[32m0.02333\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 045 | loss: 0.02333 - R2: 1.0002 -- iter: 0512/1168\n",
      "Training Step: 845  | total loss: \u001b[1m\u001b[32m0.02303\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 045 | loss: 0.02303 - R2: 1.0007 -- iter: 0576/1168\n",
      "Training Step: 846  | total loss: \u001b[1m\u001b[32m0.02346\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 045 | loss: 0.02346 - R2: 1.0006 -- iter: 0640/1168\n",
      "Training Step: 847  | total loss: \u001b[1m\u001b[32m0.02166\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 045 | loss: 0.02166 - R2: 1.0002 -- iter: 0704/1168\n",
      "Training Step: 848  | total loss: \u001b[1m\u001b[32m0.02166\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 045 | loss: 0.02166 - R2: 1.0002 -- iter: 0768/1168\n",
      "Training Step: 849  | total loss: \u001b[1m\u001b[32m0.02205\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 045 | loss: 0.02205 - R2: 1.0002 -- iter: 0832/1168\n",
      "Training Step: 850  | total loss: \u001b[1m\u001b[32m0.02111\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 045 | loss: 0.02111 - R2: 0.9999 -- iter: 0896/1168\n",
      "Training Step: 851  | total loss: \u001b[1m\u001b[32m0.02028\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 045 | loss: 0.02028 - R2: 0.9999 -- iter: 0960/1168\n",
      "Training Step: 852  | total loss: \u001b[1m\u001b[32m0.02028\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 045 | loss: 0.02028 - R2: 0.9999 -- iter: 1024/1168\n",
      "Training Step: 853  | total loss: \u001b[1m\u001b[32m0.01986\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 045 | loss: 0.01986 - R2: 1.0000 -- iter: 1088/1168\n",
      "Training Step: 854  | total loss: \u001b[1m\u001b[32m0.01923\u001b[0m\u001b[0m | time: 0.169s\n",
      "| SGD | epoch: 045 | loss: 0.01923 - R2: 1.0003 -- iter: 1152/1168\n",
      "Training Step: 855  | total loss: \u001b[1m\u001b[32m0.01918\u001b[0m\u001b[0m | time: 1.188s\n",
      "| SGD | epoch: 045 | loss: 0.01918 - R2: 1.0002 | val_loss: 0.02702 - val_acc: 1.0018 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 856  | total loss: \u001b[1m\u001b[32m0.01831\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 046 | loss: 0.01831 - R2: 1.0003 -- iter: 0064/1168\n",
      "Training Step: 857  | total loss: \u001b[1m\u001b[32m0.01900\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 046 | loss: 0.01900 - R2: 1.0000 -- iter: 0128/1168\n",
      "Training Step: 858  | total loss: \u001b[1m\u001b[32m0.01887\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 046 | loss: 0.01887 - R2: 0.9996 -- iter: 0192/1168\n",
      "Training Step: 859  | total loss: \u001b[1m\u001b[32m0.01946\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 046 | loss: 0.01946 - R2: 0.9991 -- iter: 0256/1168\n",
      "Training Step: 860  | total loss: \u001b[1m\u001b[32m0.01976\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 046 | loss: 0.01976 - R2: 0.9989 -- iter: 0320/1168\n",
      "Training Step: 861  | total loss: \u001b[1m\u001b[32m0.02148\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 046 | loss: 0.02148 - R2: 1.0001 -- iter: 0384/1168\n",
      "Training Step: 862  | total loss: \u001b[1m\u001b[32m0.02112\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 046 | loss: 0.02112 - R2: 0.9999 -- iter: 0448/1168\n",
      "Training Step: 863  | total loss: \u001b[1m\u001b[32m0.02112\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 046 | loss: 0.02112 - R2: 0.9999 -- iter: 0512/1168\n",
      "Training Step: 864  | total loss: \u001b[1m\u001b[32m0.02197\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 046 | loss: 0.02197 - R2: 0.9995 -- iter: 0576/1168\n",
      "Training Step: 865  | total loss: \u001b[1m\u001b[32m0.02197\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 046 | loss: 0.02197 - R2: 0.9995 -- iter: 0640/1168\n",
      "Training Step: 866  | total loss: \u001b[1m\u001b[32m0.02087\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 046 | loss: 0.02087 - R2: 0.9995 -- iter: 0704/1168\n",
      "Training Step: 867  | total loss: \u001b[1m\u001b[32m0.05065\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 046 | loss: 0.05065 - R2: 1.0005 -- iter: 0768/1168\n",
      "Training Step: 868  | total loss: \u001b[1m\u001b[32m0.04865\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 046 | loss: 0.04865 - R2: 1.0000 -- iter: 0832/1168\n",
      "Training Step: 869  | total loss: \u001b[1m\u001b[32m0.04865\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 046 | loss: 0.04865 - R2: 0.9999 -- iter: 0896/1168\n",
      "Training Step: 870  | total loss: \u001b[1m\u001b[32m0.04192\u001b[0m\u001b[0m | time: 0.173s\n",
      "| SGD | epoch: 046 | loss: 0.04192 - R2: 0.9999 -- iter: 0960/1168\n",
      "Training Step: 871  | total loss: \u001b[1m\u001b[32m0.04104\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 046 | loss: 0.04104 - R2: 1.0002 -- iter: 1024/1168\n",
      "Training Step: 872  | total loss: \u001b[1m\u001b[32m0.03940\u001b[0m\u001b[0m | time: 0.181s\n",
      "| SGD | epoch: 046 | loss: 0.03940 - R2: 0.9994 -- iter: 1088/1168\n",
      "Training Step: 873  | total loss: \u001b[1m\u001b[32m0.03940\u001b[0m\u001b[0m | time: 0.183s\n",
      "| SGD | epoch: 046 | loss: 0.03940 - R2: 0.9994 -- iter: 1152/1168\n",
      "Training Step: 874  | total loss: \u001b[1m\u001b[32m0.03553\u001b[0m\u001b[0m | time: 1.198s\n",
      "| SGD | epoch: 046 | loss: 0.03553 - R2: 0.9992 | val_loss: 0.02907 - val_acc: 1.0053 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 875  | total loss: \u001b[1m\u001b[32m0.03368\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 047 | loss: 0.03368 - R2: 0.9991 -- iter: 0064/1168\n",
      "Training Step: 876  | total loss: \u001b[1m\u001b[32m0.03368\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 047 | loss: 0.03368 - R2: 0.9991 -- iter: 0128/1168\n",
      "Training Step: 877  | total loss: \u001b[1m\u001b[32m0.03001\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 047 | loss: 0.03001 - R2: 0.9994 -- iter: 0192/1168\n",
      "Training Step: 878  | total loss: \u001b[1m\u001b[32m0.02894\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 047 | loss: 0.02894 - R2: 1.0000 -- iter: 0256/1168\n",
      "Training Step: 879  | total loss: \u001b[1m\u001b[32m0.02894\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 047 | loss: 0.02894 - R2: 1.0000 -- iter: 0320/1168\n",
      "Training Step: 880  | total loss: \u001b[1m\u001b[32m0.02660\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 047 | loss: 0.02660 - R2: 0.9993 -- iter: 0384/1168\n",
      "Training Step: 881  | total loss: \u001b[1m\u001b[32m0.02719\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 047 | loss: 0.02719 - R2: 0.9998 -- iter: 0448/1168\n",
      "Training Step: 882  | total loss: \u001b[1m\u001b[32m0.02662\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 047 | loss: 0.02662 - R2: 1.0002 -- iter: 0512/1168\n",
      "Training Step: 883  | total loss: \u001b[1m\u001b[32m0.02662\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 047 | loss: 0.02662 - R2: 1.0002 -- iter: 0576/1168\n",
      "Training Step: 884  | total loss: \u001b[1m\u001b[32m0.02693\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 047 | loss: 0.02693 - R2: 1.0005 -- iter: 0640/1168\n",
      "Training Step: 885  | total loss: \u001b[1m\u001b[32m0.02463\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 047 | loss: 0.02463 - R2: 1.0002 -- iter: 0704/1168\n",
      "Training Step: 886  | total loss: \u001b[1m\u001b[32m0.02463\u001b[0m\u001b[0m | time: 0.169s\n",
      "| SGD | epoch: 047 | loss: 0.02463 - R2: 1.0002 -- iter: 0768/1168\n",
      "Training Step: 887  | total loss: \u001b[1m\u001b[32m0.02292\u001b[0m\u001b[0m | time: 0.173s\n",
      "| SGD | epoch: 047 | loss: 0.02292 - R2: 0.9999 -- iter: 0832/1168\n",
      "Training Step: 888  | total loss: \u001b[1m\u001b[32m0.02292\u001b[0m\u001b[0m | time: 0.176s\n",
      "| SGD | epoch: 047 | loss: 0.02292 - R2: 1.0001 -- iter: 0896/1168\n",
      "Training Step: 889  | total loss: \u001b[1m\u001b[32m0.02345\u001b[0m\u001b[0m | time: 0.180s\n",
      "| SGD | epoch: 047 | loss: 0.02345 - R2: 1.0004 -- iter: 0960/1168\n",
      "Training Step: 890  | total loss: \u001b[1m\u001b[32m0.02351\u001b[0m\u001b[0m | time: 0.190s\n",
      "| SGD | epoch: 047 | loss: 0.02351 - R2: 1.0002 -- iter: 1024/1168\n",
      "Training Step: 891  | total loss: \u001b[1m\u001b[32m0.02232\u001b[0m\u001b[0m | time: 0.194s\n",
      "| SGD | epoch: 047 | loss: 0.02232 - R2: 1.0000 -- iter: 1088/1168\n",
      "Training Step: 892  | total loss: \u001b[1m\u001b[32m0.02136\u001b[0m\u001b[0m | time: 0.200s\n",
      "| SGD | epoch: 047 | loss: 0.02136 - R2: 0.9998 -- iter: 1152/1168\n",
      "Training Step: 893  | total loss: \u001b[1m\u001b[32m0.02189\u001b[0m\u001b[0m | time: 1.206s\n",
      "| SGD | epoch: 047 | loss: 0.02189 - R2: 0.9999 | val_loss: 0.02873 - val_acc: 1.0032 -- iter: 1168/1168\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 894  | total loss: \u001b[1m\u001b[32m0.02189\u001b[0m\u001b[0m | time: 0.023s\n",
      "| SGD | epoch: 048 | loss: 0.02189 - R2: 0.9999 -- iter: 0064/1168\n",
      "Training Step: 895  | total loss: \u001b[1m\u001b[32m0.02204\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 048 | loss: 0.02204 - R2: 1.0000 -- iter: 0128/1168\n",
      "Training Step: 896  | total loss: \u001b[1m\u001b[32m0.02176\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 048 | loss: 0.02176 - R2: 1.0003 -- iter: 0192/1168\n",
      "Training Step: 897  | total loss: \u001b[1m\u001b[32m0.02113\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 048 | loss: 0.02113 - R2: 0.9999 -- iter: 0256/1168\n",
      "Training Step: 898  | total loss: \u001b[1m\u001b[32m0.02017\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 048 | loss: 0.02017 - R2: 0.9998 -- iter: 0320/1168\n",
      "Training Step: 899  | total loss: \u001b[1m\u001b[32m0.01942\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 048 | loss: 0.01942 - R2: 0.9999 -- iter: 0384/1168\n",
      "Training Step: 900  | total loss: \u001b[1m\u001b[32m0.02645\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 048 | loss: 0.02645 - R2: 1.0016 -- iter: 0448/1168\n",
      "Training Step: 901  | total loss: \u001b[1m\u001b[32m0.03044\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 048 | loss: 0.03044 - R2: 1.0016 -- iter: 0512/1168\n",
      "Training Step: 902  | total loss: \u001b[1m\u001b[32m0.02959\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 048 | loss: 0.02959 - R2: 1.0004 -- iter: 0576/1168\n",
      "Training Step: 903  | total loss: \u001b[1m\u001b[32m0.02795\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 048 | loss: 0.02795 - R2: 1.0002 -- iter: 0640/1168\n",
      "Training Step: 904  | total loss: \u001b[1m\u001b[32m0.02778\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 048 | loss: 0.02778 - R2: 0.9988 -- iter: 0704/1168\n",
      "Training Step: 905  | total loss: \u001b[1m\u001b[32m0.02715\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 048 | loss: 0.02715 - R2: 0.9988 -- iter: 0768/1168\n",
      "Training Step: 906  | total loss: \u001b[1m\u001b[32m0.02584\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 048 | loss: 0.02584 - R2: 0.9988 -- iter: 0832/1168\n",
      "Training Step: 907  | total loss: \u001b[1m\u001b[32m0.02502\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 048 | loss: 0.02502 - R2: 0.9992 -- iter: 0896/1168\n",
      "Training Step: 908  | total loss: \u001b[1m\u001b[32m0.02517\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 048 | loss: 0.02517 - R2: 0.9992 -- iter: 0960/1168\n",
      "Training Step: 909  | total loss: \u001b[1m\u001b[32m0.02480\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 048 | loss: 0.02480 - R2: 0.9999 -- iter: 1024/1168\n",
      "Training Step: 910  | total loss: \u001b[1m\u001b[32m0.02497\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 048 | loss: 0.02497 - R2: 0.9997 -- iter: 1088/1168\n",
      "Training Step: 911  | total loss: \u001b[1m\u001b[32m0.02402\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 048 | loss: 0.02402 - R2: 0.9989 -- iter: 1152/1168\n",
      "Training Step: 912  | total loss: \u001b[1m\u001b[32m0.02487\u001b[0m\u001b[0m | time: 1.160s\n",
      "| SGD | epoch: 048 | loss: 0.02487 - R2: 0.9999 | val_loss: 0.02771 - val_acc: 1.0024 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 913  | total loss: \u001b[1m\u001b[32m0.02487\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 049 | loss: 0.02487 - R2: 0.9999 -- iter: 0064/1168\n",
      "Training Step: 914  | total loss: \u001b[1m\u001b[32m0.02369\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 049 | loss: 0.02369 - R2: 0.9999 -- iter: 0128/1168\n",
      "Training Step: 915  | total loss: \u001b[1m\u001b[32m0.02244\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 049 | loss: 0.02244 - R2: 0.9998 -- iter: 0192/1168\n",
      "Training Step: 916  | total loss: \u001b[1m\u001b[32m0.02244\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 049 | loss: 0.02244 - R2: 0.9998 -- iter: 0256/1168\n",
      "Training Step: 917  | total loss: \u001b[1m\u001b[32m0.02154\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 049 | loss: 0.02154 - R2: 0.9999 -- iter: 0320/1168\n",
      "Training Step: 918  | total loss: \u001b[1m\u001b[32m0.02089\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 049 | loss: 0.02089 - R2: 0.9996 -- iter: 0384/1168\n",
      "Training Step: 919  | total loss: \u001b[1m\u001b[32m0.02184\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 049 | loss: 0.02184 - R2: 1.0001 -- iter: 0448/1168\n",
      "Training Step: 920  | total loss: \u001b[1m\u001b[32m0.02184\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 049 | loss: 0.02184 - R2: 1.0001 -- iter: 0512/1168\n",
      "Training Step: 921  | total loss: \u001b[1m\u001b[32m0.02127\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 049 | loss: 0.02127 - R2: 1.0002 -- iter: 0576/1168\n",
      "Training Step: 922  | total loss: \u001b[1m\u001b[32m0.02127\u001b[0m\u001b[0m | time: 0.165s\n",
      "| SGD | epoch: 049 | loss: 0.02127 - R2: 1.0002 -- iter: 0640/1168\n",
      "Training Step: 923  | total loss: \u001b[1m\u001b[32m0.02178\u001b[0m\u001b[0m | time: 0.172s\n",
      "| SGD | epoch: 049 | loss: 0.02178 - R2: 0.9999 -- iter: 0704/1168\n",
      "Training Step: 924  | total loss: \u001b[1m\u001b[32m0.02251\u001b[0m\u001b[0m | time: 0.176s\n",
      "| SGD | epoch: 049 | loss: 0.02251 - R2: 0.9998 -- iter: 0768/1168\n",
      "Training Step: 925  | total loss: \u001b[1m\u001b[32m0.02251\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 049 | loss: 0.02251 - R2: 0.9998 -- iter: 0832/1168\n",
      "Training Step: 926  | total loss: \u001b[1m\u001b[32m0.02154\u001b[0m\u001b[0m | time: 0.181s\n",
      "| SGD | epoch: 049 | loss: 0.02154 - R2: 1.0000 -- iter: 0896/1168\n",
      "Training Step: 927  | total loss: \u001b[1m\u001b[32m0.02119\u001b[0m\u001b[0m | time: 0.183s\n",
      "| SGD | epoch: 049 | loss: 0.02119 - R2: 1.0000 -- iter: 0960/1168\n",
      "Training Step: 928  | total loss: \u001b[1m\u001b[32m0.02077\u001b[0m\u001b[0m | time: 0.185s\n",
      "| SGD | epoch: 049 | loss: 0.02077 - R2: 0.9994 -- iter: 1024/1168\n",
      "Training Step: 929  | total loss: \u001b[1m\u001b[32m0.02000\u001b[0m\u001b[0m | time: 0.189s\n",
      "| SGD | epoch: 049 | loss: 0.02000 - R2: 0.9995 -- iter: 1088/1168\n",
      "Training Step: 930  | total loss: \u001b[1m\u001b[32m0.02000\u001b[0m\u001b[0m | time: 0.193s\n",
      "| SGD | epoch: 049 | loss: 0.02000 - R2: 0.9995 -- iter: 1152/1168\n",
      "Training Step: 931  | total loss: \u001b[1m\u001b[32m0.02180\u001b[0m\u001b[0m | time: 1.201s\n",
      "| SGD | epoch: 049 | loss: 0.02180 - R2: 0.9999 | val_loss: 0.02816 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 932  | total loss: \u001b[1m\u001b[32m0.02135\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 050 | loss: 0.02135 - R2: 0.9998 -- iter: 0064/1168\n",
      "Training Step: 933  | total loss: \u001b[1m\u001b[32m0.02068\u001b[0m\u001b[0m | time: 0.006s\n",
      "| SGD | epoch: 050 | loss: 0.02068 - R2: 0.9997 -- iter: 0128/1168\n",
      "Training Step: 934  | total loss: \u001b[1m\u001b[32m0.02102\u001b[0m\u001b[0m | time: 0.010s\n",
      "| SGD | epoch: 050 | loss: 0.02102 - R2: 0.9999 -- iter: 0192/1168\n",
      "Training Step: 935  | total loss: \u001b[1m\u001b[32m0.02083\u001b[0m\u001b[0m | time: 0.020s\n",
      "| SGD | epoch: 050 | loss: 0.02083 - R2: 1.0000 -- iter: 0256/1168\n",
      "Training Step: 936  | total loss: \u001b[1m\u001b[32m0.02105\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 050 | loss: 0.02105 - R2: 1.0003 -- iter: 0320/1168\n",
      "Training Step: 937  | total loss: \u001b[1m\u001b[32m0.02105\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 050 | loss: 0.02105 - R2: 1.0003 -- iter: 0384/1168\n",
      "Training Step: 938  | total loss: \u001b[1m\u001b[32m0.02031\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 050 | loss: 0.02031 - R2: 0.9997 -- iter: 0448/1168\n",
      "Training Step: 939  | total loss: \u001b[1m\u001b[32m0.01974\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 050 | loss: 0.01974 - R2: 0.9997 -- iter: 0512/1168\n",
      "Training Step: 940  | total loss: \u001b[1m\u001b[32m0.01853\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 050 | loss: 0.01853 - R2: 0.9997 -- iter: 0576/1168\n",
      "Training Step: 941  | total loss: \u001b[1m\u001b[32m0.01869\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 050 | loss: 0.01869 - R2: 0.9999 -- iter: 0640/1168\n",
      "Training Step: 942  | total loss: \u001b[1m\u001b[32m0.01818\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 050 | loss: 0.01818 - R2: 0.9997 -- iter: 0704/1168\n",
      "Training Step: 943  | total loss: \u001b[1m\u001b[32m0.01757\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 050 | loss: 0.01757 - R2: 0.9993 -- iter: 0768/1168\n",
      "Training Step: 944  | total loss: \u001b[1m\u001b[32m0.01797\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 050 | loss: 0.01797 - R2: 1.0000 -- iter: 0832/1168\n",
      "Training Step: 945  | total loss: \u001b[1m\u001b[32m0.01888\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 050 | loss: 0.01888 - R2: 1.0004 -- iter: 0896/1168\n",
      "Training Step: 946  | total loss: \u001b[1m\u001b[32m0.01888\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 050 | loss: 0.01888 - R2: 1.0004 -- iter: 0960/1168\n",
      "Training Step: 947  | total loss: \u001b[1m\u001b[32m0.04201\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 050 | loss: 0.04201 - R2: 1.0012 -- iter: 1024/1168\n",
      "Training Step: 948  | total loss: \u001b[1m\u001b[32m0.03985\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 050 | loss: 0.03985 - R2: 1.0008 -- iter: 1088/1168\n",
      "Training Step: 949  | total loss: \u001b[1m\u001b[32m0.03749\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 050 | loss: 0.03749 - R2: 1.0003 -- iter: 1152/1168\n",
      "Training Step: 950  | total loss: \u001b[1m\u001b[32m0.03480\u001b[0m\u001b[0m | time: 1.135s\n",
      "| SGD | epoch: 050 | loss: 0.03480 - R2: 1.0001 | val_loss: 0.02747 - val_acc: 0.9999 -- iter: 1168/1168\n",
      "--\n",
      "---------------------------------\n",
      "Run id: MEN2I5\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "INFO:tensorflow:Summary name StandardError/ (raw) is illegal; using StandardError/__raw_ instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Training samples: 1168\n",
      "Validation samples: 292\n",
      "--\n",
      "Training Step: 1  | time: 0.043s\n",
      "| SGD | epoch: 001 | loss: 0.00000 - R2: 0.0000 -- iter: 0064/1168\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m129.04245\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 001 | loss: 129.04245 - R2: 0.0000 -- iter: 0128/1168\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m132.48660\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 001 | loss: 132.48660 - R2: 0.0014 -- iter: 0192/1168\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m124.09962\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 001 | loss: 124.09962 - R2: 0.0054 -- iter: 0256/1168\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m115.59547\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 001 | loss: 115.59547 - R2: 0.0116 -- iter: 0320/1168\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m99.43223\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 001 | loss: 99.43223 - R2: 0.0302 -- iter: 0384/1168\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m92.18421\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 001 | loss: 92.18421 - R2: 0.0422 -- iter: 0448/1168\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m92.18421\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 001 | loss: 92.18421 - R2: 0.0559 -- iter: 0512/1168\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m78.56851\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 001 | loss: 78.56851 - R2: 0.0559 -- iter: 0576/1168\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m71.47784\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 001 | loss: 71.47784 - R2: 0.0912 -- iter: 0640/1168\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m65.11227\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 001 | loss: 65.11227 - R2: 0.1127 -- iter: 0704/1168\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m65.11227\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 001 | loss: 65.11227 - R2: 0.1127 -- iter: 0768/1168\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m52.34727\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 001 | loss: 52.34727 - R2: 0.1673 -- iter: 0832/1168\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m52.34727\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 001 | loss: 52.34727 - R2: 0.1673 -- iter: 0896/1168\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m45.38641\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 001 | loss: 45.38641 - R2: 0.2040 -- iter: 0960/1168\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m39.02819\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 001 | loss: 39.02819 - R2: 0.2447 -- iter: 1024/1168\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m32.43303\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 001 | loss: 32.43303 - R2: 0.2950 -- iter: 1088/1168\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m26.50397\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 001 | loss: 26.50397 - R2: 0.3505 -- iter: 1152/1168\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m21.01986\u001b[0m\u001b[0m | time: 1.125s\n",
      "| SGD | epoch: 001 | loss: 21.01986 - R2: 0.4141 | val_loss: 3.47631 - val_acc: 0.7190 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m16.53679\u001b[0m\u001b[0m | time: 0.006s\n",
      "| SGD | epoch: 002 | loss: 16.53679 - R2: 0.4786 -- iter: 0064/1168\n",
      "Training Step: 21  | total loss: \u001b[1m\u001b[32m12.47293\u001b[0m\u001b[0m | time: 0.013s\n",
      "| SGD | epoch: 002 | loss: 12.47293 - R2: 0.5548 -- iter: 0128/1168\n",
      "Training Step: 22  | total loss: \u001b[1m\u001b[32m9.09947\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 002 | loss: 9.09947 - R2: 0.6384 -- iter: 0192/1168\n",
      "Training Step: 23  | total loss: \u001b[1m\u001b[32m6.53852\u001b[0m\u001b[0m | time: 0.018s\n",
      "| SGD | epoch: 002 | loss: 6.53852 - R2: 0.7254 -- iter: 0256/1168\n",
      "Training Step: 24  | total loss: \u001b[1m\u001b[32m4.75053\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 002 | loss: 4.75053 - R2: 0.8088 -- iter: 0320/1168\n",
      "Training Step: 25  | total loss: \u001b[1m\u001b[32m3.47776\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 002 | loss: 3.47776 - R2: 0.8621 -- iter: 0384/1168\n",
      "Training Step: 26  | total loss: \u001b[1m\u001b[32m2.58965\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 002 | loss: 2.58965 - R2: 0.8937 -- iter: 0448/1168\n",
      "Training Step: 27  | total loss: \u001b[1m\u001b[32m1.93649\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 002 | loss: 1.93649 - R2: 0.9224 -- iter: 0512/1168\n",
      "Training Step: 28  | total loss: \u001b[1m\u001b[32m1.46902\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 002 | loss: 1.46902 - R2: 0.9550 -- iter: 0576/1168\n",
      "Training Step: 29  | total loss: \u001b[1m\u001b[32m0.87485\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 002 | loss: 0.87485 - R2: 0.9659 -- iter: 0640/1168\n",
      "Training Step: 30  | total loss: \u001b[1m\u001b[32m0.87485\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 002 | loss: 0.87485 - R2: 0.9659 -- iter: 0704/1168\n",
      "Training Step: 31  | total loss: \u001b[1m\u001b[32m0.68431\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 002 | loss: 0.68431 - R2: 0.9754 -- iter: 0768/1168\n",
      "Training Step: 32  | total loss: \u001b[1m\u001b[32m0.43199\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 002 | loss: 0.43199 - R2: 0.9848 -- iter: 0832/1168\n",
      "Training Step: 33  | total loss: \u001b[1m\u001b[32m0.37262\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 002 | loss: 0.37262 - R2: 0.9896 -- iter: 0896/1168\n",
      "Training Step: 34  | total loss: \u001b[1m\u001b[32m0.37262\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 002 | loss: 0.37262 - R2: 0.9896 -- iter: 0960/1168\n",
      "Training Step: 35  | total loss: \u001b[1m\u001b[32m0.30127\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 002 | loss: 0.30127 - R2: 0.9909 -- iter: 1024/1168\n",
      "Training Step: 36  | total loss: \u001b[1m\u001b[32m0.20370\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 002 | loss: 0.20370 - R2: 0.9932 -- iter: 1088/1168\n",
      "Training Step: 37  | total loss: \u001b[1m\u001b[32m0.20370\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 002 | loss: 0.20370 - R2: 0.9932 -- iter: 1152/1168\n",
      "Training Step: 38  | total loss: \u001b[1m\u001b[32m0.14419\u001b[0m\u001b[0m | time: 1.090s\n",
      "| SGD | epoch: 002 | loss: 0.14419 - R2: 0.9955 | val_loss: 0.02750 - val_acc: 0.9973 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 39  | total loss: \u001b[1m\u001b[32m0.14419\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 003 | loss: 0.14419 - R2: 0.9955 -- iter: 0064/1168\n",
      "Training Step: 40  | total loss: \u001b[1m\u001b[32m0.12089\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 003 | loss: 0.12089 - R2: 0.9950 -- iter: 0128/1168\n",
      "Training Step: 41  | total loss: \u001b[1m\u001b[32m0.10100\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 003 | loss: 0.10100 - R2: 0.9957 -- iter: 0192/1168\n",
      "Training Step: 42  | total loss: \u001b[1m\u001b[32m0.07703\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 003 | loss: 0.07703 - R2: 0.9979 -- iter: 0256/1168\n",
      "Training Step: 43  | total loss: \u001b[1m\u001b[32m0.07703\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 003 | loss: 0.07703 - R2: 0.9979 -- iter: 0320/1168\n",
      "Training Step: 44  | total loss: \u001b[1m\u001b[32m0.06804\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 003 | loss: 0.06804 - R2: 0.9978 -- iter: 0384/1168\n",
      "Training Step: 45  | total loss: \u001b[1m\u001b[32m0.05430\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 003 | loss: 0.05430 - R2: 0.9998 -- iter: 0448/1168\n",
      "Training Step: 46  | total loss: \u001b[1m\u001b[32m0.05540\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 003 | loss: 0.05540 - R2: 1.0000 -- iter: 0512/1168\n",
      "Training Step: 47  | total loss: \u001b[1m\u001b[32m0.04996\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 003 | loss: 0.04996 - R2: 1.0004 -- iter: 0576/1168\n",
      "Training Step: 48  | total loss: \u001b[1m\u001b[32m0.04755\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 003 | loss: 0.04755 - R2: 1.0004 -- iter: 0640/1168\n",
      "Training Step: 49  | total loss: \u001b[1m\u001b[32m0.04408\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 003 | loss: 0.04408 - R2: 0.9993 -- iter: 0704/1168\n",
      "Training Step: 50  | total loss: \u001b[1m\u001b[32m0.04408\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 003 | loss: 0.04408 - R2: 0.9993 -- iter: 0768/1168\n",
      "Training Step: 51  | total loss: \u001b[1m\u001b[32m0.04051\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 003 | loss: 0.04051 - R2: 0.9996 -- iter: 0832/1168\n",
      "Training Step: 52  | total loss: \u001b[1m\u001b[32m0.03636\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 003 | loss: 0.03636 - R2: 1.0000 -- iter: 0896/1168\n",
      "Training Step: 53  | total loss: \u001b[1m\u001b[32m0.03636\u001b[0m\u001b[0m | time: 0.185s\n",
      "| SGD | epoch: 003 | loss: 0.03636 - R2: 1.0000 -- iter: 0960/1168\n",
      "Training Step: 54  | total loss: \u001b[1m\u001b[32m0.03199\u001b[0m\u001b[0m | time: 0.189s\n",
      "| SGD | epoch: 003 | loss: 0.03199 - R2: 0.9997 -- iter: 1024/1168\n",
      "Training Step: 55  | total loss: \u001b[1m\u001b[32m0.03199\u001b[0m\u001b[0m | time: 0.192s\n",
      "| SGD | epoch: 003 | loss: 0.03199 - R2: 0.9997 -- iter: 1088/1168\n",
      "Training Step: 56  | total loss: \u001b[1m\u001b[32m0.02977\u001b[0m\u001b[0m | time: 0.194s\n",
      "| SGD | epoch: 003 | loss: 0.02977 - R2: 1.0004 -- iter: 1152/1168\n",
      "Training Step: 57  | total loss: \u001b[1m\u001b[32m0.02853\u001b[0m\u001b[0m | time: 1.202s\n",
      "| SGD | epoch: 003 | loss: 0.02853 - R2: 1.0003 | val_loss: 0.01922 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 58  | total loss: \u001b[1m\u001b[32m0.02853\u001b[0m\u001b[0m | time: 0.019s\n",
      "| SGD | epoch: 004 | loss: 0.02853 - R2: 1.0003 -- iter: 0064/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 59  | total loss: \u001b[1m\u001b[32m0.03424\u001b[0m\u001b[0m | time: 0.023s\n",
      "| SGD | epoch: 004 | loss: 0.03424 - R2: 1.0014 -- iter: 0128/1168\n",
      "Training Step: 60  | total loss: \u001b[1m\u001b[32m0.03313\u001b[0m\u001b[0m | time: 0.026s\n",
      "| SGD | epoch: 004 | loss: 0.03313 - R2: 1.0015 -- iter: 0192/1168\n",
      "Training Step: 61  | total loss: \u001b[1m\u001b[32m0.03049\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 004 | loss: 0.03049 - R2: 1.0009 -- iter: 0256/1168\n",
      "Training Step: 62  | total loss: \u001b[1m\u001b[32m0.03049\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 004 | loss: 0.03049 - R2: 1.0009 -- iter: 0320/1168\n",
      "Training Step: 63  | total loss: \u001b[1m\u001b[32m0.02816\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 004 | loss: 0.02816 - R2: 0.9996 -- iter: 0384/1168\n",
      "Training Step: 64  | total loss: \u001b[1m\u001b[32m0.02730\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 004 | loss: 0.02730 - R2: 0.9990 -- iter: 0448/1168\n",
      "Training Step: 65  | total loss: \u001b[1m\u001b[32m0.02730\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 004 | loss: 0.02730 - R2: 0.9987 -- iter: 0512/1168\n",
      "Training Step: 66  | total loss: \u001b[1m\u001b[32m0.02609\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 004 | loss: 0.02609 - R2: 0.9993 -- iter: 0576/1168\n",
      "Training Step: 67  | total loss: \u001b[1m\u001b[32m0.06345\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 004 | loss: 0.06345 - R2: 0.9971 -- iter: 0640/1168\n",
      "Training Step: 68  | total loss: \u001b[1m\u001b[32m0.06130\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 004 | loss: 0.06130 - R2: 0.9987 -- iter: 0704/1168\n",
      "Training Step: 69  | total loss: \u001b[1m\u001b[32m0.05722\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 004 | loss: 0.05722 - R2: 0.9986 -- iter: 0768/1168\n",
      "Training Step: 70  | total loss: \u001b[1m\u001b[32m0.05321\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 004 | loss: 0.05321 - R2: 0.9989 -- iter: 0832/1168\n",
      "Training Step: 71  | total loss: \u001b[1m\u001b[32m0.04829\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 004 | loss: 0.04829 - R2: 0.9987 -- iter: 0896/1168\n",
      "Training Step: 72  | total loss: \u001b[1m\u001b[32m0.04829\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 004 | loss: 0.04829 - R2: 0.9987 -- iter: 0960/1168\n",
      "Training Step: 73  | total loss: \u001b[1m\u001b[32m0.04434\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 004 | loss: 0.04434 - R2: 0.9987 -- iter: 1024/1168\n",
      "Training Step: 74  | total loss: \u001b[1m\u001b[32m0.04185\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 004 | loss: 0.04185 - R2: 0.9988 -- iter: 1088/1168\n",
      "Training Step: 75  | total loss: \u001b[1m\u001b[32m0.03738\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 004 | loss: 0.03738 - R2: 0.9993 -- iter: 1152/1168\n",
      "Training Step: 76  | total loss: \u001b[1m\u001b[32m0.03738\u001b[0m\u001b[0m | time: 1.104s\n",
      "| SGD | epoch: 004 | loss: 0.03738 - R2: 0.9993 | val_loss: 0.01899 - val_acc: 0.9984 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 77  | total loss: \u001b[1m\u001b[32m0.03405\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 005 | loss: 0.03405 - R2: 1.0000 -- iter: 0064/1168\n",
      "Training Step: 78  | total loss: \u001b[1m\u001b[32m0.03405\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 005 | loss: 0.03405 - R2: 1.0005 -- iter: 0128/1168\n",
      "Training Step: 79  | total loss: \u001b[1m\u001b[32m0.03294\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 005 | loss: 0.03294 - R2: 1.0000 -- iter: 0192/1168\n",
      "Training Step: 80  | total loss: \u001b[1m\u001b[32m0.03182\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 005 | loss: 0.03182 - R2: 1.0011 -- iter: 0256/1168\n",
      "Training Step: 81  | total loss: \u001b[1m\u001b[32m0.03066\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 005 | loss: 0.03066 - R2: 1.0020 -- iter: 0320/1168\n",
      "Training Step: 82  | total loss: \u001b[1m\u001b[32m0.02957\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 005 | loss: 0.02957 - R2: 1.0017 -- iter: 0384/1168\n",
      "Training Step: 83  | total loss: \u001b[1m\u001b[32m0.02804\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 005 | loss: 0.02804 - R2: 1.0010 -- iter: 0448/1168\n",
      "Training Step: 84  | total loss: \u001b[1m\u001b[32m0.02691\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 005 | loss: 0.02691 - R2: 1.0009 -- iter: 0512/1168\n",
      "Training Step: 85  | total loss: \u001b[1m\u001b[32m0.02621\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 005 | loss: 0.02621 - R2: 1.0010 -- iter: 0576/1168\n",
      "Training Step: 86  | total loss: \u001b[1m\u001b[32m0.02588\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 005 | loss: 0.02588 - R2: 1.0009 -- iter: 0640/1168\n",
      "Training Step: 87  | total loss: \u001b[1m\u001b[32m0.02765\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 005 | loss: 0.02765 - R2: 1.0012 -- iter: 0704/1168\n",
      "Training Step: 88  | total loss: \u001b[1m\u001b[32m0.06520\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 005 | loss: 0.06520 - R2: 1.0011 -- iter: 0768/1168\n",
      "Training Step: 89  | total loss: \u001b[1m\u001b[32m0.06202\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 005 | loss: 0.06202 - R2: 1.0011 -- iter: 0832/1168\n",
      "Training Step: 90  | total loss: \u001b[1m\u001b[32m0.05831\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 005 | loss: 0.05831 - R2: 1.0009 -- iter: 0896/1168\n",
      "Training Step: 91  | total loss: \u001b[1m\u001b[32m0.05753\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 005 | loss: 0.05753 - R2: 1.0015 -- iter: 0960/1168\n",
      "Training Step: 92  | total loss: \u001b[1m\u001b[32m0.05050\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 005 | loss: 0.05050 - R2: 1.0004 -- iter: 1024/1168\n",
      "Training Step: 93  | total loss: \u001b[1m\u001b[32m0.05050\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 005 | loss: 0.05050 - R2: 1.0004 -- iter: 1088/1168\n",
      "Training Step: 94  | total loss: \u001b[1m\u001b[32m0.04681\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 005 | loss: 0.04681 - R2: 1.0001 -- iter: 1152/1168\n",
      "Training Step: 95  | total loss: \u001b[1m\u001b[32m0.04418\u001b[0m\u001b[0m | time: 1.148s\n",
      "| SGD | epoch: 005 | loss: 0.04418 - R2: 0.9998 | val_loss: 0.01927 - val_acc: 0.9967 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 96  | total loss: \u001b[1m\u001b[32m0.04136\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 006 | loss: 0.04136 - R2: 0.9991 -- iter: 0064/1168\n",
      "Training Step: 97  | total loss: \u001b[1m\u001b[32m0.03738\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 006 | loss: 0.03738 - R2: 0.9991 -- iter: 0128/1168\n",
      "Training Step: 98  | total loss: \u001b[1m\u001b[32m0.03738\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 006 | loss: 0.03738 - R2: 0.9992 -- iter: 0192/1168\n",
      "Training Step: 99  | total loss: \u001b[1m\u001b[32m0.03568\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 006 | loss: 0.03568 - R2: 0.9992 -- iter: 0256/1168\n",
      "Training Step: 100  | total loss: \u001b[1m\u001b[32m0.03281\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 006 | loss: 0.03281 - R2: 1.0001 -- iter: 0320/1168\n",
      "Training Step: 101  | total loss: \u001b[1m\u001b[32m0.03240\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 006 | loss: 0.03240 - R2: 0.9994 -- iter: 0384/1168\n",
      "Training Step: 102  | total loss: \u001b[1m\u001b[32m0.03103\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 006 | loss: 0.03103 - R2: 0.9998 -- iter: 0448/1168\n",
      "Training Step: 103  | total loss: \u001b[1m\u001b[32m0.03103\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 006 | loss: 0.03103 - R2: 0.9998 -- iter: 0512/1168\n",
      "Training Step: 104  | total loss: \u001b[1m\u001b[32m0.02988\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 006 | loss: 0.02988 - R2: 0.9998 -- iter: 0576/1168\n",
      "Training Step: 105  | total loss: \u001b[1m\u001b[32m0.02847\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 006 | loss: 0.02847 - R2: 0.9994 -- iter: 0640/1168\n",
      "Training Step: 106  | total loss: \u001b[1m\u001b[32m0.02834\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 006 | loss: 0.02834 - R2: 0.9993 -- iter: 0704/1168\n",
      "Training Step: 107  | total loss: \u001b[1m\u001b[32m0.02746\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 006 | loss: 0.02746 - R2: 0.9995 -- iter: 0768/1168\n",
      "Training Step: 108  | total loss: \u001b[1m\u001b[32m0.02718\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 006 | loss: 0.02718 - R2: 0.9996 -- iter: 0832/1168\n",
      "Training Step: 109  | total loss: \u001b[1m\u001b[32m0.02610\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 006 | loss: 0.02610 - R2: 0.9996 -- iter: 0896/1168\n",
      "Training Step: 110  | total loss: \u001b[1m\u001b[32m0.02599\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 006 | loss: 0.02599 - R2: 0.9995 -- iter: 0960/1168\n",
      "Training Step: 111  | total loss: \u001b[1m\u001b[32m0.02538\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 006 | loss: 0.02538 - R2: 0.9995 -- iter: 1024/1168\n",
      "Training Step: 112  | total loss: \u001b[1m\u001b[32m0.02652\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 006 | loss: 0.02652 - R2: 0.9990 -- iter: 1088/1168\n",
      "Training Step: 113  | total loss: \u001b[1m\u001b[32m0.02533\u001b[0m\u001b[0m | time: 0.182s\n",
      "| SGD | epoch: 006 | loss: 0.02533 - R2: 0.9994 -- iter: 1152/1168\n",
      "Training Step: 114  | total loss: \u001b[1m\u001b[32m0.02560\u001b[0m\u001b[0m | time: 1.196s\n",
      "| SGD | epoch: 006 | loss: 0.02560 - R2: 0.9996 | val_loss: 0.01906 - val_acc: 0.9969 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 115  | total loss: \u001b[1m\u001b[32m0.02560\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 007 | loss: 0.02560 - R2: 0.9996 -- iter: 0064/1168\n",
      "Training Step: 116  | total loss: \u001b[1m\u001b[32m0.02485\u001b[0m\u001b[0m | time: 0.012s\n",
      "| SGD | epoch: 007 | loss: 0.02485 - R2: 0.9991 -- iter: 0128/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 117  | total loss: \u001b[1m\u001b[32m0.02486\u001b[0m\u001b[0m | time: 0.018s\n",
      "| SGD | epoch: 007 | loss: 0.02486 - R2: 0.9987 -- iter: 0192/1168\n",
      "Training Step: 118  | total loss: \u001b[1m\u001b[32m0.02486\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 007 | loss: 0.02486 - R2: 0.9987 -- iter: 0256/1168\n",
      "Training Step: 119  | total loss: \u001b[1m\u001b[32m0.02446\u001b[0m\u001b[0m | time: 0.026s\n",
      "| SGD | epoch: 007 | loss: 0.02446 - R2: 0.9987 -- iter: 0320/1168\n",
      "Training Step: 120  | total loss: \u001b[1m\u001b[32m0.02449\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 007 | loss: 0.02449 - R2: 0.9986 -- iter: 0384/1168\n",
      "Training Step: 121  | total loss: \u001b[1m\u001b[32m0.02447\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 007 | loss: 0.02447 - R2: 0.9985 -- iter: 0448/1168\n",
      "Training Step: 122  | total loss: \u001b[1m\u001b[32m0.02348\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 007 | loss: 0.02348 - R2: 0.9985 -- iter: 0512/1168\n",
      "Training Step: 123  | total loss: \u001b[1m\u001b[32m0.02415\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 007 | loss: 0.02415 - R2: 0.9991 -- iter: 0576/1168\n",
      "Training Step: 124  | total loss: \u001b[1m\u001b[32m0.02304\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 007 | loss: 0.02304 - R2: 0.9990 -- iter: 0640/1168\n",
      "Training Step: 125  | total loss: \u001b[1m\u001b[32m0.02304\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 007 | loss: 0.02304 - R2: 0.9990 -- iter: 0704/1168\n",
      "Training Step: 126  | total loss: \u001b[1m\u001b[32m0.02211\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 007 | loss: 0.02211 - R2: 0.9989 -- iter: 0768/1168\n",
      "Training Step: 127  | total loss: \u001b[1m\u001b[32m0.02181\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 007 | loss: 0.02181 - R2: 0.9994 -- iter: 0832/1168\n",
      "Training Step: 128  | total loss: \u001b[1m\u001b[32m0.02183\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 007 | loss: 0.02183 - R2: 0.9992 -- iter: 0896/1168\n",
      "Training Step: 129  | total loss: \u001b[1m\u001b[32m0.02141\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 007 | loss: 0.02141 - R2: 0.9988 -- iter: 0960/1168\n",
      "Training Step: 130  | total loss: \u001b[1m\u001b[32m0.02149\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 007 | loss: 0.02149 - R2: 0.9990 -- iter: 1024/1168\n",
      "Training Step: 131  | total loss: \u001b[1m\u001b[32m0.02310\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 007 | loss: 0.02310 - R2: 0.9987 -- iter: 1088/1168\n",
      "Training Step: 132  | total loss: \u001b[1m\u001b[32m0.02231\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 007 | loss: 0.02231 - R2: 0.9987 -- iter: 1152/1168\n",
      "Training Step: 133  | total loss: \u001b[1m\u001b[32m0.02162\u001b[0m\u001b[0m | time: 1.092s\n",
      "| SGD | epoch: 007 | loss: 0.02162 - R2: 0.9995 | val_loss: 0.01891 - val_acc: 0.9973 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 134  | total loss: \u001b[1m\u001b[32m0.02162\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 008 | loss: 0.02162 - R2: 0.9994 -- iter: 0064/1168\n",
      "Training Step: 135  | total loss: \u001b[1m\u001b[32m0.02086\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 008 | loss: 0.02086 - R2: 0.9994 -- iter: 0128/1168\n",
      "Training Step: 136  | total loss: \u001b[1m\u001b[32m0.02087\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 008 | loss: 0.02087 - R2: 0.9991 -- iter: 0192/1168\n",
      "Training Step: 137  | total loss: \u001b[1m\u001b[32m0.02087\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 008 | loss: 0.02087 - R2: 0.9991 -- iter: 0256/1168\n",
      "Training Step: 138  | total loss: \u001b[1m\u001b[32m0.02095\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 008 | loss: 0.02095 - R2: 0.9991 -- iter: 0320/1168\n",
      "Training Step: 139  | total loss: \u001b[1m\u001b[32m0.02135\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 008 | loss: 0.02135 - R2: 0.9991 -- iter: 0384/1168\n",
      "Training Step: 140  | total loss: \u001b[1m\u001b[32m0.02210\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 008 | loss: 0.02210 - R2: 0.9995 -- iter: 0448/1168\n",
      "Training Step: 141  | total loss: \u001b[1m\u001b[32m0.02210\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 008 | loss: 0.02210 - R2: 0.9995 -- iter: 0512/1168\n",
      "Training Step: 142  | total loss: \u001b[1m\u001b[32m0.02121\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 008 | loss: 0.02121 - R2: 0.9995 -- iter: 0576/1168\n",
      "Training Step: 143  | total loss: \u001b[1m\u001b[32m0.02372\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 008 | loss: 0.02372 - R2: 1.0001 -- iter: 0640/1168\n",
      "Training Step: 144  | total loss: \u001b[1m\u001b[32m0.02307\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 008 | loss: 0.02307 - R2: 1.0001 -- iter: 0704/1168\n",
      "Training Step: 145  | total loss: \u001b[1m\u001b[32m0.02307\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 008 | loss: 0.02307 - R2: 1.0001 -- iter: 0768/1168\n",
      "Training Step: 146  | total loss: \u001b[1m\u001b[32m0.02310\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 008 | loss: 0.02310 - R2: 0.9999 -- iter: 0832/1168\n",
      "Training Step: 147  | total loss: \u001b[1m\u001b[32m0.04106\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 008 | loss: 0.04106 - R2: 1.0004 -- iter: 0896/1168\n",
      "Training Step: 148  | total loss: \u001b[1m\u001b[32m0.03812\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 008 | loss: 0.03812 - R2: 1.0002 -- iter: 0960/1168\n",
      "Training Step: 149  | total loss: \u001b[1m\u001b[32m0.03664\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 008 | loss: 0.03664 - R2: 0.9998 -- iter: 1024/1168\n",
      "Training Step: 150  | total loss: \u001b[1m\u001b[32m0.03664\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 008 | loss: 0.03664 - R2: 0.9998 -- iter: 1088/1168\n",
      "Training Step: 151  | total loss: \u001b[1m\u001b[32m0.03256\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 008 | loss: 0.03256 - R2: 0.9992 -- iter: 1152/1168\n",
      "Training Step: 152  | total loss: \u001b[1m\u001b[32m0.03203\u001b[0m\u001b[0m | time: 1.139s\n",
      "| SGD | epoch: 008 | loss: 0.03203 - R2: 0.9993 | val_loss: 0.01887 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 153  | total loss: \u001b[1m\u001b[32m0.03203\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 009 | loss: 0.03203 - R2: 0.9993 -- iter: 0064/1168\n",
      "Training Step: 154  | total loss: \u001b[1m\u001b[32m0.03386\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 009 | loss: 0.03386 - R2: 0.9995 -- iter: 0128/1168\n",
      "Training Step: 155  | total loss: \u001b[1m\u001b[32m0.03245\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 009 | loss: 0.03245 - R2: 0.9995 -- iter: 0192/1168\n",
      "Training Step: 156  | total loss: \u001b[1m\u001b[32m0.03108\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 009 | loss: 0.03108 - R2: 0.9994 -- iter: 0256/1168\n",
      "Training Step: 157  | total loss: \u001b[1m\u001b[32m0.03108\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 009 | loss: 0.03108 - R2: 0.9994 -- iter: 0320/1168\n",
      "Training Step: 158  | total loss: \u001b[1m\u001b[32m0.02925\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 009 | loss: 0.02925 - R2: 0.9994 -- iter: 0384/1168\n",
      "Training Step: 159  | total loss: \u001b[1m\u001b[32m0.02785\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 009 | loss: 0.02785 - R2: 0.9992 -- iter: 0448/1168\n",
      "Training Step: 160  | total loss: \u001b[1m\u001b[32m0.02530\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 009 | loss: 0.02530 - R2: 0.9994 -- iter: 0512/1168\n",
      "Training Step: 161  | total loss: \u001b[1m\u001b[32m0.02530\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 009 | loss: 0.02530 - R2: 0.9991 -- iter: 0576/1168\n",
      "Training Step: 162  | total loss: \u001b[1m\u001b[32m0.02477\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 009 | loss: 0.02477 - R2: 0.9991 -- iter: 0640/1168\n",
      "Training Step: 163  | total loss: \u001b[1m\u001b[32m0.02616\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 009 | loss: 0.02616 - R2: 0.9994 -- iter: 0704/1168\n",
      "Training Step: 164  | total loss: \u001b[1m\u001b[32m0.02540\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 009 | loss: 0.02540 - R2: 0.9997 -- iter: 0768/1168\n",
      "Training Step: 165  | total loss: \u001b[1m\u001b[32m0.02510\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 009 | loss: 0.02510 - R2: 1.0002 -- iter: 0832/1168\n",
      "Training Step: 166  | total loss: \u001b[1m\u001b[32m0.02520\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 009 | loss: 0.02520 - R2: 1.0002 -- iter: 0896/1168\n",
      "Training Step: 167  | total loss: \u001b[1m\u001b[32m0.02456\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 009 | loss: 0.02456 - R2: 1.0002 -- iter: 0960/1168\n",
      "Training Step: 168  | total loss: \u001b[1m\u001b[32m0.02339\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 009 | loss: 0.02339 - R2: 1.0003 -- iter: 1024/1168\n",
      "Training Step: 169  | total loss: \u001b[1m\u001b[32m0.02475\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 009 | loss: 0.02475 - R2: 1.0006 -- iter: 1088/1168\n",
      "Training Step: 170  | total loss: \u001b[1m\u001b[32m0.02654\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 009 | loss: 0.02654 - R2: 1.0007 -- iter: 1152/1168\n",
      "Training Step: 171  | total loss: \u001b[1m\u001b[32m0.02506\u001b[0m\u001b[0m | time: 1.161s\n",
      "| SGD | epoch: 009 | loss: 0.02506 - R2: 1.0002 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 172  | total loss: \u001b[1m\u001b[32m0.02437\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 010 | loss: 0.02437 - R2: 1.0000 -- iter: 0064/1168\n",
      "Training Step: 173  | total loss: \u001b[1m\u001b[32m0.02251\u001b[0m\u001b[0m | time: 0.020s\n",
      "| SGD | epoch: 010 | loss: 0.02251 - R2: 0.9998 -- iter: 0128/1168\n",
      "Training Step: 174  | total loss: \u001b[1m\u001b[32m0.02224\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 010 | loss: 0.02224 - R2: 0.9999 -- iter: 0192/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 175  | total loss: \u001b[1m\u001b[32m0.02224\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 010 | loss: 0.02224 - R2: 0.9999 -- iter: 0256/1168\n",
      "Training Step: 176  | total loss: \u001b[1m\u001b[32m0.02177\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 010 | loss: 0.02177 - R2: 1.0001 -- iter: 0320/1168\n",
      "Training Step: 177  | total loss: \u001b[1m\u001b[32m0.02183\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 010 | loss: 0.02183 - R2: 0.9999 -- iter: 0384/1168\n",
      "Training Step: 178  | total loss: \u001b[1m\u001b[32m0.02125\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 010 | loss: 0.02125 - R2: 0.9999 -- iter: 0448/1168\n",
      "Training Step: 179  | total loss: \u001b[1m\u001b[32m0.02086\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 010 | loss: 0.02086 - R2: 0.9995 -- iter: 0512/1168\n",
      "Training Step: 180  | total loss: \u001b[1m\u001b[32m0.02056\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 010 | loss: 0.02056 - R2: 0.9995 -- iter: 0576/1168\n",
      "Training Step: 181  | total loss: \u001b[1m\u001b[32m0.02029\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 010 | loss: 0.02029 - R2: 0.9993 -- iter: 0640/1168\n",
      "Training Step: 182  | total loss: \u001b[1m\u001b[32m0.02124\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 010 | loss: 0.02124 - R2: 0.9994 -- iter: 0704/1168\n",
      "Training Step: 183  | total loss: \u001b[1m\u001b[32m0.02194\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 010 | loss: 0.02194 - R2: 0.9992 -- iter: 0768/1168\n",
      "Training Step: 184  | total loss: \u001b[1m\u001b[32m0.02096\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 010 | loss: 0.02096 - R2: 0.9991 -- iter: 0832/1168\n",
      "Training Step: 185  | total loss: \u001b[1m\u001b[32m0.02220\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 010 | loss: 0.02220 - R2: 0.9994 -- iter: 0896/1168\n",
      "Training Step: 186  | total loss: \u001b[1m\u001b[32m0.02220\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 010 | loss: 0.02220 - R2: 0.9995 -- iter: 0960/1168\n",
      "Training Step: 187  | total loss: \u001b[1m\u001b[32m0.02191\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 010 | loss: 0.02191 - R2: 0.9995 -- iter: 1024/1168\n",
      "Training Step: 188  | total loss: \u001b[1m\u001b[32m0.04983\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 010 | loss: 0.04983 - R2: 1.0001 -- iter: 1088/1168\n",
      "Training Step: 189  | total loss: \u001b[1m\u001b[32m0.04838\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 010 | loss: 0.04838 - R2: 1.0005 -- iter: 1152/1168\n",
      "Training Step: 190  | total loss: \u001b[1m\u001b[32m0.04563\u001b[0m\u001b[0m | time: 1.120s\n",
      "| SGD | epoch: 010 | loss: 0.04563 - R2: 1.0000 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 191  | total loss: \u001b[1m\u001b[32m0.04079\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 011 | loss: 0.04079 - R2: 0.9995 -- iter: 0064/1168\n",
      "Training Step: 192  | total loss: \u001b[1m\u001b[32m0.03815\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 011 | loss: 0.03815 - R2: 0.9995 -- iter: 0128/1168\n",
      "Training Step: 193  | total loss: \u001b[1m\u001b[32m0.03631\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 011 | loss: 0.03631 - R2: 0.9991 -- iter: 0192/1168\n",
      "Training Step: 194  | total loss: \u001b[1m\u001b[32m0.03516\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 011 | loss: 0.03516 - R2: 0.9993 -- iter: 0256/1168\n",
      "Training Step: 195  | total loss: \u001b[1m\u001b[32m0.03334\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 011 | loss: 0.03334 - R2: 0.9990 -- iter: 0320/1168\n",
      "Training Step: 196  | total loss: \u001b[1m\u001b[32m0.03426\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 011 | loss: 0.03426 - R2: 0.9999 -- iter: 0384/1168\n",
      "Training Step: 197  | total loss: \u001b[1m\u001b[32m0.03276\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 011 | loss: 0.03276 - R2: 1.0001 -- iter: 0448/1168\n",
      "Training Step: 198  | total loss: \u001b[1m\u001b[32m0.03183\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 011 | loss: 0.03183 - R2: 0.9999 -- iter: 0512/1168\n",
      "Training Step: 199  | total loss: \u001b[1m\u001b[32m0.03183\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 011 | loss: 0.03183 - R2: 0.9999 -- iter: 0576/1168\n",
      "Training Step: 200  | total loss: \u001b[1m\u001b[32m0.03022\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 011 | loss: 0.03022 - R2: 0.9996 -- iter: 0640/1168\n",
      "Training Step: 201  | total loss: \u001b[1m\u001b[32m0.03022\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 011 | loss: 0.03022 - R2: 0.9996 -- iter: 0704/1168\n",
      "Training Step: 202  | total loss: \u001b[1m\u001b[32m0.02810\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 011 | loss: 0.02810 - R2: 0.9995 -- iter: 0768/1168\n",
      "Training Step: 203  | total loss: \u001b[1m\u001b[32m0.02810\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 011 | loss: 0.02810 - R2: 0.9995 -- iter: 0832/1168\n",
      "Training Step: 204  | total loss: \u001b[1m\u001b[32m0.02778\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 011 | loss: 0.02778 - R2: 0.9997 -- iter: 0896/1168\n",
      "Training Step: 205  | total loss: \u001b[1m\u001b[32m0.02708\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 011 | loss: 0.02708 - R2: 0.9994 -- iter: 0960/1168\n",
      "Training Step: 206  | total loss: \u001b[1m\u001b[32m0.02501\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 011 | loss: 0.02501 - R2: 0.9995 -- iter: 1024/1168\n",
      "Training Step: 207  | total loss: \u001b[1m\u001b[32m0.02501\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 011 | loss: 0.02501 - R2: 1.0004 -- iter: 1088/1168\n",
      "Training Step: 208  | total loss: \u001b[1m\u001b[32m0.05353\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 011 | loss: 0.05353 - R2: 1.0003 -- iter: 1152/1168\n",
      "Training Step: 209  | total loss: \u001b[1m\u001b[32m0.05353\u001b[0m\u001b[0m | time: 1.175s\n",
      "| SGD | epoch: 011 | loss: 0.05353 - R2: 1.0002 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 210  | total loss: \u001b[1m\u001b[32m0.04995\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 012 | loss: 0.04995 - R2: 1.0002 -- iter: 0064/1168\n",
      "Training Step: 211  | total loss: \u001b[1m\u001b[32m0.04713\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 012 | loss: 0.04713 - R2: 1.0005 -- iter: 0128/1168\n",
      "Training Step: 212  | total loss: \u001b[1m\u001b[32m0.04095\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 012 | loss: 0.04095 - R2: 1.0006 -- iter: 0192/1168\n",
      "Training Step: 213  | total loss: \u001b[1m\u001b[32m0.03933\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 012 | loss: 0.03933 - R2: 1.0005 -- iter: 0256/1168\n",
      "Training Step: 214  | total loss: \u001b[1m\u001b[32m0.03709\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 012 | loss: 0.03709 - R2: 1.0001 -- iter: 0320/1168\n",
      "Training Step: 215  | total loss: \u001b[1m\u001b[32m0.03610\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 012 | loss: 0.03610 - R2: 0.9994 -- iter: 0384/1168\n",
      "Training Step: 216  | total loss: \u001b[1m\u001b[32m0.03365\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 012 | loss: 0.03365 - R2: 0.9994 -- iter: 0448/1168\n",
      "Training Step: 217  | total loss: \u001b[1m\u001b[32m0.03365\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 012 | loss: 0.03365 - R2: 0.9994 -- iter: 0512/1168\n",
      "Training Step: 218  | total loss: \u001b[1m\u001b[32m0.03280\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 012 | loss: 0.03280 - R2: 0.9993 -- iter: 0576/1168\n",
      "Training Step: 219  | total loss: \u001b[1m\u001b[32m0.03143\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 012 | loss: 0.03143 - R2: 0.9993 -- iter: 0640/1168\n",
      "Training Step: 220  | total loss: \u001b[1m\u001b[32m0.02878\u001b[0m\u001b[0m | time: 0.172s\n",
      "| SGD | epoch: 012 | loss: 0.02878 - R2: 0.9997 -- iter: 0704/1168\n",
      "Training Step: 221  | total loss: \u001b[1m\u001b[32m0.02638\u001b[0m\u001b[0m | time: 0.182s\n",
      "| SGD | epoch: 012 | loss: 0.02638 - R2: 0.9997 -- iter: 0768/1168\n",
      "Training Step: 222  | total loss: \u001b[1m\u001b[32m0.02468\u001b[0m\u001b[0m | time: 0.186s\n",
      "| SGD | epoch: 012 | loss: 0.02468 - R2: 0.9997 -- iter: 0832/1168\n",
      "Training Step: 223  | total loss: \u001b[1m\u001b[32m0.02473\u001b[0m\u001b[0m | time: 0.188s\n",
      "| SGD | epoch: 012 | loss: 0.02473 - R2: 0.9996 -- iter: 0896/1168\n",
      "Training Step: 224  | total loss: \u001b[1m\u001b[32m0.02696\u001b[0m\u001b[0m | time: 0.190s\n",
      "| SGD | epoch: 012 | loss: 0.02696 - R2: 0.9992 -- iter: 0960/1168\n",
      "Training Step: 225  | total loss: \u001b[1m\u001b[32m0.02700\u001b[0m\u001b[0m | time: 0.196s\n",
      "| SGD | epoch: 012 | loss: 0.02700 - R2: 0.9996 -- iter: 1024/1168\n",
      "Training Step: 226  | total loss: \u001b[1m\u001b[32m0.02700\u001b[0m\u001b[0m | time: 0.217s\n",
      "| SGD | epoch: 012 | loss: 0.02700 - R2: 1.0002 -- iter: 1088/1168\n",
      "Training Step: 227  | total loss: \u001b[1m\u001b[32m0.02783\u001b[0m\u001b[0m | time: 0.225s\n",
      "| SGD | epoch: 012 | loss: 0.02783 - R2: 1.0002 -- iter: 1152/1168\n",
      "Training Step: 228  | total loss: \u001b[1m\u001b[32m0.05854\u001b[0m\u001b[0m | time: 1.291s\n",
      "| SGD | epoch: 012 | loss: 0.05854 - R2: 0.9992 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 229  | total loss: \u001b[1m\u001b[32m0.05384\u001b[0m\u001b[0m | time: 0.005s\n",
      "| SGD | epoch: 013 | loss: 0.05384 - R2: 0.9992 -- iter: 0064/1168\n",
      "Training Step: 230  | total loss: \u001b[1m\u001b[32m0.05164\u001b[0m\u001b[0m | time: 0.008s\n",
      "| SGD | epoch: 013 | loss: 0.05164 - R2: 0.9995 -- iter: 0128/1168\n",
      "Training Step: 231  | total loss: \u001b[1m\u001b[32m0.04934\u001b[0m\u001b[0m | time: 0.010s\n",
      "| SGD | epoch: 013 | loss: 0.04934 - R2: 0.9998 -- iter: 0192/1168\n",
      "Training Step: 232  | total loss: \u001b[1m\u001b[32m0.04636\u001b[0m\u001b[0m | time: 0.013s\n",
      "| SGD | epoch: 013 | loss: 0.04636 - R2: 0.9996 -- iter: 0256/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 233  | total loss: \u001b[1m\u001b[32m0.04148\u001b[0m\u001b[0m | time: 0.016s\n",
      "| SGD | epoch: 013 | loss: 0.04148 - R2: 1.0003 -- iter: 0320/1168\n",
      "Training Step: 234  | total loss: \u001b[1m\u001b[32m0.03925\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 013 | loss: 0.03925 - R2: 0.9998 -- iter: 0384/1168\n",
      "Training Step: 235  | total loss: \u001b[1m\u001b[32m0.03925\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 013 | loss: 0.03925 - R2: 0.9998 -- iter: 0448/1168\n",
      "Training Step: 236  | total loss: \u001b[1m\u001b[32m0.03810\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 013 | loss: 0.03810 - R2: 0.9995 -- iter: 0512/1168\n",
      "Training Step: 237  | total loss: \u001b[1m\u001b[32m0.03885\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 013 | loss: 0.03885 - R2: 0.9997 -- iter: 0576/1168\n",
      "Training Step: 238  | total loss: \u001b[1m\u001b[32m0.03639\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 013 | loss: 0.03639 - R2: 1.0000 -- iter: 0640/1168\n",
      "Training Step: 239  | total loss: \u001b[1m\u001b[32m0.03466\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 013 | loss: 0.03466 - R2: 0.9997 -- iter: 0704/1168\n",
      "Training Step: 240  | total loss: \u001b[1m\u001b[32m0.03204\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 013 | loss: 0.03204 - R2: 0.9996 -- iter: 0768/1168\n",
      "Training Step: 241  | total loss: \u001b[1m\u001b[32m0.02968\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 013 | loss: 0.02968 - R2: 0.9994 -- iter: 0832/1168\n",
      "Training Step: 242  | total loss: \u001b[1m\u001b[32m0.02837\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 013 | loss: 0.02837 - R2: 0.9996 -- iter: 0896/1168\n",
      "Training Step: 243  | total loss: \u001b[1m\u001b[32m0.02837\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 013 | loss: 0.02837 - R2: 0.9996 -- iter: 0960/1168\n",
      "Training Step: 244  | total loss: \u001b[1m\u001b[32m0.02553\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 013 | loss: 0.02553 - R2: 0.9990 -- iter: 1024/1168\n",
      "Training Step: 245  | total loss: \u001b[1m\u001b[32m0.02618\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 013 | loss: 0.02618 - R2: 0.9993 -- iter: 1088/1168\n",
      "Training Step: 246  | total loss: \u001b[1m\u001b[32m0.02557\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 013 | loss: 0.02557 - R2: 0.9991 -- iter: 1152/1168\n",
      "Training Step: 247  | total loss: \u001b[1m\u001b[32m0.02557\u001b[0m\u001b[0m | time: 1.066s\n",
      "| SGD | epoch: 013 | loss: 0.02557 - R2: 0.9988 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 248  | total loss: \u001b[1m\u001b[32m0.02412\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 014 | loss: 0.02412 - R2: 0.9986 -- iter: 0064/1168\n",
      "Training Step: 249  | total loss: \u001b[1m\u001b[32m0.02354\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 014 | loss: 0.02354 - R2: 0.9990 -- iter: 0128/1168\n",
      "Training Step: 250  | total loss: \u001b[1m\u001b[32m0.02294\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 014 | loss: 0.02294 - R2: 0.9986 -- iter: 0192/1168\n",
      "Training Step: 251  | total loss: \u001b[1m\u001b[32m0.02275\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 014 | loss: 0.02275 - R2: 0.9982 -- iter: 0256/1168\n",
      "Training Step: 252  | total loss: \u001b[1m\u001b[32m0.02275\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 014 | loss: 0.02275 - R2: 0.9982 -- iter: 0320/1168\n",
      "Training Step: 253  | total loss: \u001b[1m\u001b[32m0.02221\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 014 | loss: 0.02221 - R2: 0.9984 -- iter: 0384/1168\n",
      "Training Step: 254  | total loss: \u001b[1m\u001b[32m0.02198\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 014 | loss: 0.02198 - R2: 0.9985 -- iter: 0448/1168\n",
      "Training Step: 255  | total loss: \u001b[1m\u001b[32m0.02238\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 014 | loss: 0.02238 - R2: 0.9982 -- iter: 0512/1168\n",
      "Training Step: 256  | total loss: \u001b[1m\u001b[32m0.02448\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 014 | loss: 0.02448 - R2: 0.9983 -- iter: 0576/1168\n",
      "Training Step: 257  | total loss: \u001b[1m\u001b[32m0.02396\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 014 | loss: 0.02396 - R2: 0.9984 -- iter: 0640/1168\n",
      "Training Step: 258  | total loss: \u001b[1m\u001b[32m0.02388\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 014 | loss: 0.02388 - R2: 0.9982 -- iter: 0704/1168\n",
      "Training Step: 259  | total loss: \u001b[1m\u001b[32m0.02321\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 014 | loss: 0.02321 - R2: 0.9981 -- iter: 0768/1168\n",
      "Training Step: 260  | total loss: \u001b[1m\u001b[32m0.02154\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 014 | loss: 0.02154 - R2: 0.9981 -- iter: 0832/1168\n",
      "Training Step: 261  | total loss: \u001b[1m\u001b[32m0.02004\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 014 | loss: 0.02004 - R2: 0.9978 -- iter: 0896/1168\n",
      "Training Step: 262  | total loss: \u001b[1m\u001b[32m0.02086\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 014 | loss: 0.02086 - R2: 0.9990 -- iter: 0960/1168\n",
      "Training Step: 263  | total loss: \u001b[1m\u001b[32m0.02082\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 014 | loss: 0.02082 - R2: 0.9987 -- iter: 1024/1168\n",
      "Training Step: 264  | total loss: \u001b[1m\u001b[32m0.02082\u001b[0m\u001b[0m | time: 0.203s\n",
      "| SGD | epoch: 014 | loss: 0.02082 - R2: 0.9987 -- iter: 1088/1168\n",
      "Training Step: 265  | total loss: \u001b[1m\u001b[32m0.02188\u001b[0m\u001b[0m | time: 0.211s\n",
      "| SGD | epoch: 014 | loss: 0.02188 - R2: 0.9993 -- iter: 1152/1168\n",
      "Training Step: 266  | total loss: \u001b[1m\u001b[32m0.02080\u001b[0m\u001b[0m | time: 1.218s\n",
      "| SGD | epoch: 014 | loss: 0.02080 - R2: 0.9996 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 267  | total loss: \u001b[1m\u001b[32m0.02080\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 015 | loss: 0.02080 - R2: 0.9996 -- iter: 0064/1168\n",
      "Training Step: 268  | total loss: \u001b[1m\u001b[32m0.04329\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 015 | loss: 0.04329 - R2: 0.9991 -- iter: 0128/1168\n",
      "Training Step: 269  | total loss: \u001b[1m\u001b[32m0.04096\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 015 | loss: 0.04096 - R2: 0.9994 -- iter: 0192/1168\n",
      "Training Step: 270  | total loss: \u001b[1m\u001b[32m0.03690\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 015 | loss: 0.03690 - R2: 0.9996 -- iter: 0256/1168\n",
      "Training Step: 271  | total loss: \u001b[1m\u001b[32m0.03690\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 015 | loss: 0.03690 - R2: 0.9996 -- iter: 0320/1168\n",
      "Training Step: 272  | total loss: \u001b[1m\u001b[32m0.03441\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 015 | loss: 0.03441 - R2: 0.9996 -- iter: 0384/1168\n",
      "Training Step: 273  | total loss: \u001b[1m\u001b[32m0.03184\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 015 | loss: 0.03184 - R2: 0.9998 -- iter: 0448/1168\n",
      "Training Step: 274  | total loss: \u001b[1m\u001b[32m0.03137\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 015 | loss: 0.03137 - R2: 0.9997 -- iter: 0512/1168\n",
      "Training Step: 275  | total loss: \u001b[1m\u001b[32m0.03080\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 015 | loss: 0.03080 - R2: 0.9994 -- iter: 0576/1168\n",
      "Training Step: 276  | total loss: \u001b[1m\u001b[32m0.02913\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 015 | loss: 0.02913 - R2: 0.9989 -- iter: 0640/1168\n",
      "Training Step: 277  | total loss: \u001b[1m\u001b[32m0.02810\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 015 | loss: 0.02810 - R2: 0.9992 -- iter: 0704/1168\n",
      "Training Step: 278  | total loss: \u001b[1m\u001b[32m0.02668\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 015 | loss: 0.02668 - R2: 0.9991 -- iter: 0768/1168\n",
      "Training Step: 279  | total loss: \u001b[1m\u001b[32m0.02509\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 015 | loss: 0.02509 - R2: 0.9994 -- iter: 0832/1168\n",
      "Training Step: 280  | total loss: \u001b[1m\u001b[32m0.02509\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 015 | loss: 0.02509 - R2: 0.9994 -- iter: 0896/1168\n",
      "Training Step: 281  | total loss: \u001b[1m\u001b[32m0.02366\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 015 | loss: 0.02366 - R2: 0.9995 -- iter: 0960/1168\n",
      "Training Step: 282  | total loss: \u001b[1m\u001b[32m0.02571\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 015 | loss: 0.02571 - R2: 0.9998 -- iter: 1024/1168\n",
      "Training Step: 283  | total loss: \u001b[1m\u001b[32m0.02533\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 015 | loss: 0.02533 - R2: 0.9998 -- iter: 1088/1168\n",
      "Training Step: 284  | total loss: \u001b[1m\u001b[32m0.02518\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 015 | loss: 0.02518 - R2: 0.9991 -- iter: 1152/1168\n",
      "Training Step: 285  | total loss: \u001b[1m\u001b[32m0.02475\u001b[0m\u001b[0m | time: 1.154s\n",
      "| SGD | epoch: 015 | loss: 0.02475 - R2: 0.9990 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 286  | total loss: \u001b[1m\u001b[32m0.02579\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 016 | loss: 0.02579 - R2: 0.9994 -- iter: 0064/1168\n",
      "Training Step: 287  | total loss: \u001b[1m\u001b[32m0.02541\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 016 | loss: 0.02541 - R2: 0.9991 -- iter: 0128/1168\n",
      "Training Step: 288  | total loss: \u001b[1m\u001b[32m0.04886\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 016 | loss: 0.04886 - R2: 1.0005 -- iter: 0192/1168\n",
      "Training Step: 289  | total loss: \u001b[1m\u001b[32m0.04552\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 016 | loss: 0.04552 - R2: 1.0005 -- iter: 0256/1168\n",
      "Training Step: 290  | total loss: \u001b[1m\u001b[32m0.04552\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 016 | loss: 0.04552 - R2: 1.0005 -- iter: 0320/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 291  | total loss: \u001b[1m\u001b[32m0.04283\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 016 | loss: 0.04283 - R2: 1.0004 -- iter: 0384/1168\n",
      "Training Step: 292  | total loss: \u001b[1m\u001b[32m0.04067\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 016 | loss: 0.04067 - R2: 1.0001 -- iter: 0448/1168\n",
      "Training Step: 293  | total loss: \u001b[1m\u001b[32m0.03839\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 016 | loss: 0.03839 - R2: 1.0003 -- iter: 0512/1168\n",
      "Training Step: 294  | total loss: \u001b[1m\u001b[32m0.03839\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 016 | loss: 0.03839 - R2: 1.0003 -- iter: 0576/1168\n",
      "Training Step: 295  | total loss: \u001b[1m\u001b[32m0.03666\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 016 | loss: 0.03666 - R2: 1.0002 -- iter: 0640/1168\n",
      "Training Step: 296  | total loss: \u001b[1m\u001b[32m0.03561\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 016 | loss: 0.03561 - R2: 0.9998 -- iter: 0704/1168\n",
      "Training Step: 297  | total loss: \u001b[1m\u001b[32m0.03322\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 016 | loss: 0.03322 - R2: 0.9999 -- iter: 0768/1168\n",
      "Training Step: 298  | total loss: \u001b[1m\u001b[32m0.03219\u001b[0m\u001b[0m | time: 0.185s\n",
      "| SGD | epoch: 016 | loss: 0.03219 - R2: 0.9990 -- iter: 0832/1168\n",
      "Training Step: 299  | total loss: \u001b[1m\u001b[32m0.03219\u001b[0m\u001b[0m | time: 0.192s\n",
      "| SGD | epoch: 016 | loss: 0.03219 - R2: 0.9990 -- iter: 0896/1168\n",
      "Training Step: 300  | total loss: \u001b[1m\u001b[32m0.03387\u001b[0m\u001b[0m | time: 0.196s\n",
      "| SGD | epoch: 016 | loss: 0.03387 - R2: 0.9976 -- iter: 0960/1168\n",
      "Training Step: 301  | total loss: \u001b[1m\u001b[32m0.03539\u001b[0m\u001b[0m | time: 0.200s\n",
      "| SGD | epoch: 016 | loss: 0.03539 - R2: 0.9964 -- iter: 1024/1168\n",
      "Training Step: 302  | total loss: \u001b[1m\u001b[32m0.03160\u001b[0m\u001b[0m | time: 0.206s\n",
      "| SGD | epoch: 016 | loss: 0.03160 - R2: 0.9971 -- iter: 1088/1168\n",
      "Training Step: 303  | total loss: \u001b[1m\u001b[32m0.03160\u001b[0m\u001b[0m | time: 0.218s\n",
      "| SGD | epoch: 016 | loss: 0.03160 - R2: 0.9971 -- iter: 1152/1168\n",
      "Training Step: 304  | total loss: \u001b[1m\u001b[32m0.03053\u001b[0m\u001b[0m | time: 1.225s\n",
      "| SGD | epoch: 016 | loss: 0.03053 - R2: 0.9975 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 305  | total loss: \u001b[1m\u001b[32m0.02945\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 017 | loss: 0.02945 - R2: 0.9982 -- iter: 0064/1168\n",
      "Training Step: 306  | total loss: \u001b[1m\u001b[32m0.02789\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 017 | loss: 0.02789 - R2: 0.9986 -- iter: 0128/1168\n",
      "Training Step: 307  | total loss: \u001b[1m\u001b[32m0.02785\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 017 | loss: 0.02785 - R2: 0.9986 -- iter: 0192/1168\n",
      "Training Step: 308  | total loss: \u001b[1m\u001b[32m0.02769\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 017 | loss: 0.02769 - R2: 0.9989 -- iter: 0256/1168\n",
      "Training Step: 309  | total loss: \u001b[1m\u001b[32m0.02748\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 017 | loss: 0.02748 - R2: 0.9987 -- iter: 0320/1168\n",
      "Training Step: 310  | total loss: \u001b[1m\u001b[32m0.02868\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 017 | loss: 0.02868 - R2: 0.9990 -- iter: 0384/1168\n",
      "Training Step: 311  | total loss: \u001b[1m\u001b[32m0.02832\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 017 | loss: 0.02832 - R2: 0.9991 -- iter: 0448/1168\n",
      "Training Step: 312  | total loss: \u001b[1m\u001b[32m0.02911\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 017 | loss: 0.02911 - R2: 0.9995 -- iter: 0512/1168\n",
      "Training Step: 313  | total loss: \u001b[1m\u001b[32m0.02792\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 017 | loss: 0.02792 - R2: 0.9993 -- iter: 0576/1168\n",
      "Training Step: 314  | total loss: \u001b[1m\u001b[32m0.02652\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 017 | loss: 0.02652 - R2: 0.9995 -- iter: 0640/1168\n",
      "Training Step: 315  | total loss: \u001b[1m\u001b[32m0.02652\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 017 | loss: 0.02652 - R2: 0.9995 -- iter: 0704/1168\n",
      "Training Step: 316  | total loss: \u001b[1m\u001b[32m0.02578\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 017 | loss: 0.02578 - R2: 0.9994 -- iter: 0768/1168\n",
      "Training Step: 317  | total loss: \u001b[1m\u001b[32m0.02530\u001b[0m\u001b[0m | time: 0.171s\n",
      "| SGD | epoch: 017 | loss: 0.02530 - R2: 0.9993 -- iter: 0832/1168\n",
      "Training Step: 318  | total loss: \u001b[1m\u001b[32m0.02427\u001b[0m\u001b[0m | time: 0.175s\n",
      "| SGD | epoch: 017 | loss: 0.02427 - R2: 0.9996 -- iter: 0896/1168\n",
      "Training Step: 319  | total loss: \u001b[1m\u001b[32m0.02368\u001b[0m\u001b[0m | time: 0.181s\n",
      "| SGD | epoch: 017 | loss: 0.02368 - R2: 0.9988 -- iter: 0960/1168\n",
      "Training Step: 320  | total loss: \u001b[1m\u001b[32m0.02201\u001b[0m\u001b[0m | time: 0.185s\n",
      "| SGD | epoch: 017 | loss: 0.02201 - R2: 0.9992 -- iter: 1024/1168\n",
      "Training Step: 321  | total loss: \u001b[1m\u001b[32m0.02201\u001b[0m\u001b[0m | time: 0.187s\n",
      "| SGD | epoch: 017 | loss: 0.02201 - R2: 0.9992 -- iter: 1088/1168\n",
      "Training Step: 322  | total loss: \u001b[1m\u001b[32m0.02095\u001b[0m\u001b[0m | time: 0.189s\n",
      "| SGD | epoch: 017 | loss: 0.02095 - R2: 0.9992 -- iter: 1152/1168\n",
      "Training Step: 323  | total loss: \u001b[1m\u001b[32m0.02130\u001b[0m\u001b[0m | time: 1.200s\n",
      "| SGD | epoch: 017 | loss: 0.02130 - R2: 0.9997 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 324  | total loss: \u001b[1m\u001b[32m0.02184\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 018 | loss: 0.02184 - R2: 0.9993 -- iter: 0064/1168\n",
      "Training Step: 325  | total loss: \u001b[1m\u001b[32m0.02178\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 018 | loss: 0.02178 - R2: 0.9995 -- iter: 0128/1168\n",
      "Training Step: 326  | total loss: \u001b[1m\u001b[32m0.02106\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 018 | loss: 0.02106 - R2: 0.9992 -- iter: 0192/1168\n",
      "Training Step: 327  | total loss: \u001b[1m\u001b[32m0.02076\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 018 | loss: 0.02076 - R2: 0.9994 -- iter: 0256/1168\n",
      "Training Step: 328  | total loss: \u001b[1m\u001b[32m0.04508\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 018 | loss: 0.04508 - R2: 0.9995 -- iter: 0320/1168\n",
      "Training Step: 329  | total loss: \u001b[1m\u001b[32m0.04195\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 018 | loss: 0.04195 - R2: 0.9992 -- iter: 0384/1168\n",
      "Training Step: 330  | total loss: \u001b[1m\u001b[32m0.04198\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 018 | loss: 0.04198 - R2: 0.9994 -- iter: 0448/1168\n",
      "Training Step: 331  | total loss: \u001b[1m\u001b[32m0.03930\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 018 | loss: 0.03930 - R2: 0.9992 -- iter: 0512/1168\n",
      "Training Step: 332  | total loss: \u001b[1m\u001b[32m0.03689\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 018 | loss: 0.03689 - R2: 0.9989 -- iter: 0576/1168\n",
      "Training Step: 333  | total loss: \u001b[1m\u001b[32m0.03693\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 018 | loss: 0.03693 - R2: 0.9989 -- iter: 0640/1168\n",
      "Training Step: 334  | total loss: \u001b[1m\u001b[32m0.03484\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 018 | loss: 0.03484 - R2: 0.9991 -- iter: 0704/1168\n",
      "Training Step: 335  | total loss: \u001b[1m\u001b[32m0.03269\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 018 | loss: 0.03269 - R2: 0.9987 -- iter: 0768/1168\n",
      "Training Step: 336  | total loss: \u001b[1m\u001b[32m0.03136\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 018 | loss: 0.03136 - R2: 0.9989 -- iter: 0832/1168\n",
      "Training Step: 337  | total loss: \u001b[1m\u001b[32m0.02954\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 018 | loss: 0.02954 - R2: 0.9990 -- iter: 0896/1168\n",
      "Training Step: 338  | total loss: \u001b[1m\u001b[32m0.02857\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 018 | loss: 0.02857 - R2: 0.9992 -- iter: 0960/1168\n",
      "Training Step: 339  | total loss: \u001b[1m\u001b[32m0.02755\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 018 | loss: 0.02755 - R2: 0.9995 -- iter: 1024/1168\n",
      "Training Step: 340  | total loss: \u001b[1m\u001b[32m0.02626\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 018 | loss: 0.02626 - R2: 1.0003 -- iter: 1088/1168\n",
      "Training Step: 341  | total loss: \u001b[1m\u001b[32m0.02510\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 018 | loss: 0.02510 - R2: 1.0011 -- iter: 1152/1168\n",
      "Training Step: 342  | total loss: \u001b[1m\u001b[32m0.02412\u001b[0m\u001b[0m | time: 1.157s\n",
      "| SGD | epoch: 018 | loss: 0.02412 - R2: 1.0008 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 343  | total loss: \u001b[1m\u001b[32m0.02365\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 019 | loss: 0.02365 - R2: 1.0015 -- iter: 0064/1168\n",
      "Training Step: 344  | total loss: \u001b[1m\u001b[32m0.02394\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 019 | loss: 0.02394 - R2: 1.0015 -- iter: 0128/1168\n",
      "Training Step: 345  | total loss: \u001b[1m\u001b[32m0.02344\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 019 | loss: 0.02344 - R2: 1.0011 -- iter: 0192/1168\n",
      "Training Step: 346  | total loss: \u001b[1m\u001b[32m0.02426\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 019 | loss: 0.02426 - R2: 1.0011 -- iter: 0256/1168\n",
      "Training Step: 347  | total loss: \u001b[1m\u001b[32m0.02535\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 019 | loss: 0.02535 - R2: 1.0002 -- iter: 0320/1168\n",
      "Training Step: 348  | total loss: \u001b[1m\u001b[32m0.02535\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 019 | loss: 0.02535 - R2: 1.0002 -- iter: 0384/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 349  | total loss: \u001b[1m\u001b[32m0.02480\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 019 | loss: 0.02480 - R2: 0.9994 -- iter: 0448/1168\n",
      "Training Step: 350  | total loss: \u001b[1m\u001b[32m0.02413\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 019 | loss: 0.02413 - R2: 0.9994 -- iter: 0512/1168\n",
      "Training Step: 351  | total loss: \u001b[1m\u001b[32m0.02623\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 019 | loss: 0.02623 - R2: 1.0000 -- iter: 0576/1168\n",
      "Training Step: 352  | total loss: \u001b[1m\u001b[32m0.02623\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 019 | loss: 0.02623 - R2: 1.0000 -- iter: 0640/1168\n",
      "Training Step: 353  | total loss: \u001b[1m\u001b[32m0.02506\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 019 | loss: 0.02506 - R2: 0.9997 -- iter: 0704/1168\n",
      "Training Step: 354  | total loss: \u001b[1m\u001b[32m0.02483\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 019 | loss: 0.02483 - R2: 0.9998 -- iter: 0768/1168\n",
      "Training Step: 355  | total loss: \u001b[1m\u001b[32m0.02412\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 019 | loss: 0.02412 - R2: 0.9998 -- iter: 0832/1168\n",
      "Training Step: 356  | total loss: \u001b[1m\u001b[32m0.02390\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 019 | loss: 0.02390 - R2: 0.9996 -- iter: 0896/1168\n",
      "Training Step: 357  | total loss: \u001b[1m\u001b[32m0.02404\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 019 | loss: 0.02404 - R2: 0.9998 -- iter: 0960/1168\n",
      "Training Step: 358  | total loss: \u001b[1m\u001b[32m0.02369\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 019 | loss: 0.02369 - R2: 0.9995 -- iter: 1024/1168\n",
      "Training Step: 359  | total loss: \u001b[1m\u001b[32m0.02322\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 019 | loss: 0.02322 - R2: 0.9994 -- iter: 1088/1168\n",
      "Training Step: 360  | total loss: \u001b[1m\u001b[32m0.02311\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 019 | loss: 0.02311 - R2: 0.9994 -- iter: 1152/1168\n",
      "Training Step: 361  | total loss: \u001b[1m\u001b[32m0.02301\u001b[0m\u001b[0m | time: 1.157s\n",
      "| SGD | epoch: 019 | loss: 0.02301 - R2: 0.9993 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 362  | total loss: \u001b[1m\u001b[32m0.02233\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 020 | loss: 0.02233 - R2: 0.9997 -- iter: 0064/1168\n",
      "Training Step: 363  | total loss: \u001b[1m\u001b[32m0.02233\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 020 | loss: 0.02233 - R2: 0.9997 -- iter: 0128/1168\n",
      "Training Step: 364  | total loss: \u001b[1m\u001b[32m0.02171\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 020 | loss: 0.02171 - R2: 0.9997 -- iter: 0192/1168\n",
      "Training Step: 365  | total loss: \u001b[1m\u001b[32m0.02068\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 020 | loss: 0.02068 - R2: 0.9994 -- iter: 0256/1168\n",
      "Training Step: 366  | total loss: \u001b[1m\u001b[32m0.02235\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 020 | loss: 0.02235 - R2: 0.9997 -- iter: 0320/1168\n",
      "Training Step: 367  | total loss: \u001b[1m\u001b[32m0.02216\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 020 | loss: 0.02216 - R2: 0.9997 -- iter: 0384/1168\n",
      "Training Step: 368  | total loss: \u001b[1m\u001b[32m0.02201\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 020 | loss: 0.02201 - R2: 0.9989 -- iter: 0448/1168\n",
      "Training Step: 369  | total loss: \u001b[1m\u001b[32m0.02259\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 020 | loss: 0.02259 - R2: 0.9992 -- iter: 0512/1168\n",
      "Training Step: 370  | total loss: \u001b[1m\u001b[32m0.02259\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 020 | loss: 0.02259 - R2: 0.9992 -- iter: 0576/1168\n",
      "Training Step: 371  | total loss: \u001b[1m\u001b[32m0.02400\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 020 | loss: 0.02400 - R2: 0.9998 -- iter: 0640/1168\n",
      "Training Step: 372  | total loss: \u001b[1m\u001b[32m0.02415\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 020 | loss: 0.02415 - R2: 1.0001 -- iter: 0704/1168\n",
      "Training Step: 373  | total loss: \u001b[1m\u001b[32m0.02291\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 020 | loss: 0.02291 - R2: 1.0000 -- iter: 0768/1168\n",
      "Training Step: 374  | total loss: \u001b[1m\u001b[32m0.02155\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 020 | loss: 0.02155 - R2: 1.0000 -- iter: 0832/1168\n",
      "Training Step: 375  | total loss: \u001b[1m\u001b[32m0.02079\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 020 | loss: 0.02079 - R2: 0.9996 -- iter: 0896/1168\n",
      "Training Step: 376  | total loss: \u001b[1m\u001b[32m0.02012\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 020 | loss: 0.02012 - R2: 0.9990 -- iter: 0960/1168\n",
      "Training Step: 377  | total loss: \u001b[1m\u001b[32m0.02108\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 020 | loss: 0.02108 - R2: 0.9993 -- iter: 1024/1168\n",
      "Training Step: 378  | total loss: \u001b[1m\u001b[32m0.02108\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 020 | loss: 0.02108 - R2: 0.9993 -- iter: 1088/1168\n",
      "Training Step: 379  | total loss: \u001b[1m\u001b[32m0.02158\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 020 | loss: 0.02158 - R2: 0.9999 -- iter: 1152/1168\n",
      "Training Step: 380  | total loss: \u001b[1m\u001b[32m0.02023\u001b[0m\u001b[0m | time: 1.175s\n",
      "| SGD | epoch: 020 | loss: 0.02023 - R2: 0.9995 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 381  | total loss: \u001b[1m\u001b[32m0.02210\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 021 | loss: 0.02210 - R2: 0.9989 -- iter: 0064/1168\n",
      "Training Step: 382  | total loss: \u001b[1m\u001b[32m0.02210\u001b[0m\u001b[0m | time: 0.026s\n",
      "| SGD | epoch: 021 | loss: 0.02210 - R2: 0.9989 -- iter: 0128/1168\n",
      "Training Step: 383  | total loss: \u001b[1m\u001b[32m0.02242\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 021 | loss: 0.02242 - R2: 0.9990 -- iter: 0192/1168\n",
      "Training Step: 384  | total loss: \u001b[1m\u001b[32m0.02071\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 021 | loss: 0.02071 - R2: 0.9988 -- iter: 0256/1168\n",
      "Training Step: 385  | total loss: \u001b[1m\u001b[32m0.02071\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 021 | loss: 0.02071 - R2: 0.9988 -- iter: 0320/1168\n",
      "Training Step: 386  | total loss: \u001b[1m\u001b[32m0.02006\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 021 | loss: 0.02006 - R2: 0.9986 -- iter: 0384/1168\n",
      "Training Step: 387  | total loss: \u001b[1m\u001b[32m0.02006\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 021 | loss: 0.02006 - R2: 0.9994 -- iter: 0448/1168\n",
      "Training Step: 388  | total loss: \u001b[1m\u001b[32m0.01996\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 021 | loss: 0.01996 - R2: 0.9990 -- iter: 0512/1168\n",
      "Training Step: 389  | total loss: \u001b[1m\u001b[32m0.02051\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 021 | loss: 0.02051 - R2: 0.9996 -- iter: 0576/1168\n",
      "Training Step: 390  | total loss: \u001b[1m\u001b[32m0.02035\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 021 | loss: 0.02035 - R2: 0.9995 -- iter: 0640/1168\n",
      "Training Step: 391  | total loss: \u001b[1m\u001b[32m0.02035\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 021 | loss: 0.02035 - R2: 0.9991 -- iter: 0704/1168\n",
      "Training Step: 392  | total loss: \u001b[1m\u001b[32m0.01999\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 021 | loss: 0.01999 - R2: 0.9993 -- iter: 0768/1168\n",
      "Training Step: 393  | total loss: \u001b[1m\u001b[32m0.01999\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 021 | loss: 0.01999 - R2: 0.9993 -- iter: 0832/1168\n",
      "Training Step: 394  | total loss: \u001b[1m\u001b[32m0.02036\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 021 | loss: 0.02036 - R2: 0.9992 -- iter: 0896/1168\n",
      "Training Step: 395  | total loss: \u001b[1m\u001b[32m0.02220\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 021 | loss: 0.02220 - R2: 0.9988 -- iter: 0960/1168\n",
      "Training Step: 396  | total loss: \u001b[1m\u001b[32m0.02233\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 021 | loss: 0.02233 - R2: 0.9982 -- iter: 1024/1168\n",
      "Training Step: 397  | total loss: \u001b[1m\u001b[32m0.02161\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 021 | loss: 0.02161 - R2: 0.9986 -- iter: 1088/1168\n",
      "Training Step: 398  | total loss: \u001b[1m\u001b[32m0.02219\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 021 | loss: 0.02219 - R2: 0.9983 -- iter: 1152/1168\n",
      "Training Step: 399  | total loss: \u001b[1m\u001b[32m0.02219\u001b[0m\u001b[0m | time: 1.167s\n",
      "| SGD | epoch: 021 | loss: 0.02219 - R2: 0.9983 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 400  | total loss: \u001b[1m\u001b[32m0.02285\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 022 | loss: 0.02285 - R2: 0.9973 -- iter: 0064/1168\n",
      "Training Step: 401  | total loss: \u001b[1m\u001b[32m0.02322\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 022 | loss: 0.02322 - R2: 0.9981 -- iter: 0128/1168\n",
      "Training Step: 402  | total loss: \u001b[1m\u001b[32m0.02255\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 022 | loss: 0.02255 - R2: 0.9985 -- iter: 0192/1168\n",
      "Training Step: 403  | total loss: \u001b[1m\u001b[32m0.02243\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 022 | loss: 0.02243 - R2: 0.9987 -- iter: 0256/1168\n",
      "Training Step: 404  | total loss: \u001b[1m\u001b[32m0.02194\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 022 | loss: 0.02194 - R2: 0.9987 -- iter: 0320/1168\n",
      "Training Step: 405  | total loss: \u001b[1m\u001b[32m0.02194\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 022 | loss: 0.02194 - R2: 0.9991 -- iter: 0384/1168\n",
      "Training Step: 406  | total loss: \u001b[1m\u001b[32m0.02328\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 022 | loss: 0.02328 - R2: 0.9997 -- iter: 0448/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 407  | total loss: \u001b[1m\u001b[32m0.02328\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 022 | loss: 0.02328 - R2: 0.9996 -- iter: 0512/1168\n",
      "Training Step: 408  | total loss: \u001b[1m\u001b[32m0.02185\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 022 | loss: 0.02185 - R2: 0.9997 -- iter: 0576/1168\n",
      "Training Step: 409  | total loss: \u001b[1m\u001b[32m0.02185\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 022 | loss: 0.02185 - R2: 0.9997 -- iter: 0640/1168\n",
      "Training Step: 410  | total loss: \u001b[1m\u001b[32m0.02098\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 022 | loss: 0.02098 - R2: 1.0000 -- iter: 0704/1168\n",
      "Training Step: 411  | total loss: \u001b[1m\u001b[32m0.02098\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 022 | loss: 0.02098 - R2: 0.9998 -- iter: 0768/1168\n",
      "Training Step: 412  | total loss: \u001b[1m\u001b[32m0.02021\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 022 | loss: 0.02021 - R2: 0.9999 -- iter: 0832/1168\n",
      "Training Step: 413  | total loss: \u001b[1m\u001b[32m0.02160\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 022 | loss: 0.02160 - R2: 0.9997 -- iter: 0896/1168\n",
      "Training Step: 414  | total loss: \u001b[1m\u001b[32m0.02033\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 022 | loss: 0.02033 - R2: 0.9997 -- iter: 0960/1168\n",
      "Training Step: 415  | total loss: \u001b[1m\u001b[32m0.02207\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 022 | loss: 0.02207 - R2: 1.0000 -- iter: 1024/1168\n",
      "Training Step: 416  | total loss: \u001b[1m\u001b[32m0.02439\u001b[0m\u001b[0m | time: 0.184s\n",
      "| SGD | epoch: 022 | loss: 0.02439 - R2: 1.0004 -- iter: 1088/1168\n",
      "Training Step: 417  | total loss: \u001b[1m\u001b[32m0.02379\u001b[0m\u001b[0m | time: 0.212s\n",
      "| SGD | epoch: 022 | loss: 0.02379 - R2: 1.0001 -- iter: 1152/1168\n",
      "Training Step: 418  | total loss: \u001b[1m\u001b[32m0.02379\u001b[0m\u001b[0m | time: 1.218s\n",
      "| SGD | epoch: 022 | loss: 0.02379 - R2: 1.0001 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 419  | total loss: \u001b[1m\u001b[32m0.02512\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 023 | loss: 0.02512 - R2: 1.0001 -- iter: 0064/1168\n",
      "Training Step: 420  | total loss: \u001b[1m\u001b[32m0.02359\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 023 | loss: 0.02359 - R2: 1.0005 -- iter: 0128/1168\n",
      "Training Step: 421  | total loss: \u001b[1m\u001b[32m0.02329\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 023 | loss: 0.02329 - R2: 1.0000 -- iter: 0192/1168\n",
      "Training Step: 422  | total loss: \u001b[1m\u001b[32m0.02329\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 023 | loss: 0.02329 - R2: 1.0000 -- iter: 0256/1168\n",
      "Training Step: 423  | total loss: \u001b[1m\u001b[32m0.02379\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 023 | loss: 0.02379 - R2: 0.9999 -- iter: 0320/1168\n",
      "Training Step: 424  | total loss: \u001b[1m\u001b[32m0.02238\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 023 | loss: 0.02238 - R2: 0.9997 -- iter: 0384/1168\n",
      "Training Step: 425  | total loss: \u001b[1m\u001b[32m0.02097\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 023 | loss: 0.02097 - R2: 0.9994 -- iter: 0448/1168\n",
      "Training Step: 426  | total loss: \u001b[1m\u001b[32m0.02108\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 023 | loss: 0.02108 - R2: 0.9991 -- iter: 0512/1168\n",
      "Training Step: 427  | total loss: \u001b[1m\u001b[32m0.04280\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 023 | loss: 0.04280 - R2: 0.9988 -- iter: 0576/1168\n",
      "Training Step: 428  | total loss: \u001b[1m\u001b[32m0.04280\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 023 | loss: 0.04280 - R2: 0.9988 -- iter: 0640/1168\n",
      "Training Step: 429  | total loss: \u001b[1m\u001b[32m0.04055\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 023 | loss: 0.04055 - R2: 0.9984 -- iter: 0704/1168\n",
      "Training Step: 430  | total loss: \u001b[1m\u001b[32m0.04227\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 023 | loss: 0.04227 - R2: 0.9987 -- iter: 0768/1168\n",
      "Training Step: 431  | total loss: \u001b[1m\u001b[32m0.03761\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 023 | loss: 0.03761 - R2: 0.9978 -- iter: 0832/1168\n",
      "Training Step: 432  | total loss: \u001b[1m\u001b[32m0.03761\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 023 | loss: 0.03761 - R2: 0.9978 -- iter: 0896/1168\n",
      "Training Step: 433  | total loss: \u001b[1m\u001b[32m0.03820\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 023 | loss: 0.03820 - R2: 0.9987 -- iter: 0960/1168\n",
      "Training Step: 434  | total loss: \u001b[1m\u001b[32m0.03654\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 023 | loss: 0.03654 - R2: 0.9989 -- iter: 1024/1168\n",
      "Training Step: 435  | total loss: \u001b[1m\u001b[32m0.03376\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 023 | loss: 0.03376 - R2: 0.9987 -- iter: 1088/1168\n",
      "Training Step: 436  | total loss: \u001b[1m\u001b[32m0.03323\u001b[0m\u001b[0m | time: 0.181s\n",
      "| SGD | epoch: 023 | loss: 0.03323 - R2: 0.9991 -- iter: 1152/1168\n",
      "Training Step: 437  | total loss: \u001b[1m\u001b[32m0.03111\u001b[0m\u001b[0m | time: 1.187s\n",
      "| SGD | epoch: 023 | loss: 0.03111 - R2: 0.9994 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 438  | total loss: \u001b[1m\u001b[32m0.02930\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 024 | loss: 0.02930 - R2: 0.9995 -- iter: 0064/1168\n",
      "Training Step: 439  | total loss: \u001b[1m\u001b[32m0.02668\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 024 | loss: 0.02668 - R2: 0.9987 -- iter: 0128/1168\n",
      "Training Step: 440  | total loss: \u001b[1m\u001b[32m0.02668\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 024 | loss: 0.02668 - R2: 0.9987 -- iter: 0192/1168\n",
      "Training Step: 441  | total loss: \u001b[1m\u001b[32m0.02530\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 024 | loss: 0.02530 - R2: 0.9977 -- iter: 0256/1168\n",
      "Training Step: 442  | total loss: \u001b[1m\u001b[32m0.02463\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 024 | loss: 0.02463 - R2: 0.9980 -- iter: 0320/1168\n",
      "Training Step: 443  | total loss: \u001b[1m\u001b[32m0.02463\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 024 | loss: 0.02463 - R2: 0.9980 -- iter: 0384/1168\n",
      "Training Step: 444  | total loss: \u001b[1m\u001b[32m0.02443\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 024 | loss: 0.02443 - R2: 0.9984 -- iter: 0448/1168\n",
      "Training Step: 445  | total loss: \u001b[1m\u001b[32m0.02484\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 024 | loss: 0.02484 - R2: 0.9986 -- iter: 0512/1168\n",
      "Training Step: 446  | total loss: \u001b[1m\u001b[32m0.02308\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 024 | loss: 0.02308 - R2: 0.9991 -- iter: 0576/1168\n",
      "Training Step: 447  | total loss: \u001b[1m\u001b[32m0.02308\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 024 | loss: 0.02308 - R2: 0.9991 -- iter: 0640/1168\n",
      "Training Step: 448  | total loss: \u001b[1m\u001b[32m0.02275\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 024 | loss: 0.02275 - R2: 0.9989 -- iter: 0704/1168\n",
      "Training Step: 449  | total loss: \u001b[1m\u001b[32m0.02306\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 024 | loss: 0.02306 - R2: 0.9988 -- iter: 0768/1168\n",
      "Training Step: 450  | total loss: \u001b[1m\u001b[32m0.02365\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 024 | loss: 0.02365 - R2: 0.9986 -- iter: 0832/1168\n",
      "Training Step: 451  | total loss: \u001b[1m\u001b[32m0.02365\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 024 | loss: 0.02365 - R2: 0.9986 -- iter: 0896/1168\n",
      "Training Step: 452  | total loss: \u001b[1m\u001b[32m0.02437\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 024 | loss: 0.02437 - R2: 0.9995 -- iter: 0960/1168\n",
      "Training Step: 453  | total loss: \u001b[1m\u001b[32m0.02437\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 024 | loss: 0.02437 - R2: 0.9995 -- iter: 1024/1168\n",
      "Training Step: 454  | total loss: \u001b[1m\u001b[32m0.02422\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 024 | loss: 0.02422 - R2: 0.9994 -- iter: 1088/1168\n",
      "Training Step: 455  | total loss: \u001b[1m\u001b[32m0.02337\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 024 | loss: 0.02337 - R2: 0.9991 -- iter: 1152/1168\n",
      "Training Step: 456  | total loss: \u001b[1m\u001b[32m0.02201\u001b[0m\u001b[0m | time: 1.151s\n",
      "| SGD | epoch: 024 | loss: 0.02201 - R2: 0.9995 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 457  | total loss: \u001b[1m\u001b[32m0.02121\u001b[0m\u001b[0m | time: 0.006s\n",
      "| SGD | epoch: 025 | loss: 0.02121 - R2: 0.9997 -- iter: 0064/1168\n",
      "Training Step: 458  | total loss: \u001b[1m\u001b[32m0.02121\u001b[0m\u001b[0m | time: 0.009s\n",
      "| SGD | epoch: 025 | loss: 0.02121 - R2: 0.9997 -- iter: 0128/1168\n",
      "Training Step: 459  | total loss: \u001b[1m\u001b[32m0.02168\u001b[0m\u001b[0m | time: 0.011s\n",
      "| SGD | epoch: 025 | loss: 0.02168 - R2: 0.9995 -- iter: 0192/1168\n",
      "Training Step: 460  | total loss: \u001b[1m\u001b[32m0.02105\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 025 | loss: 0.02105 - R2: 0.9992 -- iter: 0256/1168\n",
      "Training Step: 461  | total loss: \u001b[1m\u001b[32m0.02105\u001b[0m\u001b[0m | time: 0.018s\n",
      "| SGD | epoch: 025 | loss: 0.02105 - R2: 0.9989 -- iter: 0320/1168\n",
      "Training Step: 462  | total loss: \u001b[1m\u001b[32m0.02345\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 025 | loss: 0.02345 - R2: 0.9990 -- iter: 0384/1168\n",
      "Training Step: 463  | total loss: \u001b[1m\u001b[32m0.02345\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 025 | loss: 0.02345 - R2: 0.9990 -- iter: 0448/1168\n",
      "Training Step: 464  | total loss: \u001b[1m\u001b[32m0.02268\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 025 | loss: 0.02268 - R2: 0.9992 -- iter: 0512/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 465  | total loss: \u001b[1m\u001b[32m0.02256\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 025 | loss: 0.02256 - R2: 0.9988 -- iter: 0576/1168\n",
      "Training Step: 466  | total loss: \u001b[1m\u001b[32m0.02127\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 025 | loss: 0.02127 - R2: 0.9989 -- iter: 0640/1168\n",
      "Training Step: 467  | total loss: \u001b[1m\u001b[32m0.02127\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 025 | loss: 0.02127 - R2: 0.9989 -- iter: 0704/1168\n",
      "Training Step: 468  | total loss: \u001b[1m\u001b[32m0.02078\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 025 | loss: 0.02078 - R2: 0.9992 -- iter: 0768/1168\n",
      "Training Step: 469  | total loss: \u001b[1m\u001b[32m0.02093\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 025 | loss: 0.02093 - R2: 0.9989 -- iter: 0832/1168\n",
      "Training Step: 470  | total loss: \u001b[1m\u001b[32m0.02176\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 025 | loss: 0.02176 - R2: 0.9988 -- iter: 0896/1168\n",
      "Training Step: 471  | total loss: \u001b[1m\u001b[32m0.02083\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 025 | loss: 0.02083 - R2: 0.9988 -- iter: 0960/1168\n",
      "Training Step: 472  | total loss: \u001b[1m\u001b[32m0.02093\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 025 | loss: 0.02093 - R2: 0.9990 -- iter: 1024/1168\n",
      "Training Step: 473  | total loss: \u001b[1m\u001b[32m0.02056\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 025 | loss: 0.02056 - R2: 0.9987 -- iter: 1088/1168\n",
      "Training Step: 474  | total loss: \u001b[1m\u001b[32m0.01972\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 025 | loss: 0.01972 - R2: 0.9988 -- iter: 1152/1168\n",
      "Training Step: 475  | total loss: \u001b[1m\u001b[32m0.02263\u001b[0m\u001b[0m | time: 1.084s\n",
      "| SGD | epoch: 025 | loss: 0.02263 - R2: 0.9993 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 476  | total loss: \u001b[1m\u001b[32m0.02263\u001b[0m\u001b[0m | time: 0.009s\n",
      "| SGD | epoch: 026 | loss: 0.02263 - R2: 0.9994 -- iter: 0064/1168\n",
      "Training Step: 477  | total loss: \u001b[1m\u001b[32m0.02153\u001b[0m\u001b[0m | time: 0.013s\n",
      "| SGD | epoch: 026 | loss: 0.02153 - R2: 0.9993 -- iter: 0128/1168\n",
      "Training Step: 478  | total loss: \u001b[1m\u001b[32m0.02140\u001b[0m\u001b[0m | time: 0.018s\n",
      "| SGD | epoch: 026 | loss: 0.02140 - R2: 0.9991 -- iter: 0192/1168\n",
      "Training Step: 479  | total loss: \u001b[1m\u001b[32m0.02351\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 026 | loss: 0.02351 - R2: 0.9982 -- iter: 0256/1168\n",
      "Training Step: 480  | total loss: \u001b[1m\u001b[32m0.02542\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 026 | loss: 0.02542 - R2: 0.9974 -- iter: 0320/1168\n",
      "Training Step: 481  | total loss: \u001b[1m\u001b[32m0.02542\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 026 | loss: 0.02542 - R2: 0.9974 -- iter: 0384/1168\n",
      "Training Step: 482  | total loss: \u001b[1m\u001b[32m0.02574\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 026 | loss: 0.02574 - R2: 0.9984 -- iter: 0448/1168\n",
      "Training Step: 483  | total loss: \u001b[1m\u001b[32m0.02442\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 026 | loss: 0.02442 - R2: 0.9989 -- iter: 0512/1168\n",
      "Training Step: 484  | total loss: \u001b[1m\u001b[32m0.02355\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 026 | loss: 0.02355 - R2: 0.9988 -- iter: 0576/1168\n",
      "Training Step: 485  | total loss: \u001b[1m\u001b[32m0.02205\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 026 | loss: 0.02205 - R2: 0.9984 -- iter: 0640/1168\n",
      "Training Step: 486  | total loss: \u001b[1m\u001b[32m0.02205\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 026 | loss: 0.02205 - R2: 0.9984 -- iter: 0704/1168\n",
      "Training Step: 487  | total loss: \u001b[1m\u001b[32m0.06058\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 026 | loss: 0.06058 - R2: 0.9995 -- iter: 0768/1168\n",
      "Training Step: 488  | total loss: \u001b[1m\u001b[32m0.06058\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 026 | loss: 0.06058 - R2: 0.9995 -- iter: 0832/1168\n",
      "Training Step: 489  | total loss: \u001b[1m\u001b[32m0.05240\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 026 | loss: 0.05240 - R2: 0.9991 -- iter: 0896/1168\n",
      "Training Step: 490  | total loss: \u001b[1m\u001b[32m0.05240\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 026 | loss: 0.05240 - R2: 0.9991 -- iter: 0960/1168\n",
      "Training Step: 491  | total loss: \u001b[1m\u001b[32m0.04762\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 026 | loss: 0.04762 - R2: 0.9990 -- iter: 1024/1168\n",
      "Training Step: 492  | total loss: \u001b[1m\u001b[32m0.04762\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 026 | loss: 0.04762 - R2: 0.9990 -- iter: 1088/1168\n",
      "Training Step: 493  | total loss: \u001b[1m\u001b[32m0.04281\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 026 | loss: 0.04281 - R2: 0.9989 -- iter: 1152/1168\n",
      "Training Step: 494  | total loss: \u001b[1m\u001b[32m0.04281\u001b[0m\u001b[0m | time: 1.113s\n",
      "| SGD | epoch: 026 | loss: 0.04281 - R2: 0.9989 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 495  | total loss: \u001b[1m\u001b[32m0.04104\u001b[0m\u001b[0m | time: 0.020s\n",
      "| SGD | epoch: 027 | loss: 0.04104 - R2: 0.9999 -- iter: 0064/1168\n",
      "Training Step: 496  | total loss: \u001b[1m\u001b[32m0.03840\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 027 | loss: 0.03840 - R2: 0.9999 -- iter: 0128/1168\n",
      "Training Step: 497  | total loss: \u001b[1m\u001b[32m0.03778\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 027 | loss: 0.03778 - R2: 0.9999 -- iter: 0192/1168\n",
      "Training Step: 498  | total loss: \u001b[1m\u001b[32m0.03575\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 027 | loss: 0.03575 - R2: 0.9998 -- iter: 0256/1168\n",
      "Training Step: 499  | total loss: \u001b[1m\u001b[32m0.03299\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 027 | loss: 0.03299 - R2: 0.9999 -- iter: 0320/1168\n",
      "Training Step: 500  | total loss: \u001b[1m\u001b[32m0.03050\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 027 | loss: 0.03050 - R2: 0.9999 -- iter: 0384/1168\n",
      "Training Step: 501  | total loss: \u001b[1m\u001b[32m0.02927\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 027 | loss: 0.02927 - R2: 1.0003 -- iter: 0448/1168\n",
      "Training Step: 502  | total loss: \u001b[1m\u001b[32m0.02840\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 027 | loss: 0.02840 - R2: 0.9997 -- iter: 0512/1168\n",
      "Training Step: 503  | total loss: \u001b[1m\u001b[32m0.02840\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 027 | loss: 0.02840 - R2: 0.9997 -- iter: 0576/1168\n",
      "Training Step: 504  | total loss: \u001b[1m\u001b[32m0.02586\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 027 | loss: 0.02586 - R2: 1.0001 -- iter: 0640/1168\n",
      "Training Step: 505  | total loss: \u001b[1m\u001b[32m0.02586\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 027 | loss: 0.02586 - R2: 1.0001 -- iter: 0704/1168\n",
      "Training Step: 506  | total loss: \u001b[1m\u001b[32m0.02520\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 027 | loss: 0.02520 - R2: 0.9994 -- iter: 0768/1168\n",
      "Training Step: 507  | total loss: \u001b[1m\u001b[32m0.02514\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 027 | loss: 0.02514 - R2: 0.9994 -- iter: 0832/1168\n",
      "Training Step: 508  | total loss: \u001b[1m\u001b[32m0.05630\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 027 | loss: 0.05630 - R2: 0.9979 -- iter: 0896/1168\n",
      "Training Step: 509  | total loss: \u001b[1m\u001b[32m0.05281\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 027 | loss: 0.05281 - R2: 0.9981 -- iter: 0960/1168\n",
      "Training Step: 510  | total loss: \u001b[1m\u001b[32m0.04932\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 027 | loss: 0.04932 - R2: 0.9983 -- iter: 1024/1168\n",
      "Training Step: 511  | total loss: \u001b[1m\u001b[32m0.04628\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 027 | loss: 0.04628 - R2: 0.9982 -- iter: 1088/1168\n",
      "Training Step: 512  | total loss: \u001b[1m\u001b[32m0.04285\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 027 | loss: 0.04285 - R2: 0.9983 -- iter: 1152/1168\n",
      "Training Step: 513  | total loss: \u001b[1m\u001b[32m0.03967\u001b[0m\u001b[0m | time: 1.140s\n",
      "| SGD | epoch: 027 | loss: 0.03967 - R2: 0.9983 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 514  | total loss: \u001b[1m\u001b[32m0.04056\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 028 | loss: 0.04056 - R2: 0.9996 -- iter: 0064/1168\n",
      "Training Step: 515  | total loss: \u001b[1m\u001b[32m0.03986\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 028 | loss: 0.03986 - R2: 0.9994 -- iter: 0128/1168\n",
      "Training Step: 516  | total loss: \u001b[1m\u001b[32m0.03828\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 028 | loss: 0.03828 - R2: 0.9994 -- iter: 0192/1168\n",
      "Training Step: 517  | total loss: \u001b[1m\u001b[32m0.03464\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 028 | loss: 0.03464 - R2: 0.9991 -- iter: 0256/1168\n",
      "Training Step: 518  | total loss: \u001b[1m\u001b[32m0.03255\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 028 | loss: 0.03255 - R2: 0.9990 -- iter: 0320/1168\n",
      "Training Step: 519  | total loss: \u001b[1m\u001b[32m0.03137\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 028 | loss: 0.03137 - R2: 0.9982 -- iter: 0384/1168\n",
      "Training Step: 520  | total loss: \u001b[1m\u001b[32m0.03137\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 028 | loss: 0.03137 - R2: 0.9982 -- iter: 0448/1168\n",
      "Training Step: 521  | total loss: \u001b[1m\u001b[32m0.03031\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 028 | loss: 0.03031 - R2: 0.9975 -- iter: 0512/1168\n",
      "Training Step: 522  | total loss: \u001b[1m\u001b[32m0.02804\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 028 | loss: 0.02804 - R2: 0.9979 -- iter: 0576/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 523  | total loss: \u001b[1m\u001b[32m0.02804\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 028 | loss: 0.02804 - R2: 0.9977 -- iter: 0640/1168\n",
      "Training Step: 524  | total loss: \u001b[1m\u001b[32m0.02833\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 028 | loss: 0.02833 - R2: 0.9980 -- iter: 0704/1168\n",
      "Training Step: 525  | total loss: \u001b[1m\u001b[32m0.02833\u001b[0m\u001b[0m | time: 0.176s\n",
      "| SGD | epoch: 028 | loss: 0.02833 - R2: 0.9980 -- iter: 0768/1168\n",
      "Training Step: 526  | total loss: \u001b[1m\u001b[32m0.02803\u001b[0m\u001b[0m | time: 0.186s\n",
      "| SGD | epoch: 028 | loss: 0.02803 - R2: 0.9985 -- iter: 0832/1168\n",
      "Training Step: 527  | total loss: \u001b[1m\u001b[32m0.02683\u001b[0m\u001b[0m | time: 0.188s\n",
      "| SGD | epoch: 028 | loss: 0.02683 - R2: 0.9985 -- iter: 0896/1168\n",
      "Training Step: 528  | total loss: \u001b[1m\u001b[32m0.02558\u001b[0m\u001b[0m | time: 0.192s\n",
      "| SGD | epoch: 028 | loss: 0.02558 - R2: 0.9987 -- iter: 0960/1168\n",
      "Training Step: 529  | total loss: \u001b[1m\u001b[32m0.02500\u001b[0m\u001b[0m | time: 0.213s\n",
      "| SGD | epoch: 028 | loss: 0.02500 - R2: 0.9988 -- iter: 1024/1168\n",
      "Training Step: 530  | total loss: \u001b[1m\u001b[32m0.02442\u001b[0m\u001b[0m | time: 0.221s\n",
      "| SGD | epoch: 028 | loss: 0.02442 - R2: 0.9987 -- iter: 1088/1168\n",
      "Training Step: 531  | total loss: \u001b[1m\u001b[32m0.02442\u001b[0m\u001b[0m | time: 0.226s\n",
      "| SGD | epoch: 028 | loss: 0.02442 - R2: 0.9987 -- iter: 1152/1168\n",
      "Training Step: 532  | total loss: \u001b[1m\u001b[32m0.02414\u001b[0m\u001b[0m | time: 1.238s\n",
      "| SGD | epoch: 028 | loss: 0.02414 - R2: 0.9984 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 533  | total loss: \u001b[1m\u001b[32m0.02520\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 029 | loss: 0.02520 - R2: 0.9992 -- iter: 0064/1168\n",
      "Training Step: 534  | total loss: \u001b[1m\u001b[32m0.02446\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 029 | loss: 0.02446 - R2: 0.9986 -- iter: 0128/1168\n",
      "Training Step: 535  | total loss: \u001b[1m\u001b[32m0.02400\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 029 | loss: 0.02400 - R2: 0.9986 -- iter: 0192/1168\n",
      "Training Step: 536  | total loss: \u001b[1m\u001b[32m0.02338\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 029 | loss: 0.02338 - R2: 0.9986 -- iter: 0256/1168\n",
      "Training Step: 537  | total loss: \u001b[1m\u001b[32m0.02376\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 029 | loss: 0.02376 - R2: 0.9992 -- iter: 0320/1168\n",
      "Training Step: 538  | total loss: \u001b[1m\u001b[32m0.02544\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 029 | loss: 0.02544 - R2: 0.9994 -- iter: 0384/1168\n",
      "Training Step: 539  | total loss: \u001b[1m\u001b[32m0.02573\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 029 | loss: 0.02573 - R2: 0.9995 -- iter: 0448/1168\n",
      "Training Step: 540  | total loss: \u001b[1m\u001b[32m0.02420\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 029 | loss: 0.02420 - R2: 0.9989 -- iter: 0512/1168\n",
      "Training Step: 541  | total loss: \u001b[1m\u001b[32m0.02420\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 029 | loss: 0.02420 - R2: 0.9989 -- iter: 0576/1168\n",
      "Training Step: 542  | total loss: \u001b[1m\u001b[32m0.02318\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 029 | loss: 0.02318 - R2: 0.9988 -- iter: 0640/1168\n",
      "Training Step: 543  | total loss: \u001b[1m\u001b[32m0.02318\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 029 | loss: 0.02318 - R2: 0.9988 -- iter: 0704/1168\n",
      "Training Step: 544  | total loss: \u001b[1m\u001b[32m0.02322\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 029 | loss: 0.02322 - R2: 0.9987 -- iter: 0768/1168\n",
      "Training Step: 545  | total loss: \u001b[1m\u001b[32m0.02211\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 029 | loss: 0.02211 - R2: 0.9992 -- iter: 0832/1168\n",
      "Training Step: 546  | total loss: \u001b[1m\u001b[32m0.02167\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 029 | loss: 0.02167 - R2: 0.9993 -- iter: 0896/1168\n",
      "Training Step: 547  | total loss: \u001b[1m\u001b[32m0.02167\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 029 | loss: 0.02167 - R2: 0.9993 -- iter: 0960/1168\n",
      "Training Step: 548  | total loss: \u001b[1m\u001b[32m0.03230\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 029 | loss: 0.03230 - R2: 0.9991 -- iter: 1024/1168\n",
      "Training Step: 549  | total loss: \u001b[1m\u001b[32m0.03105\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 029 | loss: 0.03105 - R2: 0.9995 -- iter: 1088/1168\n",
      "Training Step: 550  | total loss: \u001b[1m\u001b[32m0.02959\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 029 | loss: 0.02959 - R2: 0.9990 -- iter: 1152/1168\n",
      "Training Step: 551  | total loss: \u001b[1m\u001b[32m0.02984\u001b[0m\u001b[0m | time: 1.108s\n",
      "| SGD | epoch: 029 | loss: 0.02984 - R2: 0.9993 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 552  | total loss: \u001b[1m\u001b[32m0.02883\u001b[0m\u001b[0m | time: 0.016s\n",
      "| SGD | epoch: 030 | loss: 0.02883 - R2: 0.9986 -- iter: 0064/1168\n",
      "Training Step: 553  | total loss: \u001b[1m\u001b[32m0.02768\u001b[0m\u001b[0m | time: 0.023s\n",
      "| SGD | epoch: 030 | loss: 0.02768 - R2: 0.9993 -- iter: 0128/1168\n",
      "Training Step: 554  | total loss: \u001b[1m\u001b[32m0.03021\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 030 | loss: 0.03021 - R2: 0.9999 -- iter: 0192/1168\n",
      "Training Step: 555  | total loss: \u001b[1m\u001b[32m0.02876\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 030 | loss: 0.02876 - R2: 0.9993 -- iter: 0256/1168\n",
      "Training Step: 556  | total loss: \u001b[1m\u001b[32m0.02872\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 030 | loss: 0.02872 - R2: 1.0002 -- iter: 0320/1168\n",
      "Training Step: 557  | total loss: \u001b[1m\u001b[32m0.02872\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 030 | loss: 0.02872 - R2: 1.0002 -- iter: 0384/1168\n",
      "Training Step: 558  | total loss: \u001b[1m\u001b[32m0.02590\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 030 | loss: 0.02590 - R2: 1.0001 -- iter: 0448/1168\n",
      "Training Step: 559  | total loss: \u001b[1m\u001b[32m0.02616\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 030 | loss: 0.02616 - R2: 0.9995 -- iter: 0512/1168\n",
      "Training Step: 560  | total loss: \u001b[1m\u001b[32m0.02616\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 030 | loss: 0.02616 - R2: 0.9990 -- iter: 0576/1168\n",
      "Training Step: 561  | total loss: \u001b[1m\u001b[32m0.02640\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 030 | loss: 0.02640 - R2: 0.9990 -- iter: 0640/1168\n",
      "Training Step: 562  | total loss: \u001b[1m\u001b[32m0.02579\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 030 | loss: 0.02579 - R2: 0.9987 -- iter: 0704/1168\n",
      "Training Step: 563  | total loss: \u001b[1m\u001b[32m0.02516\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 030 | loss: 0.02516 - R2: 0.9983 -- iter: 0768/1168\n",
      "Training Step: 564  | total loss: \u001b[1m\u001b[32m0.02425\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 030 | loss: 0.02425 - R2: 0.9983 -- iter: 0832/1168\n",
      "Training Step: 565  | total loss: \u001b[1m\u001b[32m0.02386\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 030 | loss: 0.02386 - R2: 0.9981 -- iter: 0896/1168\n",
      "Training Step: 566  | total loss: \u001b[1m\u001b[32m0.02157\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 030 | loss: 0.02157 - R2: 0.9990 -- iter: 0960/1168\n",
      "Training Step: 567  | total loss: \u001b[1m\u001b[32m0.02144\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 030 | loss: 0.02144 - R2: 0.9990 -- iter: 1024/1168\n",
      "Training Step: 568  | total loss: \u001b[1m\u001b[32m0.02144\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 030 | loss: 0.02144 - R2: 0.9990 -- iter: 1088/1168\n",
      "Training Step: 569  | total loss: \u001b[1m\u001b[32m0.02082\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 030 | loss: 0.02082 - R2: 0.9988 -- iter: 1152/1168\n",
      "Training Step: 570  | total loss: \u001b[1m\u001b[32m0.02130\u001b[0m\u001b[0m | time: 1.089s\n",
      "| SGD | epoch: 030 | loss: 0.02130 - R2: 0.9988 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 571  | total loss: \u001b[1m\u001b[32m0.02036\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 031 | loss: 0.02036 - R2: 0.9992 -- iter: 0064/1168\n",
      "Training Step: 572  | total loss: \u001b[1m\u001b[32m0.02027\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 031 | loss: 0.02027 - R2: 0.9996 -- iter: 0128/1168\n",
      "Training Step: 573  | total loss: \u001b[1m\u001b[32m0.01987\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 031 | loss: 0.01987 - R2: 0.9992 -- iter: 0192/1168\n",
      "Training Step: 574  | total loss: \u001b[1m\u001b[32m0.02179\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 031 | loss: 0.02179 - R2: 0.9999 -- iter: 0256/1168\n",
      "Training Step: 575  | total loss: \u001b[1m\u001b[32m0.02124\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 031 | loss: 0.02124 - R2: 0.9999 -- iter: 0320/1168\n",
      "Training Step: 576  | total loss: \u001b[1m\u001b[32m0.02076\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 031 | loss: 0.02076 - R2: 1.0000 -- iter: 0384/1168\n",
      "Training Step: 577  | total loss: \u001b[1m\u001b[32m0.02243\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 031 | loss: 0.02243 - R2: 1.0000 -- iter: 0448/1168\n",
      "Training Step: 578  | total loss: \u001b[1m\u001b[32m0.02249\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 031 | loss: 0.02249 - R2: 0.9995 -- iter: 0512/1168\n",
      "Training Step: 579  | total loss: \u001b[1m\u001b[32m0.02283\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 031 | loss: 0.02283 - R2: 0.9994 -- iter: 0576/1168\n",
      "Training Step: 580  | total loss: \u001b[1m\u001b[32m0.02774\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 031 | loss: 0.02774 - R2: 1.0002 -- iter: 0640/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 581  | total loss: \u001b[1m\u001b[32m0.03217\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 031 | loss: 0.03217 - R2: 1.0010 -- iter: 0704/1168\n",
      "Training Step: 582  | total loss: \u001b[1m\u001b[32m0.03068\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 031 | loss: 0.03068 - R2: 1.0012 -- iter: 0768/1168\n",
      "Training Step: 583  | total loss: \u001b[1m\u001b[32m0.02968\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 031 | loss: 0.02968 - R2: 1.0008 -- iter: 0832/1168\n",
      "Training Step: 584  | total loss: \u001b[1m\u001b[32m0.02675\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 031 | loss: 0.02675 - R2: 1.0000 -- iter: 0896/1168\n",
      "Training Step: 585  | total loss: \u001b[1m\u001b[32m0.02532\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 031 | loss: 0.02532 - R2: 0.9997 -- iter: 0960/1168\n",
      "Training Step: 586  | total loss: \u001b[1m\u001b[32m0.02644\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 031 | loss: 0.02644 - R2: 0.9999 -- iter: 1024/1168\n",
      "Training Step: 587  | total loss: \u001b[1m\u001b[32m0.02644\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 031 | loss: 0.02644 - R2: 0.9999 -- iter: 1088/1168\n",
      "Training Step: 588  | total loss: \u001b[1m\u001b[32m0.04429\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 031 | loss: 0.04429 - R2: 0.9990 -- iter: 1152/1168\n",
      "Training Step: 589  | total loss: \u001b[1m\u001b[32m0.04429\u001b[0m\u001b[0m | time: 1.162s\n",
      "| SGD | epoch: 031 | loss: 0.04429 - R2: 0.9990 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 590  | total loss: \u001b[1m\u001b[32m0.04166\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 032 | loss: 0.04166 - R2: 0.9990 -- iter: 0064/1168\n",
      "Training Step: 591  | total loss: \u001b[1m\u001b[32m0.03928\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 032 | loss: 0.03928 - R2: 0.9989 -- iter: 0128/1168\n",
      "Training Step: 592  | total loss: \u001b[1m\u001b[32m0.03968\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 032 | loss: 0.03968 - R2: 0.9998 -- iter: 0192/1168\n",
      "Training Step: 593  | total loss: \u001b[1m\u001b[32m0.03968\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 032 | loss: 0.03968 - R2: 0.9998 -- iter: 0256/1168\n",
      "Training Step: 594  | total loss: \u001b[1m\u001b[32m0.03687\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 032 | loss: 0.03687 - R2: 0.9997 -- iter: 0320/1168\n",
      "Training Step: 595  | total loss: \u001b[1m\u001b[32m0.03492\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 032 | loss: 0.03492 - R2: 0.9998 -- iter: 0384/1168\n",
      "Training Step: 596  | total loss: \u001b[1m\u001b[32m0.03492\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 032 | loss: 0.03492 - R2: 0.9998 -- iter: 0448/1168\n",
      "Training Step: 597  | total loss: \u001b[1m\u001b[32m0.03285\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 032 | loss: 0.03285 - R2: 0.9997 -- iter: 0512/1168\n",
      "Training Step: 598  | total loss: \u001b[1m\u001b[32m0.03125\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 032 | loss: 0.03125 - R2: 0.9993 -- iter: 0576/1168\n",
      "Training Step: 599  | total loss: \u001b[1m\u001b[32m0.03088\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 032 | loss: 0.03088 - R2: 0.9991 -- iter: 0640/1168\n",
      "Training Step: 600  | total loss: \u001b[1m\u001b[32m0.03021\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 032 | loss: 0.03021 - R2: 0.9987 -- iter: 0704/1168\n",
      "Training Step: 601  | total loss: \u001b[1m\u001b[32m0.02961\u001b[0m\u001b[0m | time: 0.180s\n",
      "| SGD | epoch: 032 | loss: 0.02961 - R2: 0.9984 -- iter: 0768/1168\n",
      "Training Step: 602  | total loss: \u001b[1m\u001b[32m0.02778\u001b[0m\u001b[0m | time: 0.183s\n",
      "| SGD | epoch: 032 | loss: 0.02778 - R2: 0.9983 -- iter: 0832/1168\n",
      "Training Step: 603  | total loss: \u001b[1m\u001b[32m0.02618\u001b[0m\u001b[0m | time: 0.187s\n",
      "| SGD | epoch: 032 | loss: 0.02618 - R2: 0.9989 -- iter: 0896/1168\n",
      "Training Step: 604  | total loss: \u001b[1m\u001b[32m0.02507\u001b[0m\u001b[0m | time: 0.192s\n",
      "| SGD | epoch: 032 | loss: 0.02507 - R2: 0.9989 -- iter: 0960/1168\n",
      "Training Step: 605  | total loss: \u001b[1m\u001b[32m0.02432\u001b[0m\u001b[0m | time: 0.196s\n",
      "| SGD | epoch: 032 | loss: 0.02432 - R2: 0.9988 -- iter: 1024/1168\n",
      "Training Step: 606  | total loss: \u001b[1m\u001b[32m0.02523\u001b[0m\u001b[0m | time: 0.199s\n",
      "| SGD | epoch: 032 | loss: 0.02523 - R2: 0.9992 -- iter: 1088/1168\n",
      "Training Step: 607  | total loss: \u001b[1m\u001b[32m0.02523\u001b[0m\u001b[0m | time: 0.203s\n",
      "| SGD | epoch: 032 | loss: 0.02523 - R2: 0.9992 -- iter: 1152/1168\n",
      "Training Step: 608  | total loss: \u001b[1m\u001b[32m0.02474\u001b[0m\u001b[0m | time: 1.229s\n",
      "| SGD | epoch: 032 | loss: 0.02474 - R2: 0.9994 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 609  | total loss: \u001b[1m\u001b[32m0.02409\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 033 | loss: 0.02409 - R2: 0.9998 -- iter: 0064/1168\n",
      "Training Step: 610  | total loss: \u001b[1m\u001b[32m0.02346\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 033 | loss: 0.02346 - R2: 0.9995 -- iter: 0128/1168\n",
      "Training Step: 611  | total loss: \u001b[1m\u001b[32m0.02346\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 033 | loss: 0.02346 - R2: 1.0003 -- iter: 0192/1168\n",
      "Training Step: 612  | total loss: \u001b[1m\u001b[32m0.02322\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 033 | loss: 0.02322 - R2: 1.0001 -- iter: 0256/1168\n",
      "Training Step: 613  | total loss: \u001b[1m\u001b[32m0.02211\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 033 | loss: 0.02211 - R2: 0.9995 -- iter: 0320/1168\n",
      "Training Step: 614  | total loss: \u001b[1m\u001b[32m0.02201\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 033 | loss: 0.02201 - R2: 0.9997 -- iter: 0384/1168\n",
      "Training Step: 615  | total loss: \u001b[1m\u001b[32m0.02125\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 033 | loss: 0.02125 - R2: 0.9993 -- iter: 0448/1168\n",
      "Training Step: 616  | total loss: \u001b[1m\u001b[32m0.02132\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 033 | loss: 0.02132 - R2: 0.9991 -- iter: 0512/1168\n",
      "Training Step: 617  | total loss: \u001b[1m\u001b[32m0.02132\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 033 | loss: 0.02132 - R2: 0.9990 -- iter: 0576/1168\n",
      "Training Step: 618  | total loss: \u001b[1m\u001b[32m0.02147\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 033 | loss: 0.02147 - R2: 0.9990 -- iter: 0640/1168\n",
      "Training Step: 619  | total loss: \u001b[1m\u001b[32m0.02613\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 033 | loss: 0.02613 - R2: 0.9996 -- iter: 0704/1168\n",
      "Training Step: 620  | total loss: \u001b[1m\u001b[32m0.02515\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 033 | loss: 0.02515 - R2: 0.9992 -- iter: 0768/1168\n",
      "Training Step: 621  | total loss: \u001b[1m\u001b[32m0.02426\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 033 | loss: 0.02426 - R2: 0.9990 -- iter: 0832/1168\n",
      "Training Step: 622  | total loss: \u001b[1m\u001b[32m0.02285\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 033 | loss: 0.02285 - R2: 0.9990 -- iter: 0896/1168\n",
      "Training Step: 623  | total loss: \u001b[1m\u001b[32m0.02285\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 033 | loss: 0.02285 - R2: 0.9990 -- iter: 0960/1168\n",
      "Training Step: 624  | total loss: \u001b[1m\u001b[32m0.02285\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 033 | loss: 0.02285 - R2: 0.9990 -- iter: 1024/1168\n",
      "Training Step: 625  | total loss: \u001b[1m\u001b[32m0.02230\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 033 | loss: 0.02230 - R2: 0.9989 -- iter: 1088/1168\n",
      "Training Step: 626  | total loss: \u001b[1m\u001b[32m0.02219\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 033 | loss: 0.02219 - R2: 0.9989 -- iter: 1152/1168\n",
      "Training Step: 627  | total loss: \u001b[1m\u001b[32m0.02219\u001b[0m\u001b[0m | time: 1.165s\n",
      "| SGD | epoch: 033 | loss: 0.02219 - R2: 0.9989 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 628  | total loss: \u001b[1m\u001b[32m0.04468\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 034 | loss: 0.04468 - R2: 0.9976 -- iter: 0064/1168\n",
      "Training Step: 629  | total loss: \u001b[1m\u001b[32m0.04192\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 034 | loss: 0.04192 - R2: 0.9980 -- iter: 0128/1168\n",
      "Training Step: 630  | total loss: \u001b[1m\u001b[32m0.04192\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 034 | loss: 0.04192 - R2: 0.9980 -- iter: 0192/1168\n",
      "Training Step: 631  | total loss: \u001b[1m\u001b[32m0.03712\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 034 | loss: 0.03712 - R2: 0.9987 -- iter: 0256/1168\n",
      "Training Step: 632  | total loss: \u001b[1m\u001b[32m0.03529\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 034 | loss: 0.03529 - R2: 0.9987 -- iter: 0320/1168\n",
      "Training Step: 633  | total loss: \u001b[1m\u001b[32m0.03529\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 034 | loss: 0.03529 - R2: 0.9987 -- iter: 0384/1168\n",
      "Training Step: 634  | total loss: \u001b[1m\u001b[32m0.03400\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 034 | loss: 0.03400 - R2: 0.9986 -- iter: 0448/1168\n",
      "Training Step: 635  | total loss: \u001b[1m\u001b[32m0.03335\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 034 | loss: 0.03335 - R2: 0.9986 -- iter: 0512/1168\n",
      "Training Step: 636  | total loss: \u001b[1m\u001b[32m0.03309\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 034 | loss: 0.03309 - R2: 0.9987 -- iter: 0576/1168\n",
      "Training Step: 637  | total loss: \u001b[1m\u001b[32m0.03430\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 034 | loss: 0.03430 - R2: 0.9993 -- iter: 0640/1168\n",
      "Training Step: 638  | total loss: \u001b[1m\u001b[32m0.03294\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 034 | loss: 0.03294 - R2: 0.9990 -- iter: 0704/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 639  | total loss: \u001b[1m\u001b[32m0.03167\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 034 | loss: 0.03167 - R2: 0.9990 -- iter: 0768/1168\n",
      "Training Step: 640  | total loss: \u001b[1m\u001b[32m0.02813\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 034 | loss: 0.02813 - R2: 0.9992 -- iter: 0832/1168\n",
      "Training Step: 641  | total loss: \u001b[1m\u001b[32m0.02813\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 034 | loss: 0.02813 - R2: 0.9992 -- iter: 0896/1168\n",
      "Training Step: 642  | total loss: \u001b[1m\u001b[32m0.02656\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 034 | loss: 0.02656 - R2: 0.9993 -- iter: 0960/1168\n",
      "Training Step: 643  | total loss: \u001b[1m\u001b[32m0.02586\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 034 | loss: 0.02586 - R2: 0.9991 -- iter: 1024/1168\n",
      "Training Step: 644  | total loss: \u001b[1m\u001b[32m0.02442\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 034 | loss: 0.02442 - R2: 0.9991 -- iter: 1088/1168\n",
      "Training Step: 645  | total loss: \u001b[1m\u001b[32m0.02734\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 034 | loss: 0.02734 - R2: 0.9998 -- iter: 1152/1168\n",
      "Training Step: 646  | total loss: \u001b[1m\u001b[32m0.02589\u001b[0m\u001b[0m | time: 1.112s\n",
      "| SGD | epoch: 034 | loss: 0.02589 - R2: 0.9997 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 647  | total loss: \u001b[1m\u001b[32m0.05110\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 035 | loss: 0.05110 - R2: 0.9996 -- iter: 0064/1168\n",
      "Training Step: 648  | total loss: \u001b[1m\u001b[32m0.04747\u001b[0m\u001b[0m | time: 0.016s\n",
      "| SGD | epoch: 035 | loss: 0.04747 - R2: 0.9998 -- iter: 0128/1168\n",
      "Training Step: 649  | total loss: \u001b[1m\u001b[32m0.04747\u001b[0m\u001b[0m | time: 0.018s\n",
      "| SGD | epoch: 035 | loss: 0.04747 - R2: 0.9998 -- iter: 0192/1168\n",
      "Training Step: 650  | total loss: \u001b[1m\u001b[32m0.04449\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 035 | loss: 0.04449 - R2: 0.9995 -- iter: 0256/1168\n",
      "Training Step: 651  | total loss: \u001b[1m\u001b[32m0.04212\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 035 | loss: 0.04212 - R2: 0.9997 -- iter: 0320/1168\n",
      "Training Step: 652  | total loss: \u001b[1m\u001b[32m0.04212\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 035 | loss: 0.04212 - R2: 0.9997 -- iter: 0384/1168\n",
      "Training Step: 653  | total loss: \u001b[1m\u001b[32m0.03802\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 035 | loss: 0.03802 - R2: 0.9994 -- iter: 0448/1168\n",
      "Training Step: 654  | total loss: \u001b[1m\u001b[32m0.03599\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 035 | loss: 0.03599 - R2: 0.9993 -- iter: 0512/1168\n",
      "Training Step: 655  | total loss: \u001b[1m\u001b[32m0.03381\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 035 | loss: 0.03381 - R2: 0.9993 -- iter: 0576/1168\n",
      "Training Step: 656  | total loss: \u001b[1m\u001b[32m0.03381\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 035 | loss: 0.03381 - R2: 0.9993 -- iter: 0640/1168\n",
      "Training Step: 657  | total loss: \u001b[1m\u001b[32m0.03323\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 035 | loss: 0.03323 - R2: 0.9990 -- iter: 0704/1168\n",
      "Training Step: 658  | total loss: \u001b[1m\u001b[32m0.03001\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 035 | loss: 0.03001 - R2: 0.9996 -- iter: 0768/1168\n",
      "Training Step: 659  | total loss: \u001b[1m\u001b[32m0.02761\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 035 | loss: 0.02761 - R2: 0.9992 -- iter: 0832/1168\n",
      "Training Step: 660  | total loss: \u001b[1m\u001b[32m0.02546\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 035 | loss: 0.02546 - R2: 0.9988 -- iter: 0896/1168\n",
      "Training Step: 661  | total loss: \u001b[1m\u001b[32m0.02475\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 035 | loss: 0.02475 - R2: 0.9988 -- iter: 0960/1168\n",
      "Training Step: 662  | total loss: \u001b[1m\u001b[32m0.02449\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 035 | loss: 0.02449 - R2: 0.9989 -- iter: 1024/1168\n",
      "Training Step: 663  | total loss: \u001b[1m\u001b[32m0.02405\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 035 | loss: 0.02405 - R2: 0.9988 -- iter: 1088/1168\n",
      "Training Step: 664  | total loss: \u001b[1m\u001b[32m0.02240\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 035 | loss: 0.02240 - R2: 0.9991 -- iter: 1152/1168\n",
      "Training Step: 665  | total loss: \u001b[1m\u001b[32m0.02240\u001b[0m\u001b[0m | time: 1.133s\n",
      "| SGD | epoch: 035 | loss: 0.02240 - R2: 0.9991 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 666  | total loss: \u001b[1m\u001b[32m0.02257\u001b[0m\u001b[0m | time: 0.005s\n",
      "| SGD | epoch: 036 | loss: 0.02257 - R2: 0.9989 -- iter: 0064/1168\n",
      "Training Step: 667  | total loss: \u001b[1m\u001b[32m0.05973\u001b[0m\u001b[0m | time: 0.011s\n",
      "| SGD | epoch: 036 | loss: 0.05973 - R2: 0.9987 -- iter: 0128/1168\n",
      "Training Step: 668  | total loss: \u001b[1m\u001b[32m0.05973\u001b[0m\u001b[0m | time: 0.016s\n",
      "| SGD | epoch: 036 | loss: 0.05973 - R2: 0.9990 -- iter: 0192/1168\n",
      "Training Step: 669  | total loss: \u001b[1m\u001b[32m0.05583\u001b[0m\u001b[0m | time: 0.019s\n",
      "| SGD | epoch: 036 | loss: 0.05583 - R2: 0.9990 -- iter: 0256/1168\n",
      "Training Step: 670  | total loss: \u001b[1m\u001b[32m0.05203\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 036 | loss: 0.05203 - R2: 0.9990 -- iter: 0320/1168\n",
      "Training Step: 671  | total loss: \u001b[1m\u001b[32m0.04622\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 036 | loss: 0.04622 - R2: 0.9992 -- iter: 0384/1168\n",
      "Training Step: 672  | total loss: \u001b[1m\u001b[32m0.04622\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 036 | loss: 0.04622 - R2: 0.9992 -- iter: 0448/1168\n",
      "Training Step: 673  | total loss: \u001b[1m\u001b[32m0.04451\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 036 | loss: 0.04451 - R2: 0.9998 -- iter: 0512/1168\n",
      "Training Step: 674  | total loss: \u001b[1m\u001b[32m0.04276\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 036 | loss: 0.04276 - R2: 0.9997 -- iter: 0576/1168\n",
      "Training Step: 675  | total loss: \u001b[1m\u001b[32m0.04276\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 036 | loss: 0.04276 - R2: 0.9997 -- iter: 0640/1168\n",
      "Training Step: 676  | total loss: \u001b[1m\u001b[32m0.04017\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 036 | loss: 0.04017 - R2: 0.9997 -- iter: 0704/1168\n",
      "Training Step: 677  | total loss: \u001b[1m\u001b[32m0.03754\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 036 | loss: 0.03754 - R2: 1.0001 -- iter: 0768/1168\n",
      "Training Step: 678  | total loss: \u001b[1m\u001b[32m0.03302\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 036 | loss: 0.03302 - R2: 1.0000 -- iter: 0832/1168\n",
      "Training Step: 679  | total loss: \u001b[1m\u001b[32m0.03302\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 036 | loss: 0.03302 - R2: 1.0000 -- iter: 0896/1168\n",
      "Training Step: 680  | total loss: \u001b[1m\u001b[32m0.02909\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 036 | loss: 0.02909 - R2: 0.9991 -- iter: 0960/1168\n",
      "Training Step: 681  | total loss: \u001b[1m\u001b[32m0.02755\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 036 | loss: 0.02755 - R2: 0.9991 -- iter: 1024/1168\n",
      "Training Step: 682  | total loss: \u001b[1m\u001b[32m0.02755\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 036 | loss: 0.02755 - R2: 0.9991 -- iter: 1088/1168\n",
      "Training Step: 683  | total loss: \u001b[1m\u001b[32m0.02777\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 036 | loss: 0.02777 - R2: 0.9989 -- iter: 1152/1168\n",
      "Training Step: 684  | total loss: \u001b[1m\u001b[32m0.02661\u001b[0m\u001b[0m | time: 1.109s\n",
      "| SGD | epoch: 036 | loss: 0.02661 - R2: 0.9992 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 685  | total loss: \u001b[1m\u001b[32m0.02742\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 037 | loss: 0.02742 - R2: 0.9988 -- iter: 0064/1168\n",
      "Training Step: 686  | total loss: \u001b[1m\u001b[32m0.02742\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 037 | loss: 0.02742 - R2: 0.9988 -- iter: 0128/1168\n",
      "Training Step: 687  | total loss: \u001b[1m\u001b[32m0.02623\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 037 | loss: 0.02623 - R2: 0.9989 -- iter: 0192/1168\n",
      "Training Step: 688  | total loss: \u001b[1m\u001b[32m0.02577\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 037 | loss: 0.02577 - R2: 0.9983 -- iter: 0256/1168\n",
      "Training Step: 689  | total loss: \u001b[1m\u001b[32m0.02545\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 037 | loss: 0.02545 - R2: 0.9987 -- iter: 0320/1168\n",
      "Training Step: 690  | total loss: \u001b[1m\u001b[32m0.02440\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 037 | loss: 0.02440 - R2: 0.9988 -- iter: 0384/1168\n",
      "Training Step: 691  | total loss: \u001b[1m\u001b[32m0.02339\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 037 | loss: 0.02339 - R2: 0.9991 -- iter: 0448/1168\n",
      "Training Step: 692  | total loss: \u001b[1m\u001b[32m0.02280\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 037 | loss: 0.02280 - R2: 0.9992 -- iter: 0512/1168\n",
      "Training Step: 693  | total loss: \u001b[1m\u001b[32m0.02165\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 037 | loss: 0.02165 - R2: 0.9994 -- iter: 0576/1168\n",
      "Training Step: 694  | total loss: \u001b[1m\u001b[32m0.02171\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 037 | loss: 0.02171 - R2: 0.9996 -- iter: 0640/1168\n",
      "Training Step: 695  | total loss: \u001b[1m\u001b[32m0.02234\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 037 | loss: 0.02234 - R2: 0.9997 -- iter: 0704/1168\n",
      "Training Step: 696  | total loss: \u001b[1m\u001b[32m0.02190\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 037 | loss: 0.02190 - R2: 0.9995 -- iter: 0768/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 697  | total loss: \u001b[1m\u001b[32m0.02126\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 037 | loss: 0.02126 - R2: 0.9996 -- iter: 0832/1168\n",
      "Training Step: 698  | total loss: \u001b[1m\u001b[32m0.02069\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 037 | loss: 0.02069 - R2: 0.9997 -- iter: 0896/1168\n",
      "Training Step: 699  | total loss: \u001b[1m\u001b[32m0.02017\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 037 | loss: 0.02017 - R2: 0.9991 -- iter: 0960/1168\n",
      "Training Step: 700  | total loss: \u001b[1m\u001b[32m0.01970\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 037 | loss: 0.01970 - R2: 0.9984 -- iter: 1024/1168\n",
      "Training Step: 701  | total loss: \u001b[1m\u001b[32m0.01970\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 037 | loss: 0.01970 - R2: 0.9984 -- iter: 1088/1168\n",
      "Training Step: 702  | total loss: \u001b[1m\u001b[32m0.02191\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 037 | loss: 0.02191 - R2: 0.9987 -- iter: 1152/1168\n",
      "Training Step: 703  | total loss: \u001b[1m\u001b[32m0.02186\u001b[0m\u001b[0m | time: 1.126s\n",
      "| SGD | epoch: 037 | loss: 0.02186 - R2: 0.9984 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 704  | total loss: \u001b[1m\u001b[32m0.02391\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 038 | loss: 0.02391 - R2: 0.9984 -- iter: 0064/1168\n",
      "Training Step: 705  | total loss: \u001b[1m\u001b[32m0.02414\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 038 | loss: 0.02414 - R2: 0.9980 -- iter: 0128/1168\n",
      "Training Step: 706  | total loss: \u001b[1m\u001b[32m0.02438\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 038 | loss: 0.02438 - R2: 0.9983 -- iter: 0192/1168\n",
      "Training Step: 707  | total loss: \u001b[1m\u001b[32m0.02319\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 038 | loss: 0.02319 - R2: 0.9986 -- iter: 0256/1168\n",
      "Training Step: 708  | total loss: \u001b[1m\u001b[32m0.05095\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 038 | loss: 0.05095 - R2: 0.9986 -- iter: 0320/1168\n",
      "Training Step: 709  | total loss: \u001b[1m\u001b[32m0.04781\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 038 | loss: 0.04781 - R2: 0.9992 -- iter: 0384/1168\n",
      "Training Step: 710  | total loss: \u001b[1m\u001b[32m0.04419\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 038 | loss: 0.04419 - R2: 0.9987 -- iter: 0448/1168\n",
      "Training Step: 711  | total loss: \u001b[1m\u001b[32m0.04226\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 038 | loss: 0.04226 - R2: 0.9984 -- iter: 0512/1168\n",
      "Training Step: 712  | total loss: \u001b[1m\u001b[32m0.04124\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 038 | loss: 0.04124 - R2: 0.9987 -- iter: 0576/1168\n",
      "Training Step: 713  | total loss: \u001b[1m\u001b[32m0.04222\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 038 | loss: 0.04222 - R2: 0.9989 -- iter: 0640/1168\n",
      "Training Step: 714  | total loss: \u001b[1m\u001b[32m0.03735\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 038 | loss: 0.03735 - R2: 0.9987 -- iter: 0704/1168\n",
      "Training Step: 715  | total loss: \u001b[1m\u001b[32m0.03735\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 038 | loss: 0.03735 - R2: 0.9987 -- iter: 0768/1168\n",
      "Training Step: 716  | total loss: \u001b[1m\u001b[32m0.03574\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 038 | loss: 0.03574 - R2: 0.9988 -- iter: 0832/1168\n",
      "Training Step: 717  | total loss: \u001b[1m\u001b[32m0.03285\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 038 | loss: 0.03285 - R2: 0.9985 -- iter: 0896/1168\n",
      "Training Step: 718  | total loss: \u001b[1m\u001b[32m0.03126\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 038 | loss: 0.03126 - R2: 0.9987 -- iter: 0960/1168\n",
      "Training Step: 719  | total loss: \u001b[1m\u001b[32m0.03126\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 038 | loss: 0.03126 - R2: 0.9987 -- iter: 1024/1168\n",
      "Training Step: 720  | total loss: \u001b[1m\u001b[32m0.02974\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 038 | loss: 0.02974 - R2: 0.9987 -- iter: 1088/1168\n",
      "Training Step: 721  | total loss: \u001b[1m\u001b[32m0.02837\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 038 | loss: 0.02837 - R2: 0.9987 -- iter: 1152/1168\n",
      "Training Step: 722  | total loss: \u001b[1m\u001b[32m0.02802\u001b[0m\u001b[0m | time: 1.134s\n",
      "| SGD | epoch: 038 | loss: 0.02802 - R2: 0.9990 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 723  | total loss: \u001b[1m\u001b[32m0.02696\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 039 | loss: 0.02696 - R2: 0.9993 -- iter: 0064/1168\n",
      "Training Step: 724  | total loss: \u001b[1m\u001b[32m0.02696\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 039 | loss: 0.02696 - R2: 0.9993 -- iter: 0128/1168\n",
      "Training Step: 725  | total loss: \u001b[1m\u001b[32m0.02625\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 039 | loss: 0.02625 - R2: 0.9997 -- iter: 0192/1168\n",
      "Training Step: 726  | total loss: \u001b[1m\u001b[32m0.02519\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 039 | loss: 0.02519 - R2: 0.9995 -- iter: 0256/1168\n",
      "Training Step: 727  | total loss: \u001b[1m\u001b[32m0.02381\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 039 | loss: 0.02381 - R2: 0.9995 -- iter: 0320/1168\n",
      "Training Step: 728  | total loss: \u001b[1m\u001b[32m0.05128\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 039 | loss: 0.05128 - R2: 0.9992 -- iter: 0384/1168\n",
      "Training Step: 729  | total loss: \u001b[1m\u001b[32m0.05020\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 039 | loss: 0.05020 - R2: 0.9994 -- iter: 0448/1168\n",
      "Training Step: 730  | total loss: \u001b[1m\u001b[32m0.04670\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 039 | loss: 0.04670 - R2: 0.9996 -- iter: 0512/1168\n",
      "Training Step: 731  | total loss: \u001b[1m\u001b[32m0.04435\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 039 | loss: 0.04435 - R2: 0.9993 -- iter: 0576/1168\n",
      "Training Step: 732  | total loss: \u001b[1m\u001b[32m0.03866\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 039 | loss: 0.03866 - R2: 0.9995 -- iter: 0640/1168\n",
      "Training Step: 733  | total loss: \u001b[1m\u001b[32m0.03688\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 039 | loss: 0.03688 - R2: 0.9992 -- iter: 0704/1168\n",
      "Training Step: 734  | total loss: \u001b[1m\u001b[32m0.03688\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 039 | loss: 0.03688 - R2: 0.9992 -- iter: 0768/1168\n",
      "Training Step: 735  | total loss: \u001b[1m\u001b[32m0.03590\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 039 | loss: 0.03590 - R2: 0.9995 -- iter: 0832/1168\n",
      "Training Step: 736  | total loss: \u001b[1m\u001b[32m0.03496\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 039 | loss: 0.03496 - R2: 0.9994 -- iter: 0896/1168\n",
      "Training Step: 737  | total loss: \u001b[1m\u001b[32m0.03259\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 039 | loss: 0.03259 - R2: 0.9996 -- iter: 0960/1168\n",
      "Training Step: 738  | total loss: \u001b[1m\u001b[32m0.03111\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 039 | loss: 0.03111 - R2: 0.9991 -- iter: 1024/1168\n",
      "Training Step: 739  | total loss: \u001b[1m\u001b[32m0.02821\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 039 | loss: 0.02821 - R2: 0.9988 -- iter: 1088/1168\n",
      "Training Step: 740  | total loss: \u001b[1m\u001b[32m0.02821\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 039 | loss: 0.02821 - R2: 0.9988 -- iter: 1152/1168\n",
      "Training Step: 741  | total loss: \u001b[1m\u001b[32m0.02678\u001b[0m\u001b[0m | time: 1.119s\n",
      "| SGD | epoch: 039 | loss: 0.02678 - R2: 0.9985 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 742  | total loss: \u001b[1m\u001b[32m0.02504\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 040 | loss: 0.02504 - R2: 0.9988 -- iter: 0064/1168\n",
      "Training Step: 743  | total loss: \u001b[1m\u001b[32m0.02800\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 040 | loss: 0.02800 - R2: 0.9996 -- iter: 0128/1168\n",
      "Training Step: 744  | total loss: \u001b[1m\u001b[32m0.02800\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 040 | loss: 0.02800 - R2: 0.9996 -- iter: 0192/1168\n",
      "Training Step: 745  | total loss: \u001b[1m\u001b[32m0.02659\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 040 | loss: 0.02659 - R2: 0.9995 -- iter: 0256/1168\n",
      "Training Step: 746  | total loss: \u001b[1m\u001b[32m0.02578\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 040 | loss: 0.02578 - R2: 0.9995 -- iter: 0320/1168\n",
      "Training Step: 747  | total loss: \u001b[1m\u001b[32m0.05368\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 040 | loss: 0.05368 - R2: 1.0004 -- iter: 0384/1168\n",
      "Training Step: 748  | total loss: \u001b[1m\u001b[32m0.05037\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 040 | loss: 0.05037 - R2: 1.0000 -- iter: 0448/1168\n",
      "Training Step: 749  | total loss: \u001b[1m\u001b[32m0.04793\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 040 | loss: 0.04793 - R2: 1.0005 -- iter: 0512/1168\n",
      "Training Step: 750  | total loss: \u001b[1m\u001b[32m0.04469\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 040 | loss: 0.04469 - R2: 1.0003 -- iter: 0576/1168\n",
      "Training Step: 751  | total loss: \u001b[1m\u001b[32m0.04469\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 040 | loss: 0.04469 - R2: 1.0003 -- iter: 0640/1168\n",
      "Training Step: 752  | total loss: \u001b[1m\u001b[32m0.04163\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 040 | loss: 0.04163 - R2: 1.0004 -- iter: 0704/1168\n",
      "Training Step: 753  | total loss: \u001b[1m\u001b[32m0.04163\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 040 | loss: 0.04163 - R2: 1.0007 -- iter: 0768/1168\n",
      "Training Step: 754  | total loss: \u001b[1m\u001b[32m0.03910\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 040 | loss: 0.03910 - R2: 1.0002 -- iter: 0832/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 755  | total loss: \u001b[1m\u001b[32m0.03699\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 040 | loss: 0.03699 - R2: 0.9997 -- iter: 0896/1168\n",
      "Training Step: 756  | total loss: \u001b[1m\u001b[32m0.03494\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 040 | loss: 0.03494 - R2: 1.0000 -- iter: 0960/1168\n",
      "Training Step: 757  | total loss: \u001b[1m\u001b[32m0.03156\u001b[0m\u001b[0m | time: 0.169s\n",
      "| SGD | epoch: 040 | loss: 0.03156 - R2: 0.9996 -- iter: 1024/1168\n",
      "Training Step: 758  | total loss: \u001b[1m\u001b[32m0.03156\u001b[0m\u001b[0m | time: 0.176s\n",
      "| SGD | epoch: 040 | loss: 0.03156 - R2: 0.9994 -- iter: 1088/1168\n",
      "Training Step: 759  | total loss: \u001b[1m\u001b[32m0.02998\u001b[0m\u001b[0m | time: 0.179s\n",
      "| SGD | epoch: 040 | loss: 0.02998 - R2: 0.9994 -- iter: 1152/1168\n",
      "Training Step: 760  | total loss: \u001b[1m\u001b[32m0.02859\u001b[0m\u001b[0m | time: 1.183s\n",
      "| SGD | epoch: 040 | loss: 0.02859 - R2: 0.9987 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 761  | total loss: \u001b[1m\u001b[32m0.02734\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 041 | loss: 0.02734 - R2: 0.9987 -- iter: 0064/1168\n",
      "Training Step: 762  | total loss: \u001b[1m\u001b[32m0.02661\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 041 | loss: 0.02661 - R2: 0.9988 -- iter: 0128/1168\n",
      "Training Step: 763  | total loss: \u001b[1m\u001b[32m0.02697\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 041 | loss: 0.02697 - R2: 0.9989 -- iter: 0192/1168\n",
      "Training Step: 764  | total loss: \u001b[1m\u001b[32m0.02579\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 041 | loss: 0.02579 - R2: 0.9992 -- iter: 0256/1168\n",
      "Training Step: 765  | total loss: \u001b[1m\u001b[32m0.02552\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 041 | loss: 0.02552 - R2: 0.9990 -- iter: 0320/1168\n",
      "Training Step: 766  | total loss: \u001b[1m\u001b[32m0.02477\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 041 | loss: 0.02477 - R2: 0.9990 -- iter: 0384/1168\n",
      "Training Step: 767  | total loss: \u001b[1m\u001b[32m0.02477\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 041 | loss: 0.02477 - R2: 0.9991 -- iter: 0448/1168\n",
      "Training Step: 768  | total loss: \u001b[1m\u001b[32m0.04833\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 041 | loss: 0.04833 - R2: 1.0002 -- iter: 0512/1168\n",
      "Training Step: 769  | total loss: \u001b[1m\u001b[32m0.04490\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 041 | loss: 0.04490 - R2: 1.0001 -- iter: 0576/1168\n",
      "Training Step: 770  | total loss: \u001b[1m\u001b[32m0.04490\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 041 | loss: 0.04490 - R2: 1.0001 -- iter: 0640/1168\n",
      "Training Step: 771  | total loss: \u001b[1m\u001b[32m0.04232\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 041 | loss: 0.04232 - R2: 0.9999 -- iter: 0704/1168\n",
      "Training Step: 772  | total loss: \u001b[1m\u001b[32m0.03768\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 041 | loss: 0.03768 - R2: 0.9997 -- iter: 0768/1168\n",
      "Training Step: 773  | total loss: \u001b[1m\u001b[32m0.03768\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 041 | loss: 0.03768 - R2: 0.9997 -- iter: 0832/1168\n",
      "Training Step: 774  | total loss: \u001b[1m\u001b[32m0.03705\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 041 | loss: 0.03705 - R2: 1.0000 -- iter: 0896/1168\n",
      "Training Step: 775  | total loss: \u001b[1m\u001b[32m0.03705\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 041 | loss: 0.03705 - R2: 1.0000 -- iter: 0960/1168\n",
      "Training Step: 776  | total loss: \u001b[1m\u001b[32m0.03566\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 041 | loss: 0.03566 - R2: 1.0001 -- iter: 1024/1168\n",
      "Training Step: 777  | total loss: \u001b[1m\u001b[32m0.03333\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 041 | loss: 0.03333 - R2: 0.9998 -- iter: 1088/1168\n",
      "Training Step: 778  | total loss: \u001b[1m\u001b[32m0.03205\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 041 | loss: 0.03205 - R2: 0.9995 -- iter: 1152/1168\n",
      "Training Step: 779  | total loss: \u001b[1m\u001b[32m0.02907\u001b[0m\u001b[0m | time: 1.124s\n",
      "| SGD | epoch: 041 | loss: 0.02907 - R2: 0.9994 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 780  | total loss: \u001b[1m\u001b[32m0.02907\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 042 | loss: 0.02907 - R2: 0.9996 -- iter: 0064/1168\n",
      "Training Step: 781  | total loss: \u001b[1m\u001b[32m0.02751\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 042 | loss: 0.02751 - R2: 0.9996 -- iter: 0128/1168\n",
      "Training Step: 782  | total loss: \u001b[1m\u001b[32m0.02754\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 042 | loss: 0.02754 - R2: 0.9998 -- iter: 0192/1168\n",
      "Training Step: 783  | total loss: \u001b[1m\u001b[32m0.02665\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 042 | loss: 0.02665 - R2: 0.9995 -- iter: 0256/1168\n",
      "Training Step: 784  | total loss: \u001b[1m\u001b[32m0.02745\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 042 | loss: 0.02745 - R2: 0.9998 -- iter: 0320/1168\n",
      "Training Step: 785  | total loss: \u001b[1m\u001b[32m0.02565\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 042 | loss: 0.02565 - R2: 0.9999 -- iter: 0384/1168\n",
      "Training Step: 786  | total loss: \u001b[1m\u001b[32m0.02698\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 042 | loss: 0.02698 - R2: 0.9999 -- iter: 0448/1168\n",
      "Training Step: 787  | total loss: \u001b[1m\u001b[32m0.02689\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 042 | loss: 0.02689 - R2: 0.9999 -- iter: 0512/1168\n",
      "Training Step: 788  | total loss: \u001b[1m\u001b[32m0.05208\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 042 | loss: 0.05208 - R2: 0.9993 -- iter: 0576/1168\n",
      "Training Step: 789  | total loss: \u001b[1m\u001b[32m0.04983\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 042 | loss: 0.04983 - R2: 0.9996 -- iter: 0640/1168\n",
      "Training Step: 790  | total loss: \u001b[1m\u001b[32m0.04683\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 042 | loss: 0.04683 - R2: 0.9993 -- iter: 0704/1168\n",
      "Training Step: 791  | total loss: \u001b[1m\u001b[32m0.04389\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 042 | loss: 0.04389 - R2: 0.9998 -- iter: 0768/1168\n",
      "Training Step: 792  | total loss: \u001b[1m\u001b[32m0.04188\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 042 | loss: 0.04188 - R2: 0.9997 -- iter: 0832/1168\n",
      "Training Step: 793  | total loss: \u001b[1m\u001b[32m0.03997\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 042 | loss: 0.03997 - R2: 0.9995 -- iter: 0896/1168\n",
      "Training Step: 794  | total loss: \u001b[1m\u001b[32m0.03618\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 042 | loss: 0.03618 - R2: 0.9996 -- iter: 0960/1168\n",
      "Training Step: 795  | total loss: \u001b[1m\u001b[32m0.03479\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 042 | loss: 0.03479 - R2: 0.9993 -- iter: 1024/1168\n",
      "Training Step: 796  | total loss: \u001b[1m\u001b[32m0.03479\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 042 | loss: 0.03479 - R2: 0.9993 -- iter: 1088/1168\n",
      "Training Step: 797  | total loss: \u001b[1m\u001b[32m0.03328\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 042 | loss: 0.03328 - R2: 0.9988 -- iter: 1152/1168\n",
      "Training Step: 798  | total loss: \u001b[1m\u001b[32m0.03154\u001b[0m\u001b[0m | time: 1.098s\n",
      "| SGD | epoch: 042 | loss: 0.03154 - R2: 0.9987 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 799  | total loss: \u001b[1m\u001b[32m0.03155\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 043 | loss: 0.03155 - R2: 0.9988 -- iter: 0064/1168\n",
      "Training Step: 800  | total loss: \u001b[1m\u001b[32m0.03155\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 043 | loss: 0.03155 - R2: 0.9988 -- iter: 0128/1168\n",
      "Training Step: 801  | total loss: \u001b[1m\u001b[32m0.03155\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 043 | loss: 0.03155 - R2: 0.9988 -- iter: 0192/1168\n",
      "Training Step: 802  | total loss: \u001b[1m\u001b[32m0.03021\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 043 | loss: 0.03021 - R2: 0.9996 -- iter: 0256/1168\n",
      "Training Step: 803  | total loss: \u001b[1m\u001b[32m0.02948\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 043 | loss: 0.02948 - R2: 0.9990 -- iter: 0320/1168\n",
      "Training Step: 804  | total loss: \u001b[1m\u001b[32m0.02948\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 043 | loss: 0.02948 - R2: 0.9990 -- iter: 0384/1168\n",
      "Training Step: 805  | total loss: \u001b[1m\u001b[32m0.02863\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 043 | loss: 0.02863 - R2: 0.9991 -- iter: 0448/1168\n",
      "Training Step: 806  | total loss: \u001b[1m\u001b[32m0.02794\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 043 | loss: 0.02794 - R2: 0.9990 -- iter: 0512/1168\n",
      "Training Step: 807  | total loss: \u001b[1m\u001b[32m0.02721\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 043 | loss: 0.02721 - R2: 0.9992 -- iter: 0576/1168\n",
      "Training Step: 808  | total loss: \u001b[1m\u001b[32m0.04302\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 043 | loss: 0.04302 - R2: 0.9990 -- iter: 0640/1168\n",
      "Training Step: 809  | total loss: \u001b[1m\u001b[32m0.04302\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 043 | loss: 0.04302 - R2: 0.9990 -- iter: 0704/1168\n",
      "Training Step: 810  | total loss: \u001b[1m\u001b[32m0.03827\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 043 | loss: 0.03827 - R2: 0.9987 -- iter: 0768/1168\n",
      "Training Step: 811  | total loss: \u001b[1m\u001b[32m0.03827\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 043 | loss: 0.03827 - R2: 0.9986 -- iter: 0832/1168\n",
      "Training Step: 812  | total loss: \u001b[1m\u001b[32m0.03664\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 043 | loss: 0.03664 - R2: 0.9986 -- iter: 0896/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 813  | total loss: \u001b[1m\u001b[32m0.03551\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 043 | loss: 0.03551 - R2: 0.9994 -- iter: 0960/1168\n",
      "Training Step: 814  | total loss: \u001b[1m\u001b[32m0.03380\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 043 | loss: 0.03380 - R2: 0.9991 -- iter: 1024/1168\n",
      "Training Step: 815  | total loss: \u001b[1m\u001b[32m0.03246\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 043 | loss: 0.03246 - R2: 0.9997 -- iter: 1088/1168\n",
      "Training Step: 816  | total loss: \u001b[1m\u001b[32m0.03070\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 043 | loss: 0.03070 - R2: 0.9998 -- iter: 1152/1168\n",
      "Training Step: 817  | total loss: \u001b[1m\u001b[32m0.02995\u001b[0m\u001b[0m | time: 1.167s\n",
      "| SGD | epoch: 043 | loss: 0.02995 - R2: 0.9994 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 818  | total loss: \u001b[1m\u001b[32m0.02995\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 044 | loss: 0.02995 - R2: 0.9994 -- iter: 0064/1168\n",
      "Training Step: 819  | total loss: \u001b[1m\u001b[32m0.02858\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 044 | loss: 0.02858 - R2: 0.9990 -- iter: 0128/1168\n",
      "Training Step: 820  | total loss: \u001b[1m\u001b[32m0.02674\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 044 | loss: 0.02674 - R2: 0.9994 -- iter: 0192/1168\n",
      "Training Step: 821  | total loss: \u001b[1m\u001b[32m0.02508\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 044 | loss: 0.02508 - R2: 0.9998 -- iter: 0256/1168\n",
      "Training Step: 822  | total loss: \u001b[1m\u001b[32m0.02406\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 044 | loss: 0.02406 - R2: 0.9998 -- iter: 0320/1168\n",
      "Training Step: 823  | total loss: \u001b[1m\u001b[32m0.02399\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 044 | loss: 0.02399 - R2: 0.9999 -- iter: 0384/1168\n",
      "Training Step: 824  | total loss: \u001b[1m\u001b[32m0.02347\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 044 | loss: 0.02347 - R2: 0.9993 -- iter: 0448/1168\n",
      "Training Step: 825  | total loss: \u001b[1m\u001b[32m0.02307\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 044 | loss: 0.02307 - R2: 0.9993 -- iter: 0512/1168\n",
      "Training Step: 826  | total loss: \u001b[1m\u001b[32m0.02307\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 044 | loss: 0.02307 - R2: 0.9993 -- iter: 0576/1168\n",
      "Training Step: 827  | total loss: \u001b[1m\u001b[32m0.02299\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 044 | loss: 0.02299 - R2: 0.9999 -- iter: 0640/1168\n",
      "Training Step: 828  | total loss: \u001b[1m\u001b[32m0.04789\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 044 | loss: 0.04789 - R2: 1.0008 -- iter: 0704/1168\n",
      "Training Step: 829  | total loss: \u001b[1m\u001b[32m0.04523\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 044 | loss: 0.04523 - R2: 1.0010 -- iter: 0768/1168\n",
      "Training Step: 830  | total loss: \u001b[1m\u001b[32m0.04229\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 044 | loss: 0.04229 - R2: 1.0009 -- iter: 0832/1168\n",
      "Training Step: 831  | total loss: \u001b[1m\u001b[32m0.03728\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 044 | loss: 0.03728 - R2: 1.0004 -- iter: 0896/1168\n",
      "Training Step: 832  | total loss: \u001b[1m\u001b[32m0.03728\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 044 | loss: 0.03728 - R2: 1.0004 -- iter: 0960/1168\n",
      "Training Step: 833  | total loss: \u001b[1m\u001b[32m0.03517\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 044 | loss: 0.03517 - R2: 1.0003 -- iter: 1024/1168\n",
      "Training Step: 834  | total loss: \u001b[1m\u001b[32m0.03317\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 044 | loss: 0.03317 - R2: 1.0002 -- iter: 1088/1168\n",
      "Training Step: 835  | total loss: \u001b[1m\u001b[32m0.03401\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 044 | loss: 0.03401 - R2: 1.0003 -- iter: 1152/1168\n",
      "Training Step: 836  | total loss: \u001b[1m\u001b[32m0.03335\u001b[0m\u001b[0m | time: 1.145s\n",
      "| SGD | epoch: 044 | loss: 0.03335 - R2: 1.0001 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 837  | total loss: \u001b[1m\u001b[32m0.03182\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 045 | loss: 0.03182 - R2: 0.9995 -- iter: 0064/1168\n",
      "Training Step: 838  | total loss: \u001b[1m\u001b[32m0.03051\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 045 | loss: 0.03051 - R2: 0.9997 -- iter: 0128/1168\n",
      "Training Step: 839  | total loss: \u001b[1m\u001b[32m0.02925\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 045 | loss: 0.02925 - R2: 0.9993 -- iter: 0192/1168\n",
      "Training Step: 840  | total loss: \u001b[1m\u001b[32m0.02768\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 045 | loss: 0.02768 - R2: 0.9994 -- iter: 0256/1168\n",
      "Training Step: 841  | total loss: \u001b[1m\u001b[32m0.02633\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 045 | loss: 0.02633 - R2: 0.9992 -- iter: 0320/1168\n",
      "Training Step: 842  | total loss: \u001b[1m\u001b[32m0.02633\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 045 | loss: 0.02633 - R2: 0.9992 -- iter: 0384/1168\n",
      "Training Step: 843  | total loss: \u001b[1m\u001b[32m0.02613\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 045 | loss: 0.02613 - R2: 0.9992 -- iter: 0448/1168\n",
      "Training Step: 844  | total loss: \u001b[1m\u001b[32m0.02588\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 045 | loss: 0.02588 - R2: 1.0000 -- iter: 0512/1168\n",
      "Training Step: 845  | total loss: \u001b[1m\u001b[32m0.02588\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 045 | loss: 0.02588 - R2: 0.9999 -- iter: 0576/1168\n",
      "Training Step: 846  | total loss: \u001b[1m\u001b[32m0.02474\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 045 | loss: 0.02474 - R2: 0.9998 -- iter: 0640/1168\n",
      "Training Step: 847  | total loss: \u001b[1m\u001b[32m0.02607\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 045 | loss: 0.02607 - R2: 0.9998 -- iter: 0704/1168\n",
      "Training Step: 848  | total loss: \u001b[1m\u001b[32m0.02549\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 045 | loss: 0.02549 - R2: 1.0001 -- iter: 0768/1168\n",
      "Training Step: 849  | total loss: \u001b[1m\u001b[32m0.02650\u001b[0m\u001b[0m | time: 0.169s\n",
      "| SGD | epoch: 045 | loss: 0.02650 - R2: 1.0005 -- iter: 0832/1168\n",
      "Training Step: 850  | total loss: \u001b[1m\u001b[32m0.02524\u001b[0m\u001b[0m | time: 0.171s\n",
      "| SGD | epoch: 045 | loss: 0.02524 - R2: 1.0005 -- iter: 0896/1168\n",
      "Training Step: 851  | total loss: \u001b[1m\u001b[32m0.02664\u001b[0m\u001b[0m | time: 0.176s\n",
      "| SGD | epoch: 045 | loss: 0.02664 - R2: 1.0005 -- iter: 0960/1168\n",
      "Training Step: 852  | total loss: \u001b[1m\u001b[32m0.02545\u001b[0m\u001b[0m | time: 0.179s\n",
      "| SGD | epoch: 045 | loss: 0.02545 - R2: 1.0000 -- iter: 1024/1168\n",
      "Training Step: 853  | total loss: \u001b[1m\u001b[32m0.02515\u001b[0m\u001b[0m | time: 0.181s\n",
      "| SGD | epoch: 045 | loss: 0.02515 - R2: 0.9996 -- iter: 1088/1168\n",
      "Training Step: 854  | total loss: \u001b[1m\u001b[32m0.02487\u001b[0m\u001b[0m | time: 0.183s\n",
      "| SGD | epoch: 045 | loss: 0.02487 - R2: 0.9995 -- iter: 1152/1168\n",
      "Training Step: 855  | total loss: \u001b[1m\u001b[32m0.02462\u001b[0m\u001b[0m | time: 1.196s\n",
      "| SGD | epoch: 045 | loss: 0.02462 - R2: 0.9999 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 856  | total loss: \u001b[1m\u001b[32m0.02371\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 046 | loss: 0.02371 - R2: 0.9999 -- iter: 0064/1168\n",
      "Training Step: 857  | total loss: \u001b[1m\u001b[32m0.02415\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 046 | loss: 0.02415 - R2: 1.0001 -- iter: 0128/1168\n",
      "Training Step: 858  | total loss: \u001b[1m\u001b[32m0.02268\u001b[0m\u001b[0m | time: 0.007s\n",
      "| SGD | epoch: 046 | loss: 0.02268 - R2: 1.0000 -- iter: 0192/1168\n",
      "Training Step: 859  | total loss: \u001b[1m\u001b[32m0.02268\u001b[0m\u001b[0m | time: 0.010s\n",
      "| SGD | epoch: 046 | loss: 0.02268 - R2: 1.0000 -- iter: 0256/1168\n",
      "Training Step: 860  | total loss: \u001b[1m\u001b[32m0.02167\u001b[0m\u001b[0m | time: 0.013s\n",
      "| SGD | epoch: 046 | loss: 0.02167 - R2: 0.9988 -- iter: 0320/1168\n",
      "Training Step: 861  | total loss: \u001b[1m\u001b[32m0.02167\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 046 | loss: 0.02167 - R2: 0.9988 -- iter: 0384/1168\n",
      "Training Step: 862  | total loss: \u001b[1m\u001b[32m0.02148\u001b[0m\u001b[0m | time: 0.017s\n",
      "| SGD | epoch: 046 | loss: 0.02148 - R2: 0.9989 -- iter: 0448/1168\n",
      "Training Step: 863  | total loss: \u001b[1m\u001b[32m0.02115\u001b[0m\u001b[0m | time: 0.019s\n",
      "| SGD | epoch: 046 | loss: 0.02115 - R2: 0.9989 -- iter: 0512/1168\n",
      "Training Step: 864  | total loss: \u001b[1m\u001b[32m0.02245\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 046 | loss: 0.02245 - R2: 0.9988 -- iter: 0576/1168\n",
      "Training Step: 865  | total loss: \u001b[1m\u001b[32m0.02297\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 046 | loss: 0.02297 - R2: 0.9987 -- iter: 0640/1168\n",
      "Training Step: 866  | total loss: \u001b[1m\u001b[32m0.02209\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 046 | loss: 0.02209 - R2: 0.9985 -- iter: 0704/1168\n",
      "Training Step: 867  | total loss: \u001b[1m\u001b[32m0.05825\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 046 | loss: 0.05825 - R2: 0.9984 -- iter: 0768/1168\n",
      "Training Step: 868  | total loss: \u001b[1m\u001b[32m0.05825\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 046 | loss: 0.05825 - R2: 0.9984 -- iter: 0832/1168\n",
      "Training Step: 869  | total loss: \u001b[1m\u001b[32m0.05763\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 046 | loss: 0.05763 - R2: 0.9990 -- iter: 0896/1168\n",
      "Training Step: 870  | total loss: \u001b[1m\u001b[32m0.05348\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 046 | loss: 0.05348 - R2: 0.9995 -- iter: 0960/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 871  | total loss: \u001b[1m\u001b[32m0.05010\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 046 | loss: 0.05010 - R2: 0.9992 -- iter: 1024/1168\n",
      "Training Step: 872  | total loss: \u001b[1m\u001b[32m0.04650\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 046 | loss: 0.04650 - R2: 0.9989 -- iter: 1088/1168\n",
      "Training Step: 873  | total loss: \u001b[1m\u001b[32m0.04417\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 046 | loss: 0.04417 - R2: 0.9993 -- iter: 1152/1168\n",
      "Training Step: 874  | total loss: \u001b[1m\u001b[32m0.04012\u001b[0m\u001b[0m | time: 1.057s\n",
      "| SGD | epoch: 046 | loss: 0.04012 - R2: 0.9989 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 875  | total loss: \u001b[1m\u001b[32m0.03852\u001b[0m\u001b[0m | time: 0.007s\n",
      "| SGD | epoch: 047 | loss: 0.03852 - R2: 0.9987 -- iter: 0064/1168\n",
      "Training Step: 876  | total loss: \u001b[1m\u001b[32m0.03852\u001b[0m\u001b[0m | time: 0.011s\n",
      "| SGD | epoch: 047 | loss: 0.03852 - R2: 0.9987 -- iter: 0128/1168\n",
      "Training Step: 877  | total loss: \u001b[1m\u001b[32m0.03571\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 047 | loss: 0.03571 - R2: 0.9985 -- iter: 0192/1168\n",
      "Training Step: 878  | total loss: \u001b[1m\u001b[32m0.03421\u001b[0m\u001b[0m | time: 0.018s\n",
      "| SGD | epoch: 047 | loss: 0.03421 - R2: 0.9989 -- iter: 0256/1168\n",
      "Training Step: 879  | total loss: \u001b[1m\u001b[32m0.03232\u001b[0m\u001b[0m | time: 0.020s\n",
      "| SGD | epoch: 047 | loss: 0.03232 - R2: 0.9991 -- iter: 0320/1168\n",
      "Training Step: 880  | total loss: \u001b[1m\u001b[32m0.03528\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 047 | loss: 0.03528 - R2: 0.9999 -- iter: 0384/1168\n",
      "Training Step: 881  | total loss: \u001b[1m\u001b[32m0.03559\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 047 | loss: 0.03559 - R2: 1.0003 -- iter: 0448/1168\n",
      "Training Step: 882  | total loss: \u001b[1m\u001b[32m0.03440\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 047 | loss: 0.03440 - R2: 1.0006 -- iter: 0512/1168\n",
      "Training Step: 883  | total loss: \u001b[1m\u001b[32m0.03440\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 047 | loss: 0.03440 - R2: 1.0006 -- iter: 0576/1168\n",
      "Training Step: 884  | total loss: \u001b[1m\u001b[32m0.03238\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 047 | loss: 0.03238 - R2: 1.0005 -- iter: 0640/1168\n",
      "Training Step: 885  | total loss: \u001b[1m\u001b[32m0.03057\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 047 | loss: 0.03057 - R2: 1.0004 -- iter: 0704/1168\n",
      "Training Step: 886  | total loss: \u001b[1m\u001b[32m0.03009\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 047 | loss: 0.03009 - R2: 1.0001 -- iter: 0768/1168\n",
      "Training Step: 887  | total loss: \u001b[1m\u001b[32m0.03009\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 047 | loss: 0.03009 - R2: 1.0010 -- iter: 0832/1168\n",
      "Training Step: 888  | total loss: \u001b[1m\u001b[32m0.05077\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 047 | loss: 0.05077 - R2: 1.0013 -- iter: 0896/1168\n",
      "Training Step: 889  | total loss: \u001b[1m\u001b[32m0.05077\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 047 | loss: 0.05077 - R2: 1.0010 -- iter: 0960/1168\n",
      "Training Step: 890  | total loss: \u001b[1m\u001b[32m0.04757\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 047 | loss: 0.04757 - R2: 1.0010 -- iter: 1024/1168\n",
      "Training Step: 891  | total loss: \u001b[1m\u001b[32m0.04135\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 047 | loss: 0.04135 - R2: 1.0007 -- iter: 1088/1168\n",
      "Training Step: 892  | total loss: \u001b[1m\u001b[32m0.04135\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 047 | loss: 0.04135 - R2: 1.0007 -- iter: 1152/1168\n",
      "Training Step: 893  | total loss: \u001b[1m\u001b[32m0.03825\u001b[0m\u001b[0m | time: 1.079s\n",
      "| SGD | epoch: 047 | loss: 0.03825 - R2: 1.0009 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 894  | total loss: \u001b[1m\u001b[32m0.03639\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 048 | loss: 0.03639 - R2: 1.0009 -- iter: 0064/1168\n",
      "Training Step: 895  | total loss: \u001b[1m\u001b[32m0.03380\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 048 | loss: 0.03380 - R2: 1.0009 -- iter: 0128/1168\n",
      "Training Step: 896  | total loss: \u001b[1m\u001b[32m0.03030\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 048 | loss: 0.03030 - R2: 1.0002 -- iter: 0192/1168\n",
      "Training Step: 897  | total loss: \u001b[1m\u001b[32m0.03046\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 048 | loss: 0.03046 - R2: 0.9995 -- iter: 0256/1168\n",
      "Training Step: 898  | total loss: \u001b[1m\u001b[32m0.03218\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 048 | loss: 0.03218 - R2: 0.9995 -- iter: 0320/1168\n",
      "Training Step: 899  | total loss: \u001b[1m\u001b[32m0.03218\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 048 | loss: 0.03218 - R2: 0.9995 -- iter: 0384/1168\n",
      "Training Step: 900  | total loss: \u001b[1m\u001b[32m0.03001\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 048 | loss: 0.03001 - R2: 0.9995 -- iter: 0448/1168\n",
      "Training Step: 901  | total loss: \u001b[1m\u001b[32m0.02805\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 048 | loss: 0.02805 - R2: 0.9995 -- iter: 0512/1168\n",
      "Training Step: 902  | total loss: \u001b[1m\u001b[32m0.02685\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 048 | loss: 0.02685 - R2: 0.9999 -- iter: 0576/1168\n",
      "Training Step: 903  | total loss: \u001b[1m\u001b[32m0.02599\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 048 | loss: 0.02599 - R2: 0.9995 -- iter: 0640/1168\n",
      "Training Step: 904  | total loss: \u001b[1m\u001b[32m0.02569\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 048 | loss: 0.02569 - R2: 0.9995 -- iter: 0704/1168\n",
      "Training Step: 905  | total loss: \u001b[1m\u001b[32m0.02569\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 048 | loss: 0.02569 - R2: 0.9995 -- iter: 0768/1168\n",
      "Training Step: 906  | total loss: \u001b[1m\u001b[32m0.02555\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 048 | loss: 0.02555 - R2: 0.9996 -- iter: 0832/1168\n",
      "Training Step: 907  | total loss: \u001b[1m\u001b[32m0.02557\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 048 | loss: 0.02557 - R2: 0.9993 -- iter: 0896/1168\n",
      "Training Step: 908  | total loss: \u001b[1m\u001b[32m0.04689\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 048 | loss: 0.04689 - R2: 0.9984 -- iter: 0960/1168\n",
      "Training Step: 909  | total loss: \u001b[1m\u001b[32m0.04689\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 048 | loss: 0.04689 - R2: 0.9984 -- iter: 1024/1168\n",
      "Training Step: 910  | total loss: \u001b[1m\u001b[32m0.04132\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 048 | loss: 0.04132 - R2: 0.9983 -- iter: 1088/1168\n",
      "Training Step: 911  | total loss: \u001b[1m\u001b[32m0.03922\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 048 | loss: 0.03922 - R2: 0.9987 -- iter: 1152/1168\n",
      "Training Step: 912  | total loss: \u001b[1m\u001b[32m0.03978\u001b[0m\u001b[0m | time: 1.169s\n",
      "| SGD | epoch: 048 | loss: 0.03978 - R2: 0.9988 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 913  | total loss: \u001b[1m\u001b[32m0.03978\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 049 | loss: 0.03978 - R2: 0.9988 -- iter: 0064/1168\n",
      "Training Step: 914  | total loss: \u001b[1m\u001b[32m0.03603\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 049 | loss: 0.03603 - R2: 0.9991 -- iter: 0128/1168\n",
      "Training Step: 915  | total loss: \u001b[1m\u001b[32m0.03603\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 049 | loss: 0.03603 - R2: 0.9991 -- iter: 0192/1168\n",
      "Training Step: 916  | total loss: \u001b[1m\u001b[32m0.03341\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 049 | loss: 0.03341 - R2: 0.9992 -- iter: 0256/1168\n",
      "Training Step: 917  | total loss: \u001b[1m\u001b[32m0.03203\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 049 | loss: 0.03203 - R2: 0.9990 -- iter: 0320/1168\n",
      "Training Step: 918  | total loss: \u001b[1m\u001b[32m0.03203\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 049 | loss: 0.03203 - R2: 0.9990 -- iter: 0384/1168\n",
      "Training Step: 919  | total loss: \u001b[1m\u001b[32m0.02909\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 049 | loss: 0.02909 - R2: 0.9991 -- iter: 0448/1168\n",
      "Training Step: 920  | total loss: \u001b[1m\u001b[32m0.02748\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 049 | loss: 0.02748 - R2: 0.9992 -- iter: 0512/1168\n",
      "Training Step: 921  | total loss: \u001b[1m\u001b[32m0.02652\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 049 | loss: 0.02652 - R2: 0.9991 -- iter: 0576/1168\n",
      "Training Step: 922  | total loss: \u001b[1m\u001b[32m0.02586\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 049 | loss: 0.02586 - R2: 0.9992 -- iter: 0640/1168\n",
      "Training Step: 923  | total loss: \u001b[1m\u001b[32m0.02530\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 049 | loss: 0.02530 - R2: 0.9994 -- iter: 0704/1168\n",
      "Training Step: 924  | total loss: \u001b[1m\u001b[32m0.02514\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 049 | loss: 0.02514 - R2: 0.9997 -- iter: 0768/1168\n",
      "Training Step: 925  | total loss: \u001b[1m\u001b[32m0.02472\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 049 | loss: 0.02472 - R2: 0.9996 -- iter: 0832/1168\n",
      "Training Step: 926  | total loss: \u001b[1m\u001b[32m0.02492\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 049 | loss: 0.02492 - R2: 0.9994 -- iter: 0896/1168\n",
      "Training Step: 927  | total loss: \u001b[1m\u001b[32m0.02492\u001b[0m\u001b[0m | time: 0.169s\n",
      "| SGD | epoch: 049 | loss: 0.02492 - R2: 0.9994 -- iter: 0960/1168\n",
      "Training Step: 928  | total loss: \u001b[1m\u001b[32m0.06204\u001b[0m\u001b[0m | time: 0.182s\n",
      "| SGD | epoch: 049 | loss: 0.06204 - R2: 0.9993 -- iter: 1024/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 929  | total loss: \u001b[1m\u001b[32m0.05765\u001b[0m\u001b[0m | time: 0.184s\n",
      "| SGD | epoch: 049 | loss: 0.05765 - R2: 0.9993 -- iter: 1088/1168\n",
      "Training Step: 930  | total loss: \u001b[1m\u001b[32m0.05336\u001b[0m\u001b[0m | time: 0.187s\n",
      "| SGD | epoch: 049 | loss: 0.05336 - R2: 0.9993 -- iter: 1152/1168\n",
      "Training Step: 931  | total loss: \u001b[1m\u001b[32m0.04881\u001b[0m\u001b[0m | time: 1.193s\n",
      "| SGD | epoch: 049 | loss: 0.04881 - R2: 0.9997 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 932  | total loss: \u001b[1m\u001b[32m0.04881\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 050 | loss: 0.04881 - R2: 0.9997 -- iter: 0064/1168\n",
      "Training Step: 933  | total loss: \u001b[1m\u001b[32m0.04506\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 050 | loss: 0.04506 - R2: 0.9999 -- iter: 0128/1168\n",
      "Training Step: 934  | total loss: \u001b[1m\u001b[32m0.04248\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 050 | loss: 0.04248 - R2: 0.9997 -- iter: 0192/1168\n",
      "Training Step: 935  | total loss: \u001b[1m\u001b[32m0.04020\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 050 | loss: 0.04020 - R2: 0.9995 -- iter: 0256/1168\n",
      "Training Step: 936  | total loss: \u001b[1m\u001b[32m0.03790\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 050 | loss: 0.03790 - R2: 0.9996 -- iter: 0320/1168\n",
      "Training Step: 937  | total loss: \u001b[1m\u001b[32m0.03654\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 050 | loss: 0.03654 - R2: 0.9994 -- iter: 0384/1168\n",
      "Training Step: 938  | total loss: \u001b[1m\u001b[32m0.03496\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 050 | loss: 0.03496 - R2: 0.9999 -- iter: 0448/1168\n",
      "Training Step: 939  | total loss: \u001b[1m\u001b[32m0.03496\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 050 | loss: 0.03496 - R2: 0.9999 -- iter: 0512/1168\n",
      "Training Step: 940  | total loss: \u001b[1m\u001b[32m0.03243\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 050 | loss: 0.03243 - R2: 0.9996 -- iter: 0576/1168\n",
      "Training Step: 941  | total loss: \u001b[1m\u001b[32m0.03015\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 050 | loss: 0.03015 - R2: 0.9994 -- iter: 0640/1168\n",
      "Training Step: 942  | total loss: \u001b[1m\u001b[32m0.02913\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 050 | loss: 0.02913 - R2: 0.9995 -- iter: 0704/1168\n",
      "Training Step: 943  | total loss: \u001b[1m\u001b[32m0.02825\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 050 | loss: 0.02825 - R2: 0.9993 -- iter: 0768/1168\n",
      "Training Step: 944  | total loss: \u001b[1m\u001b[32m0.02665\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 050 | loss: 0.02665 - R2: 0.9992 -- iter: 0832/1168\n",
      "Training Step: 945  | total loss: \u001b[1m\u001b[32m0.02606\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 050 | loss: 0.02606 - R2: 0.9991 -- iter: 0896/1168\n",
      "Training Step: 946  | total loss: \u001b[1m\u001b[32m0.02601\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 050 | loss: 0.02601 - R2: 0.9993 -- iter: 0960/1168\n",
      "Training Step: 947  | total loss: \u001b[1m\u001b[32m0.05625\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 050 | loss: 0.05625 - R2: 0.9992 -- iter: 1024/1168\n",
      "Training Step: 948  | total loss: \u001b[1m\u001b[32m0.05625\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 050 | loss: 0.05625 - R2: 0.9992 -- iter: 1088/1168\n",
      "Training Step: 949  | total loss: \u001b[1m\u001b[32m0.05218\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 050 | loss: 0.05218 - R2: 0.9992 -- iter: 1152/1168\n",
      "Training Step: 950  | total loss: \u001b[1m\u001b[32m0.05265\u001b[0m\u001b[0m | time: 1.161s\n",
      "| SGD | epoch: 050 | loss: 0.05265 - R2: 0.9995 | val_loss: 0.01886 - val_acc: 0.9974 -- iter: 1168/1168\n",
      "--\n",
      "---------------------------------\n",
      "Run id: JXOIMX\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "INFO:tensorflow:Summary name StandardError/ (raw) is illegal; using StandardError/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 1168\n",
      "Validation samples: 292\n",
      "--\n",
      "Training Step: 1  | total loss: \u001b[1m\u001b[32m129.17216\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 001 | loss: 129.17216 - R2: 0.0000 -- iter: 0064/1168\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m129.17216\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 001 | loss: 129.17216 - R2: 0.0000 -- iter: 0128/1168\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m125.56593\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 001 | loss: 125.56593 - R2: 0.0053 -- iter: 0192/1168\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m125.56593\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 001 | loss: 125.56593 - R2: 0.0053 -- iter: 0256/1168\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m116.53680\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 001 | loss: 116.53680 - R2: 0.0115 -- iter: 0320/1168\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m108.74973\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 001 | loss: 108.74973 - R2: 0.0197 -- iter: 0384/1168\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m100.45542\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 001 | loss: 100.45542 - R2: 0.0299 -- iter: 0448/1168\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m92.50266\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 001 | loss: 92.50266 - R2: 0.0422 -- iter: 0512/1168\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m85.90976\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 001 | loss: 85.90976 - R2: 0.0558 -- iter: 0576/1168\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m78.32690\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 001 | loss: 78.32690 - R2: 0.0726 -- iter: 0640/1168\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m71.42880\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 001 | loss: 71.42880 - R2: 0.0916 -- iter: 0704/1168\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m64.30135\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 001 | loss: 64.30135 - R2: 0.1406 -- iter: 0768/1168\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m57.66155\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 001 | loss: 57.66155 - R2: 0.1406 -- iter: 0832/1168\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m44.13796\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 001 | loss: 44.13796 - R2: 0.2091 -- iter: 0896/1168\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m44.13796\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 001 | loss: 44.13796 - R2: 0.2091 -- iter: 0960/1168\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m37.50828\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 001 | loss: 37.50828 - R2: 0.3008 -- iter: 1024/1168\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m31.48417\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 001 | loss: 31.48417 - R2: 0.3008 -- iter: 1088/1168\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m25.75759\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 001 | loss: 25.75759 - R2: 0.3563 -- iter: 1152/1168\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m20.36010\u001b[0m\u001b[0m | time: 1.106s\n",
      "| SGD | epoch: 001 | loss: 20.36010 - R2: 0.4216 | val_loss: 3.02367 - val_acc: 0.7380 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m15.58181\u001b[0m\u001b[0m | time: 0.010s\n",
      "| SGD | epoch: 002 | loss: 15.58181 - R2: 0.4945 -- iter: 0064/1168\n",
      "Training Step: 21  | total loss: \u001b[1m\u001b[32m11.47023\u001b[0m\u001b[0m | time: 0.020s\n",
      "| SGD | epoch: 002 | loss: 11.47023 - R2: 0.5788 -- iter: 0128/1168\n",
      "Training Step: 22  | total loss: \u001b[1m\u001b[32m8.27942\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 002 | loss: 8.27942 - R2: 0.6649 -- iter: 0192/1168\n",
      "Training Step: 23  | total loss: \u001b[1m\u001b[32m5.94712\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 002 | loss: 5.94712 - R2: 0.7566 -- iter: 0256/1168\n",
      "Training Step: 24  | total loss: \u001b[1m\u001b[32m4.29629\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 002 | loss: 4.29629 - R2: 0.8272 -- iter: 0320/1168\n",
      "Training Step: 25  | total loss: \u001b[1m\u001b[32m3.15550\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 002 | loss: 3.15550 - R2: 0.8754 -- iter: 0384/1168\n",
      "Training Step: 26  | total loss: \u001b[1m\u001b[32m2.34301\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 002 | loss: 2.34301 - R2: 0.9058 -- iter: 0448/1168\n",
      "Training Step: 27  | total loss: \u001b[1m\u001b[32m1.76666\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 002 | loss: 1.76666 - R2: 0.9325 -- iter: 0512/1168\n",
      "Training Step: 28  | total loss: \u001b[1m\u001b[32m1.33222\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 002 | loss: 1.33222 - R2: 0.9480 -- iter: 0576/1168\n",
      "Training Step: 29  | total loss: \u001b[1m\u001b[32m1.02001\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 002 | loss: 1.02001 - R2: 0.9593 -- iter: 0640/1168\n",
      "Training Step: 30  | total loss: \u001b[1m\u001b[32m0.78713\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 002 | loss: 0.78713 - R2: 0.9708 -- iter: 0704/1168\n",
      "Training Step: 31  | total loss: \u001b[1m\u001b[32m0.61573\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 002 | loss: 0.61573 - R2: 0.9779 -- iter: 0768/1168\n",
      "Training Step: 32  | total loss: \u001b[1m\u001b[32m0.48452\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 002 | loss: 0.48452 - R2: 0.9829 -- iter: 0832/1168\n",
      "Training Step: 33  | total loss: \u001b[1m\u001b[32m0.38338\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 002 | loss: 0.38338 - R2: 0.9855 -- iter: 0896/1168\n",
      "Training Step: 34  | total loss: \u001b[1m\u001b[32m0.30991\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 002 | loss: 0.30991 - R2: 0.9902 -- iter: 0960/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 35  | total loss: \u001b[1m\u001b[32m0.25084\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 002 | loss: 0.25084 - R2: 0.9912 -- iter: 1024/1168\n",
      "Training Step: 36  | total loss: \u001b[1m\u001b[32m0.20783\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 002 | loss: 0.20783 - R2: 0.9934 -- iter: 1088/1168\n",
      "Training Step: 37  | total loss: \u001b[1m\u001b[32m0.17345\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 002 | loss: 0.17345 - R2: 0.9955 -- iter: 1152/1168\n",
      "Training Step: 38  | total loss: \u001b[1m\u001b[32m0.14962\u001b[0m\u001b[0m | time: 1.107s\n",
      "| SGD | epoch: 002 | loss: 0.14962 - R2: 0.9958 | val_loss: 0.02435 - val_acc: 0.9919 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 39  | total loss: \u001b[1m\u001b[32m0.12463\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 003 | loss: 0.12463 - R2: 0.9971 -- iter: 0064/1168\n",
      "Training Step: 40  | total loss: \u001b[1m\u001b[32m0.10835\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 003 | loss: 0.10835 - R2: 0.9988 -- iter: 0128/1168\n",
      "Training Step: 41  | total loss: \u001b[1m\u001b[32m0.09314\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 003 | loss: 0.09314 - R2: 0.9994 -- iter: 0192/1168\n",
      "Training Step: 42  | total loss: \u001b[1m\u001b[32m0.07991\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 003 | loss: 0.07991 - R2: 0.9977 -- iter: 0256/1168\n",
      "Training Step: 43  | total loss: \u001b[1m\u001b[32m0.06891\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 003 | loss: 0.06891 - R2: 0.9975 -- iter: 0320/1168\n",
      "Training Step: 44  | total loss: \u001b[1m\u001b[32m0.06288\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 003 | loss: 0.06288 - R2: 0.9974 -- iter: 0384/1168\n",
      "Training Step: 45  | total loss: \u001b[1m\u001b[32m0.05443\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 003 | loss: 0.05443 - R2: 0.9985 -- iter: 0448/1168\n",
      "Training Step: 46  | total loss: \u001b[1m\u001b[32m0.04926\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 003 | loss: 0.04926 - R2: 0.9986 -- iter: 0512/1168\n",
      "Training Step: 47  | total loss: \u001b[1m\u001b[32m0.05118\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 003 | loss: 0.05118 - R2: 0.9987 -- iter: 0576/1168\n",
      "Training Step: 48  | total loss: \u001b[1m\u001b[32m0.04602\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 003 | loss: 0.04602 - R2: 0.9984 -- iter: 0640/1168\n",
      "Training Step: 49  | total loss: \u001b[1m\u001b[32m0.04455\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 003 | loss: 0.04455 - R2: 0.9995 -- iter: 0704/1168\n",
      "Training Step: 50  | total loss: \u001b[1m\u001b[32m0.04390\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 003 | loss: 0.04390 - R2: 1.0008 -- iter: 0768/1168\n",
      "Training Step: 51  | total loss: \u001b[1m\u001b[32m0.03931\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 003 | loss: 0.03931 - R2: 0.9999 -- iter: 0832/1168\n",
      "Training Step: 52  | total loss: \u001b[1m\u001b[32m0.03953\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 003 | loss: 0.03953 - R2: 0.9995 -- iter: 0896/1168\n",
      "Training Step: 53  | total loss: \u001b[1m\u001b[32m0.03687\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 003 | loss: 0.03687 - R2: 0.9981 -- iter: 0960/1168\n",
      "Training Step: 54  | total loss: \u001b[1m\u001b[32m0.03565\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 003 | loss: 0.03565 - R2: 0.9982 -- iter: 1024/1168\n",
      "Training Step: 55  | total loss: \u001b[1m\u001b[32m0.03352\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 003 | loss: 0.03352 - R2: 0.9990 -- iter: 1088/1168\n",
      "Training Step: 56  | total loss: \u001b[1m\u001b[32m0.03168\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 003 | loss: 0.03168 - R2: 0.9990 -- iter: 1152/1168\n",
      "Training Step: 57  | total loss: \u001b[1m\u001b[32m0.03016\u001b[0m\u001b[0m | time: 1.106s\n",
      "| SGD | epoch: 003 | loss: 0.03016 - R2: 0.9989 | val_loss: 0.01837 - val_acc: 0.9961 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 58  | total loss: \u001b[1m\u001b[32m0.02695\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 004 | loss: 0.02695 - R2: 0.9998 -- iter: 0064/1168\n",
      "Training Step: 59  | total loss: \u001b[1m\u001b[32m0.02695\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 004 | loss: 0.02695 - R2: 0.9998 -- iter: 0128/1168\n",
      "Training Step: 60  | total loss: \u001b[1m\u001b[32m0.02465\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 004 | loss: 0.02465 - R2: 0.9994 -- iter: 0192/1168\n",
      "Training Step: 61  | total loss: \u001b[1m\u001b[32m0.02255\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 004 | loss: 0.02255 - R2: 0.9992 -- iter: 0256/1168\n",
      "Training Step: 62  | total loss: \u001b[1m\u001b[32m0.02174\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 004 | loss: 0.02174 - R2: 0.9992 -- iter: 0320/1168\n",
      "Training Step: 63  | total loss: \u001b[1m\u001b[32m0.02012\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 004 | loss: 0.02012 - R2: 1.0005 -- iter: 0384/1168\n",
      "Training Step: 64  | total loss: \u001b[1m\u001b[32m0.02001\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 004 | loss: 0.02001 - R2: 1.0006 -- iter: 0448/1168\n",
      "Training Step: 65  | total loss: \u001b[1m\u001b[32m0.02054\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 004 | loss: 0.02054 - R2: 1.0009 -- iter: 0512/1168\n",
      "Training Step: 66  | total loss: \u001b[1m\u001b[32m0.02066\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 004 | loss: 0.02066 - R2: 1.0006 -- iter: 0576/1168\n",
      "Training Step: 67  | total loss: \u001b[1m\u001b[32m0.02066\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 004 | loss: 0.02066 - R2: 1.0006 -- iter: 0640/1168\n",
      "Training Step: 68  | total loss: \u001b[1m\u001b[32m0.06067\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 004 | loss: 0.06067 - R2: 1.0005 -- iter: 0704/1168\n",
      "Training Step: 69  | total loss: \u001b[1m\u001b[32m0.05255\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 004 | loss: 0.05255 - R2: 1.0001 -- iter: 0768/1168\n",
      "Training Step: 70  | total loss: \u001b[1m\u001b[32m0.04795\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 004 | loss: 0.04795 - R2: 1.0000 -- iter: 0832/1168\n",
      "Training Step: 71  | total loss: \u001b[1m\u001b[32m0.04496\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 004 | loss: 0.04496 - R2: 0.9995 -- iter: 0896/1168\n",
      "Training Step: 72  | total loss: \u001b[1m\u001b[32m0.04150\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 004 | loss: 0.04150 - R2: 0.9995 -- iter: 0960/1168\n",
      "Training Step: 73  | total loss: \u001b[1m\u001b[32m0.03901\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 004 | loss: 0.03901 - R2: 0.9991 -- iter: 1024/1168\n",
      "Training Step: 74  | total loss: \u001b[1m\u001b[32m0.03620\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 004 | loss: 0.03620 - R2: 0.9994 -- iter: 1088/1168\n",
      "Training Step: 75  | total loss: \u001b[1m\u001b[32m0.03531\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 004 | loss: 0.03531 - R2: 0.9998 -- iter: 1152/1168\n",
      "Training Step: 76  | total loss: \u001b[1m\u001b[32m0.03413\u001b[0m\u001b[0m | time: 1.132s\n",
      "| SGD | epoch: 004 | loss: 0.03413 - R2: 0.9997 | val_loss: 0.01819 - val_acc: 0.9957 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 77  | total loss: \u001b[1m\u001b[32m0.03289\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 005 | loss: 0.03289 - R2: 0.9997 -- iter: 0064/1168\n",
      "Training Step: 78  | total loss: \u001b[1m\u001b[32m0.03289\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 005 | loss: 0.03289 - R2: 0.9999 -- iter: 0128/1168\n",
      "Training Step: 79  | total loss: \u001b[1m\u001b[32m0.03003\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 005 | loss: 0.03003 - R2: 0.9998 -- iter: 0192/1168\n",
      "Training Step: 80  | total loss: \u001b[1m\u001b[32m0.02873\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 005 | loss: 0.02873 - R2: 1.0000 -- iter: 0256/1168\n",
      "Training Step: 81  | total loss: \u001b[1m\u001b[32m0.02873\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 005 | loss: 0.02873 - R2: 1.0000 -- iter: 0320/1168\n",
      "Training Step: 82  | total loss: \u001b[1m\u001b[32m0.02702\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 005 | loss: 0.02702 - R2: 1.0007 -- iter: 0384/1168\n",
      "Training Step: 83  | total loss: \u001b[1m\u001b[32m0.02841\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 005 | loss: 0.02841 - R2: 1.0002 -- iter: 0448/1168\n",
      "Training Step: 84  | total loss: \u001b[1m\u001b[32m0.02693\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 005 | loss: 0.02693 - R2: 1.0002 -- iter: 0512/1168\n",
      "Training Step: 85  | total loss: \u001b[1m\u001b[32m0.02903\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 005 | loss: 0.02903 - R2: 1.0005 -- iter: 0576/1168\n",
      "Training Step: 86  | total loss: \u001b[1m\u001b[32m0.02904\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 005 | loss: 0.02904 - R2: 1.0010 -- iter: 0640/1168\n",
      "Training Step: 87  | total loss: \u001b[1m\u001b[32m0.02731\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 005 | loss: 0.02731 - R2: 1.0005 -- iter: 0704/1168\n",
      "Training Step: 88  | total loss: \u001b[1m\u001b[32m0.02774\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 005 | loss: 0.02774 - R2: 1.0008 -- iter: 0768/1168\n",
      "Training Step: 89  | total loss: \u001b[1m\u001b[32m0.02801\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 005 | loss: 0.02801 - R2: 1.0008 -- iter: 0832/1168\n",
      "Training Step: 90  | total loss: \u001b[1m\u001b[32m0.02830\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 005 | loss: 0.02830 - R2: 1.0008 -- iter: 0896/1168\n",
      "Training Step: 91  | total loss: \u001b[1m\u001b[32m0.02830\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 005 | loss: 0.02830 - R2: 1.0008 -- iter: 0960/1168\n",
      "Training Step: 92  | total loss: \u001b[1m\u001b[32m0.02721\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 005 | loss: 0.02721 - R2: 1.0003 -- iter: 1024/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 93  | total loss: \u001b[1m\u001b[32m0.02562\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 005 | loss: 0.02562 - R2: 0.9998 -- iter: 1088/1168\n",
      "Training Step: 94  | total loss: \u001b[1m\u001b[32m0.02562\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 005 | loss: 0.02562 - R2: 1.0003 -- iter: 1152/1168\n",
      "Training Step: 95  | total loss: \u001b[1m\u001b[32m0.02460\u001b[0m\u001b[0m | time: 1.189s\n",
      "| SGD | epoch: 005 | loss: 0.02460 - R2: 0.9999 | val_loss: 0.01837 - val_acc: 0.9949 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 96  | total loss: \u001b[1m\u001b[32m0.02460\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 006 | loss: 0.02460 - R2: 0.9999 -- iter: 0064/1168\n",
      "Training Step: 97  | total loss: \u001b[1m\u001b[32m0.02512\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 006 | loss: 0.02512 - R2: 1.0001 -- iter: 0128/1168\n",
      "Training Step: 98  | total loss: \u001b[1m\u001b[32m0.02525\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 006 | loss: 0.02525 - R2: 1.0000 -- iter: 0192/1168\n",
      "Training Step: 99  | total loss: \u001b[1m\u001b[32m0.02326\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 006 | loss: 0.02326 - R2: 0.9995 -- iter: 0256/1168\n",
      "Training Step: 100  | total loss: \u001b[1m\u001b[32m0.02146\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 006 | loss: 0.02146 - R2: 0.9991 -- iter: 0320/1168\n",
      "Training Step: 101  | total loss: \u001b[1m\u001b[32m0.02169\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 006 | loss: 0.02169 - R2: 0.9991 -- iter: 0384/1168\n",
      "Training Step: 102  | total loss: \u001b[1m\u001b[32m0.02062\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 006 | loss: 0.02062 - R2: 0.9991 -- iter: 0448/1168\n",
      "Training Step: 103  | total loss: \u001b[1m\u001b[32m0.02062\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 006 | loss: 0.02062 - R2: 0.9991 -- iter: 0512/1168\n",
      "Training Step: 104  | total loss: \u001b[1m\u001b[32m0.02023\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 006 | loss: 0.02023 - R2: 0.9994 -- iter: 0576/1168\n",
      "Training Step: 105  | total loss: \u001b[1m\u001b[32m0.02048\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 006 | loss: 0.02048 - R2: 0.9993 -- iter: 0640/1168\n",
      "Training Step: 106  | total loss: \u001b[1m\u001b[32m0.02044\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 006 | loss: 0.02044 - R2: 0.9989 -- iter: 0704/1168\n",
      "Training Step: 107  | total loss: \u001b[1m\u001b[32m0.02044\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 006 | loss: 0.02044 - R2: 0.9989 -- iter: 0768/1168\n",
      "Training Step: 108  | total loss: \u001b[1m\u001b[32m0.01942\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 006 | loss: 0.01942 - R2: 0.9989 -- iter: 0832/1168\n",
      "Training Step: 109  | total loss: \u001b[1m\u001b[32m0.01928\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 006 | loss: 0.01928 - R2: 0.9989 -- iter: 0896/1168\n",
      "Training Step: 110  | total loss: \u001b[1m\u001b[32m0.02019\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 006 | loss: 0.02019 - R2: 0.9990 -- iter: 0960/1168\n",
      "Training Step: 111  | total loss: \u001b[1m\u001b[32m0.01949\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 006 | loss: 0.01949 - R2: 0.9989 -- iter: 1024/1168\n",
      "Training Step: 112  | total loss: \u001b[1m\u001b[32m0.01992\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 006 | loss: 0.01992 - R2: 0.9990 -- iter: 1088/1168\n",
      "Training Step: 113  | total loss: \u001b[1m\u001b[32m0.01941\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 006 | loss: 0.01941 - R2: 0.9990 -- iter: 1152/1168\n",
      "Training Step: 114  | total loss: \u001b[1m\u001b[32m0.02027\u001b[0m\u001b[0m | time: 1.144s\n",
      "| SGD | epoch: 006 | loss: 0.02027 - R2: 0.9993 | val_loss: 0.01804 - val_acc: 0.9955 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 115  | total loss: \u001b[1m\u001b[32m0.02027\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 007 | loss: 0.02027 - R2: 0.9993 -- iter: 0064/1168\n",
      "Training Step: 116  | total loss: \u001b[1m\u001b[32m0.01922\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 007 | loss: 0.01922 - R2: 0.9994 -- iter: 0128/1168\n",
      "Training Step: 117  | total loss: \u001b[1m\u001b[32m0.01922\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 007 | loss: 0.01922 - R2: 0.9994 -- iter: 0192/1168\n",
      "Training Step: 118  | total loss: \u001b[1m\u001b[32m0.01860\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 007 | loss: 0.01860 - R2: 0.9999 -- iter: 0256/1168\n",
      "Training Step: 119  | total loss: \u001b[1m\u001b[32m0.01874\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 007 | loss: 0.01874 - R2: 1.0007 -- iter: 0320/1168\n",
      "Training Step: 120  | total loss: \u001b[1m\u001b[32m0.01874\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 007 | loss: 0.01874 - R2: 1.0007 -- iter: 0384/1168\n",
      "Training Step: 121  | total loss: \u001b[1m\u001b[32m0.01885\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 007 | loss: 0.01885 - R2: 1.0013 -- iter: 0448/1168\n",
      "Training Step: 122  | total loss: \u001b[1m\u001b[32m0.02000\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 007 | loss: 0.02000 - R2: 1.0013 -- iter: 0512/1168\n",
      "Training Step: 123  | total loss: \u001b[1m\u001b[32m0.02235\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 007 | loss: 0.02235 - R2: 1.0013 -- iter: 0576/1168\n",
      "Training Step: 124  | total loss: \u001b[1m\u001b[32m0.02252\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 007 | loss: 0.02252 - R2: 1.0010 -- iter: 0640/1168\n",
      "Training Step: 125  | total loss: \u001b[1m\u001b[32m0.02252\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 007 | loss: 0.02252 - R2: 1.0010 -- iter: 0704/1168\n",
      "Training Step: 126  | total loss: \u001b[1m\u001b[32m0.02333\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 007 | loss: 0.02333 - R2: 1.0006 -- iter: 0768/1168\n",
      "Training Step: 127  | total loss: \u001b[1m\u001b[32m0.02333\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 007 | loss: 0.02333 - R2: 1.0006 -- iter: 0832/1168\n",
      "Training Step: 128  | total loss: \u001b[1m\u001b[32m0.04870\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 007 | loss: 0.04870 - R2: 1.0003 -- iter: 0896/1168\n",
      "Training Step: 129  | total loss: \u001b[1m\u001b[32m0.04536\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 007 | loss: 0.04536 - R2: 1.0002 -- iter: 0960/1168\n",
      "Training Step: 130  | total loss: \u001b[1m\u001b[32m0.04536\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 007 | loss: 0.04536 - R2: 1.0006 -- iter: 1024/1168\n",
      "Training Step: 131  | total loss: \u001b[1m\u001b[32m0.04154\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 007 | loss: 0.04154 - R2: 1.0002 -- iter: 1088/1168\n",
      "Training Step: 132  | total loss: \u001b[1m\u001b[32m0.04154\u001b[0m\u001b[0m | time: 0.172s\n",
      "| SGD | epoch: 007 | loss: 0.04154 - R2: 1.0005 -- iter: 1152/1168\n",
      "Training Step: 133  | total loss: \u001b[1m\u001b[32m0.03576\u001b[0m\u001b[0m | time: 1.178s\n",
      "| SGD | epoch: 007 | loss: 0.03576 - R2: 1.0007 | val_loss: 0.01816 - val_acc: 0.9951 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 134  | total loss: \u001b[1m\u001b[32m0.03611\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 008 | loss: 0.03611 - R2: 1.0005 -- iter: 0064/1168\n",
      "Training Step: 135  | total loss: \u001b[1m\u001b[32m0.03611\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 008 | loss: 0.03611 - R2: 1.0005 -- iter: 0128/1168\n",
      "Training Step: 136  | total loss: \u001b[1m\u001b[32m0.03418\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 008 | loss: 0.03418 - R2: 1.0005 -- iter: 0192/1168\n",
      "Training Step: 137  | total loss: \u001b[1m\u001b[32m0.03327\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 008 | loss: 0.03327 - R2: 1.0002 -- iter: 0256/1168\n",
      "Training Step: 138  | total loss: \u001b[1m\u001b[32m0.03141\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 008 | loss: 0.03141 - R2: 1.0002 -- iter: 0320/1168\n",
      "Training Step: 139  | total loss: \u001b[1m\u001b[32m0.02863\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 008 | loss: 0.02863 - R2: 1.0000 -- iter: 0384/1168\n",
      "Training Step: 140  | total loss: \u001b[1m\u001b[32m0.02693\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 008 | loss: 0.02693 - R2: 1.0000 -- iter: 0448/1168\n",
      "Training Step: 141  | total loss: \u001b[1m\u001b[32m0.02693\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 008 | loss: 0.02693 - R2: 1.0000 -- iter: 0512/1168\n",
      "Training Step: 142  | total loss: \u001b[1m\u001b[32m0.02572\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 008 | loss: 0.02572 - R2: 0.9997 -- iter: 0576/1168\n",
      "Training Step: 143  | total loss: \u001b[1m\u001b[32m0.02541\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 008 | loss: 0.02541 - R2: 0.9999 -- iter: 0640/1168\n",
      "Training Step: 144  | total loss: \u001b[1m\u001b[32m0.02503\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 008 | loss: 0.02503 - R2: 0.9998 -- iter: 0704/1168\n",
      "Training Step: 145  | total loss: \u001b[1m\u001b[32m0.02485\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 008 | loss: 0.02485 - R2: 0.9998 -- iter: 0768/1168\n",
      "Training Step: 146  | total loss: \u001b[1m\u001b[32m0.02485\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 008 | loss: 0.02485 - R2: 0.9998 -- iter: 0832/1168\n",
      "Training Step: 147  | total loss: \u001b[1m\u001b[32m0.02441\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 008 | loss: 0.02441 - R2: 0.9993 -- iter: 0896/1168\n",
      "Training Step: 148  | total loss: \u001b[1m\u001b[32m0.04972\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 008 | loss: 0.04972 - R2: 1.0000 -- iter: 0960/1168\n",
      "Training Step: 149  | total loss: \u001b[1m\u001b[32m0.04760\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 008 | loss: 0.04760 - R2: 1.0000 -- iter: 1024/1168\n",
      "Training Step: 150  | total loss: \u001b[1m\u001b[32m0.04406\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 008 | loss: 0.04406 - R2: 0.9999 -- iter: 1088/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 151  | total loss: \u001b[1m\u001b[32m0.04108\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 008 | loss: 0.04108 - R2: 0.9993 -- iter: 1152/1168\n",
      "Training Step: 152  | total loss: \u001b[1m\u001b[32m0.03921\u001b[0m\u001b[0m | time: 1.113s\n",
      "| SGD | epoch: 008 | loss: 0.03921 - R2: 0.9995 | val_loss: 0.01812 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 153  | total loss: \u001b[1m\u001b[32m0.03770\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 009 | loss: 0.03770 - R2: 0.9988 -- iter: 0064/1168\n",
      "Training Step: 154  | total loss: \u001b[1m\u001b[32m0.03847\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 009 | loss: 0.03847 - R2: 0.9991 -- iter: 0128/1168\n",
      "Training Step: 155  | total loss: \u001b[1m\u001b[32m0.03622\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 009 | loss: 0.03622 - R2: 0.9993 -- iter: 0192/1168\n",
      "Training Step: 156  | total loss: \u001b[1m\u001b[32m0.03356\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 009 | loss: 0.03356 - R2: 0.9993 -- iter: 0256/1168\n",
      "Training Step: 157  | total loss: \u001b[1m\u001b[32m0.03134\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 009 | loss: 0.03134 - R2: 0.9993 -- iter: 0320/1168\n",
      "Training Step: 158  | total loss: \u001b[1m\u001b[32m0.03039\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 009 | loss: 0.03039 - R2: 0.9990 -- iter: 0384/1168\n",
      "Training Step: 159  | total loss: \u001b[1m\u001b[32m0.02980\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 009 | loss: 0.02980 - R2: 0.9997 -- iter: 0448/1168\n",
      "Training Step: 160  | total loss: \u001b[1m\u001b[32m0.02957\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 009 | loss: 0.02957 - R2: 1.0009 -- iter: 0512/1168\n",
      "Training Step: 161  | total loss: \u001b[1m\u001b[32m0.02957\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 009 | loss: 0.02957 - R2: 1.0009 -- iter: 0576/1168\n",
      "Training Step: 162  | total loss: \u001b[1m\u001b[32m0.02887\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 009 | loss: 0.02887 - R2: 1.0015 -- iter: 0640/1168\n",
      "Training Step: 163  | total loss: \u001b[1m\u001b[32m0.02774\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 009 | loss: 0.02774 - R2: 1.0011 -- iter: 0704/1168\n",
      "Training Step: 164  | total loss: \u001b[1m\u001b[32m0.02887\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 009 | loss: 0.02887 - R2: 1.0012 -- iter: 0768/1168\n",
      "Training Step: 165  | total loss: \u001b[1m\u001b[32m0.02887\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 009 | loss: 0.02887 - R2: 1.0012 -- iter: 0832/1168\n",
      "Training Step: 166  | total loss: \u001b[1m\u001b[32m0.02669\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 009 | loss: 0.02669 - R2: 1.0004 -- iter: 0896/1168\n",
      "Training Step: 167  | total loss: \u001b[1m\u001b[32m0.02669\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 009 | loss: 0.02669 - R2: 1.0004 -- iter: 0960/1168\n",
      "Training Step: 168  | total loss: \u001b[1m\u001b[32m0.05555\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 009 | loss: 0.05555 - R2: 0.9988 -- iter: 1024/1168\n",
      "Training Step: 169  | total loss: \u001b[1m\u001b[32m0.05128\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 009 | loss: 0.05128 - R2: 0.9990 -- iter: 1088/1168\n",
      "Training Step: 170  | total loss: \u001b[1m\u001b[32m0.04847\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 009 | loss: 0.04847 - R2: 0.9991 -- iter: 1152/1168\n",
      "Training Step: 171  | total loss: \u001b[1m\u001b[32m0.04505\u001b[0m\u001b[0m | time: 1.111s\n",
      "| SGD | epoch: 009 | loss: 0.04505 - R2: 0.9992 | val_loss: 0.01813 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 172  | total loss: \u001b[1m\u001b[32m0.04505\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 010 | loss: 0.04505 - R2: 0.9992 -- iter: 0064/1168\n",
      "Training Step: 173  | total loss: \u001b[1m\u001b[32m0.04292\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 010 | loss: 0.04292 - R2: 0.9993 -- iter: 0128/1168\n",
      "Training Step: 174  | total loss: \u001b[1m\u001b[32m0.03772\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 010 | loss: 0.03772 - R2: 0.9991 -- iter: 0192/1168\n",
      "Training Step: 175  | total loss: \u001b[1m\u001b[32m0.03772\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 010 | loss: 0.03772 - R2: 0.9991 -- iter: 0256/1168\n",
      "Training Step: 176  | total loss: \u001b[1m\u001b[32m0.03633\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 010 | loss: 0.03633 - R2: 0.9992 -- iter: 0320/1168\n",
      "Training Step: 177  | total loss: \u001b[1m\u001b[32m0.03305\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 010 | loss: 0.03305 - R2: 0.9989 -- iter: 0384/1168\n",
      "Training Step: 178  | total loss: \u001b[1m\u001b[32m0.03383\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 010 | loss: 0.03383 - R2: 0.9991 -- iter: 0448/1168\n",
      "Training Step: 179  | total loss: \u001b[1m\u001b[32m0.03200\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 010 | loss: 0.03200 - R2: 0.9991 -- iter: 0512/1168\n",
      "Training Step: 180  | total loss: \u001b[1m\u001b[32m0.03034\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 010 | loss: 0.03034 - R2: 1.0019 -- iter: 0576/1168\n",
      "Training Step: 181  | total loss: \u001b[1m\u001b[32m0.02886\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 010 | loss: 0.02886 - R2: 1.0015 -- iter: 0640/1168\n",
      "Training Step: 182  | total loss: \u001b[1m\u001b[32m0.02886\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 010 | loss: 0.02886 - R2: 1.0015 -- iter: 0704/1168\n",
      "Training Step: 183  | total loss: \u001b[1m\u001b[32m0.02803\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 010 | loss: 0.02803 - R2: 1.0016 -- iter: 0768/1168\n",
      "Training Step: 184  | total loss: \u001b[1m\u001b[32m0.02803\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 010 | loss: 0.02803 - R2: 1.0016 -- iter: 0832/1168\n",
      "Training Step: 185  | total loss: \u001b[1m\u001b[32m0.02772\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 010 | loss: 0.02772 - R2: 1.0014 -- iter: 0896/1168\n",
      "Training Step: 186  | total loss: \u001b[1m\u001b[32m0.02772\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 010 | loss: 0.02772 - R2: 1.0013 -- iter: 0960/1168\n",
      "Training Step: 187  | total loss: \u001b[1m\u001b[32m0.02776\u001b[0m\u001b[0m | time: 0.187s\n",
      "| SGD | epoch: 010 | loss: 0.02776 - R2: 1.0026 -- iter: 1024/1168\n",
      "Training Step: 188  | total loss: \u001b[1m\u001b[32m0.05259\u001b[0m\u001b[0m | time: 0.196s\n",
      "| SGD | epoch: 010 | loss: 0.05259 - R2: 1.0026 -- iter: 1088/1168\n",
      "Training Step: 189  | total loss: \u001b[1m\u001b[32m0.04648\u001b[0m\u001b[0m | time: 0.205s\n",
      "| SGD | epoch: 010 | loss: 0.04648 - R2: 1.0021 -- iter: 1152/1168\n",
      "Training Step: 190  | total loss: \u001b[1m\u001b[32m0.04648\u001b[0m\u001b[0m | time: 1.211s\n",
      "| SGD | epoch: 010 | loss: 0.04648 - R2: 1.0021 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 191  | total loss: \u001b[1m\u001b[32m0.04506\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 011 | loss: 0.04506 - R2: 1.0028 -- iter: 0064/1168\n",
      "Training Step: 192  | total loss: \u001b[1m\u001b[32m0.04124\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 011 | loss: 0.04124 - R2: 1.0020 -- iter: 0128/1168\n",
      "Training Step: 193  | total loss: \u001b[1m\u001b[32m0.03878\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 011 | loss: 0.03878 - R2: 1.0017 -- iter: 0192/1168\n",
      "Training Step: 194  | total loss: \u001b[1m\u001b[32m0.03934\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 011 | loss: 0.03934 - R2: 1.0015 -- iter: 0256/1168\n",
      "Training Step: 195  | total loss: \u001b[1m\u001b[32m0.03697\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 011 | loss: 0.03697 - R2: 1.0011 -- iter: 0320/1168\n",
      "Training Step: 196  | total loss: \u001b[1m\u001b[32m0.03697\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 011 | loss: 0.03697 - R2: 1.0011 -- iter: 0384/1168\n",
      "Training Step: 197  | total loss: \u001b[1m\u001b[32m0.03282\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 011 | loss: 0.03282 - R2: 1.0008 -- iter: 0448/1168\n",
      "Training Step: 198  | total loss: \u001b[1m\u001b[32m0.03116\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 011 | loss: 0.03116 - R2: 1.0008 -- iter: 0512/1168\n",
      "Training Step: 199  | total loss: \u001b[1m\u001b[32m0.03116\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 011 | loss: 0.03116 - R2: 1.0008 -- iter: 0576/1168\n",
      "Training Step: 200  | total loss: \u001b[1m\u001b[32m0.03327\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 011 | loss: 0.03327 - R2: 1.0010 -- iter: 0640/1168\n",
      "Training Step: 201  | total loss: \u001b[1m\u001b[32m0.03306\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 011 | loss: 0.03306 - R2: 1.0009 -- iter: 0704/1168\n",
      "Training Step: 202  | total loss: \u001b[1m\u001b[32m0.03306\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 011 | loss: 0.03306 - R2: 1.0009 -- iter: 0768/1168\n",
      "Training Step: 203  | total loss: \u001b[1m\u001b[32m0.02998\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 011 | loss: 0.02998 - R2: 0.9999 -- iter: 0832/1168\n",
      "Training Step: 204  | total loss: \u001b[1m\u001b[32m0.02876\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 011 | loss: 0.02876 - R2: 1.0000 -- iter: 0896/1168\n",
      "Training Step: 205  | total loss: \u001b[1m\u001b[32m0.02876\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 011 | loss: 0.02876 - R2: 1.0000 -- iter: 0960/1168\n",
      "Training Step: 206  | total loss: \u001b[1m\u001b[32m0.02791\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 011 | loss: 0.02791 - R2: 0.9997 -- iter: 1024/1168\n",
      "Training Step: 207  | total loss: \u001b[1m\u001b[32m0.02700\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 011 | loss: 0.02700 - R2: 0.9995 -- iter: 1088/1168\n",
      "Training Step: 208  | total loss: \u001b[1m\u001b[32m0.04905\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 011 | loss: 0.04905 - R2: 0.9998 -- iter: 1152/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 209  | total loss: \u001b[1m\u001b[32m0.04570\u001b[0m\u001b[0m | time: 1.123s\n",
      "| SGD | epoch: 011 | loss: 0.04570 - R2: 0.9999 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 210  | total loss: \u001b[1m\u001b[32m0.04570\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 012 | loss: 0.04570 - R2: 0.9999 -- iter: 0064/1168\n",
      "Training Step: 211  | total loss: \u001b[1m\u001b[32m0.04271\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 012 | loss: 0.04271 - R2: 1.0002 -- iter: 0128/1168\n",
      "Training Step: 212  | total loss: \u001b[1m\u001b[32m0.03775\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 012 | loss: 0.03775 - R2: 1.0004 -- iter: 0192/1168\n",
      "Training Step: 213  | total loss: \u001b[1m\u001b[32m0.03775\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 012 | loss: 0.03775 - R2: 1.0004 -- iter: 0256/1168\n",
      "Training Step: 214  | total loss: \u001b[1m\u001b[32m0.03532\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 012 | loss: 0.03532 - R2: 1.0006 -- iter: 0320/1168\n",
      "Training Step: 215  | total loss: \u001b[1m\u001b[32m0.03305\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 012 | loss: 0.03305 - R2: 1.0000 -- iter: 0384/1168\n",
      "Training Step: 216  | total loss: \u001b[1m\u001b[32m0.03143\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 012 | loss: 0.03143 - R2: 0.9997 -- iter: 0448/1168\n",
      "Training Step: 217  | total loss: \u001b[1m\u001b[32m0.03143\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 012 | loss: 0.03143 - R2: 0.9997 -- iter: 0512/1168\n",
      "Training Step: 218  | total loss: \u001b[1m\u001b[32m0.03155\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 012 | loss: 0.03155 - R2: 0.9997 -- iter: 0576/1168\n",
      "Training Step: 219  | total loss: \u001b[1m\u001b[32m0.03201\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 012 | loss: 0.03201 - R2: 1.0004 -- iter: 0640/1168\n",
      "Training Step: 220  | total loss: \u001b[1m\u001b[32m0.03079\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 012 | loss: 0.03079 - R2: 1.0005 -- iter: 0704/1168\n",
      "Training Step: 221  | total loss: \u001b[1m\u001b[32m0.02969\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 012 | loss: 0.02969 - R2: 1.0006 -- iter: 0768/1168\n",
      "Training Step: 222  | total loss: \u001b[1m\u001b[32m0.02874\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 012 | loss: 0.02874 - R2: 1.0002 -- iter: 0832/1168\n",
      "Training Step: 223  | total loss: \u001b[1m\u001b[32m0.02588\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 012 | loss: 0.02588 - R2: 1.0000 -- iter: 0896/1168\n",
      "Training Step: 224  | total loss: \u001b[1m\u001b[32m0.02524\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 012 | loss: 0.02524 - R2: 1.0002 -- iter: 0960/1168\n",
      "Training Step: 225  | total loss: \u001b[1m\u001b[32m0.02513\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 012 | loss: 0.02513 - R2: 1.0002 -- iter: 1024/1168\n",
      "Training Step: 226  | total loss: \u001b[1m\u001b[32m0.02513\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 012 | loss: 0.02513 - R2: 1.0002 -- iter: 1088/1168\n",
      "Training Step: 227  | total loss: \u001b[1m\u001b[32m0.02468\u001b[0m\u001b[0m | time: 0.180s\n",
      "| SGD | epoch: 012 | loss: 0.02468 - R2: 0.9993 -- iter: 1152/1168\n",
      "Training Step: 228  | total loss: \u001b[1m\u001b[32m0.05015\u001b[0m\u001b[0m | time: 1.190s\n",
      "| SGD | epoch: 012 | loss: 0.05015 - R2: 0.9985 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 229  | total loss: \u001b[1m\u001b[32m0.05015\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 013 | loss: 0.05015 - R2: 0.9991 -- iter: 0064/1168\n",
      "Training Step: 230  | total loss: \u001b[1m\u001b[32m0.04787\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 013 | loss: 0.04787 - R2: 0.9989 -- iter: 0128/1168\n",
      "Training Step: 231  | total loss: \u001b[1m\u001b[32m0.04576\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 013 | loss: 0.04576 - R2: 0.9992 -- iter: 0192/1168\n",
      "Training Step: 232  | total loss: \u001b[1m\u001b[32m0.04274\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 013 | loss: 0.04274 - R2: 0.9988 -- iter: 0256/1168\n",
      "Training Step: 233  | total loss: \u001b[1m\u001b[32m0.04010\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 013 | loss: 0.04010 - R2: 0.9988 -- iter: 0320/1168\n",
      "Training Step: 234  | total loss: \u001b[1m\u001b[32m0.03795\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 013 | loss: 0.03795 - R2: 0.9989 -- iter: 0384/1168\n",
      "Training Step: 235  | total loss: \u001b[1m\u001b[32m0.03515\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 013 | loss: 0.03515 - R2: 0.9993 -- iter: 0448/1168\n",
      "Training Step: 236  | total loss: \u001b[1m\u001b[32m0.03545\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 013 | loss: 0.03545 - R2: 1.0000 -- iter: 0512/1168\n",
      "Training Step: 237  | total loss: \u001b[1m\u001b[32m0.03369\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 013 | loss: 0.03369 - R2: 0.9998 -- iter: 0576/1168\n",
      "Training Step: 238  | total loss: \u001b[1m\u001b[32m0.03470\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 013 | loss: 0.03470 - R2: 1.0006 -- iter: 0640/1168\n",
      "Training Step: 239  | total loss: \u001b[1m\u001b[32m0.03288\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 013 | loss: 0.03288 - R2: 1.0000 -- iter: 0704/1168\n",
      "Training Step: 240  | total loss: \u001b[1m\u001b[32m0.03121\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 013 | loss: 0.03121 - R2: 0.9992 -- iter: 0768/1168\n",
      "Training Step: 241  | total loss: \u001b[1m\u001b[32m0.02971\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 013 | loss: 0.02971 - R2: 0.9985 -- iter: 0832/1168\n",
      "Training Step: 242  | total loss: \u001b[1m\u001b[32m0.02910\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 013 | loss: 0.02910 - R2: 0.9983 -- iter: 0896/1168\n",
      "Training Step: 243  | total loss: \u001b[1m\u001b[32m0.02780\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 013 | loss: 0.02780 - R2: 0.9980 -- iter: 0960/1168\n",
      "Training Step: 244  | total loss: \u001b[1m\u001b[32m0.02664\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 013 | loss: 0.02664 - R2: 0.9984 -- iter: 1024/1168\n",
      "Training Step: 245  | total loss: \u001b[1m\u001b[32m0.02551\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 013 | loss: 0.02551 - R2: 0.9980 -- iter: 1088/1168\n",
      "Training Step: 246  | total loss: \u001b[1m\u001b[32m0.02486\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 013 | loss: 0.02486 - R2: 0.9981 -- iter: 1152/1168\n",
      "Training Step: 247  | total loss: \u001b[1m\u001b[32m0.02492\u001b[0m\u001b[0m | time: 1.112s\n",
      "| SGD | epoch: 013 | loss: 0.02492 - R2: 0.9986 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 248  | total loss: \u001b[1m\u001b[32m0.02267\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 014 | loss: 0.02267 - R2: 0.9989 -- iter: 0064/1168\n",
      "Training Step: 249  | total loss: \u001b[1m\u001b[32m0.02267\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 014 | loss: 0.02267 - R2: 0.9989 -- iter: 0128/1168\n",
      "Training Step: 250  | total loss: \u001b[1m\u001b[32m0.02394\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 014 | loss: 0.02394 - R2: 0.9993 -- iter: 0192/1168\n",
      "Training Step: 251  | total loss: \u001b[1m\u001b[32m0.02299\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 014 | loss: 0.02299 - R2: 0.9992 -- iter: 0256/1168\n",
      "Training Step: 252  | total loss: \u001b[1m\u001b[32m0.02347\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 014 | loss: 0.02347 - R2: 0.9990 -- iter: 0320/1168\n",
      "Training Step: 253  | total loss: \u001b[1m\u001b[32m0.02283\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 014 | loss: 0.02283 - R2: 0.9991 -- iter: 0384/1168\n",
      "Training Step: 254  | total loss: \u001b[1m\u001b[32m0.02283\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 014 | loss: 0.02283 - R2: 0.9991 -- iter: 0448/1168\n",
      "Training Step: 255  | total loss: \u001b[1m\u001b[32m0.02186\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 014 | loss: 0.02186 - R2: 0.9988 -- iter: 0512/1168\n",
      "Training Step: 256  | total loss: \u001b[1m\u001b[32m0.02247\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 014 | loss: 0.02247 - R2: 0.9990 -- iter: 0576/1168\n",
      "Training Step: 257  | total loss: \u001b[1m\u001b[32m0.02264\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 014 | loss: 0.02264 - R2: 0.9995 -- iter: 0640/1168\n",
      "Training Step: 258  | total loss: \u001b[1m\u001b[32m0.02264\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 014 | loss: 0.02264 - R2: 0.9996 -- iter: 0704/1168\n",
      "Training Step: 259  | total loss: \u001b[1m\u001b[32m0.02224\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 014 | loss: 0.02224 - R2: 0.9993 -- iter: 0768/1168\n",
      "Training Step: 260  | total loss: \u001b[1m\u001b[32m0.02056\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 014 | loss: 0.02056 - R2: 0.9990 -- iter: 0832/1168\n",
      "Training Step: 261  | total loss: \u001b[1m\u001b[32m0.02078\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 014 | loss: 0.02078 - R2: 0.9987 -- iter: 0896/1168\n",
      "Training Step: 262  | total loss: \u001b[1m\u001b[32m0.01991\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 014 | loss: 0.01991 - R2: 0.9989 -- iter: 0960/1168\n",
      "Training Step: 263  | total loss: \u001b[1m\u001b[32m0.01991\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 014 | loss: 0.01991 - R2: 0.9989 -- iter: 1024/1168\n",
      "Training Step: 264  | total loss: \u001b[1m\u001b[32m0.02167\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 014 | loss: 0.02167 - R2: 0.9990 -- iter: 1088/1168\n",
      "Training Step: 265  | total loss: \u001b[1m\u001b[32m0.02119\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 014 | loss: 0.02119 - R2: 0.9990 -- iter: 1152/1168\n",
      "Training Step: 266  | total loss: \u001b[1m\u001b[32m0.02065\u001b[0m\u001b[0m | time: 1.143s\n",
      "| SGD | epoch: 014 | loss: 0.02065 - R2: 0.9995 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 267  | total loss: \u001b[1m\u001b[32m0.05557\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 015 | loss: 0.05557 - R2: 0.9983 -- iter: 0064/1168\n",
      "Training Step: 268  | total loss: \u001b[1m\u001b[32m0.05557\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 015 | loss: 0.05557 - R2: 0.9983 -- iter: 0128/1168\n",
      "Training Step: 269  | total loss: \u001b[1m\u001b[32m0.04828\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 015 | loss: 0.04828 - R2: 0.9985 -- iter: 0192/1168\n",
      "Training Step: 270  | total loss: \u001b[1m\u001b[32m0.04479\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 015 | loss: 0.04479 - R2: 0.9985 -- iter: 0256/1168\n",
      "Training Step: 271  | total loss: \u001b[1m\u001b[32m0.04304\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 015 | loss: 0.04304 - R2: 0.9986 -- iter: 0320/1168\n",
      "Training Step: 272  | total loss: \u001b[1m\u001b[32m0.04304\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 015 | loss: 0.04304 - R2: 0.9986 -- iter: 0384/1168\n",
      "Training Step: 273  | total loss: \u001b[1m\u001b[32m0.04093\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 015 | loss: 0.04093 - R2: 0.9990 -- iter: 0448/1168\n",
      "Training Step: 274  | total loss: \u001b[1m\u001b[32m0.03910\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 015 | loss: 0.03910 - R2: 0.9993 -- iter: 0512/1168\n",
      "Training Step: 275  | total loss: \u001b[1m\u001b[32m0.03664\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 015 | loss: 0.03664 - R2: 0.9993 -- iter: 0576/1168\n",
      "Training Step: 276  | total loss: \u001b[1m\u001b[32m0.03681\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 015 | loss: 0.03681 - R2: 0.9993 -- iter: 0640/1168\n",
      "Training Step: 277  | total loss: \u001b[1m\u001b[32m0.03598\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 015 | loss: 0.03598 - R2: 0.9999 -- iter: 0704/1168\n",
      "Training Step: 278  | total loss: \u001b[1m\u001b[32m0.03703\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 015 | loss: 0.03703 - R2: 0.9999 -- iter: 0768/1168\n",
      "Training Step: 279  | total loss: \u001b[1m\u001b[32m0.03471\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 015 | loss: 0.03471 - R2: 0.9999 -- iter: 0832/1168\n",
      "Training Step: 280  | total loss: \u001b[1m\u001b[32m0.03300\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 015 | loss: 0.03300 - R2: 1.0005 -- iter: 0896/1168\n",
      "Training Step: 281  | total loss: \u001b[1m\u001b[32m0.03114\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 015 | loss: 0.03114 - R2: 1.0013 -- iter: 0960/1168\n",
      "Training Step: 282  | total loss: \u001b[1m\u001b[32m0.03010\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 015 | loss: 0.03010 - R2: 1.0009 -- iter: 1024/1168\n",
      "Training Step: 283  | total loss: \u001b[1m\u001b[32m0.02876\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 015 | loss: 0.02876 - R2: 1.0002 -- iter: 1088/1168\n",
      "Training Step: 284  | total loss: \u001b[1m\u001b[32m0.02876\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 015 | loss: 0.02876 - R2: 1.0002 -- iter: 1152/1168\n",
      "Training Step: 285  | total loss: \u001b[1m\u001b[32m0.02858\u001b[0m\u001b[0m | time: 1.099s\n",
      "| SGD | epoch: 015 | loss: 0.02858 - R2: 1.0001 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 286  | total loss: \u001b[1m\u001b[32m0.02704\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 016 | loss: 0.02704 - R2: 1.0003 -- iter: 0064/1168\n",
      "Training Step: 287  | total loss: \u001b[1m\u001b[32m0.02535\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 016 | loss: 0.02535 - R2: 0.9999 -- iter: 0128/1168\n",
      "Training Step: 288  | total loss: \u001b[1m\u001b[32m0.05060\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 016 | loss: 0.05060 - R2: 0.9998 -- iter: 0192/1168\n",
      "Training Step: 289  | total loss: \u001b[1m\u001b[32m0.04808\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 016 | loss: 0.04808 - R2: 0.9998 -- iter: 0256/1168\n",
      "Training Step: 290  | total loss: \u001b[1m\u001b[32m0.04530\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 016 | loss: 0.04530 - R2: 0.9998 -- iter: 0320/1168\n",
      "Training Step: 291  | total loss: \u001b[1m\u001b[32m0.04263\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 016 | loss: 0.04263 - R2: 1.0001 -- iter: 0384/1168\n",
      "Training Step: 292  | total loss: \u001b[1m\u001b[32m0.04142\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 016 | loss: 0.04142 - R2: 1.0004 -- iter: 0448/1168\n",
      "Training Step: 293  | total loss: \u001b[1m\u001b[32m0.03862\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 016 | loss: 0.03862 - R2: 1.0001 -- iter: 0512/1168\n",
      "Training Step: 294  | total loss: \u001b[1m\u001b[32m0.03615\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 016 | loss: 0.03615 - R2: 1.0002 -- iter: 0576/1168\n",
      "Training Step: 295  | total loss: \u001b[1m\u001b[32m0.03376\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 016 | loss: 0.03376 - R2: 0.9996 -- iter: 0640/1168\n",
      "Training Step: 296  | total loss: \u001b[1m\u001b[32m0.03065\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 016 | loss: 0.03065 - R2: 0.9994 -- iter: 0704/1168\n",
      "Training Step: 297  | total loss: \u001b[1m\u001b[32m0.02929\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 016 | loss: 0.02929 - R2: 0.9996 -- iter: 0768/1168\n",
      "Training Step: 298  | total loss: \u001b[1m\u001b[32m0.02929\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 016 | loss: 0.02929 - R2: 0.9996 -- iter: 0832/1168\n",
      "Training Step: 299  | total loss: \u001b[1m\u001b[32m0.03088\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 016 | loss: 0.03088 - R2: 1.0000 -- iter: 0896/1168\n",
      "Training Step: 300  | total loss: \u001b[1m\u001b[32m0.02999\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 016 | loss: 0.02999 - R2: 0.9990 -- iter: 0960/1168\n",
      "Training Step: 301  | total loss: \u001b[1m\u001b[32m0.02919\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 016 | loss: 0.02919 - R2: 0.9982 -- iter: 1024/1168\n",
      "Training Step: 302  | total loss: \u001b[1m\u001b[32m0.02744\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 016 | loss: 0.02744 - R2: 0.9979 -- iter: 1088/1168\n",
      "Training Step: 303  | total loss: \u001b[1m\u001b[32m0.02760\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 016 | loss: 0.02760 - R2: 0.9977 -- iter: 1152/1168\n",
      "Training Step: 304  | total loss: \u001b[1m\u001b[32m0.02643\u001b[0m\u001b[0m | time: 1.163s\n",
      "| SGD | epoch: 016 | loss: 0.02643 - R2: 0.9975 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 305  | total loss: \u001b[1m\u001b[32m0.02570\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 017 | loss: 0.02570 - R2: 0.9977 -- iter: 0064/1168\n",
      "Training Step: 306  | total loss: \u001b[1m\u001b[32m0.02616\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 017 | loss: 0.02616 - R2: 0.9984 -- iter: 0128/1168\n",
      "Training Step: 307  | total loss: \u001b[1m\u001b[32m0.02644\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 017 | loss: 0.02644 - R2: 0.9991 -- iter: 0192/1168\n",
      "Training Step: 308  | total loss: \u001b[1m\u001b[32m0.03880\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 017 | loss: 0.03880 - R2: 0.9999 -- iter: 0256/1168\n",
      "Training Step: 309  | total loss: \u001b[1m\u001b[32m0.03793\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 017 | loss: 0.03793 - R2: 1.0003 -- iter: 0320/1168\n",
      "Training Step: 310  | total loss: \u001b[1m\u001b[32m0.03557\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 017 | loss: 0.03557 - R2: 1.0001 -- iter: 0384/1168\n",
      "Training Step: 311  | total loss: \u001b[1m\u001b[32m0.03557\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 017 | loss: 0.03557 - R2: 1.0001 -- iter: 0448/1168\n",
      "Training Step: 312  | total loss: \u001b[1m\u001b[32m0.03461\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 017 | loss: 0.03461 - R2: 1.0001 -- iter: 0512/1168\n",
      "Training Step: 313  | total loss: \u001b[1m\u001b[32m0.03073\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 017 | loss: 0.03073 - R2: 0.9999 -- iter: 0576/1168\n",
      "Training Step: 314  | total loss: \u001b[1m\u001b[32m0.02939\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 017 | loss: 0.02939 - R2: 0.9998 -- iter: 0640/1168\n",
      "Training Step: 315  | total loss: \u001b[1m\u001b[32m0.02874\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 017 | loss: 0.02874 - R2: 0.9991 -- iter: 0704/1168\n",
      "Training Step: 316  | total loss: \u001b[1m\u001b[32m0.02741\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 017 | loss: 0.02741 - R2: 0.9991 -- iter: 0768/1168\n",
      "Training Step: 317  | total loss: \u001b[1m\u001b[32m0.02741\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 017 | loss: 0.02741 - R2: 0.9991 -- iter: 0832/1168\n",
      "Training Step: 318  | total loss: \u001b[1m\u001b[32m0.02650\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 017 | loss: 0.02650 - R2: 0.9992 -- iter: 0896/1168\n",
      "Training Step: 319  | total loss: \u001b[1m\u001b[32m0.02510\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 017 | loss: 0.02510 - R2: 0.9990 -- iter: 0960/1168\n",
      "Training Step: 320  | total loss: \u001b[1m\u001b[32m0.02477\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 017 | loss: 0.02477 - R2: 0.9986 -- iter: 1024/1168\n",
      "Training Step: 321  | total loss: \u001b[1m\u001b[32m0.02448\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 017 | loss: 0.02448 - R2: 0.9982 -- iter: 1088/1168\n",
      "Training Step: 322  | total loss: \u001b[1m\u001b[32m0.02496\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 017 | loss: 0.02496 - R2: 0.9990 -- iter: 1152/1168\n",
      "Training Step: 323  | total loss: \u001b[1m\u001b[32m0.02641\u001b[0m\u001b[0m | time: 1.120s\n",
      "| SGD | epoch: 017 | loss: 0.02641 - R2: 0.9989 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 324  | total loss: \u001b[1m\u001b[32m0.02641\u001b[0m\u001b[0m | time: 0.012s\n",
      "| SGD | epoch: 018 | loss: 0.02641 - R2: 0.9989 -- iter: 0064/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 325  | total loss: \u001b[1m\u001b[32m0.02492\u001b[0m\u001b[0m | time: 0.018s\n",
      "| SGD | epoch: 018 | loss: 0.02492 - R2: 0.9995 -- iter: 0128/1168\n",
      "Training Step: 326  | total loss: \u001b[1m\u001b[32m0.02412\u001b[0m\u001b[0m | time: 0.023s\n",
      "| SGD | epoch: 018 | loss: 0.02412 - R2: 0.9993 -- iter: 0192/1168\n",
      "Training Step: 327  | total loss: \u001b[1m\u001b[32m0.02524\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 018 | loss: 0.02524 - R2: 0.9998 -- iter: 0256/1168\n",
      "Training Step: 328  | total loss: \u001b[1m\u001b[32m0.02524\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 018 | loss: 0.02524 - R2: 0.9998 -- iter: 0320/1168\n",
      "Training Step: 329  | total loss: \u001b[1m\u001b[32m0.02720\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 018 | loss: 0.02720 - R2: 1.0002 -- iter: 0384/1168\n",
      "Training Step: 330  | total loss: \u001b[1m\u001b[32m0.02653\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 018 | loss: 0.02653 - R2: 1.0002 -- iter: 0448/1168\n",
      "Training Step: 331  | total loss: \u001b[1m\u001b[32m0.02530\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 018 | loss: 0.02530 - R2: 1.0001 -- iter: 0512/1168\n",
      "Training Step: 332  | total loss: \u001b[1m\u001b[32m0.02451\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 018 | loss: 0.02451 - R2: 1.0000 -- iter: 0576/1168\n",
      "Training Step: 333  | total loss: \u001b[1m\u001b[32m0.02480\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 018 | loss: 0.02480 - R2: 1.0004 -- iter: 0640/1168\n",
      "Training Step: 334  | total loss: \u001b[1m\u001b[32m0.02668\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 018 | loss: 0.02668 - R2: 1.0011 -- iter: 0704/1168\n",
      "Training Step: 335  | total loss: \u001b[1m\u001b[32m0.02540\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 018 | loss: 0.02540 - R2: 1.0005 -- iter: 0768/1168\n",
      "Training Step: 336  | total loss: \u001b[1m\u001b[32m0.02466\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 018 | loss: 0.02466 - R2: 1.0005 -- iter: 0832/1168\n",
      "Training Step: 337  | total loss: \u001b[1m\u001b[32m0.02573\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 018 | loss: 0.02573 - R2: 1.0006 -- iter: 0896/1168\n",
      "Training Step: 338  | total loss: \u001b[1m\u001b[32m0.02390\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 018 | loss: 0.02390 - R2: 0.9997 -- iter: 0960/1168\n",
      "Training Step: 339  | total loss: \u001b[1m\u001b[32m0.02390\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 018 | loss: 0.02390 - R2: 0.9997 -- iter: 1024/1168\n",
      "Training Step: 340  | total loss: \u001b[1m\u001b[32m0.02286\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 018 | loss: 0.02286 - R2: 0.9991 -- iter: 1088/1168\n",
      "Training Step: 341  | total loss: \u001b[1m\u001b[32m0.02192\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 018 | loss: 0.02192 - R2: 0.9985 -- iter: 1152/1168\n",
      "Training Step: 342  | total loss: \u001b[1m\u001b[32m0.02220\u001b[0m\u001b[0m | time: 1.105s\n",
      "| SGD | epoch: 018 | loss: 0.02220 - R2: 0.9979 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 343  | total loss: \u001b[1m\u001b[32m0.02150\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 019 | loss: 0.02150 - R2: 0.9983 -- iter: 0064/1168\n",
      "Training Step: 344  | total loss: \u001b[1m\u001b[32m0.02150\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 019 | loss: 0.02150 - R2: 0.9983 -- iter: 0128/1168\n",
      "Training Step: 345  | total loss: \u001b[1m\u001b[32m0.02010\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 019 | loss: 0.02010 - R2: 0.9984 -- iter: 0192/1168\n",
      "Training Step: 346  | total loss: \u001b[1m\u001b[32m0.02010\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 019 | loss: 0.02010 - R2: 0.9984 -- iter: 0256/1168\n",
      "Training Step: 347  | total loss: \u001b[1m\u001b[32m0.02021\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 019 | loss: 0.02021 - R2: 0.9986 -- iter: 0320/1168\n",
      "Training Step: 348  | total loss: \u001b[1m\u001b[32m0.01918\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 019 | loss: 0.01918 - R2: 0.9986 -- iter: 0384/1168\n",
      "Training Step: 349  | total loss: \u001b[1m\u001b[32m0.01858\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 019 | loss: 0.01858 - R2: 0.9980 -- iter: 0448/1168\n",
      "Training Step: 350  | total loss: \u001b[1m\u001b[32m0.01896\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 019 | loss: 0.01896 - R2: 0.9982 -- iter: 0512/1168\n",
      "Training Step: 351  | total loss: \u001b[1m\u001b[32m0.01898\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 019 | loss: 0.01898 - R2: 0.9978 -- iter: 0576/1168\n",
      "Training Step: 352  | total loss: \u001b[1m\u001b[32m0.02239\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 019 | loss: 0.02239 - R2: 0.9986 -- iter: 0640/1168\n",
      "Training Step: 353  | total loss: \u001b[1m\u001b[32m0.02239\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 019 | loss: 0.02239 - R2: 0.9986 -- iter: 0704/1168\n",
      "Training Step: 354  | total loss: \u001b[1m\u001b[32m0.02272\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 019 | loss: 0.02272 - R2: 0.9986 -- iter: 0768/1168\n",
      "Training Step: 355  | total loss: \u001b[1m\u001b[32m0.02447\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 019 | loss: 0.02447 - R2: 0.9990 -- iter: 0832/1168\n",
      "Training Step: 356  | total loss: \u001b[1m\u001b[32m0.02493\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 019 | loss: 0.02493 - R2: 0.9994 -- iter: 0896/1168\n",
      "Training Step: 357  | total loss: \u001b[1m\u001b[32m0.02349\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 019 | loss: 0.02349 - R2: 0.9998 -- iter: 0960/1168\n",
      "Training Step: 358  | total loss: \u001b[1m\u001b[32m0.02457\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 019 | loss: 0.02457 - R2: 0.9998 -- iter: 1024/1168\n",
      "Training Step: 359  | total loss: \u001b[1m\u001b[32m0.02364\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 019 | loss: 0.02364 - R2: 1.0001 -- iter: 1088/1168\n",
      "Training Step: 360  | total loss: \u001b[1m\u001b[32m0.02364\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 019 | loss: 0.02364 - R2: 1.0001 -- iter: 1152/1168\n",
      "Training Step: 361  | total loss: \u001b[1m\u001b[32m0.02281\u001b[0m\u001b[0m | time: 1.147s\n",
      "| SGD | epoch: 019 | loss: 0.02281 - R2: 1.0003 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 362  | total loss: \u001b[1m\u001b[32m0.02153\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 020 | loss: 0.02153 - R2: 1.0001 -- iter: 0064/1168\n",
      "Training Step: 363  | total loss: \u001b[1m\u001b[32m0.02059\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 020 | loss: 0.02059 - R2: 1.0003 -- iter: 0128/1168\n",
      "Training Step: 364  | total loss: \u001b[1m\u001b[32m0.01967\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 020 | loss: 0.01967 - R2: 1.0002 -- iter: 0192/1168\n",
      "Training Step: 365  | total loss: \u001b[1m\u001b[32m0.01951\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 020 | loss: 0.01951 - R2: 0.9997 -- iter: 0256/1168\n",
      "Training Step: 366  | total loss: \u001b[1m\u001b[32m0.01951\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 020 | loss: 0.01951 - R2: 0.9999 -- iter: 0320/1168\n",
      "Training Step: 367  | total loss: \u001b[1m\u001b[32m0.01885\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 020 | loss: 0.01885 - R2: 0.9984 -- iter: 0384/1168\n",
      "Training Step: 368  | total loss: \u001b[1m\u001b[32m0.04882\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 020 | loss: 0.04882 - R2: 0.9984 -- iter: 0448/1168\n",
      "Training Step: 369  | total loss: \u001b[1m\u001b[32m0.04564\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 020 | loss: 0.04564 - R2: 0.9985 -- iter: 0512/1168\n",
      "Training Step: 370  | total loss: \u001b[1m\u001b[32m0.04224\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 020 | loss: 0.04224 - R2: 0.9983 -- iter: 0576/1168\n",
      "Training Step: 371  | total loss: \u001b[1m\u001b[32m0.03984\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 020 | loss: 0.03984 - R2: 0.9983 -- iter: 0640/1168\n",
      "Training Step: 372  | total loss: \u001b[1m\u001b[32m0.03566\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 020 | loss: 0.03566 - R2: 0.9980 -- iter: 0704/1168\n",
      "Training Step: 373  | total loss: \u001b[1m\u001b[32m0.03566\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 020 | loss: 0.03566 - R2: 0.9978 -- iter: 0768/1168\n",
      "Training Step: 374  | total loss: \u001b[1m\u001b[32m0.03432\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 020 | loss: 0.03432 - R2: 0.9983 -- iter: 0832/1168\n",
      "Training Step: 375  | total loss: \u001b[1m\u001b[32m0.03176\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 020 | loss: 0.03176 - R2: 0.9985 -- iter: 0896/1168\n",
      "Training Step: 376  | total loss: \u001b[1m\u001b[32m0.02990\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 020 | loss: 0.02990 - R2: 0.9988 -- iter: 0960/1168\n",
      "Training Step: 377  | total loss: \u001b[1m\u001b[32m0.03116\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 020 | loss: 0.03116 - R2: 0.9991 -- iter: 1024/1168\n",
      "Training Step: 378  | total loss: \u001b[1m\u001b[32m0.02972\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 020 | loss: 0.02972 - R2: 0.9987 -- iter: 1088/1168\n",
      "Training Step: 379  | total loss: \u001b[1m\u001b[32m0.02972\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 020 | loss: 0.02972 - R2: 0.9987 -- iter: 1152/1168\n",
      "Training Step: 380  | total loss: \u001b[1m\u001b[32m0.02607\u001b[0m\u001b[0m | time: 1.134s\n",
      "| SGD | epoch: 020 | loss: 0.02607 - R2: 0.9975 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 381  | total loss: \u001b[1m\u001b[32m0.02503\u001b[0m\u001b[0m | time: 0.016s\n",
      "| SGD | epoch: 021 | loss: 0.02503 - R2: 0.9974 -- iter: 0064/1168\n",
      "Training Step: 382  | total loss: \u001b[1m\u001b[32m0.02511\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 021 | loss: 0.02511 - R2: 0.9983 -- iter: 0128/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 383  | total loss: \u001b[1m\u001b[32m0.02511\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 021 | loss: 0.02511 - R2: 0.9983 -- iter: 0192/1168\n",
      "Training Step: 384  | total loss: \u001b[1m\u001b[32m0.02392\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 021 | loss: 0.02392 - R2: 0.9983 -- iter: 0256/1168\n",
      "Training Step: 385  | total loss: \u001b[1m\u001b[32m0.02362\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 021 | loss: 0.02362 - R2: 0.9987 -- iter: 0320/1168\n",
      "Training Step: 386  | total loss: \u001b[1m\u001b[32m0.02596\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 021 | loss: 0.02596 - R2: 0.9994 -- iter: 0384/1168\n",
      "Training Step: 387  | total loss: \u001b[1m\u001b[32m0.04556\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 021 | loss: 0.04556 - R2: 1.0011 -- iter: 0448/1168\n",
      "Training Step: 388  | total loss: \u001b[1m\u001b[32m0.04556\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 021 | loss: 0.04556 - R2: 1.0011 -- iter: 0512/1168\n",
      "Training Step: 389  | total loss: \u001b[1m\u001b[32m0.04337\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 021 | loss: 0.04337 - R2: 1.0010 -- iter: 0576/1168\n",
      "Training Step: 390  | total loss: \u001b[1m\u001b[32m0.04302\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 021 | loss: 0.04302 - R2: 1.0011 -- iter: 0640/1168\n",
      "Training Step: 391  | total loss: \u001b[1m\u001b[32m0.04074\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 021 | loss: 0.04074 - R2: 1.0009 -- iter: 0704/1168\n",
      "Training Step: 392  | total loss: \u001b[1m\u001b[32m0.03926\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 021 | loss: 0.03926 - R2: 1.0007 -- iter: 0768/1168\n",
      "Training Step: 393  | total loss: \u001b[1m\u001b[32m0.03698\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 021 | loss: 0.03698 - R2: 1.0009 -- iter: 0832/1168\n",
      "Training Step: 394  | total loss: \u001b[1m\u001b[32m0.03480\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 021 | loss: 0.03480 - R2: 1.0010 -- iter: 0896/1168\n",
      "Training Step: 395  | total loss: \u001b[1m\u001b[32m0.03328\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 021 | loss: 0.03328 - R2: 1.0014 -- iter: 0960/1168\n",
      "Training Step: 396  | total loss: \u001b[1m\u001b[32m0.03328\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 021 | loss: 0.03328 - R2: 1.0014 -- iter: 1024/1168\n",
      "Training Step: 397  | total loss: \u001b[1m\u001b[32m0.02998\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 021 | loss: 0.02998 - R2: 1.0011 -- iter: 1088/1168\n",
      "Training Step: 398  | total loss: \u001b[1m\u001b[32m0.02998\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 021 | loss: 0.02998 - R2: 1.0011 -- iter: 1152/1168\n",
      "Training Step: 399  | total loss: \u001b[1m\u001b[32m0.02722\u001b[0m\u001b[0m | time: 1.084s\n",
      "| SGD | epoch: 021 | loss: 0.02722 - R2: 1.0005 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 400  | total loss: \u001b[1m\u001b[32m0.02722\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 022 | loss: 0.02722 - R2: 1.0005 -- iter: 0064/1168\n",
      "Training Step: 401  | total loss: \u001b[1m\u001b[32m0.02586\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 022 | loss: 0.02586 - R2: 1.0006 -- iter: 0128/1168\n",
      "Training Step: 402  | total loss: \u001b[1m\u001b[32m0.02460\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 022 | loss: 0.02460 - R2: 1.0004 -- iter: 0192/1168\n",
      "Training Step: 403  | total loss: \u001b[1m\u001b[32m0.02344\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 022 | loss: 0.02344 - R2: 1.0005 -- iter: 0256/1168\n",
      "Training Step: 404  | total loss: \u001b[1m\u001b[32m0.02282\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 022 | loss: 0.02282 - R2: 1.0000 -- iter: 0320/1168\n",
      "Training Step: 405  | total loss: \u001b[1m\u001b[32m0.02290\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 022 | loss: 0.02290 - R2: 0.9998 -- iter: 0384/1168\n",
      "Training Step: 406  | total loss: \u001b[1m\u001b[32m0.02222\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 022 | loss: 0.02222 - R2: 0.9996 -- iter: 0448/1168\n",
      "Training Step: 407  | total loss: \u001b[1m\u001b[32m0.02222\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 022 | loss: 0.02222 - R2: 0.9996 -- iter: 0512/1168\n",
      "Training Step: 408  | total loss: \u001b[1m\u001b[32m0.03984\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 022 | loss: 0.03984 - R2: 0.9996 -- iter: 0576/1168\n",
      "Training Step: 409  | total loss: \u001b[1m\u001b[32m0.03729\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 022 | loss: 0.03729 - R2: 0.9992 -- iter: 0640/1168\n",
      "Training Step: 410  | total loss: \u001b[1m\u001b[32m0.03388\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 022 | loss: 0.03388 - R2: 0.9990 -- iter: 0704/1168\n",
      "Training Step: 411  | total loss: \u001b[1m\u001b[32m0.03388\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 022 | loss: 0.03388 - R2: 0.9990 -- iter: 0768/1168\n",
      "Training Step: 412  | total loss: \u001b[1m\u001b[32m0.03332\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 022 | loss: 0.03332 - R2: 0.9994 -- iter: 0832/1168\n",
      "Training Step: 413  | total loss: \u001b[1m\u001b[32m0.03321\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 022 | loss: 0.03321 - R2: 1.0002 -- iter: 0896/1168\n",
      "Training Step: 414  | total loss: \u001b[1m\u001b[32m0.03274\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 022 | loss: 0.03274 - R2: 1.0002 -- iter: 0960/1168\n",
      "Training Step: 415  | total loss: \u001b[1m\u001b[32m0.03111\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 022 | loss: 0.03111 - R2: 1.0001 -- iter: 1024/1168\n",
      "Training Step: 416  | total loss: \u001b[1m\u001b[32m0.03228\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 022 | loss: 0.03228 - R2: 1.0001 -- iter: 1088/1168\n",
      "Training Step: 417  | total loss: \u001b[1m\u001b[32m0.03104\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 022 | loss: 0.03104 - R2: 1.0001 -- iter: 1152/1168\n",
      "Training Step: 418  | total loss: \u001b[1m\u001b[32m0.02940\u001b[0m\u001b[0m | time: 1.112s\n",
      "| SGD | epoch: 022 | loss: 0.02940 - R2: 0.9998 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 419  | total loss: \u001b[1m\u001b[32m0.02868\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 023 | loss: 0.02868 - R2: 0.9998 -- iter: 0064/1168\n",
      "Training Step: 420  | total loss: \u001b[1m\u001b[32m0.02746\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 023 | loss: 0.02746 - R2: 1.0011 -- iter: 0128/1168\n",
      "Training Step: 421  | total loss: \u001b[1m\u001b[32m0.02625\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 023 | loss: 0.02625 - R2: 1.0009 -- iter: 0192/1168\n",
      "Training Step: 422  | total loss: \u001b[1m\u001b[32m0.02494\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 023 | loss: 0.02494 - R2: 1.0008 -- iter: 0256/1168\n",
      "Training Step: 423  | total loss: \u001b[1m\u001b[32m0.02494\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 023 | loss: 0.02494 - R2: 1.0008 -- iter: 0320/1168\n",
      "Training Step: 424  | total loss: \u001b[1m\u001b[32m0.02515\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 023 | loss: 0.02515 - R2: 1.0009 -- iter: 0384/1168\n",
      "Training Step: 425  | total loss: \u001b[1m\u001b[32m0.02403\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 023 | loss: 0.02403 - R2: 1.0000 -- iter: 0448/1168\n",
      "Training Step: 426  | total loss: \u001b[1m\u001b[32m0.02350\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 023 | loss: 0.02350 - R2: 1.0001 -- iter: 0512/1168\n",
      "Training Step: 427  | total loss: \u001b[1m\u001b[32m0.02254\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 023 | loss: 0.02254 - R2: 1.0001 -- iter: 0576/1168\n",
      "Training Step: 428  | total loss: \u001b[1m\u001b[32m0.04366\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 023 | loss: 0.04366 - R2: 0.9997 -- iter: 0640/1168\n",
      "Training Step: 429  | total loss: \u001b[1m\u001b[32m0.04366\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 023 | loss: 0.04366 - R2: 0.9997 -- iter: 0704/1168\n",
      "Training Step: 430  | total loss: \u001b[1m\u001b[32m0.03916\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 023 | loss: 0.03916 - R2: 0.9994 -- iter: 0768/1168\n",
      "Training Step: 431  | total loss: \u001b[1m\u001b[32m0.03916\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 023 | loss: 0.03916 - R2: 0.9994 -- iter: 0832/1168\n",
      "Training Step: 432  | total loss: \u001b[1m\u001b[32m0.03702\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 023 | loss: 0.03702 - R2: 0.9992 -- iter: 0896/1168\n",
      "Training Step: 433  | total loss: \u001b[1m\u001b[32m0.03268\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 023 | loss: 0.03268 - R2: 0.9992 -- iter: 0960/1168\n",
      "Training Step: 434  | total loss: \u001b[1m\u001b[32m0.03268\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 023 | loss: 0.03268 - R2: 0.9992 -- iter: 1024/1168\n",
      "Training Step: 435  | total loss: \u001b[1m\u001b[32m0.03228\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 023 | loss: 0.03228 - R2: 0.9996 -- iter: 1088/1168\n",
      "Training Step: 436  | total loss: \u001b[1m\u001b[32m0.02935\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 023 | loss: 0.02935 - R2: 0.9993 -- iter: 1152/1168\n",
      "Training Step: 437  | total loss: \u001b[1m\u001b[32m0.02935\u001b[0m\u001b[0m | time: 1.137s\n",
      "| SGD | epoch: 023 | loss: 0.02935 - R2: 0.9991 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 438  | total loss: \u001b[1m\u001b[32m0.02687\u001b[0m\u001b[0m | time: 0.006s\n",
      "| SGD | epoch: 024 | loss: 0.02687 - R2: 0.9993 -- iter: 0064/1168\n",
      "Training Step: 439  | total loss: \u001b[1m\u001b[32m0.02870\u001b[0m\u001b[0m | time: 0.009s\n",
      "| SGD | epoch: 024 | loss: 0.02870 - R2: 0.9993 -- iter: 0128/1168\n",
      "Training Step: 440  | total loss: \u001b[1m\u001b[32m0.02870\u001b[0m\u001b[0m | time: 0.013s\n",
      "| SGD | epoch: 024 | loss: 0.02870 - R2: 0.9996 -- iter: 0192/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 441  | total loss: \u001b[1m\u001b[32m0.02970\u001b[0m\u001b[0m | time: 0.017s\n",
      "| SGD | epoch: 024 | loss: 0.02970 - R2: 0.9997 -- iter: 0256/1168\n",
      "Training Step: 442  | total loss: \u001b[1m\u001b[32m0.02885\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 024 | loss: 0.02885 - R2: 0.9993 -- iter: 0320/1168\n",
      "Training Step: 443  | total loss: \u001b[1m\u001b[32m0.02701\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 024 | loss: 0.02701 - R2: 0.9991 -- iter: 0384/1168\n",
      "Training Step: 444  | total loss: \u001b[1m\u001b[32m0.02867\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 024 | loss: 0.02867 - R2: 0.9998 -- iter: 0448/1168\n",
      "Training Step: 445  | total loss: \u001b[1m\u001b[32m0.03022\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 024 | loss: 0.03022 - R2: 1.0003 -- iter: 0512/1168\n",
      "Training Step: 446  | total loss: \u001b[1m\u001b[32m0.02933\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 024 | loss: 0.02933 - R2: 1.0004 -- iter: 0576/1168\n",
      "Training Step: 447  | total loss: \u001b[1m\u001b[32m0.02933\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 024 | loss: 0.02933 - R2: 0.9990 -- iter: 0640/1168\n",
      "Training Step: 448  | total loss: \u001b[1m\u001b[32m0.04718\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 024 | loss: 0.04718 - R2: 0.9991 -- iter: 0704/1168\n",
      "Training Step: 449  | total loss: \u001b[1m\u001b[32m0.04428\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 024 | loss: 0.04428 - R2: 0.9989 -- iter: 0768/1168\n",
      "Training Step: 450  | total loss: \u001b[1m\u001b[32m0.04428\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 024 | loss: 0.04428 - R2: 0.9989 -- iter: 0832/1168\n",
      "Training Step: 451  | total loss: \u001b[1m\u001b[32m0.03881\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 024 | loss: 0.03881 - R2: 0.9988 -- iter: 0896/1168\n",
      "Training Step: 452  | total loss: \u001b[1m\u001b[32m0.03761\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 024 | loss: 0.03761 - R2: 0.9993 -- iter: 0960/1168\n",
      "Training Step: 453  | total loss: \u001b[1m\u001b[32m0.03761\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 024 | loss: 0.03761 - R2: 0.9994 -- iter: 1024/1168\n",
      "Training Step: 454  | total loss: \u001b[1m\u001b[32m0.03378\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 024 | loss: 0.03378 - R2: 0.9992 -- iter: 1088/1168\n",
      "Training Step: 455  | total loss: \u001b[1m\u001b[32m0.03253\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 024 | loss: 0.03253 - R2: 0.9989 -- iter: 1152/1168\n",
      "Training Step: 456  | total loss: \u001b[1m\u001b[32m0.03186\u001b[0m\u001b[0m | time: 1.070s\n",
      "| SGD | epoch: 024 | loss: 0.03186 - R2: 0.9993 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 457  | total loss: \u001b[1m\u001b[32m0.03145\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 025 | loss: 0.03145 - R2: 0.9995 -- iter: 0064/1168\n",
      "Training Step: 458  | total loss: \u001b[1m\u001b[32m0.02959\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 025 | loss: 0.02959 - R2: 0.9994 -- iter: 0128/1168\n",
      "Training Step: 459  | total loss: \u001b[1m\u001b[32m0.02959\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 025 | loss: 0.02959 - R2: 0.9994 -- iter: 0192/1168\n",
      "Training Step: 460  | total loss: \u001b[1m\u001b[32m0.03113\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 025 | loss: 0.03113 - R2: 0.9986 -- iter: 0256/1168\n",
      "Training Step: 461  | total loss: \u001b[1m\u001b[32m0.03252\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 025 | loss: 0.03252 - R2: 0.9978 -- iter: 0320/1168\n",
      "Training Step: 462  | total loss: \u001b[1m\u001b[32m0.03091\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 025 | loss: 0.03091 - R2: 0.9979 -- iter: 0384/1168\n",
      "Training Step: 463  | total loss: \u001b[1m\u001b[32m0.02862\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 025 | loss: 0.02862 - R2: 0.9979 -- iter: 0448/1168\n",
      "Training Step: 464  | total loss: \u001b[1m\u001b[32m0.02660\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 025 | loss: 0.02660 - R2: 0.9980 -- iter: 0512/1168\n",
      "Training Step: 465  | total loss: \u001b[1m\u001b[32m0.02660\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 025 | loss: 0.02660 - R2: 0.9980 -- iter: 0576/1168\n",
      "Training Step: 466  | total loss: \u001b[1m\u001b[32m0.02579\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 025 | loss: 0.02579 - R2: 0.9984 -- iter: 0640/1168\n",
      "Training Step: 467  | total loss: \u001b[1m\u001b[32m0.02718\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 025 | loss: 0.02718 - R2: 0.9990 -- iter: 0704/1168\n",
      "Training Step: 468  | total loss: \u001b[1m\u001b[32m0.05699\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 025 | loss: 0.05699 - R2: 1.0022 -- iter: 0768/1168\n",
      "Training Step: 469  | total loss: \u001b[1m\u001b[32m0.05409\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 025 | loss: 0.05409 - R2: 1.0023 -- iter: 0832/1168\n",
      "Training Step: 470  | total loss: \u001b[1m\u001b[32m0.05060\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 025 | loss: 0.05060 - R2: 1.0022 -- iter: 0896/1168\n",
      "Training Step: 471  | total loss: \u001b[1m\u001b[32m0.04343\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 025 | loss: 0.04343 - R2: 1.0007 -- iter: 0960/1168\n",
      "Training Step: 472  | total loss: \u001b[1m\u001b[32m0.04102\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 025 | loss: 0.04102 - R2: 1.0005 -- iter: 1024/1168\n",
      "Training Step: 473  | total loss: \u001b[1m\u001b[32m0.03803\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 025 | loss: 0.03803 - R2: 1.0003 -- iter: 1088/1168\n",
      "Training Step: 474  | total loss: \u001b[1m\u001b[32m0.03577\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 025 | loss: 0.03577 - R2: 1.0001 -- iter: 1152/1168\n",
      "Training Step: 475  | total loss: \u001b[1m\u001b[32m0.03393\u001b[0m\u001b[0m | time: 1.146s\n",
      "| SGD | epoch: 025 | loss: 0.03393 - R2: 1.0002 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 476  | total loss: \u001b[1m\u001b[32m0.03276\u001b[0m\u001b[0m | time: 0.007s\n",
      "| SGD | epoch: 026 | loss: 0.03276 - R2: 1.0002 -- iter: 0064/1168\n",
      "Training Step: 477  | total loss: \u001b[1m\u001b[32m0.03276\u001b[0m\u001b[0m | time: 0.010s\n",
      "| SGD | epoch: 026 | loss: 0.03276 - R2: 0.9998 -- iter: 0128/1168\n",
      "Training Step: 478  | total loss: \u001b[1m\u001b[32m0.02947\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 026 | loss: 0.02947 - R2: 0.9994 -- iter: 0192/1168\n",
      "Training Step: 479  | total loss: \u001b[1m\u001b[32m0.02789\u001b[0m\u001b[0m | time: 0.020s\n",
      "| SGD | epoch: 026 | loss: 0.02789 - R2: 0.9993 -- iter: 0256/1168\n",
      "Training Step: 480  | total loss: \u001b[1m\u001b[32m0.02646\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 026 | loss: 0.02646 - R2: 0.9993 -- iter: 0320/1168\n",
      "Training Step: 481  | total loss: \u001b[1m\u001b[32m0.02545\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 026 | loss: 0.02545 - R2: 0.9992 -- iter: 0384/1168\n",
      "Training Step: 482  | total loss: \u001b[1m\u001b[32m0.02540\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 026 | loss: 0.02540 - R2: 0.9995 -- iter: 0448/1168\n",
      "Training Step: 483  | total loss: \u001b[1m\u001b[32m0.02540\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 026 | loss: 0.02540 - R2: 0.9995 -- iter: 0512/1168\n",
      "Training Step: 484  | total loss: \u001b[1m\u001b[32m0.02659\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 026 | loss: 0.02659 - R2: 0.9999 -- iter: 0576/1168\n",
      "Training Step: 485  | total loss: \u001b[1m\u001b[32m0.02659\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 026 | loss: 0.02659 - R2: 0.9999 -- iter: 0640/1168\n",
      "Training Step: 486  | total loss: \u001b[1m\u001b[32m0.02929\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 026 | loss: 0.02929 - R2: 1.0008 -- iter: 0704/1168\n",
      "Training Step: 487  | total loss: \u001b[1m\u001b[32m0.05883\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 026 | loss: 0.05883 - R2: 0.9982 -- iter: 0768/1168\n",
      "Training Step: 488  | total loss: \u001b[1m\u001b[32m0.05883\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 026 | loss: 0.05883 - R2: 0.9982 -- iter: 0832/1168\n",
      "Training Step: 489  | total loss: \u001b[1m\u001b[32m0.05297\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 026 | loss: 0.05297 - R2: 0.9990 -- iter: 0896/1168\n",
      "Training Step: 490  | total loss: \u001b[1m\u001b[32m0.05297\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 026 | loss: 0.05297 - R2: 0.9990 -- iter: 0960/1168\n",
      "Training Step: 491  | total loss: \u001b[1m\u001b[32m0.05090\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 026 | loss: 0.05090 - R2: 0.9993 -- iter: 1024/1168\n",
      "Training Step: 492  | total loss: \u001b[1m\u001b[32m0.05147\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 026 | loss: 0.05147 - R2: 1.0000 -- iter: 1088/1168\n",
      "Training Step: 493  | total loss: \u001b[1m\u001b[32m0.04463\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 026 | loss: 0.04463 - R2: 0.9996 -- iter: 1152/1168\n",
      "Training Step: 494  | total loss: \u001b[1m\u001b[32m0.04463\u001b[0m\u001b[0m | time: 1.159s\n",
      "| SGD | epoch: 026 | loss: 0.04463 - R2: 0.9996 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 495  | total loss: \u001b[1m\u001b[32m0.04025\u001b[0m\u001b[0m | time: 0.018s\n",
      "| SGD | epoch: 027 | loss: 0.04025 - R2: 0.9996 -- iter: 0064/1168\n",
      "Training Step: 496  | total loss: \u001b[1m\u001b[32m0.04025\u001b[0m\u001b[0m | time: 0.020s\n",
      "| SGD | epoch: 027 | loss: 0.04025 - R2: 0.9996 -- iter: 0128/1168\n",
      "Training Step: 497  | total loss: \u001b[1m\u001b[32m0.03803\u001b[0m\u001b[0m | time: 0.026s\n",
      "| SGD | epoch: 027 | loss: 0.03803 - R2: 0.9998 -- iter: 0192/1168\n",
      "Training Step: 498  | total loss: \u001b[1m\u001b[32m0.03545\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 027 | loss: 0.03545 - R2: 0.9992 -- iter: 0256/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 499  | total loss: \u001b[1m\u001b[32m0.03319\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 027 | loss: 0.03319 - R2: 0.9987 -- iter: 0320/1168\n",
      "Training Step: 500  | total loss: \u001b[1m\u001b[32m0.03259\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 027 | loss: 0.03259 - R2: 0.9983 -- iter: 0384/1168\n",
      "Training Step: 501  | total loss: \u001b[1m\u001b[32m0.03259\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 027 | loss: 0.03259 - R2: 0.9983 -- iter: 0448/1168\n",
      "Training Step: 502  | total loss: \u001b[1m\u001b[32m0.03112\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 027 | loss: 0.03112 - R2: 0.9986 -- iter: 0512/1168\n",
      "Training Step: 503  | total loss: \u001b[1m\u001b[32m0.02903\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 027 | loss: 0.02903 - R2: 0.9985 -- iter: 0576/1168\n",
      "Training Step: 504  | total loss: \u001b[1m\u001b[32m0.02815\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 027 | loss: 0.02815 - R2: 0.9986 -- iter: 0640/1168\n",
      "Training Step: 505  | total loss: \u001b[1m\u001b[32m0.02803\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 027 | loss: 0.02803 - R2: 0.9988 -- iter: 0704/1168\n",
      "Training Step: 506  | total loss: \u001b[1m\u001b[32m0.02694\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 027 | loss: 0.02694 - R2: 0.9988 -- iter: 0768/1168\n",
      "Training Step: 507  | total loss: \u001b[1m\u001b[32m0.02588\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 027 | loss: 0.02588 - R2: 1.0004 -- iter: 0832/1168\n",
      "Training Step: 508  | total loss: \u001b[1m\u001b[32m0.05278\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 027 | loss: 0.05278 - R2: 1.0001 -- iter: 0896/1168\n",
      "Training Step: 509  | total loss: \u001b[1m\u001b[32m0.05278\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 027 | loss: 0.05278 - R2: 1.0001 -- iter: 0960/1168\n",
      "Training Step: 510  | total loss: \u001b[1m\u001b[32m0.04985\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 027 | loss: 0.04985 - R2: 0.9998 -- iter: 1024/1168\n",
      "Training Step: 511  | total loss: \u001b[1m\u001b[32m0.04376\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 027 | loss: 0.04376 - R2: 0.9991 -- iter: 1088/1168\n",
      "Training Step: 512  | total loss: \u001b[1m\u001b[32m0.04152\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 027 | loss: 0.04152 - R2: 0.9997 -- iter: 1152/1168\n",
      "Training Step: 513  | total loss: \u001b[1m\u001b[32m0.03975\u001b[0m\u001b[0m | time: 1.110s\n",
      "| SGD | epoch: 027 | loss: 0.03975 - R2: 0.9997 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 514  | total loss: \u001b[1m\u001b[32m0.03975\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 028 | loss: 0.03975 - R2: 0.9997 -- iter: 0064/1168\n",
      "Training Step: 515  | total loss: \u001b[1m\u001b[32m0.03668\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 028 | loss: 0.03668 - R2: 1.0002 -- iter: 0128/1168\n",
      "Training Step: 516  | total loss: \u001b[1m\u001b[32m0.03668\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 028 | loss: 0.03668 - R2: 1.0002 -- iter: 0192/1168\n",
      "Training Step: 517  | total loss: \u001b[1m\u001b[32m0.03435\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 028 | loss: 0.03435 - R2: 1.0002 -- iter: 0256/1168\n",
      "Training Step: 518  | total loss: \u001b[1m\u001b[32m0.03251\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 028 | loss: 0.03251 - R2: 0.9998 -- iter: 0320/1168\n",
      "Training Step: 519  | total loss: \u001b[1m\u001b[32m0.03081\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 028 | loss: 0.03081 - R2: 0.9992 -- iter: 0384/1168\n",
      "Training Step: 520  | total loss: \u001b[1m\u001b[32m0.03081\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 028 | loss: 0.03081 - R2: 0.9992 -- iter: 0448/1168\n",
      "Training Step: 521  | total loss: \u001b[1m\u001b[32m0.02928\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 028 | loss: 0.02928 - R2: 0.9987 -- iter: 0512/1168\n",
      "Training Step: 522  | total loss: \u001b[1m\u001b[32m0.02829\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 028 | loss: 0.02829 - R2: 0.9987 -- iter: 0576/1168\n",
      "Training Step: 523  | total loss: \u001b[1m\u001b[32m0.02646\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 028 | loss: 0.02646 - R2: 0.9985 -- iter: 0640/1168\n",
      "Training Step: 524  | total loss: \u001b[1m\u001b[32m0.02646\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 028 | loss: 0.02646 - R2: 0.9985 -- iter: 0704/1168\n",
      "Training Step: 525  | total loss: \u001b[1m\u001b[32m0.02741\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 028 | loss: 0.02741 - R2: 0.9994 -- iter: 0768/1168\n",
      "Training Step: 526  | total loss: \u001b[1m\u001b[32m0.02741\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 028 | loss: 0.02741 - R2: 0.9994 -- iter: 0832/1168\n",
      "Training Step: 527  | total loss: \u001b[1m\u001b[32m0.02628\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 028 | loss: 0.02628 - R2: 0.9995 -- iter: 0896/1168\n",
      "Training Step: 528  | total loss: \u001b[1m\u001b[32m0.06651\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 028 | loss: 0.06651 - R2: 0.9975 -- iter: 0960/1168\n",
      "Training Step: 529  | total loss: \u001b[1m\u001b[32m0.05719\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 028 | loss: 0.05719 - R2: 0.9980 -- iter: 1024/1168\n",
      "Training Step: 530  | total loss: \u001b[1m\u001b[32m0.05719\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 028 | loss: 0.05719 - R2: 0.9980 -- iter: 1088/1168\n",
      "Training Step: 531  | total loss: \u001b[1m\u001b[32m0.05412\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 028 | loss: 0.05412 - R2: 0.9984 -- iter: 1152/1168\n",
      "Training Step: 532  | total loss: \u001b[1m\u001b[32m0.05225\u001b[0m\u001b[0m | time: 1.124s\n",
      "| SGD | epoch: 028 | loss: 0.05225 - R2: 0.9991 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 533  | total loss: \u001b[1m\u001b[32m0.04900\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 029 | loss: 0.04900 - R2: 0.9990 -- iter: 0064/1168\n",
      "Training Step: 534  | total loss: \u001b[1m\u001b[32m0.04695\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 029 | loss: 0.04695 - R2: 0.9995 -- iter: 0128/1168\n",
      "Training Step: 535  | total loss: \u001b[1m\u001b[32m0.04362\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 029 | loss: 0.04362 - R2: 0.9989 -- iter: 0192/1168\n",
      "Training Step: 536  | total loss: \u001b[1m\u001b[32m0.04050\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 029 | loss: 0.04050 - R2: 0.9993 -- iter: 0256/1168\n",
      "Training Step: 537  | total loss: \u001b[1m\u001b[32m0.03931\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 029 | loss: 0.03931 - R2: 0.9992 -- iter: 0320/1168\n",
      "Training Step: 538  | total loss: \u001b[1m\u001b[32m0.03795\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 029 | loss: 0.03795 - R2: 0.9993 -- iter: 0384/1168\n",
      "Training Step: 539  | total loss: \u001b[1m\u001b[32m0.03537\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 029 | loss: 0.03537 - R2: 0.9994 -- iter: 0448/1168\n",
      "Training Step: 540  | total loss: \u001b[1m\u001b[32m0.03304\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 029 | loss: 0.03304 - R2: 0.9995 -- iter: 0512/1168\n",
      "Training Step: 541  | total loss: \u001b[1m\u001b[32m0.03304\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 029 | loss: 0.03304 - R2: 0.9994 -- iter: 0576/1168\n",
      "Training Step: 542  | total loss: \u001b[1m\u001b[32m0.02932\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 029 | loss: 0.02932 - R2: 0.9995 -- iter: 0640/1168\n",
      "Training Step: 543  | total loss: \u001b[1m\u001b[32m0.02871\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 029 | loss: 0.02871 - R2: 0.9999 -- iter: 0704/1168\n",
      "Training Step: 544  | total loss: \u001b[1m\u001b[32m0.02823\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 029 | loss: 0.02823 - R2: 0.9999 -- iter: 0768/1168\n",
      "Training Step: 545  | total loss: \u001b[1m\u001b[32m0.02706\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 029 | loss: 0.02706 - R2: 0.9996 -- iter: 0832/1168\n",
      "Training Step: 546  | total loss: \u001b[1m\u001b[32m0.02706\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 029 | loss: 0.02706 - R2: 0.9996 -- iter: 0896/1168\n",
      "Training Step: 547  | total loss: \u001b[1m\u001b[32m0.02601\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 029 | loss: 0.02601 - R2: 1.0014 -- iter: 0960/1168\n",
      "Training Step: 548  | total loss: \u001b[1m\u001b[32m0.05466\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 029 | loss: 0.05466 - R2: 1.0014 -- iter: 1024/1168\n",
      "Training Step: 549  | total loss: \u001b[1m\u001b[32m0.05188\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 029 | loss: 0.05188 - R2: 1.0015 -- iter: 1088/1168\n",
      "Training Step: 550  | total loss: \u001b[1m\u001b[32m0.04422\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 029 | loss: 0.04422 - R2: 1.0012 -- iter: 1152/1168\n",
      "Training Step: 551  | total loss: \u001b[1m\u001b[32m0.04422\u001b[0m\u001b[0m | time: 1.118s\n",
      "| SGD | epoch: 029 | loss: 0.04422 - R2: 1.0012 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 552  | total loss: \u001b[1m\u001b[32m0.04084\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 030 | loss: 0.04084 - R2: 1.0014 -- iter: 0064/1168\n",
      "Training Step: 553  | total loss: \u001b[1m\u001b[32m0.04070\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 030 | loss: 0.04070 - R2: 1.0014 -- iter: 0128/1168\n",
      "Training Step: 554  | total loss: \u001b[1m\u001b[32m0.04070\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 030 | loss: 0.04070 - R2: 1.0014 -- iter: 0192/1168\n",
      "Training Step: 555  | total loss: \u001b[1m\u001b[32m0.03837\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 030 | loss: 0.03837 - R2: 1.0008 -- iter: 0256/1168\n",
      "Training Step: 556  | total loss: \u001b[1m\u001b[32m0.03683\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 030 | loss: 0.03683 - R2: 1.0004 -- iter: 0320/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 557  | total loss: \u001b[1m\u001b[32m0.03503\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 030 | loss: 0.03503 - R2: 1.0005 -- iter: 0384/1168\n",
      "Training Step: 558  | total loss: \u001b[1m\u001b[32m0.03517\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 030 | loss: 0.03517 - R2: 1.0005 -- iter: 0448/1168\n",
      "Training Step: 559  | total loss: \u001b[1m\u001b[32m0.03335\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 030 | loss: 0.03335 - R2: 1.0002 -- iter: 0512/1168\n",
      "Training Step: 560  | total loss: \u001b[1m\u001b[32m0.03132\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 030 | loss: 0.03132 - R2: 0.9996 -- iter: 0576/1168\n",
      "Training Step: 561  | total loss: \u001b[1m\u001b[32m0.02948\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 030 | loss: 0.02948 - R2: 0.9991 -- iter: 0640/1168\n",
      "Training Step: 562  | total loss: \u001b[1m\u001b[32m0.02799\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 030 | loss: 0.02799 - R2: 0.9988 -- iter: 0704/1168\n",
      "Training Step: 563  | total loss: \u001b[1m\u001b[32m0.02478\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 030 | loss: 0.02478 - R2: 0.9985 -- iter: 0768/1168\n",
      "Training Step: 564  | total loss: \u001b[1m\u001b[32m0.02478\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 030 | loss: 0.02478 - R2: 0.9985 -- iter: 0832/1168\n",
      "Training Step: 565  | total loss: \u001b[1m\u001b[32m0.02419\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 030 | loss: 0.02419 - R2: 0.9982 -- iter: 0896/1168\n",
      "Training Step: 566  | total loss: \u001b[1m\u001b[32m0.02329\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 030 | loss: 0.02329 - R2: 0.9986 -- iter: 0960/1168\n",
      "Training Step: 567  | total loss: \u001b[1m\u001b[32m0.02509\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 030 | loss: 0.02509 - R2: 0.9988 -- iter: 1024/1168\n",
      "Training Step: 568  | total loss: \u001b[1m\u001b[32m0.05132\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 030 | loss: 0.05132 - R2: 0.9973 -- iter: 1088/1168\n",
      "Training Step: 569  | total loss: \u001b[1m\u001b[32m0.04851\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 030 | loss: 0.04851 - R2: 0.9976 -- iter: 1152/1168\n",
      "Training Step: 570  | total loss: \u001b[1m\u001b[32m0.04254\u001b[0m\u001b[0m | time: 1.179s\n",
      "| SGD | epoch: 030 | loss: 0.04254 - R2: 0.9979 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 571  | total loss: \u001b[1m\u001b[32m0.04254\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 031 | loss: 0.04254 - R2: 0.9979 -- iter: 0064/1168\n",
      "Training Step: 572  | total loss: \u001b[1m\u001b[32m0.03992\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 031 | loss: 0.03992 - R2: 0.9981 -- iter: 0128/1168\n",
      "Training Step: 573  | total loss: \u001b[1m\u001b[32m0.03833\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 031 | loss: 0.03833 - R2: 0.9981 -- iter: 0192/1168\n",
      "Training Step: 574  | total loss: \u001b[1m\u001b[32m0.03706\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 031 | loss: 0.03706 - R2: 0.9979 -- iter: 0256/1168\n",
      "Training Step: 575  | total loss: \u001b[1m\u001b[32m0.03554\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 031 | loss: 0.03554 - R2: 0.9982 -- iter: 0320/1168\n",
      "Training Step: 576  | total loss: \u001b[1m\u001b[32m0.03470\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 031 | loss: 0.03470 - R2: 0.9980 -- iter: 0384/1168\n",
      "Training Step: 577  | total loss: \u001b[1m\u001b[32m0.03466\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 031 | loss: 0.03466 - R2: 0.9986 -- iter: 0448/1168\n",
      "Training Step: 578  | total loss: \u001b[1m\u001b[32m0.03306\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 031 | loss: 0.03306 - R2: 0.9989 -- iter: 0512/1168\n",
      "Training Step: 579  | total loss: \u001b[1m\u001b[32m0.03306\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 031 | loss: 0.03306 - R2: 0.9989 -- iter: 0576/1168\n",
      "Training Step: 580  | total loss: \u001b[1m\u001b[32m0.03065\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 031 | loss: 0.03065 - R2: 0.9994 -- iter: 0640/1168\n",
      "Training Step: 581  | total loss: \u001b[1m\u001b[32m0.02848\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 031 | loss: 0.02848 - R2: 0.9998 -- iter: 0704/1168\n",
      "Training Step: 582  | total loss: \u001b[1m\u001b[32m0.02731\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 031 | loss: 0.02731 - R2: 0.9999 -- iter: 0768/1168\n",
      "Training Step: 583  | total loss: \u001b[1m\u001b[32m0.02649\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 031 | loss: 0.02649 - R2: 0.9996 -- iter: 0832/1168\n",
      "Training Step: 584  | total loss: \u001b[1m\u001b[32m0.02593\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 031 | loss: 0.02593 - R2: 0.9993 -- iter: 0896/1168\n",
      "Training Step: 585  | total loss: \u001b[1m\u001b[32m0.02409\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 031 | loss: 0.02409 - R2: 0.9995 -- iter: 0960/1168\n",
      "Training Step: 586  | total loss: \u001b[1m\u001b[32m0.02600\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 031 | loss: 0.02600 - R2: 0.9999 -- iter: 1024/1168\n",
      "Training Step: 587  | total loss: \u001b[1m\u001b[32m0.02462\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 031 | loss: 0.02462 - R2: 0.9998 -- iter: 1088/1168\n",
      "Training Step: 588  | total loss: \u001b[1m\u001b[32m0.02343\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 031 | loss: 0.02343 - R2: 1.0000 -- iter: 1152/1168\n",
      "Training Step: 589  | total loss: \u001b[1m\u001b[32m0.02199\u001b[0m\u001b[0m | time: 1.173s\n",
      "| SGD | epoch: 031 | loss: 0.02199 - R2: 0.9997 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 590  | total loss: \u001b[1m\u001b[32m0.02143\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 032 | loss: 0.02143 - R2: 0.9998 -- iter: 0064/1168\n",
      "Training Step: 591  | total loss: \u001b[1m\u001b[32m0.02143\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 032 | loss: 0.02143 - R2: 0.9998 -- iter: 0128/1168\n",
      "Training Step: 592  | total loss: \u001b[1m\u001b[32m0.02099\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 032 | loss: 0.02099 - R2: 0.9999 -- iter: 0192/1168\n",
      "Training Step: 593  | total loss: \u001b[1m\u001b[32m0.02097\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 032 | loss: 0.02097 - R2: 1.0000 -- iter: 0256/1168\n",
      "Training Step: 594  | total loss: \u001b[1m\u001b[32m0.02137\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 032 | loss: 0.02137 - R2: 1.0003 -- iter: 0320/1168\n",
      "Training Step: 595  | total loss: \u001b[1m\u001b[32m0.02137\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 032 | loss: 0.02137 - R2: 1.0003 -- iter: 0384/1168\n",
      "Training Step: 596  | total loss: \u001b[1m\u001b[32m0.02260\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 032 | loss: 0.02260 - R2: 1.0001 -- iter: 0448/1168\n",
      "Training Step: 597  | total loss: \u001b[1m\u001b[32m0.02260\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 032 | loss: 0.02260 - R2: 1.0001 -- iter: 0512/1168\n",
      "Training Step: 598  | total loss: \u001b[1m\u001b[32m0.02358\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 032 | loss: 0.02358 - R2: 1.0004 -- iter: 0576/1168\n",
      "Training Step: 599  | total loss: \u001b[1m\u001b[32m0.02308\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 032 | loss: 0.02308 - R2: 1.0002 -- iter: 0640/1168\n",
      "Training Step: 600  | total loss: \u001b[1m\u001b[32m0.02113\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 032 | loss: 0.02113 - R2: 1.0001 -- iter: 0704/1168\n",
      "Training Step: 601  | total loss: \u001b[1m\u001b[32m0.01936\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 032 | loss: 0.01936 - R2: 1.0001 -- iter: 0768/1168\n",
      "Training Step: 602  | total loss: \u001b[1m\u001b[32m0.01942\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 032 | loss: 0.01942 - R2: 0.9996 -- iter: 0832/1168\n",
      "Training Step: 603  | total loss: \u001b[1m\u001b[32m0.02166\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 032 | loss: 0.02166 - R2: 1.0000 -- iter: 0896/1168\n",
      "Training Step: 604  | total loss: \u001b[1m\u001b[32m0.02166\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 032 | loss: 0.02166 - R2: 1.0000 -- iter: 0960/1168\n",
      "Training Step: 605  | total loss: \u001b[1m\u001b[32m0.02210\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 032 | loss: 0.02210 - R2: 0.9998 -- iter: 1024/1168\n",
      "Training Step: 606  | total loss: \u001b[1m\u001b[32m0.02136\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 032 | loss: 0.02136 - R2: 0.9994 -- iter: 1088/1168\n",
      "Training Step: 607  | total loss: \u001b[1m\u001b[32m0.02136\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 032 | loss: 0.02136 - R2: 0.9994 -- iter: 1152/1168\n",
      "Training Step: 608  | total loss: \u001b[1m\u001b[32m0.04832\u001b[0m\u001b[0m | time: 1.140s\n",
      "| SGD | epoch: 032 | loss: 0.04832 - R2: 0.9979 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 609  | total loss: \u001b[1m\u001b[32m0.04570\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 033 | loss: 0.04570 - R2: 0.9983 -- iter: 0064/1168\n",
      "Training Step: 610  | total loss: \u001b[1m\u001b[32m0.04236\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 033 | loss: 0.04236 - R2: 0.9985 -- iter: 0128/1168\n",
      "Training Step: 611  | total loss: \u001b[1m\u001b[32m0.04021\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 033 | loss: 0.04021 - R2: 0.9988 -- iter: 0192/1168\n",
      "Training Step: 612  | total loss: \u001b[1m\u001b[32m0.04021\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 033 | loss: 0.04021 - R2: 0.9987 -- iter: 0256/1168\n",
      "Training Step: 613  | total loss: \u001b[1m\u001b[32m0.03593\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 033 | loss: 0.03593 - R2: 0.9990 -- iter: 0320/1168\n",
      "Training Step: 614  | total loss: \u001b[1m\u001b[32m0.03593\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 033 | loss: 0.03593 - R2: 0.9990 -- iter: 0384/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 615  | total loss: \u001b[1m\u001b[32m0.03464\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 033 | loss: 0.03464 - R2: 0.9994 -- iter: 0448/1168\n",
      "Training Step: 616  | total loss: \u001b[1m\u001b[32m0.03282\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 033 | loss: 0.03282 - R2: 0.9991 -- iter: 0512/1168\n",
      "Training Step: 617  | total loss: \u001b[1m\u001b[32m0.03057\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 033 | loss: 0.03057 - R2: 0.9995 -- iter: 0576/1168\n",
      "Training Step: 618  | total loss: \u001b[1m\u001b[32m0.02881\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 033 | loss: 0.02881 - R2: 0.9994 -- iter: 0640/1168\n",
      "Training Step: 619  | total loss: \u001b[1m\u001b[32m0.02881\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 033 | loss: 0.02881 - R2: 0.9994 -- iter: 0704/1168\n",
      "Training Step: 620  | total loss: \u001b[1m\u001b[32m0.02508\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 033 | loss: 0.02508 - R2: 0.9999 -- iter: 0768/1168\n",
      "Training Step: 621  | total loss: \u001b[1m\u001b[32m0.02519\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 033 | loss: 0.02519 - R2: 0.9999 -- iter: 0832/1168\n",
      "Training Step: 622  | total loss: \u001b[1m\u001b[32m0.02519\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 033 | loss: 0.02519 - R2: 0.9999 -- iter: 0896/1168\n",
      "Training Step: 623  | total loss: \u001b[1m\u001b[32m0.02431\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 033 | loss: 0.02431 - R2: 1.0001 -- iter: 0960/1168\n",
      "Training Step: 624  | total loss: \u001b[1m\u001b[32m0.02467\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 033 | loss: 0.02467 - R2: 1.0000 -- iter: 1024/1168\n",
      "Training Step: 625  | total loss: \u001b[1m\u001b[32m0.02652\u001b[0m\u001b[0m | time: 0.172s\n",
      "| SGD | epoch: 033 | loss: 0.02652 - R2: 1.0002 -- iter: 1088/1168\n",
      "Training Step: 626  | total loss: \u001b[1m\u001b[32m0.02545\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 033 | loss: 0.02545 - R2: 0.9994 -- iter: 1152/1168\n",
      "Training Step: 627  | total loss: \u001b[1m\u001b[32m0.02461\u001b[0m\u001b[0m | time: 1.182s\n",
      "| SGD | epoch: 033 | loss: 0.02461 - R2: 0.9991 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 628  | total loss: \u001b[1m\u001b[32m0.04261\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 034 | loss: 0.04261 - R2: 1.0001 -- iter: 0064/1168\n",
      "Training Step: 629  | total loss: \u001b[1m\u001b[32m0.04017\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 034 | loss: 0.04017 - R2: 1.0001 -- iter: 0128/1168\n",
      "Training Step: 630  | total loss: \u001b[1m\u001b[32m0.03809\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 034 | loss: 0.03809 - R2: 1.0007 -- iter: 0192/1168\n",
      "Training Step: 631  | total loss: \u001b[1m\u001b[32m0.03682\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 034 | loss: 0.03682 - R2: 1.0005 -- iter: 0256/1168\n",
      "Training Step: 632  | total loss: \u001b[1m\u001b[32m0.03682\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 034 | loss: 0.03682 - R2: 1.0005 -- iter: 0320/1168\n",
      "Training Step: 633  | total loss: \u001b[1m\u001b[32m0.03498\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 034 | loss: 0.03498 - R2: 1.0000 -- iter: 0384/1168\n",
      "Training Step: 634  | total loss: \u001b[1m\u001b[32m0.03411\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 034 | loss: 0.03411 - R2: 0.9994 -- iter: 0448/1168\n",
      "Training Step: 635  | total loss: \u001b[1m\u001b[32m0.03202\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 034 | loss: 0.03202 - R2: 0.9997 -- iter: 0512/1168\n",
      "Training Step: 636  | total loss: \u001b[1m\u001b[32m0.03067\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 034 | loss: 0.03067 - R2: 0.9994 -- iter: 0576/1168\n",
      "Training Step: 637  | total loss: \u001b[1m\u001b[32m0.02755\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 034 | loss: 0.02755 - R2: 0.9993 -- iter: 0640/1168\n",
      "Training Step: 638  | total loss: \u001b[1m\u001b[32m0.02755\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 034 | loss: 0.02755 - R2: 0.9993 -- iter: 0704/1168\n",
      "Training Step: 639  | total loss: \u001b[1m\u001b[32m0.02559\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 034 | loss: 0.02559 - R2: 0.9992 -- iter: 0768/1168\n",
      "Training Step: 640  | total loss: \u001b[1m\u001b[32m0.02459\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 034 | loss: 0.02459 - R2: 0.9993 -- iter: 0832/1168\n",
      "Training Step: 641  | total loss: \u001b[1m\u001b[32m0.02415\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 034 | loss: 0.02415 - R2: 0.9993 -- iter: 0896/1168\n",
      "Training Step: 642  | total loss: \u001b[1m\u001b[32m0.02415\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 034 | loss: 0.02415 - R2: 0.9993 -- iter: 0960/1168\n",
      "Training Step: 643  | total loss: \u001b[1m\u001b[32m0.02489\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 034 | loss: 0.02489 - R2: 0.9993 -- iter: 1024/1168\n",
      "Training Step: 644  | total loss: \u001b[1m\u001b[32m0.02562\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 034 | loss: 0.02562 - R2: 0.9999 -- iter: 1088/1168\n",
      "Training Step: 645  | total loss: \u001b[1m\u001b[32m0.02522\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 034 | loss: 0.02522 - R2: 0.9997 -- iter: 1152/1168\n",
      "Training Step: 646  | total loss: \u001b[1m\u001b[32m0.02450\u001b[0m\u001b[0m | time: 1.120s\n",
      "| SGD | epoch: 034 | loss: 0.02450 - R2: 1.0000 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 647  | total loss: \u001b[1m\u001b[32m0.05501\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 035 | loss: 0.05501 - R2: 0.9985 -- iter: 0064/1168\n",
      "Training Step: 648  | total loss: \u001b[1m\u001b[32m0.05253\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 035 | loss: 0.05253 - R2: 0.9991 -- iter: 0128/1168\n",
      "Training Step: 649  | total loss: \u001b[1m\u001b[32m0.05253\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 035 | loss: 0.05253 - R2: 0.9991 -- iter: 0192/1168\n",
      "Training Step: 650  | total loss: \u001b[1m\u001b[32m0.04884\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 035 | loss: 0.04884 - R2: 0.9993 -- iter: 0256/1168\n",
      "Training Step: 651  | total loss: \u001b[1m\u001b[32m0.04638\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 035 | loss: 0.04638 - R2: 0.9992 -- iter: 0320/1168\n",
      "Training Step: 652  | total loss: \u001b[1m\u001b[32m0.04367\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 035 | loss: 0.04367 - R2: 0.9991 -- iter: 0384/1168\n",
      "Training Step: 653  | total loss: \u001b[1m\u001b[32m0.04042\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 035 | loss: 0.04042 - R2: 0.9990 -- iter: 0448/1168\n",
      "Training Step: 654  | total loss: \u001b[1m\u001b[32m0.03889\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 035 | loss: 0.03889 - R2: 0.9991 -- iter: 0512/1168\n",
      "Training Step: 655  | total loss: \u001b[1m\u001b[32m0.03661\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 035 | loss: 0.03661 - R2: 0.9991 -- iter: 0576/1168\n",
      "Training Step: 656  | total loss: \u001b[1m\u001b[32m0.03472\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 035 | loss: 0.03472 - R2: 0.9992 -- iter: 0640/1168\n",
      "Training Step: 657  | total loss: \u001b[1m\u001b[32m0.03391\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 035 | loss: 0.03391 - R2: 0.9992 -- iter: 0704/1168\n",
      "Training Step: 658  | total loss: \u001b[1m\u001b[32m0.03438\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 035 | loss: 0.03438 - R2: 0.9998 -- iter: 0768/1168\n",
      "Training Step: 659  | total loss: \u001b[1m\u001b[32m0.03386\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 035 | loss: 0.03386 - R2: 1.0003 -- iter: 0832/1168\n",
      "Training Step: 660  | total loss: \u001b[1m\u001b[32m0.03259\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 035 | loss: 0.03259 - R2: 0.9998 -- iter: 0896/1168\n",
      "Training Step: 661  | total loss: \u001b[1m\u001b[32m0.03144\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 035 | loss: 0.03144 - R2: 0.9994 -- iter: 0960/1168\n",
      "Training Step: 662  | total loss: \u001b[1m\u001b[32m0.02938\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 035 | loss: 0.02938 - R2: 0.9996 -- iter: 1024/1168\n",
      "Training Step: 663  | total loss: \u001b[1m\u001b[32m0.02845\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 035 | loss: 0.02845 - R2: 0.9998 -- iter: 1088/1168\n",
      "Training Step: 664  | total loss: \u001b[1m\u001b[32m0.02705\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 035 | loss: 0.02705 - R2: 0.9995 -- iter: 1152/1168\n",
      "Training Step: 665  | total loss: \u001b[1m\u001b[32m0.02641\u001b[0m\u001b[0m | time: 1.153s\n",
      "| SGD | epoch: 035 | loss: 0.02641 - R2: 0.9990 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 666  | total loss: \u001b[1m\u001b[32m0.02612\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 036 | loss: 0.02612 - R2: 0.9989 -- iter: 0064/1168\n",
      "Training Step: 667  | total loss: \u001b[1m\u001b[32m0.02602\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 036 | loss: 0.02602 - R2: 0.9992 -- iter: 0128/1168\n",
      "Training Step: 668  | total loss: \u001b[1m\u001b[32m0.02564\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 036 | loss: 0.02564 - R2: 0.9985 -- iter: 0192/1168\n",
      "Training Step: 669  | total loss: \u001b[1m\u001b[32m0.02441\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 036 | loss: 0.02441 - R2: 0.9985 -- iter: 0256/1168\n",
      "Training Step: 670  | total loss: \u001b[1m\u001b[32m0.02328\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 036 | loss: 0.02328 - R2: 0.9985 -- iter: 0320/1168\n",
      "Training Step: 671  | total loss: \u001b[1m\u001b[32m0.02323\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 036 | loss: 0.02323 - R2: 0.9986 -- iter: 0384/1168\n",
      "Training Step: 672  | total loss: \u001b[1m\u001b[32m0.02238\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 036 | loss: 0.02238 - R2: 0.9987 -- iter: 0448/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 673  | total loss: \u001b[1m\u001b[32m0.02094\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 036 | loss: 0.02094 - R2: 0.9984 -- iter: 0512/1168\n",
      "Training Step: 674  | total loss: \u001b[1m\u001b[32m0.02094\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 036 | loss: 0.02094 - R2: 0.9994 -- iter: 0576/1168\n",
      "Training Step: 675  | total loss: \u001b[1m\u001b[32m0.02450\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 036 | loss: 0.02450 - R2: 0.9997 -- iter: 0640/1168\n",
      "Training Step: 676  | total loss: \u001b[1m\u001b[32m0.02406\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 036 | loss: 0.02406 - R2: 0.9997 -- iter: 0704/1168\n",
      "Training Step: 677  | total loss: \u001b[1m\u001b[32m0.02408\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 036 | loss: 0.02408 - R2: 0.9997 -- iter: 0768/1168\n",
      "Training Step: 678  | total loss: \u001b[1m\u001b[32m0.02444\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 036 | loss: 0.02444 - R2: 0.9997 -- iter: 0832/1168\n",
      "Training Step: 679  | total loss: \u001b[1m\u001b[32m0.02569\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 036 | loss: 0.02569 - R2: 1.0000 -- iter: 0896/1168\n",
      "Training Step: 680  | total loss: \u001b[1m\u001b[32m0.02400\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 036 | loss: 0.02400 - R2: 0.9994 -- iter: 0960/1168\n",
      "Training Step: 681  | total loss: \u001b[1m\u001b[32m0.02247\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 036 | loss: 0.02247 - R2: 0.9988 -- iter: 1024/1168\n",
      "Training Step: 682  | total loss: \u001b[1m\u001b[32m0.02283\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 036 | loss: 0.02283 - R2: 0.9994 -- iter: 1088/1168\n",
      "Training Step: 683  | total loss: \u001b[1m\u001b[32m0.02171\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 036 | loss: 0.02171 - R2: 0.9984 -- iter: 1152/1168\n",
      "Training Step: 684  | total loss: \u001b[1m\u001b[32m0.02121\u001b[0m\u001b[0m | time: 1.146s\n",
      "| SGD | epoch: 036 | loss: 0.02121 - R2: 0.9983 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 685  | total loss: \u001b[1m\u001b[32m0.02073\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 037 | loss: 0.02073 - R2: 0.9979 -- iter: 0064/1168\n",
      "Training Step: 686  | total loss: \u001b[1m\u001b[32m0.02073\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 037 | loss: 0.02073 - R2: 0.9979 -- iter: 0128/1168\n",
      "Training Step: 687  | total loss: \u001b[1m\u001b[32m0.05654\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 037 | loss: 0.05654 - R2: 0.9966 -- iter: 0192/1168\n",
      "Training Step: 688  | total loss: \u001b[1m\u001b[32m0.05193\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 037 | loss: 0.05193 - R2: 0.9966 -- iter: 0256/1168\n",
      "Training Step: 689  | total loss: \u001b[1m\u001b[32m0.05193\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 037 | loss: 0.05193 - R2: 0.9966 -- iter: 0320/1168\n",
      "Training Step: 690  | total loss: \u001b[1m\u001b[32m0.04753\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 037 | loss: 0.04753 - R2: 0.9973 -- iter: 0384/1168\n",
      "Training Step: 691  | total loss: \u001b[1m\u001b[32m0.04753\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 037 | loss: 0.04753 - R2: 0.9973 -- iter: 0448/1168\n",
      "Training Step: 692  | total loss: \u001b[1m\u001b[32m0.04506\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 037 | loss: 0.04506 - R2: 0.9975 -- iter: 0512/1168\n",
      "Training Step: 693  | total loss: \u001b[1m\u001b[32m0.04388\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 037 | loss: 0.04388 - R2: 0.9979 -- iter: 0576/1168\n",
      "Training Step: 694  | total loss: \u001b[1m\u001b[32m0.04088\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 037 | loss: 0.04088 - R2: 0.9978 -- iter: 0640/1168\n",
      "Training Step: 695  | total loss: \u001b[1m\u001b[32m0.03856\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 037 | loss: 0.03856 - R2: 0.9981 -- iter: 0704/1168\n",
      "Training Step: 696  | total loss: \u001b[1m\u001b[32m0.03405\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 037 | loss: 0.03405 - R2: 0.9985 -- iter: 0768/1168\n",
      "Training Step: 697  | total loss: \u001b[1m\u001b[32m0.03273\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 037 | loss: 0.03273 - R2: 0.9985 -- iter: 0832/1168\n",
      "Training Step: 698  | total loss: \u001b[1m\u001b[32m0.03086\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 037 | loss: 0.03086 - R2: 0.9984 -- iter: 0896/1168\n",
      "Training Step: 699  | total loss: \u001b[1m\u001b[32m0.03086\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 037 | loss: 0.03086 - R2: 0.9984 -- iter: 0960/1168\n",
      "Training Step: 700  | total loss: \u001b[1m\u001b[32m0.02743\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 037 | loss: 0.02743 - R2: 0.9999 -- iter: 1024/1168\n",
      "Training Step: 701  | total loss: \u001b[1m\u001b[32m0.02702\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 037 | loss: 0.02702 - R2: 1.0003 -- iter: 1088/1168\n",
      "Training Step: 702  | total loss: \u001b[1m\u001b[32m0.02702\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 037 | loss: 0.02702 - R2: 1.0005 -- iter: 1152/1168\n",
      "Training Step: 703  | total loss: \u001b[1m\u001b[32m0.02592\u001b[0m\u001b[0m | time: 1.144s\n",
      "| SGD | epoch: 037 | loss: 0.02592 - R2: 1.0007 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 704  | total loss: \u001b[1m\u001b[32m0.02653\u001b[0m\u001b[0m | time: 0.017s\n",
      "| SGD | epoch: 038 | loss: 0.02653 - R2: 1.0006 -- iter: 0064/1168\n",
      "Training Step: 705  | total loss: \u001b[1m\u001b[32m0.02653\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 038 | loss: 0.02653 - R2: 1.0006 -- iter: 0128/1168\n",
      "Training Step: 706  | total loss: \u001b[1m\u001b[32m0.02695\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 038 | loss: 0.02695 - R2: 1.0009 -- iter: 0192/1168\n",
      "Training Step: 707  | total loss: \u001b[1m\u001b[32m0.02661\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 038 | loss: 0.02661 - R2: 1.0004 -- iter: 0256/1168\n",
      "Training Step: 708  | total loss: \u001b[1m\u001b[32m0.02574\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 038 | loss: 0.02574 - R2: 0.9995 -- iter: 0320/1168\n",
      "Training Step: 709  | total loss: \u001b[1m\u001b[32m0.02639\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 038 | loss: 0.02639 - R2: 0.9995 -- iter: 0384/1168\n",
      "Training Step: 710  | total loss: \u001b[1m\u001b[32m0.02521\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 038 | loss: 0.02521 - R2: 0.9995 -- iter: 0448/1168\n",
      "Training Step: 711  | total loss: \u001b[1m\u001b[32m0.02521\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 038 | loss: 0.02521 - R2: 0.9995 -- iter: 0512/1168\n",
      "Training Step: 712  | total loss: \u001b[1m\u001b[32m0.02413\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 038 | loss: 0.02413 - R2: 0.9995 -- iter: 0576/1168\n",
      "Training Step: 713  | total loss: \u001b[1m\u001b[32m0.02460\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 038 | loss: 0.02460 - R2: 0.9998 -- iter: 0640/1168\n",
      "Training Step: 714  | total loss: \u001b[1m\u001b[32m0.02348\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 038 | loss: 0.02348 - R2: 0.9997 -- iter: 0704/1168\n",
      "Training Step: 715  | total loss: \u001b[1m\u001b[32m0.02348\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 038 | loss: 0.02348 - R2: 0.9997 -- iter: 0768/1168\n",
      "Training Step: 716  | total loss: \u001b[1m\u001b[32m0.02408\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 038 | loss: 0.02408 - R2: 1.0000 -- iter: 0832/1168\n",
      "Training Step: 717  | total loss: \u001b[1m\u001b[32m0.02367\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 038 | loss: 0.02367 - R2: 0.9999 -- iter: 0896/1168\n",
      "Training Step: 718  | total loss: \u001b[1m\u001b[32m0.02325\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 038 | loss: 0.02325 - R2: 0.9994 -- iter: 0960/1168\n",
      "Training Step: 719  | total loss: \u001b[1m\u001b[32m0.02281\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 038 | loss: 0.02281 - R2: 0.9995 -- iter: 1024/1168\n",
      "Training Step: 720  | total loss: \u001b[1m\u001b[32m0.02173\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 038 | loss: 0.02173 - R2: 0.9997 -- iter: 1088/1168\n",
      "Training Step: 721  | total loss: \u001b[1m\u001b[32m0.02077\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 038 | loss: 0.02077 - R2: 0.9998 -- iter: 1152/1168\n",
      "Training Step: 722  | total loss: \u001b[1m\u001b[32m0.02200\u001b[0m\u001b[0m | time: 1.119s\n",
      "| SGD | epoch: 038 | loss: 0.02200 - R2: 0.9997 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 723  | total loss: \u001b[1m\u001b[32m0.02124\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 039 | loss: 0.02124 - R2: 0.9994 -- iter: 0064/1168\n",
      "Training Step: 724  | total loss: \u001b[1m\u001b[32m0.02124\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 039 | loss: 0.02124 - R2: 0.9995 -- iter: 0128/1168\n",
      "Training Step: 725  | total loss: \u001b[1m\u001b[32m0.02116\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 039 | loss: 0.02116 - R2: 0.9995 -- iter: 0192/1168\n",
      "Training Step: 726  | total loss: \u001b[1m\u001b[32m0.02108\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 039 | loss: 0.02108 - R2: 0.9995 -- iter: 0256/1168\n",
      "Training Step: 727  | total loss: \u001b[1m\u001b[32m0.02108\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 039 | loss: 0.02108 - R2: 0.9995 -- iter: 0320/1168\n",
      "Training Step: 728  | total loss: \u001b[1m\u001b[32m0.02068\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 039 | loss: 0.02068 - R2: 1.0000 -- iter: 0384/1168\n",
      "Training Step: 729  | total loss: \u001b[1m\u001b[32m0.02093\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 039 | loss: 0.02093 - R2: 0.9997 -- iter: 0448/1168\n",
      "Training Step: 730  | total loss: \u001b[1m\u001b[32m0.02101\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 039 | loss: 0.02101 - R2: 0.9997 -- iter: 0512/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 731  | total loss: \u001b[1m\u001b[32m0.02101\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 039 | loss: 0.02101 - R2: 0.9997 -- iter: 0576/1168\n",
      "Training Step: 732  | total loss: \u001b[1m\u001b[32m0.02029\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 039 | loss: 0.02029 - R2: 0.9997 -- iter: 0640/1168\n",
      "Training Step: 733  | total loss: \u001b[1m\u001b[32m0.01943\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 039 | loss: 0.01943 - R2: 0.9992 -- iter: 0704/1168\n",
      "Training Step: 734  | total loss: \u001b[1m\u001b[32m0.02106\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 039 | loss: 0.02106 - R2: 0.9998 -- iter: 0768/1168\n",
      "Training Step: 735  | total loss: \u001b[1m\u001b[32m0.02464\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 039 | loss: 0.02464 - R2: 1.0001 -- iter: 0832/1168\n",
      "Training Step: 736  | total loss: \u001b[1m\u001b[32m0.02358\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 039 | loss: 0.02358 - R2: 1.0001 -- iter: 0896/1168\n",
      "Training Step: 737  | total loss: \u001b[1m\u001b[32m0.02302\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 039 | loss: 0.02302 - R2: 0.9998 -- iter: 0960/1168\n",
      "Training Step: 738  | total loss: \u001b[1m\u001b[32m0.02302\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 039 | loss: 0.02302 - R2: 0.9998 -- iter: 1024/1168\n",
      "Training Step: 739  | total loss: \u001b[1m\u001b[32m0.02204\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 039 | loss: 0.02204 - R2: 0.9998 -- iter: 1088/1168\n",
      "Training Step: 740  | total loss: \u001b[1m\u001b[32m0.02150\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 039 | loss: 0.02150 - R2: 0.9997 -- iter: 1152/1168\n",
      "Training Step: 741  | total loss: \u001b[1m\u001b[32m0.02150\u001b[0m\u001b[0m | time: 1.133s\n",
      "| SGD | epoch: 039 | loss: 0.02150 - R2: 0.9997 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 742  | total loss: \u001b[1m\u001b[32m0.02051\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 040 | loss: 0.02051 - R2: 1.0001 -- iter: 0064/1168\n",
      "Training Step: 743  | total loss: \u001b[1m\u001b[32m0.02077\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 040 | loss: 0.02077 - R2: 1.0000 -- iter: 0128/1168\n",
      "Training Step: 744  | total loss: \u001b[1m\u001b[32m0.02038\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 040 | loss: 0.02038 - R2: 0.9993 -- iter: 0192/1168\n",
      "Training Step: 745  | total loss: \u001b[1m\u001b[32m0.02067\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 040 | loss: 0.02067 - R2: 0.9994 -- iter: 0256/1168\n",
      "Training Step: 746  | total loss: \u001b[1m\u001b[32m0.02020\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 040 | loss: 0.02020 - R2: 0.9998 -- iter: 0320/1168\n",
      "Training Step: 747  | total loss: \u001b[1m\u001b[32m0.02020\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 040 | loss: 0.02020 - R2: 0.9998 -- iter: 0384/1168\n",
      "Training Step: 748  | total loss: \u001b[1m\u001b[32m0.02040\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 040 | loss: 0.02040 - R2: 0.9993 -- iter: 0448/1168\n",
      "Training Step: 749  | total loss: \u001b[1m\u001b[32m0.02009\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 040 | loss: 0.02009 - R2: 0.9999 -- iter: 0512/1168\n",
      "Training Step: 750  | total loss: \u001b[1m\u001b[32m0.02008\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 040 | loss: 0.02008 - R2: 0.9999 -- iter: 0576/1168\n",
      "Training Step: 751  | total loss: \u001b[1m\u001b[32m0.02075\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 040 | loss: 0.02075 - R2: 0.9997 -- iter: 0640/1168\n",
      "Training Step: 752  | total loss: \u001b[1m\u001b[32m0.02258\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 040 | loss: 0.02258 - R2: 1.0002 -- iter: 0704/1168\n",
      "Training Step: 753  | total loss: \u001b[1m\u001b[32m0.02278\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 040 | loss: 0.02278 - R2: 0.9997 -- iter: 0768/1168\n",
      "Training Step: 754  | total loss: \u001b[1m\u001b[32m0.02305\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 040 | loss: 0.02305 - R2: 1.0001 -- iter: 0832/1168\n",
      "Training Step: 755  | total loss: \u001b[1m\u001b[32m0.02150\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 040 | loss: 0.02150 - R2: 1.0000 -- iter: 0896/1168\n",
      "Training Step: 756  | total loss: \u001b[1m\u001b[32m0.02058\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 040 | loss: 0.02058 - R2: 0.9996 -- iter: 0960/1168\n",
      "Training Step: 757  | total loss: \u001b[1m\u001b[32m0.01998\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 040 | loss: 0.01998 - R2: 0.9992 -- iter: 1024/1168\n",
      "Training Step: 758  | total loss: \u001b[1m\u001b[32m0.02013\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 040 | loss: 0.02013 - R2: 0.9994 -- iter: 1088/1168\n",
      "Training Step: 759  | total loss: \u001b[1m\u001b[32m0.02013\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 040 | loss: 0.02013 - R2: 0.9996 -- iter: 1152/1168\n",
      "Training Step: 760  | total loss: \u001b[1m\u001b[32m0.01774\u001b[0m\u001b[0m | time: 1.121s\n",
      "| SGD | epoch: 040 | loss: 0.01774 - R2: 0.9999 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 761  | total loss: \u001b[1m\u001b[32m0.01774\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 041 | loss: 0.01774 - R2: 0.9999 -- iter: 0064/1168\n",
      "Training Step: 762  | total loss: \u001b[1m\u001b[32m0.01897\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 041 | loss: 0.01897 - R2: 1.0004 -- iter: 0128/1168\n",
      "Training Step: 763  | total loss: \u001b[1m\u001b[32m0.01897\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 041 | loss: 0.01897 - R2: 1.0004 -- iter: 0192/1168\n",
      "Training Step: 764  | total loss: \u001b[1m\u001b[32m0.02072\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 041 | loss: 0.02072 - R2: 1.0000 -- iter: 0256/1168\n",
      "Training Step: 765  | total loss: \u001b[1m\u001b[32m0.02072\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 041 | loss: 0.02072 - R2: 1.0000 -- iter: 0320/1168\n",
      "Training Step: 766  | total loss: \u001b[1m\u001b[32m0.02018\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 041 | loss: 0.02018 - R2: 0.9997 -- iter: 0384/1168\n",
      "Training Step: 767  | total loss: \u001b[1m\u001b[32m0.05330\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 041 | loss: 0.05330 - R2: 0.9986 -- iter: 0448/1168\n",
      "Training Step: 768  | total loss: \u001b[1m\u001b[32m0.04977\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 041 | loss: 0.04977 - R2: 0.9988 -- iter: 0512/1168\n",
      "Training Step: 769  | total loss: \u001b[1m\u001b[32m0.04787\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 041 | loss: 0.04787 - R2: 0.9984 -- iter: 0576/1168\n",
      "Training Step: 770  | total loss: \u001b[1m\u001b[32m0.04787\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 041 | loss: 0.04787 - R2: 0.9988 -- iter: 0640/1168\n",
      "Training Step: 771  | total loss: \u001b[1m\u001b[32m0.04476\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 041 | loss: 0.04476 - R2: 0.9988 -- iter: 0704/1168\n",
      "Training Step: 772  | total loss: \u001b[1m\u001b[32m0.04505\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 041 | loss: 0.04505 - R2: 0.9997 -- iter: 0768/1168\n",
      "Training Step: 773  | total loss: \u001b[1m\u001b[32m0.04212\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 041 | loss: 0.04212 - R2: 0.9993 -- iter: 0832/1168\n",
      "Training Step: 774  | total loss: \u001b[1m\u001b[32m0.04052\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 041 | loss: 0.04052 - R2: 0.9998 -- iter: 0896/1168\n",
      "Training Step: 775  | total loss: \u001b[1m\u001b[32m0.03602\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 041 | loss: 0.03602 - R2: 0.9991 -- iter: 0960/1168\n",
      "Training Step: 776  | total loss: \u001b[1m\u001b[32m0.03484\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 041 | loss: 0.03484 - R2: 0.9989 -- iter: 1024/1168\n",
      "Training Step: 777  | total loss: \u001b[1m\u001b[32m0.03484\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 041 | loss: 0.03484 - R2: 0.9989 -- iter: 1088/1168\n",
      "Training Step: 778  | total loss: \u001b[1m\u001b[32m0.03283\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 041 | loss: 0.03283 - R2: 0.9988 -- iter: 1152/1168\n",
      "Training Step: 779  | total loss: \u001b[1m\u001b[32m0.03112\u001b[0m\u001b[0m | time: 1.169s\n",
      "| SGD | epoch: 041 | loss: 0.03112 - R2: 0.9988 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 780  | total loss: \u001b[1m\u001b[32m0.02921\u001b[0m\u001b[0m | time: 0.016s\n",
      "| SGD | epoch: 042 | loss: 0.02921 - R2: 0.9981 -- iter: 0064/1168\n",
      "Training Step: 781  | total loss: \u001b[1m\u001b[32m0.02645\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 042 | loss: 0.02645 - R2: 0.9974 -- iter: 0128/1168\n",
      "Training Step: 782  | total loss: \u001b[1m\u001b[32m0.02500\u001b[0m\u001b[0m | time: 0.026s\n",
      "| SGD | epoch: 042 | loss: 0.02500 - R2: 0.9975 -- iter: 0192/1168\n",
      "Training Step: 783  | total loss: \u001b[1m\u001b[32m0.02500\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 042 | loss: 0.02500 - R2: 0.9975 -- iter: 0256/1168\n",
      "Training Step: 784  | total loss: \u001b[1m\u001b[32m0.02361\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 042 | loss: 0.02361 - R2: 0.9981 -- iter: 0320/1168\n",
      "Training Step: 785  | total loss: \u001b[1m\u001b[32m0.02242\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 042 | loss: 0.02242 - R2: 0.9984 -- iter: 0384/1168\n",
      "Training Step: 786  | total loss: \u001b[1m\u001b[32m0.02541\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 042 | loss: 0.02541 - R2: 0.9995 -- iter: 0448/1168\n",
      "Training Step: 787  | total loss: \u001b[1m\u001b[32m0.02541\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 042 | loss: 0.02541 - R2: 0.9995 -- iter: 0512/1168\n",
      "Training Step: 788  | total loss: \u001b[1m\u001b[32m0.02495\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 042 | loss: 0.02495 - R2: 0.9993 -- iter: 0576/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 789  | total loss: \u001b[1m\u001b[32m0.02513\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 042 | loss: 0.02513 - R2: 0.9994 -- iter: 0640/1168\n",
      "Training Step: 790  | total loss: \u001b[1m\u001b[32m0.02513\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 042 | loss: 0.02513 - R2: 0.9994 -- iter: 0704/1168\n",
      "Training Step: 791  | total loss: \u001b[1m\u001b[32m0.02634\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 042 | loss: 0.02634 - R2: 0.9999 -- iter: 0768/1168\n",
      "Training Step: 792  | total loss: \u001b[1m\u001b[32m0.02495\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 042 | loss: 0.02495 - R2: 1.0005 -- iter: 0832/1168\n",
      "Training Step: 793  | total loss: \u001b[1m\u001b[32m0.02495\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 042 | loss: 0.02495 - R2: 1.0005 -- iter: 0896/1168\n",
      "Training Step: 794  | total loss: \u001b[1m\u001b[32m0.02357\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 042 | loss: 0.02357 - R2: 1.0003 -- iter: 0960/1168\n",
      "Training Step: 795  | total loss: \u001b[1m\u001b[32m0.02357\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 042 | loss: 0.02357 - R2: 1.0003 -- iter: 1024/1168\n",
      "Training Step: 796  | total loss: \u001b[1m\u001b[32m0.02207\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 042 | loss: 0.02207 - R2: 1.0008 -- iter: 1088/1168\n",
      "Training Step: 797  | total loss: \u001b[1m\u001b[32m0.02205\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 042 | loss: 0.02205 - R2: 1.0004 -- iter: 1152/1168\n",
      "Training Step: 798  | total loss: \u001b[1m\u001b[32m0.02148\u001b[0m\u001b[0m | time: 1.110s\n",
      "| SGD | epoch: 042 | loss: 0.02148 - R2: 1.0000 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 799  | total loss: \u001b[1m\u001b[32m0.03160\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 043 | loss: 0.03160 - R2: 1.0002 -- iter: 0064/1168\n",
      "Training Step: 800  | total loss: \u001b[1m\u001b[32m0.04071\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 043 | loss: 0.04071 - R2: 1.0004 -- iter: 0128/1168\n",
      "Training Step: 801  | total loss: \u001b[1m\u001b[32m0.04071\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 043 | loss: 0.04071 - R2: 1.0004 -- iter: 0192/1168\n",
      "Training Step: 802  | total loss: \u001b[1m\u001b[32m0.03849\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 043 | loss: 0.03849 - R2: 0.9996 -- iter: 0256/1168\n",
      "Training Step: 803  | total loss: \u001b[1m\u001b[32m0.03500\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 043 | loss: 0.03500 - R2: 0.9993 -- iter: 0320/1168\n",
      "Training Step: 804  | total loss: \u001b[1m\u001b[32m0.03500\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 043 | loss: 0.03500 - R2: 0.9993 -- iter: 0384/1168\n",
      "Training Step: 805  | total loss: \u001b[1m\u001b[32m0.03322\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 043 | loss: 0.03322 - R2: 0.9993 -- iter: 0448/1168\n",
      "Training Step: 806  | total loss: \u001b[1m\u001b[32m0.03065\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 043 | loss: 0.03065 - R2: 0.9993 -- iter: 0512/1168\n",
      "Training Step: 807  | total loss: \u001b[1m\u001b[32m0.02953\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 043 | loss: 0.02953 - R2: 0.9992 -- iter: 0576/1168\n",
      "Training Step: 808  | total loss: \u001b[1m\u001b[32m0.02953\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 043 | loss: 0.02953 - R2: 0.9992 -- iter: 0640/1168\n",
      "Training Step: 809  | total loss: \u001b[1m\u001b[32m0.02811\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 043 | loss: 0.02811 - R2: 0.9993 -- iter: 0704/1168\n",
      "Training Step: 810  | total loss: \u001b[1m\u001b[32m0.02694\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 043 | loss: 0.02694 - R2: 0.9991 -- iter: 0768/1168\n",
      "Training Step: 811  | total loss: \u001b[1m\u001b[32m0.02633\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 043 | loss: 0.02633 - R2: 0.9998 -- iter: 0832/1168\n",
      "Training Step: 812  | total loss: \u001b[1m\u001b[32m0.02766\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 043 | loss: 0.02766 - R2: 0.9998 -- iter: 0896/1168\n",
      "Training Step: 813  | total loss: \u001b[1m\u001b[32m0.02876\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 043 | loss: 0.02876 - R2: 0.9995 -- iter: 0960/1168\n",
      "Training Step: 814  | total loss: \u001b[1m\u001b[32m0.02715\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 043 | loss: 0.02715 - R2: 0.9995 -- iter: 1024/1168\n",
      "Training Step: 815  | total loss: \u001b[1m\u001b[32m0.02596\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 043 | loss: 0.02596 - R2: 0.9995 -- iter: 1088/1168\n",
      "Training Step: 816  | total loss: \u001b[1m\u001b[32m0.02455\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 043 | loss: 0.02455 - R2: 0.9993 -- iter: 1152/1168\n",
      "Training Step: 817  | total loss: \u001b[1m\u001b[32m0.02539\u001b[0m\u001b[0m | time: 1.092s\n",
      "| SGD | epoch: 043 | loss: 0.02539 - R2: 0.9994 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 818  | total loss: \u001b[1m\u001b[32m0.02427\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 044 | loss: 0.02427 - R2: 0.9996 -- iter: 0064/1168\n",
      "Training Step: 819  | total loss: \u001b[1m\u001b[32m0.02389\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 044 | loss: 0.02389 - R2: 0.9993 -- iter: 0128/1168\n",
      "Training Step: 820  | total loss: \u001b[1m\u001b[32m0.02409\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 044 | loss: 0.02409 - R2: 0.9976 -- iter: 0192/1168\n",
      "Training Step: 821  | total loss: \u001b[1m\u001b[32m0.02428\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 044 | loss: 0.02428 - R2: 0.9961 -- iter: 0256/1168\n",
      "Training Step: 822  | total loss: \u001b[1m\u001b[32m0.02303\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 044 | loss: 0.02303 - R2: 0.9967 -- iter: 0320/1168\n",
      "Training Step: 823  | total loss: \u001b[1m\u001b[32m0.02167\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 044 | loss: 0.02167 - R2: 0.9974 -- iter: 0384/1168\n",
      "Training Step: 824  | total loss: \u001b[1m\u001b[32m0.02167\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 044 | loss: 0.02167 - R2: 0.9974 -- iter: 0448/1168\n",
      "Training Step: 825  | total loss: \u001b[1m\u001b[32m0.02333\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 044 | loss: 0.02333 - R2: 0.9980 -- iter: 0512/1168\n",
      "Training Step: 826  | total loss: \u001b[1m\u001b[32m0.02299\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 044 | loss: 0.02299 - R2: 0.9979 -- iter: 0576/1168\n",
      "Training Step: 827  | total loss: \u001b[1m\u001b[32m0.02299\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 044 | loss: 0.02299 - R2: 0.9988 -- iter: 0640/1168\n",
      "Training Step: 828  | total loss: \u001b[1m\u001b[32m0.05530\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 044 | loss: 0.05530 - R2: 0.9988 -- iter: 0704/1168\n",
      "Training Step: 829  | total loss: \u001b[1m\u001b[32m0.05108\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 044 | loss: 0.05108 - R2: 0.9987 -- iter: 0768/1168\n",
      "Training Step: 830  | total loss: \u001b[1m\u001b[32m0.04761\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 044 | loss: 0.04761 - R2: 0.9982 -- iter: 0832/1168\n",
      "Training Step: 831  | total loss: \u001b[1m\u001b[32m0.04546\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 044 | loss: 0.04546 - R2: 0.9988 -- iter: 0896/1168\n",
      "Training Step: 832  | total loss: \u001b[1m\u001b[32m0.04546\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 044 | loss: 0.04546 - R2: 0.9988 -- iter: 0960/1168\n",
      "Training Step: 833  | total loss: \u001b[1m\u001b[32m0.03941\u001b[0m\u001b[0m | time: 0.173s\n",
      "| SGD | epoch: 044 | loss: 0.03941 - R2: 0.9992 -- iter: 1024/1168\n",
      "Training Step: 834  | total loss: \u001b[1m\u001b[32m0.03941\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 044 | loss: 0.03941 - R2: 0.9992 -- iter: 1088/1168\n",
      "Training Step: 835  | total loss: \u001b[1m\u001b[32m0.03510\u001b[0m\u001b[0m | time: 0.181s\n",
      "| SGD | epoch: 044 | loss: 0.03510 - R2: 0.9992 -- iter: 1152/1168\n",
      "Training Step: 836  | total loss: \u001b[1m\u001b[32m0.03507\u001b[0m\u001b[0m | time: 1.188s\n",
      "| SGD | epoch: 044 | loss: 0.03507 - R2: 0.9997 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 837  | total loss: \u001b[1m\u001b[32m0.03393\u001b[0m\u001b[0m | time: 0.005s\n",
      "| SGD | epoch: 045 | loss: 0.03393 - R2: 1.0000 -- iter: 0064/1168\n",
      "Training Step: 838  | total loss: \u001b[1m\u001b[32m0.03393\u001b[0m\u001b[0m | time: 0.010s\n",
      "| SGD | epoch: 045 | loss: 0.03393 - R2: 1.0000 -- iter: 0128/1168\n",
      "Training Step: 839  | total loss: \u001b[1m\u001b[32m0.03051\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 045 | loss: 0.03051 - R2: 1.0000 -- iter: 0192/1168\n",
      "Training Step: 840  | total loss: \u001b[1m\u001b[32m0.03051\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 045 | loss: 0.03051 - R2: 1.0000 -- iter: 0256/1168\n",
      "Training Step: 841  | total loss: \u001b[1m\u001b[32m0.02937\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 045 | loss: 0.02937 - R2: 1.0001 -- iter: 0320/1168\n",
      "Training Step: 842  | total loss: \u001b[1m\u001b[32m0.02825\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 045 | loss: 0.02825 - R2: 1.0002 -- iter: 0384/1168\n",
      "Training Step: 843  | total loss: \u001b[1m\u001b[32m0.02758\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 045 | loss: 0.02758 - R2: 0.9996 -- iter: 0448/1168\n",
      "Training Step: 844  | total loss: \u001b[1m\u001b[32m0.02758\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 045 | loss: 0.02758 - R2: 0.9994 -- iter: 0512/1168\n",
      "Training Step: 845  | total loss: \u001b[1m\u001b[32m0.02583\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 045 | loss: 0.02583 - R2: 0.9994 -- iter: 0576/1168\n",
      "Training Step: 846  | total loss: \u001b[1m\u001b[32m0.02537\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 045 | loss: 0.02537 - R2: 0.9993 -- iter: 0640/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 847  | total loss: \u001b[1m\u001b[32m0.05293\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 045 | loss: 0.05293 - R2: 0.9984 -- iter: 0704/1168\n",
      "Training Step: 848  | total loss: \u001b[1m\u001b[32m0.04908\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 045 | loss: 0.04908 - R2: 0.9982 -- iter: 0768/1168\n",
      "Training Step: 849  | total loss: \u001b[1m\u001b[32m0.04600\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 045 | loss: 0.04600 - R2: 0.9986 -- iter: 0832/1168\n",
      "Training Step: 850  | total loss: \u001b[1m\u001b[32m0.04357\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 045 | loss: 0.04357 - R2: 0.9985 -- iter: 0896/1168\n",
      "Training Step: 851  | total loss: \u001b[1m\u001b[32m0.04344\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 045 | loss: 0.04344 - R2: 0.9986 -- iter: 0960/1168\n",
      "Training Step: 852  | total loss: \u001b[1m\u001b[32m0.04085\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 045 | loss: 0.04085 - R2: 0.9983 -- iter: 1024/1168\n",
      "Training Step: 853  | total loss: \u001b[1m\u001b[32m0.03884\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 045 | loss: 0.03884 - R2: 0.9986 -- iter: 1088/1168\n",
      "Training Step: 854  | total loss: \u001b[1m\u001b[32m0.03688\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 045 | loss: 0.03688 - R2: 0.9990 -- iter: 1152/1168\n",
      "Training Step: 855  | total loss: \u001b[1m\u001b[32m0.03688\u001b[0m\u001b[0m | time: 1.124s\n",
      "| SGD | epoch: 045 | loss: 0.03688 - R2: 0.9990 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 856  | total loss: \u001b[1m\u001b[32m0.03390\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 046 | loss: 0.03390 - R2: 0.9997 -- iter: 0064/1168\n",
      "Training Step: 857  | total loss: \u001b[1m\u001b[32m0.03390\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 046 | loss: 0.03390 - R2: 0.9997 -- iter: 0128/1168\n",
      "Training Step: 858  | total loss: \u001b[1m\u001b[32m0.03354\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 046 | loss: 0.03354 - R2: 1.0001 -- iter: 0192/1168\n",
      "Training Step: 859  | total loss: \u001b[1m\u001b[32m0.03042\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 046 | loss: 0.03042 - R2: 0.9992 -- iter: 0256/1168\n",
      "Training Step: 860  | total loss: \u001b[1m\u001b[32m0.03042\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 046 | loss: 0.03042 - R2: 0.9992 -- iter: 0320/1168\n",
      "Training Step: 861  | total loss: \u001b[1m\u001b[32m0.02913\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 046 | loss: 0.02913 - R2: 0.9988 -- iter: 0384/1168\n",
      "Training Step: 862  | total loss: \u001b[1m\u001b[32m0.02813\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 046 | loss: 0.02813 - R2: 0.9986 -- iter: 0448/1168\n",
      "Training Step: 863  | total loss: \u001b[1m\u001b[32m0.02962\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 046 | loss: 0.02962 - R2: 0.9995 -- iter: 0512/1168\n",
      "Training Step: 864  | total loss: \u001b[1m\u001b[32m0.02855\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 046 | loss: 0.02855 - R2: 0.9997 -- iter: 0576/1168\n",
      "Training Step: 865  | total loss: \u001b[1m\u001b[32m0.02628\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 046 | loss: 0.02628 - R2: 0.9996 -- iter: 0640/1168\n",
      "Training Step: 866  | total loss: \u001b[1m\u001b[32m0.02628\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 046 | loss: 0.02628 - R2: 0.9996 -- iter: 0704/1168\n",
      "Training Step: 867  | total loss: \u001b[1m\u001b[32m0.05923\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 046 | loss: 0.05923 - R2: 0.9995 -- iter: 0768/1168\n",
      "Training Step: 868  | total loss: \u001b[1m\u001b[32m0.05923\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 046 | loss: 0.05923 - R2: 0.9995 -- iter: 0832/1168\n",
      "Training Step: 869  | total loss: \u001b[1m\u001b[32m0.05518\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 046 | loss: 0.05518 - R2: 0.9993 -- iter: 0896/1168\n",
      "Training Step: 870  | total loss: \u001b[1m\u001b[32m0.04847\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 046 | loss: 0.04847 - R2: 0.9990 -- iter: 0960/1168\n",
      "Training Step: 871  | total loss: \u001b[1m\u001b[32m0.04847\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 046 | loss: 0.04847 - R2: 0.9990 -- iter: 1024/1168\n",
      "Training Step: 872  | total loss: \u001b[1m\u001b[32m0.04551\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 046 | loss: 0.04551 - R2: 0.9991 -- iter: 1088/1168\n",
      "Training Step: 873  | total loss: \u001b[1m\u001b[32m0.04174\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 046 | loss: 0.04174 - R2: 0.9993 -- iter: 1152/1168\n",
      "Training Step: 874  | total loss: \u001b[1m\u001b[32m0.03962\u001b[0m\u001b[0m | time: 1.135s\n",
      "| SGD | epoch: 046 | loss: 0.03962 - R2: 0.9987 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 875  | total loss: \u001b[1m\u001b[32m0.03939\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 047 | loss: 0.03939 - R2: 0.9993 -- iter: 0064/1168\n",
      "Training Step: 876  | total loss: \u001b[1m\u001b[32m0.03993\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 047 | loss: 0.03993 - R2: 0.9997 -- iter: 0128/1168\n",
      "Training Step: 877  | total loss: \u001b[1m\u001b[32m0.03589\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 047 | loss: 0.03589 - R2: 0.9994 -- iter: 0192/1168\n",
      "Training Step: 878  | total loss: \u001b[1m\u001b[32m0.03589\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 047 | loss: 0.03589 - R2: 0.9994 -- iter: 0256/1168\n",
      "Training Step: 879  | total loss: \u001b[1m\u001b[32m0.03147\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 047 | loss: 0.03147 - R2: 0.9993 -- iter: 0320/1168\n",
      "Training Step: 880  | total loss: \u001b[1m\u001b[32m0.02963\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 047 | loss: 0.02963 - R2: 0.9992 -- iter: 0384/1168\n",
      "Training Step: 881  | total loss: \u001b[1m\u001b[32m0.02848\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 047 | loss: 0.02848 - R2: 0.9993 -- iter: 0448/1168\n",
      "Training Step: 882  | total loss: \u001b[1m\u001b[32m0.02848\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 047 | loss: 0.02848 - R2: 0.9993 -- iter: 0512/1168\n",
      "Training Step: 883  | total loss: \u001b[1m\u001b[32m0.02710\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 047 | loss: 0.02710 - R2: 0.9996 -- iter: 0576/1168\n",
      "Training Step: 884  | total loss: \u001b[1m\u001b[32m0.02655\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 047 | loss: 0.02655 - R2: 0.9998 -- iter: 0640/1168\n",
      "Training Step: 885  | total loss: \u001b[1m\u001b[32m0.02655\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 047 | loss: 0.02655 - R2: 0.9998 -- iter: 0704/1168\n",
      "Training Step: 886  | total loss: \u001b[1m\u001b[32m0.02646\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 047 | loss: 0.02646 - R2: 0.9998 -- iter: 0768/1168\n",
      "Training Step: 887  | total loss: \u001b[1m\u001b[32m0.02481\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 047 | loss: 0.02481 - R2: 0.9995 -- iter: 0832/1168\n",
      "Training Step: 888  | total loss: \u001b[1m\u001b[32m0.02613\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 047 | loss: 0.02613 - R2: 1.0000 -- iter: 0896/1168\n",
      "Training Step: 889  | total loss: \u001b[1m\u001b[32m0.02444\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 047 | loss: 0.02444 - R2: 0.9994 -- iter: 0960/1168\n",
      "Training Step: 890  | total loss: \u001b[1m\u001b[32m0.02444\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 047 | loss: 0.02444 - R2: 0.9994 -- iter: 1024/1168\n",
      "Training Step: 891  | total loss: \u001b[1m\u001b[32m0.02351\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 047 | loss: 0.02351 - R2: 0.9995 -- iter: 1088/1168\n",
      "Training Step: 892  | total loss: \u001b[1m\u001b[32m0.02348\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 047 | loss: 0.02348 - R2: 0.9992 -- iter: 1152/1168\n",
      "Training Step: 893  | total loss: \u001b[1m\u001b[32m0.02286\u001b[0m\u001b[0m | time: 1.144s\n",
      "| SGD | epoch: 047 | loss: 0.02286 - R2: 0.9992 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 894  | total loss: \u001b[1m\u001b[32m0.02201\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 048 | loss: 0.02201 - R2: 0.9991 -- iter: 0064/1168\n",
      "Training Step: 895  | total loss: \u001b[1m\u001b[32m0.02201\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 048 | loss: 0.02201 - R2: 0.9991 -- iter: 0128/1168\n",
      "Training Step: 896  | total loss: \u001b[1m\u001b[32m0.02316\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 048 | loss: 0.02316 - R2: 0.9994 -- iter: 0192/1168\n",
      "Training Step: 897  | total loss: \u001b[1m\u001b[32m0.02220\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 048 | loss: 0.02220 - R2: 0.9993 -- iter: 0256/1168\n",
      "Training Step: 898  | total loss: \u001b[1m\u001b[32m0.02220\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 048 | loss: 0.02220 - R2: 1.0002 -- iter: 0320/1168\n",
      "Training Step: 899  | total loss: \u001b[1m\u001b[32m0.02435\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 048 | loss: 0.02435 - R2: 1.0002 -- iter: 0384/1168\n",
      "Training Step: 900  | total loss: \u001b[1m\u001b[32m0.02293\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 048 | loss: 0.02293 - R2: 1.0005 -- iter: 0448/1168\n",
      "Training Step: 901  | total loss: \u001b[1m\u001b[32m0.02165\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 048 | loss: 0.02165 - R2: 1.0008 -- iter: 0512/1168\n",
      "Training Step: 902  | total loss: \u001b[1m\u001b[32m0.02259\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 048 | loss: 0.02259 - R2: 1.0012 -- iter: 0576/1168\n",
      "Training Step: 903  | total loss: \u001b[1m\u001b[32m0.02259\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 048 | loss: 0.02259 - R2: 1.0012 -- iter: 0640/1168\n",
      "Training Step: 904  | total loss: \u001b[1m\u001b[32m0.02296\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 048 | loss: 0.02296 - R2: 1.0012 -- iter: 0704/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 905  | total loss: \u001b[1m\u001b[32m0.02264\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 048 | loss: 0.02264 - R2: 1.0008 -- iter: 0768/1168\n",
      "Training Step: 906  | total loss: \u001b[1m\u001b[32m0.02188\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 048 | loss: 0.02188 - R2: 1.0006 -- iter: 0832/1168\n",
      "Training Step: 907  | total loss: \u001b[1m\u001b[32m0.02194\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 048 | loss: 0.02194 - R2: 1.0004 -- iter: 0896/1168\n",
      "Training Step: 908  | total loss: \u001b[1m\u001b[32m0.04517\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 048 | loss: 0.04517 - R2: 1.0000 -- iter: 0960/1168\n",
      "Training Step: 909  | total loss: \u001b[1m\u001b[32m0.04391\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 048 | loss: 0.04391 - R2: 0.9999 -- iter: 1024/1168\n",
      "Training Step: 910  | total loss: \u001b[1m\u001b[32m0.04391\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 048 | loss: 0.04391 - R2: 0.9999 -- iter: 1088/1168\n",
      "Training Step: 911  | total loss: \u001b[1m\u001b[32m0.03958\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 048 | loss: 0.03958 - R2: 1.0000 -- iter: 1152/1168\n",
      "Training Step: 912  | total loss: \u001b[1m\u001b[32m0.03958\u001b[0m\u001b[0m | time: 1.166s\n",
      "| SGD | epoch: 048 | loss: 0.03958 - R2: 0.9997 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 913  | total loss: \u001b[1m\u001b[32m0.03449\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 049 | loss: 0.03449 - R2: 0.9994 -- iter: 0064/1168\n",
      "Training Step: 914  | total loss: \u001b[1m\u001b[32m0.03581\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 049 | loss: 0.03581 - R2: 0.9998 -- iter: 0128/1168\n",
      "Training Step: 915  | total loss: \u001b[1m\u001b[32m0.03420\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 049 | loss: 0.03420 - R2: 0.9994 -- iter: 0192/1168\n",
      "Training Step: 916  | total loss: \u001b[1m\u001b[32m0.03420\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 049 | loss: 0.03420 - R2: 0.9994 -- iter: 0256/1168\n",
      "Training Step: 917  | total loss: \u001b[1m\u001b[32m0.03240\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 049 | loss: 0.03240 - R2: 0.9996 -- iter: 0320/1168\n",
      "Training Step: 918  | total loss: \u001b[1m\u001b[32m0.03180\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 049 | loss: 0.03180 - R2: 0.9996 -- iter: 0384/1168\n",
      "Training Step: 919  | total loss: \u001b[1m\u001b[32m0.02864\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 049 | loss: 0.02864 - R2: 0.9998 -- iter: 0448/1168\n",
      "Training Step: 920  | total loss: \u001b[1m\u001b[32m0.02864\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 049 | loss: 0.02864 - R2: 0.9997 -- iter: 0512/1168\n",
      "Training Step: 921  | total loss: \u001b[1m\u001b[32m0.02713\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 049 | loss: 0.02713 - R2: 0.9996 -- iter: 0576/1168\n",
      "Training Step: 922  | total loss: \u001b[1m\u001b[32m0.02667\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 049 | loss: 0.02667 - R2: 0.9996 -- iter: 0640/1168\n",
      "Training Step: 923  | total loss: \u001b[1m\u001b[32m0.02672\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 049 | loss: 0.02672 - R2: 0.9996 -- iter: 0704/1168\n",
      "Training Step: 924  | total loss: \u001b[1m\u001b[32m0.02467\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 049 | loss: 0.02467 - R2: 0.9994 -- iter: 0768/1168\n",
      "Training Step: 925  | total loss: \u001b[1m\u001b[32m0.02406\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 049 | loss: 0.02406 - R2: 0.9994 -- iter: 0832/1168\n",
      "Training Step: 926  | total loss: \u001b[1m\u001b[32m0.02342\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 049 | loss: 0.02342 - R2: 0.9990 -- iter: 0896/1168\n",
      "Training Step: 927  | total loss: \u001b[1m\u001b[32m0.02343\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 049 | loss: 0.02343 - R2: 0.9995 -- iter: 0960/1168\n",
      "Training Step: 928  | total loss: \u001b[1m\u001b[32m0.02343\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 049 | loss: 0.02343 - R2: 0.9996 -- iter: 1024/1168\n",
      "Training Step: 929  | total loss: \u001b[1m\u001b[32m0.02435\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 049 | loss: 0.02435 - R2: 0.9996 -- iter: 1088/1168\n",
      "Training Step: 930  | total loss: \u001b[1m\u001b[32m0.02340\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 049 | loss: 0.02340 - R2: 0.9994 -- iter: 1152/1168\n",
      "Training Step: 931  | total loss: \u001b[1m\u001b[32m0.02529\u001b[0m\u001b[0m | time: 1.155s\n",
      "| SGD | epoch: 049 | loss: 0.02529 - R2: 1.0001 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 932  | total loss: \u001b[1m\u001b[32m0.02575\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 050 | loss: 0.02575 - R2: 0.9998 -- iter: 0064/1168\n",
      "Training Step: 933  | total loss: \u001b[1m\u001b[32m0.02454\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 050 | loss: 0.02454 - R2: 0.9993 -- iter: 0128/1168\n",
      "Training Step: 934  | total loss: \u001b[1m\u001b[32m0.02454\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 050 | loss: 0.02454 - R2: 0.9993 -- iter: 0192/1168\n",
      "Training Step: 935  | total loss: \u001b[1m\u001b[32m0.02491\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 050 | loss: 0.02491 - R2: 0.9992 -- iter: 0256/1168\n",
      "Training Step: 936  | total loss: \u001b[1m\u001b[32m0.02423\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 050 | loss: 0.02423 - R2: 0.9990 -- iter: 0320/1168\n",
      "Training Step: 937  | total loss: \u001b[1m\u001b[32m0.02423\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 050 | loss: 0.02423 - R2: 0.9993 -- iter: 0384/1168\n",
      "Training Step: 938  | total loss: \u001b[1m\u001b[32m0.02301\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 050 | loss: 0.02301 - R2: 0.9993 -- iter: 0448/1168\n",
      "Training Step: 939  | total loss: \u001b[1m\u001b[32m0.02364\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 050 | loss: 0.02364 - R2: 0.9994 -- iter: 0512/1168\n",
      "Training Step: 940  | total loss: \u001b[1m\u001b[32m0.02364\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 050 | loss: 0.02364 - R2: 0.9994 -- iter: 0576/1168\n",
      "Training Step: 941  | total loss: \u001b[1m\u001b[32m0.02500\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 050 | loss: 0.02500 - R2: 0.9996 -- iter: 0640/1168\n",
      "Training Step: 942  | total loss: \u001b[1m\u001b[32m0.02399\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 050 | loss: 0.02399 - R2: 0.9997 -- iter: 0704/1168\n",
      "Training Step: 943  | total loss: \u001b[1m\u001b[32m0.02280\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 050 | loss: 0.02280 - R2: 0.9997 -- iter: 0768/1168\n",
      "Training Step: 944  | total loss: \u001b[1m\u001b[32m0.02214\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 050 | loss: 0.02214 - R2: 0.9995 -- iter: 0832/1168\n",
      "Training Step: 945  | total loss: \u001b[1m\u001b[32m0.02155\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 050 | loss: 0.02155 - R2: 0.9995 -- iter: 0896/1168\n",
      "Training Step: 946  | total loss: \u001b[1m\u001b[32m0.02142\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 050 | loss: 0.02142 - R2: 1.0000 -- iter: 0960/1168\n",
      "Training Step: 947  | total loss: \u001b[1m\u001b[32m0.05610\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 050 | loss: 0.05610 - R2: 1.0002 -- iter: 1024/1168\n",
      "Training Step: 948  | total loss: \u001b[1m\u001b[32m0.05271\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 050 | loss: 0.05271 - R2: 0.9999 -- iter: 1088/1168\n",
      "Training Step: 949  | total loss: \u001b[1m\u001b[32m0.04888\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 050 | loss: 0.04888 - R2: 0.9994 -- iter: 1152/1168\n",
      "Training Step: 950  | total loss: \u001b[1m\u001b[32m0.04535\u001b[0m\u001b[0m | time: 1.151s\n",
      "| SGD | epoch: 050 | loss: 0.04535 - R2: 0.9995 | val_loss: 0.01814 - val_acc: 0.9952 -- iter: 1168/1168\n",
      "--\n",
      "---------------------------------\n",
      "Run id: OVRN5X\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "INFO:tensorflow:Summary name StandardError/ (raw) is illegal; using StandardError/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 1168\n",
      "Validation samples: 292\n",
      "--\n",
      "Training Step: 1  | total loss: \u001b[1m\u001b[32m131.05470\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 001 | loss: 131.05470 - R2: 0.0000 -- iter: 0064/1168\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m133.23978\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 001 | loss: 133.23978 - R2: 0.0014 -- iter: 0128/1168\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m133.23978\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 001 | loss: 133.23978 - R2: 0.0014 -- iter: 0192/1168\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m114.80876\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 001 | loss: 114.80876 - R2: 0.0117 -- iter: 0256/1168\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m114.80876\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 001 | loss: 114.80876 - R2: 0.0197 -- iter: 0320/1168\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m100.74754\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 001 | loss: 100.74754 - R2: 0.0298 -- iter: 0384/1168\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m92.50647\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 001 | loss: 92.50647 - R2: 0.0421 -- iter: 0448/1168\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m85.21091\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 001 | loss: 85.21091 - R2: 0.0561 -- iter: 0512/1168\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m78.99751\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 001 | loss: 78.99751 - R2: 0.0717 -- iter: 0576/1168\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m78.99751\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 001 | loss: 78.99751 - R2: 0.0717 -- iter: 0640/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m71.83313\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 001 | loss: 71.83313 - R2: 0.0908 -- iter: 0704/1168\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m65.21367\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 001 | loss: 65.21367 - R2: 0.1125 -- iter: 0768/1168\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m51.73347\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 001 | loss: 51.73347 - R2: 0.1688 -- iter: 0832/1168\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m45.18029\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 001 | loss: 45.18029 - R2: 0.2040 -- iter: 0896/1168\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m38.80954\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 001 | loss: 38.80954 - R2: 0.2449 -- iter: 0960/1168\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m38.80954\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 001 | loss: 38.80954 - R2: 0.2449 -- iter: 1024/1168\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m32.54639\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 001 | loss: 32.54639 - R2: 0.2929 -- iter: 1088/1168\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m26.78389\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 001 | loss: 26.78389 - R2: 0.3466 -- iter: 1152/1168\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m21.59310\u001b[0m\u001b[0m | time: 1.153s\n",
      "| SGD | epoch: 001 | loss: 21.59310 - R2: 0.4058 | val_loss: 3.47561 - val_acc: 0.7186 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m16.93124\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 002 | loss: 16.93124 - R2: 0.4720 -- iter: 0064/1168\n",
      "Training Step: 21  | total loss: \u001b[1m\u001b[32m12.85317\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 002 | loss: 12.85317 - R2: 0.5458 -- iter: 0128/1168\n",
      "Training Step: 22  | total loss: \u001b[1m\u001b[32m9.42878\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 002 | loss: 9.42878 - R2: 0.6284 -- iter: 0192/1168\n",
      "Training Step: 23  | total loss: \u001b[1m\u001b[32m6.75836\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 002 | loss: 6.75836 - R2: 0.7212 -- iter: 0256/1168\n",
      "Training Step: 24  | total loss: \u001b[1m\u001b[32m4.90533\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 002 | loss: 4.90533 - R2: 0.7968 -- iter: 0320/1168\n",
      "Training Step: 25  | total loss: \u001b[1m\u001b[32m3.61688\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 002 | loss: 3.61688 - R2: 0.8569 -- iter: 0384/1168\n",
      "Training Step: 26  | total loss: \u001b[1m\u001b[32m2.05717\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 002 | loss: 2.05717 - R2: 0.9175 -- iter: 0448/1168\n",
      "Training Step: 27  | total loss: \u001b[1m\u001b[32m1.57294\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 002 | loss: 1.57294 - R2: 0.9175 -- iter: 0512/1168\n",
      "Training Step: 28  | total loss: \u001b[1m\u001b[32m1.57294\u001b[0m\u001b[0m | time: 0.189s\n",
      "| SGD | epoch: 002 | loss: 1.57294 - R2: 0.9399 -- iter: 0576/1168\n",
      "Training Step: 29  | total loss: \u001b[1m\u001b[32m1.21414\u001b[0m\u001b[0m | time: 0.195s\n",
      "| SGD | epoch: 002 | loss: 1.21414 - R2: 0.9574 -- iter: 0640/1168\n",
      "Training Step: 30  | total loss: \u001b[1m\u001b[32m0.95125\u001b[0m\u001b[0m | time: 0.200s\n",
      "| SGD | epoch: 002 | loss: 0.95125 - R2: 0.9667 -- iter: 0704/1168\n",
      "Training Step: 31  | total loss: \u001b[1m\u001b[32m0.79375\u001b[0m\u001b[0m | time: 0.202s\n",
      "| SGD | epoch: 002 | loss: 0.79375 - R2: 0.9695 -- iter: 0768/1168\n",
      "Training Step: 32  | total loss: \u001b[1m\u001b[32m0.65631\u001b[0m\u001b[0m | time: 0.208s\n",
      "| SGD | epoch: 002 | loss: 0.65631 - R2: 0.9776 -- iter: 0832/1168\n",
      "Training Step: 33  | total loss: \u001b[1m\u001b[32m0.54770\u001b[0m\u001b[0m | time: 0.211s\n",
      "| SGD | epoch: 002 | loss: 0.54770 - R2: 0.9839 -- iter: 0896/1168\n",
      "Training Step: 34  | total loss: \u001b[1m\u001b[32m0.40389\u001b[0m\u001b[0m | time: 0.213s\n",
      "| SGD | epoch: 002 | loss: 0.40389 - R2: 0.9904 -- iter: 0960/1168\n",
      "Training Step: 35  | total loss: \u001b[1m\u001b[32m0.40389\u001b[0m\u001b[0m | time: 0.216s\n",
      "| SGD | epoch: 002 | loss: 0.40389 - R2: 0.9904 -- iter: 1024/1168\n",
      "Training Step: 36  | total loss: \u001b[1m\u001b[32m0.32599\u001b[0m\u001b[0m | time: 0.219s\n",
      "| SGD | epoch: 002 | loss: 0.32599 - R2: 0.9896 -- iter: 1088/1168\n",
      "Training Step: 37  | total loss: \u001b[1m\u001b[32m0.32599\u001b[0m\u001b[0m | time: 0.225s\n",
      "| SGD | epoch: 002 | loss: 0.32599 - R2: 0.9903 -- iter: 1152/1168\n",
      "Training Step: 38  | total loss: \u001b[1m\u001b[32m0.30341\u001b[0m\u001b[0m | time: 1.230s\n",
      "| SGD | epoch: 002 | loss: 0.30341 - R2: 0.9924 | val_loss: 0.15136 - val_acc: 1.0052 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 39  | total loss: \u001b[1m\u001b[32m0.27507\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 003 | loss: 0.27507 - R2: 0.9944 -- iter: 0064/1168\n",
      "Training Step: 40  | total loss: \u001b[1m\u001b[32m0.25267\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 003 | loss: 0.25267 - R2: 0.9907 -- iter: 0128/1168\n",
      "Training Step: 41  | total loss: \u001b[1m\u001b[32m0.23145\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 003 | loss: 0.23145 - R2: 0.9902 -- iter: 0192/1168\n",
      "Training Step: 42  | total loss: \u001b[1m\u001b[32m0.22132\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 003 | loss: 0.22132 - R2: 0.9981 -- iter: 0256/1168\n",
      "Training Step: 43  | total loss: \u001b[1m\u001b[32m0.22320\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 003 | loss: 0.22320 - R2: 0.9986 -- iter: 0320/1168\n",
      "Training Step: 44  | total loss: \u001b[1m\u001b[32m0.20544\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 003 | loss: 0.20544 - R2: 0.9983 -- iter: 0384/1168\n",
      "Training Step: 45  | total loss: \u001b[1m\u001b[32m0.18891\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 003 | loss: 0.18891 - R2: 0.9991 -- iter: 0448/1168\n",
      "Training Step: 46  | total loss: \u001b[1m\u001b[32m0.18087\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 003 | loss: 0.18087 - R2: 1.0003 -- iter: 0512/1168\n",
      "Training Step: 47  | total loss: \u001b[1m\u001b[32m0.17978\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 003 | loss: 0.17978 - R2: 0.9992 -- iter: 0576/1168\n",
      "Training Step: 48  | total loss: \u001b[1m\u001b[32m0.18730\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 003 | loss: 0.18730 - R2: 0.9988 -- iter: 0640/1168\n",
      "Training Step: 49  | total loss: \u001b[1m\u001b[32m0.18438\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 003 | loss: 0.18438 - R2: 0.9985 -- iter: 0704/1168\n",
      "Training Step: 50  | total loss: \u001b[1m\u001b[32m0.18074\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 003 | loss: 0.18074 - R2: 1.0019 -- iter: 0768/1168\n",
      "Training Step: 51  | total loss: \u001b[1m\u001b[32m0.18074\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 003 | loss: 0.18074 - R2: 1.0019 -- iter: 0832/1168\n",
      "Training Step: 52  | total loss: \u001b[1m\u001b[32m0.17740\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 003 | loss: 0.17740 - R2: 0.9996 -- iter: 0896/1168\n",
      "Training Step: 53  | total loss: \u001b[1m\u001b[32m0.17533\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 003 | loss: 0.17533 - R2: 0.9974 -- iter: 0960/1168\n",
      "Training Step: 54  | total loss: \u001b[1m\u001b[32m0.16456\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 003 | loss: 0.16456 - R2: 0.9977 -- iter: 1024/1168\n",
      "Training Step: 55  | total loss: \u001b[1m\u001b[32m0.16096\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 003 | loss: 0.16096 - R2: 0.9988 -- iter: 1088/1168\n",
      "Training Step: 56  | total loss: \u001b[1m\u001b[32m0.16112\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 003 | loss: 0.16112 - R2: 1.0002 -- iter: 1152/1168\n",
      "Training Step: 57  | total loss: \u001b[1m\u001b[32m0.15805\u001b[0m\u001b[0m | time: 1.110s\n",
      "| SGD | epoch: 003 | loss: 0.15805 - R2: 0.9996 | val_loss: 0.15046 - val_acc: 1.0026 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 58  | total loss: \u001b[1m\u001b[32m0.15089\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 004 | loss: 0.15089 - R2: 1.0006 -- iter: 0064/1168\n",
      "Training Step: 59  | total loss: \u001b[1m\u001b[32m0.16928\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 004 | loss: 0.16928 - R2: 0.9965 -- iter: 0128/1168\n",
      "Training Step: 60  | total loss: \u001b[1m\u001b[32m0.16928\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 004 | loss: 0.16928 - R2: 0.9965 -- iter: 0192/1168\n",
      "Training Step: 61  | total loss: \u001b[1m\u001b[32m0.17822\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 004 | loss: 0.17822 - R2: 0.9958 -- iter: 0256/1168\n",
      "Training Step: 62  | total loss: \u001b[1m\u001b[32m0.17822\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 004 | loss: 0.17822 - R2: 0.9958 -- iter: 0320/1168\n",
      "Training Step: 63  | total loss: \u001b[1m\u001b[32m0.17258\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 004 | loss: 0.17258 - R2: 0.9976 -- iter: 0384/1168\n",
      "Training Step: 64  | total loss: \u001b[1m\u001b[32m0.17258\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 004 | loss: 0.17258 - R2: 0.9976 -- iter: 0448/1168\n",
      "Training Step: 65  | total loss: \u001b[1m\u001b[32m0.18050\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 004 | loss: 0.18050 - R2: 1.0001 -- iter: 0512/1168\n",
      "Training Step: 66  | total loss: \u001b[1m\u001b[32m0.18050\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 004 | loss: 0.18050 - R2: 1.0001 -- iter: 0576/1168\n",
      "Training Step: 67  | total loss: \u001b[1m\u001b[32m0.17856\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 004 | loss: 0.17856 - R2: 0.9998 -- iter: 0640/1168\n",
      "Training Step: 68  | total loss: \u001b[1m\u001b[32m0.17402\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 004 | loss: 0.17402 - R2: 0.9985 -- iter: 0704/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 69  | total loss: \u001b[1m\u001b[32m0.16620\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 004 | loss: 0.16620 - R2: 0.9999 -- iter: 0768/1168\n",
      "Training Step: 70  | total loss: \u001b[1m\u001b[32m0.16754\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 004 | loss: 0.16754 - R2: 1.0011 -- iter: 0832/1168\n",
      "Training Step: 71  | total loss: \u001b[1m\u001b[32m0.17113\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 004 | loss: 0.17113 - R2: 0.9998 -- iter: 0896/1168\n",
      "Training Step: 72  | total loss: \u001b[1m\u001b[32m0.17113\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 004 | loss: 0.17113 - R2: 0.9998 -- iter: 0960/1168\n",
      "Training Step: 73  | total loss: \u001b[1m\u001b[32m0.17469\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 004 | loss: 0.17469 - R2: 1.0014 -- iter: 1024/1168\n",
      "Training Step: 74  | total loss: \u001b[1m\u001b[32m0.16630\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 004 | loss: 0.16630 - R2: 1.0014 -- iter: 1088/1168\n",
      "Training Step: 75  | total loss: \u001b[1m\u001b[32m0.16598\u001b[0m\u001b[0m | time: 0.169s\n",
      "| SGD | epoch: 004 | loss: 0.16598 - R2: 1.0011 -- iter: 1152/1168\n",
      "Training Step: 76  | total loss: \u001b[1m\u001b[32m0.16598\u001b[0m\u001b[0m | time: 1.174s\n",
      "| SGD | epoch: 004 | loss: 0.16598 - R2: 1.0011 | val_loss: 0.15049 - val_acc: 1.0027 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 77  | total loss: \u001b[1m\u001b[32m0.16143\u001b[0m\u001b[0m | time: 0.013s\n",
      "| SGD | epoch: 005 | loss: 0.16143 - R2: 1.0007 -- iter: 0064/1168\n",
      "Training Step: 78  | total loss: \u001b[1m\u001b[32m0.16194\u001b[0m\u001b[0m | time: 0.019s\n",
      "| SGD | epoch: 005 | loss: 0.16194 - R2: 1.0013 -- iter: 0128/1168\n",
      "Training Step: 79  | total loss: \u001b[1m\u001b[32m0.16254\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 005 | loss: 0.16254 - R2: 1.0021 -- iter: 0192/1168\n",
      "Training Step: 80  | total loss: \u001b[1m\u001b[32m0.16256\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 005 | loss: 0.16256 - R2: 1.0007 -- iter: 0256/1168\n",
      "Training Step: 81  | total loss: \u001b[1m\u001b[32m0.16608\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 005 | loss: 0.16608 - R2: 0.9991 -- iter: 0320/1168\n",
      "Training Step: 82  | total loss: \u001b[1m\u001b[32m0.16608\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 005 | loss: 0.16608 - R2: 0.9995 -- iter: 0384/1168\n",
      "Training Step: 83  | total loss: \u001b[1m\u001b[32m0.16642\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 005 | loss: 0.16642 - R2: 0.9995 -- iter: 0448/1168\n",
      "Training Step: 84  | total loss: \u001b[1m\u001b[32m0.16872\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 005 | loss: 0.16872 - R2: 0.9978 -- iter: 0512/1168\n",
      "Training Step: 85  | total loss: \u001b[1m\u001b[32m0.16998\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 005 | loss: 0.16998 - R2: 0.9973 -- iter: 0576/1168\n",
      "Training Step: 86  | total loss: \u001b[1m\u001b[32m0.16597\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 005 | loss: 0.16597 - R2: 0.9980 -- iter: 0640/1168\n",
      "Training Step: 87  | total loss: \u001b[1m\u001b[32m0.16515\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 005 | loss: 0.16515 - R2: 0.9979 -- iter: 0704/1168\n",
      "Training Step: 88  | total loss: \u001b[1m\u001b[32m0.16180\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 005 | loss: 0.16180 - R2: 0.9985 -- iter: 0768/1168\n",
      "Training Step: 89  | total loss: \u001b[1m\u001b[32m0.16180\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 005 | loss: 0.16180 - R2: 0.9988 -- iter: 0832/1168\n",
      "Training Step: 90  | total loss: \u001b[1m\u001b[32m0.16144\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 005 | loss: 0.16144 - R2: 0.9988 -- iter: 0896/1168\n",
      "Training Step: 91  | total loss: \u001b[1m\u001b[32m0.15816\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 005 | loss: 0.15816 - R2: 0.9988 -- iter: 0960/1168\n",
      "Training Step: 92  | total loss: \u001b[1m\u001b[32m0.15816\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 005 | loss: 0.15816 - R2: 0.9992 -- iter: 1024/1168\n",
      "Training Step: 93  | total loss: \u001b[1m\u001b[32m0.15601\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 005 | loss: 0.15601 - R2: 0.9993 -- iter: 1088/1168\n",
      "Training Step: 94  | total loss: \u001b[1m\u001b[32m0.15832\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 005 | loss: 0.15832 - R2: 0.9995 -- iter: 1152/1168\n",
      "Training Step: 95  | total loss: \u001b[1m\u001b[32m0.15411\u001b[0m\u001b[0m | time: 1.084s\n",
      "| SGD | epoch: 005 | loss: 0.15411 - R2: 0.9995 | val_loss: 0.15066 - val_acc: 1.0033 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 96  | total loss: \u001b[1m\u001b[32m0.15279\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 006 | loss: 0.15279 - R2: 0.9996 -- iter: 0064/1168\n",
      "Training Step: 97  | total loss: \u001b[1m\u001b[32m0.15271\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 006 | loss: 0.15271 - R2: 0.9993 -- iter: 0128/1168\n",
      "Training Step: 98  | total loss: \u001b[1m\u001b[32m0.15943\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 006 | loss: 0.15943 - R2: 0.9995 -- iter: 0192/1168\n",
      "Training Step: 99  | total loss: \u001b[1m\u001b[32m0.15636\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 006 | loss: 0.15636 - R2: 0.9995 -- iter: 0256/1168\n",
      "Training Step: 100  | total loss: \u001b[1m\u001b[32m0.15711\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 006 | loss: 0.15711 - R2: 0.9960 -- iter: 0320/1168\n",
      "Training Step: 101  | total loss: \u001b[1m\u001b[32m0.15747\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 006 | loss: 0.15747 - R2: 0.9930 -- iter: 0384/1168\n",
      "Training Step: 102  | total loss: \u001b[1m\u001b[32m0.15933\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 006 | loss: 0.15933 - R2: 0.9931 -- iter: 0448/1168\n",
      "Training Step: 103  | total loss: \u001b[1m\u001b[32m0.16152\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 006 | loss: 0.16152 - R2: 0.9941 -- iter: 0512/1168\n",
      "Training Step: 104  | total loss: \u001b[1m\u001b[32m0.16561\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 006 | loss: 0.16561 - R2: 0.9952 -- iter: 0576/1168\n",
      "Training Step: 105  | total loss: \u001b[1m\u001b[32m0.16647\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 006 | loss: 0.16647 - R2: 0.9960 -- iter: 0640/1168\n",
      "Training Step: 106  | total loss: \u001b[1m\u001b[32m0.16720\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 006 | loss: 0.16720 - R2: 0.9963 -- iter: 0704/1168\n",
      "Training Step: 107  | total loss: \u001b[1m\u001b[32m0.16971\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 006 | loss: 0.16971 - R2: 0.9969 -- iter: 0768/1168\n",
      "Training Step: 108  | total loss: \u001b[1m\u001b[32m0.17262\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 006 | loss: 0.17262 - R2: 0.9969 -- iter: 0832/1168\n",
      "Training Step: 109  | total loss: \u001b[1m\u001b[32m0.17178\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 006 | loss: 0.17178 - R2: 0.9977 -- iter: 0896/1168\n",
      "Training Step: 110  | total loss: \u001b[1m\u001b[32m0.16626\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 006 | loss: 0.16626 - R2: 0.9978 -- iter: 0960/1168\n",
      "Training Step: 111  | total loss: \u001b[1m\u001b[32m0.16273\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 006 | loss: 0.16273 - R2: 0.9981 -- iter: 1024/1168\n",
      "Training Step: 112  | total loss: \u001b[1m\u001b[32m0.15977\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 006 | loss: 0.15977 - R2: 0.9982 -- iter: 1088/1168\n",
      "Training Step: 113  | total loss: \u001b[1m\u001b[32m0.15316\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 006 | loss: 0.15316 - R2: 0.9982 -- iter: 1152/1168\n",
      "Training Step: 114  | total loss: \u001b[1m\u001b[32m0.15642\u001b[0m\u001b[0m | time: 1.168s\n",
      "| SGD | epoch: 006 | loss: 0.15642 - R2: 0.9994 | val_loss: 0.15121 - val_acc: 1.0048 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 115  | total loss: \u001b[1m\u001b[32m0.15763\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 007 | loss: 0.15763 - R2: 1.0003 -- iter: 0064/1168\n",
      "Training Step: 116  | total loss: \u001b[1m\u001b[32m0.15993\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 007 | loss: 0.15993 - R2: 1.0000 -- iter: 0128/1168\n",
      "Training Step: 117  | total loss: \u001b[1m\u001b[32m0.16569\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 007 | loss: 0.16569 - R2: 0.9982 -- iter: 0192/1168\n",
      "Training Step: 118  | total loss: \u001b[1m\u001b[32m0.16569\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 007 | loss: 0.16569 - R2: 0.9982 -- iter: 0256/1168\n",
      "Training Step: 119  | total loss: \u001b[1m\u001b[32m0.16490\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 007 | loss: 0.16490 - R2: 0.9985 -- iter: 0320/1168\n",
      "Training Step: 120  | total loss: \u001b[1m\u001b[32m0.17777\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 007 | loss: 0.17777 - R2: 1.0004 -- iter: 0384/1168\n",
      "Training Step: 121  | total loss: \u001b[1m\u001b[32m0.17777\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 007 | loss: 0.17777 - R2: 1.0004 -- iter: 0448/1168\n",
      "Training Step: 122  | total loss: \u001b[1m\u001b[32m0.17228\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 007 | loss: 0.17228 - R2: 1.0005 -- iter: 0512/1168\n",
      "Training Step: 123  | total loss: \u001b[1m\u001b[32m0.17254\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 007 | loss: 0.17254 - R2: 1.0010 -- iter: 0576/1168\n",
      "Training Step: 124  | total loss: \u001b[1m\u001b[32m0.17138\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 007 | loss: 0.17138 - R2: 1.0018 -- iter: 0640/1168\n",
      "Training Step: 125  | total loss: \u001b[1m\u001b[32m0.17366\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 007 | loss: 0.17366 - R2: 1.0003 -- iter: 0704/1168\n",
      "Training Step: 126  | total loss: \u001b[1m\u001b[32m0.17302\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 007 | loss: 0.17302 - R2: 1.0019 -- iter: 0768/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 127  | total loss: \u001b[1m\u001b[32m0.17213\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 007 | loss: 0.17213 - R2: 1.0010 -- iter: 0832/1168\n",
      "Training Step: 128  | total loss: \u001b[1m\u001b[32m0.16558\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 007 | loss: 0.16558 - R2: 1.0008 -- iter: 0896/1168\n",
      "Training Step: 129  | total loss: \u001b[1m\u001b[32m0.16558\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 007 | loss: 0.16558 - R2: 1.0008 -- iter: 0960/1168\n",
      "Training Step: 130  | total loss: \u001b[1m\u001b[32m0.16136\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 007 | loss: 0.16136 - R2: 1.0008 -- iter: 1024/1168\n",
      "Training Step: 131  | total loss: \u001b[1m\u001b[32m0.16136\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 007 | loss: 0.16136 - R2: 1.0001 -- iter: 1088/1168\n",
      "Training Step: 132  | total loss: \u001b[1m\u001b[32m0.16941\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 007 | loss: 0.16941 - R2: 0.9998 -- iter: 1152/1168\n",
      "Training Step: 133  | total loss: \u001b[1m\u001b[32m0.16656\u001b[0m\u001b[0m | time: 1.172s\n",
      "| SGD | epoch: 007 | loss: 0.16656 - R2: 0.9998 | val_loss: 0.15110 - val_acc: 1.0046 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 134  | total loss: \u001b[1m\u001b[32m0.16424\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 008 | loss: 0.16424 - R2: 0.9998 -- iter: 0064/1168\n",
      "Training Step: 135  | total loss: \u001b[1m\u001b[32m0.16109\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 008 | loss: 0.16109 - R2: 0.9990 -- iter: 0128/1168\n",
      "Training Step: 136  | total loss: \u001b[1m\u001b[32m0.15848\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 008 | loss: 0.15848 - R2: 0.9985 -- iter: 0192/1168\n",
      "Training Step: 137  | total loss: \u001b[1m\u001b[32m0.15848\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 008 | loss: 0.15848 - R2: 0.9985 -- iter: 0256/1168\n",
      "Training Step: 138  | total loss: \u001b[1m\u001b[32m0.15872\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 008 | loss: 0.15872 - R2: 1.0001 -- iter: 0320/1168\n",
      "Training Step: 139  | total loss: \u001b[1m\u001b[32m0.14888\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 008 | loss: 0.14888 - R2: 1.0019 -- iter: 0384/1168\n",
      "Training Step: 140  | total loss: \u001b[1m\u001b[32m0.14000\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 008 | loss: 0.14000 - R2: 1.0036 -- iter: 0448/1168\n",
      "Training Step: 141  | total loss: \u001b[1m\u001b[32m0.14005\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 008 | loss: 0.14005 - R2: 1.0041 -- iter: 0512/1168\n",
      "Training Step: 142  | total loss: \u001b[1m\u001b[32m0.14382\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 008 | loss: 0.14382 - R2: 1.0042 -- iter: 0576/1168\n",
      "Training Step: 143  | total loss: \u001b[1m\u001b[32m0.14290\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 008 | loss: 0.14290 - R2: 1.0035 -- iter: 0640/1168\n",
      "Training Step: 144  | total loss: \u001b[1m\u001b[32m0.15254\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 008 | loss: 0.15254 - R2: 1.0029 -- iter: 0704/1168\n",
      "Training Step: 145  | total loss: \u001b[1m\u001b[32m0.15937\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 008 | loss: 0.15937 - R2: 1.0045 -- iter: 0768/1168\n",
      "Training Step: 146  | total loss: \u001b[1m\u001b[32m0.16244\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 008 | loss: 0.16244 - R2: 1.0034 -- iter: 0832/1168\n",
      "Training Step: 147  | total loss: \u001b[1m\u001b[32m0.16195\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 008 | loss: 0.16195 - R2: 1.0036 -- iter: 0896/1168\n",
      "Training Step: 148  | total loss: \u001b[1m\u001b[32m0.16195\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 008 | loss: 0.16195 - R2: 1.0036 -- iter: 0960/1168\n",
      "Training Step: 149  | total loss: \u001b[1m\u001b[32m0.16134\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 008 | loss: 0.16134 - R2: 1.0031 -- iter: 1024/1168\n",
      "Training Step: 150  | total loss: \u001b[1m\u001b[32m0.15680\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 008 | loss: 0.15680 - R2: 1.0031 -- iter: 1088/1168\n",
      "Training Step: 151  | total loss: \u001b[1m\u001b[32m0.15570\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 008 | loss: 0.15570 - R2: 1.0031 -- iter: 1152/1168\n",
      "Training Step: 152  | total loss: \u001b[1m\u001b[32m0.15826\u001b[0m\u001b[0m | time: 1.172s\n",
      "| SGD | epoch: 008 | loss: 0.15826 - R2: 1.0010 | val_loss: 0.15096 - val_acc: 1.0042 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 153  | total loss: \u001b[1m\u001b[32m0.15990\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 009 | loss: 0.15990 - R2: 1.0013 -- iter: 0064/1168\n",
      "Training Step: 154  | total loss: \u001b[1m\u001b[32m0.16361\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 009 | loss: 0.16361 - R2: 1.0000 -- iter: 0128/1168\n",
      "Training Step: 155  | total loss: \u001b[1m\u001b[32m0.16132\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 009 | loss: 0.16132 - R2: 1.0008 -- iter: 0192/1168\n",
      "Training Step: 156  | total loss: \u001b[1m\u001b[32m0.16225\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 009 | loss: 0.16225 - R2: 1.0009 -- iter: 0256/1168\n",
      "Training Step: 157  | total loss: \u001b[1m\u001b[32m0.16225\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 009 | loss: 0.16225 - R2: 1.0009 -- iter: 0320/1168\n",
      "Training Step: 158  | total loss: \u001b[1m\u001b[32m0.16511\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 009 | loss: 0.16511 - R2: 1.0004 -- iter: 0384/1168\n",
      "Training Step: 159  | total loss: \u001b[1m\u001b[32m0.16489\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 009 | loss: 0.16489 - R2: 1.0009 -- iter: 0448/1168\n",
      "Training Step: 160  | total loss: \u001b[1m\u001b[32m0.16637\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 009 | loss: 0.16637 - R2: 1.0018 -- iter: 0512/1168\n",
      "Training Step: 161  | total loss: \u001b[1m\u001b[32m0.16249\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 009 | loss: 0.16249 - R2: 1.0015 -- iter: 0576/1168\n",
      "Training Step: 162  | total loss: \u001b[1m\u001b[32m0.16269\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 009 | loss: 0.16269 - R2: 1.0013 -- iter: 0640/1168\n",
      "Training Step: 163  | total loss: \u001b[1m\u001b[32m0.16269\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 009 | loss: 0.16269 - R2: 1.0013 -- iter: 0704/1168\n",
      "Training Step: 164  | total loss: \u001b[1m\u001b[32m0.16130\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 009 | loss: 0.16130 - R2: 1.0013 -- iter: 0768/1168\n",
      "Training Step: 165  | total loss: \u001b[1m\u001b[32m0.16173\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 009 | loss: 0.16173 - R2: 1.0013 -- iter: 0832/1168\n",
      "Training Step: 166  | total loss: \u001b[1m\u001b[32m0.16551\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 009 | loss: 0.16551 - R2: 1.0013 -- iter: 0896/1168\n",
      "Training Step: 167  | total loss: \u001b[1m\u001b[32m0.16383\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 009 | loss: 0.16383 - R2: 1.0013 -- iter: 0960/1168\n",
      "Training Step: 168  | total loss: \u001b[1m\u001b[32m0.15629\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 009 | loss: 0.15629 - R2: 1.0013 -- iter: 1024/1168\n",
      "Training Step: 169  | total loss: \u001b[1m\u001b[32m0.16295\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 009 | loss: 0.16295 - R2: 1.0001 -- iter: 1088/1168\n",
      "Training Step: 170  | total loss: \u001b[1m\u001b[32m0.16447\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 009 | loss: 0.16447 - R2: 1.0010 -- iter: 1152/1168\n",
      "Training Step: 171  | total loss: \u001b[1m\u001b[32m0.16145\u001b[0m\u001b[0m | time: 1.141s\n",
      "| SGD | epoch: 009 | loss: 0.16145 - R2: 1.0009 | val_loss: 0.15093 - val_acc: 1.0041 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 172  | total loss: \u001b[1m\u001b[32m0.16576\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 010 | loss: 0.16576 - R2: 1.0018 -- iter: 0064/1168\n",
      "Training Step: 173  | total loss: \u001b[1m\u001b[32m0.16576\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 010 | loss: 0.16576 - R2: 1.0010 -- iter: 0128/1168\n",
      "Training Step: 174  | total loss: \u001b[1m\u001b[32m0.16041\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 010 | loss: 0.16041 - R2: 1.0024 -- iter: 0192/1168\n",
      "Training Step: 175  | total loss: \u001b[1m\u001b[32m0.15651\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 010 | loss: 0.15651 - R2: 1.0033 -- iter: 0256/1168\n",
      "Training Step: 176  | total loss: \u001b[1m\u001b[32m0.15947\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 010 | loss: 0.15947 - R2: 1.0045 -- iter: 0320/1168\n",
      "Training Step: 177  | total loss: \u001b[1m\u001b[32m0.15947\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 010 | loss: 0.15947 - R2: 1.0045 -- iter: 0384/1168\n",
      "Training Step: 178  | total loss: \u001b[1m\u001b[32m0.15899\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 010 | loss: 0.15899 - R2: 1.0048 -- iter: 0448/1168\n",
      "Training Step: 179  | total loss: \u001b[1m\u001b[32m0.16152\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 010 | loss: 0.16152 - R2: 1.0054 -- iter: 0512/1168\n",
      "Training Step: 180  | total loss: \u001b[1m\u001b[32m0.16646\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 010 | loss: 0.16646 - R2: 1.0054 -- iter: 0576/1168\n",
      "Training Step: 181  | total loss: \u001b[1m\u001b[32m0.17090\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 010 | loss: 0.17090 - R2: 1.0065 -- iter: 0640/1168\n",
      "Training Step: 182  | total loss: \u001b[1m\u001b[32m0.16695\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 010 | loss: 0.16695 - R2: 1.0050 -- iter: 0704/1168\n",
      "Training Step: 183  | total loss: \u001b[1m\u001b[32m0.16669\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 010 | loss: 0.16669 - R2: 1.0040 -- iter: 0768/1168\n",
      "Training Step: 184  | total loss: \u001b[1m\u001b[32m0.16401\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 010 | loss: 0.16401 - R2: 1.0016 -- iter: 0832/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 185  | total loss: \u001b[1m\u001b[32m0.16910\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 010 | loss: 0.16910 - R2: 0.9996 -- iter: 0896/1168\n",
      "Training Step: 186  | total loss: \u001b[1m\u001b[32m0.16937\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 010 | loss: 0.16937 - R2: 0.9992 -- iter: 0960/1168\n",
      "Training Step: 187  | total loss: \u001b[1m\u001b[32m0.16714\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 010 | loss: 0.16714 - R2: 0.9997 -- iter: 1024/1168\n",
      "Training Step: 188  | total loss: \u001b[1m\u001b[32m0.16714\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 010 | loss: 0.16714 - R2: 0.9997 -- iter: 1088/1168\n",
      "Training Step: 189  | total loss: \u001b[1m\u001b[32m0.16548\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 010 | loss: 0.16548 - R2: 1.0005 -- iter: 1152/1168\n",
      "Training Step: 190  | total loss: \u001b[1m\u001b[32m0.16454\u001b[0m\u001b[0m | time: 1.139s\n",
      "| SGD | epoch: 010 | loss: 0.16454 - R2: 0.9995 | val_loss: 0.15090 - val_acc: 1.0041 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 191  | total loss: \u001b[1m\u001b[32m0.16454\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 011 | loss: 0.16454 - R2: 1.0007 -- iter: 0064/1168\n",
      "Training Step: 192  | total loss: \u001b[1m\u001b[32m0.16387\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 011 | loss: 0.16387 - R2: 1.0007 -- iter: 0128/1168\n",
      "Training Step: 193  | total loss: \u001b[1m\u001b[32m0.16688\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 011 | loss: 0.16688 - R2: 1.0004 -- iter: 0192/1168\n",
      "Training Step: 194  | total loss: \u001b[1m\u001b[32m0.16058\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 011 | loss: 0.16058 - R2: 1.0005 -- iter: 0256/1168\n",
      "Training Step: 195  | total loss: \u001b[1m\u001b[32m0.16171\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 011 | loss: 0.16171 - R2: 1.0010 -- iter: 0320/1168\n",
      "Training Step: 196  | total loss: \u001b[1m\u001b[32m0.16388\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 011 | loss: 0.16388 - R2: 1.0009 -- iter: 0384/1168\n",
      "Training Step: 197  | total loss: \u001b[1m\u001b[32m0.16396\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 011 | loss: 0.16396 - R2: 1.0000 -- iter: 0448/1168\n",
      "Training Step: 198  | total loss: \u001b[1m\u001b[32m0.16396\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 011 | loss: 0.16396 - R2: 1.0000 -- iter: 0512/1168\n",
      "Training Step: 199  | total loss: \u001b[1m\u001b[32m0.16469\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 011 | loss: 0.16469 - R2: 0.9983 -- iter: 0576/1168\n",
      "Training Step: 200  | total loss: \u001b[1m\u001b[32m0.16469\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 011 | loss: 0.16469 - R2: 0.9983 -- iter: 0640/1168\n",
      "Training Step: 201  | total loss: \u001b[1m\u001b[32m0.16505\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 011 | loss: 0.16505 - R2: 0.9979 -- iter: 0704/1168\n",
      "Training Step: 202  | total loss: \u001b[1m\u001b[32m0.16505\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 011 | loss: 0.16505 - R2: 0.9979 -- iter: 0768/1168\n",
      "Training Step: 203  | total loss: \u001b[1m\u001b[32m0.16168\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 011 | loss: 0.16168 - R2: 0.9984 -- iter: 0832/1168\n",
      "Training Step: 204  | total loss: \u001b[1m\u001b[32m0.15836\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 011 | loss: 0.15836 - R2: 0.9991 -- iter: 0896/1168\n",
      "Training Step: 205  | total loss: \u001b[1m\u001b[32m0.16005\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 011 | loss: 0.16005 - R2: 0.9991 -- iter: 0960/1168\n",
      "Training Step: 206  | total loss: \u001b[1m\u001b[32m0.16856\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 011 | loss: 0.16856 - R2: 0.9977 -- iter: 1024/1168\n",
      "Training Step: 207  | total loss: \u001b[1m\u001b[32m0.16772\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 011 | loss: 0.16772 - R2: 0.9990 -- iter: 1088/1168\n",
      "Training Step: 208  | total loss: \u001b[1m\u001b[32m0.16772\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 011 | loss: 0.16772 - R2: 0.9990 -- iter: 1152/1168\n",
      "Training Step: 209  | total loss: \u001b[1m\u001b[32m0.16644\u001b[0m\u001b[0m | time: 1.127s\n",
      "| SGD | epoch: 011 | loss: 0.16644 - R2: 0.9998 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 210  | total loss: \u001b[1m\u001b[32m0.16942\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 012 | loss: 0.16942 - R2: 0.9990 -- iter: 0064/1168\n",
      "Training Step: 211  | total loss: \u001b[1m\u001b[32m0.16605\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 012 | loss: 0.16605 - R2: 1.0000 -- iter: 0128/1168\n",
      "Training Step: 212  | total loss: \u001b[1m\u001b[32m0.16605\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 012 | loss: 0.16605 - R2: 1.0000 -- iter: 0192/1168\n",
      "Training Step: 213  | total loss: \u001b[1m\u001b[32m0.16741\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 012 | loss: 0.16741 - R2: 0.9999 -- iter: 0256/1168\n",
      "Training Step: 214  | total loss: \u001b[1m\u001b[32m0.16376\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 012 | loss: 0.16376 - R2: 1.0007 -- iter: 0320/1168\n",
      "Training Step: 215  | total loss: \u001b[1m\u001b[32m0.16084\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 012 | loss: 0.16084 - R2: 1.0003 -- iter: 0384/1168\n",
      "Training Step: 216  | total loss: \u001b[1m\u001b[32m0.16374\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 012 | loss: 0.16374 - R2: 1.0005 -- iter: 0448/1168\n",
      "Training Step: 217  | total loss: \u001b[1m\u001b[32m0.16108\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 012 | loss: 0.16108 - R2: 1.0009 -- iter: 0512/1168\n",
      "Training Step: 218  | total loss: \u001b[1m\u001b[32m0.16153\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 012 | loss: 0.16153 - R2: 1.0005 -- iter: 0576/1168\n",
      "Training Step: 219  | total loss: \u001b[1m\u001b[32m0.15545\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 012 | loss: 0.15545 - R2: 1.0003 -- iter: 0640/1168\n",
      "Training Step: 220  | total loss: \u001b[1m\u001b[32m0.15182\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 012 | loss: 0.15182 - R2: 1.0010 -- iter: 0704/1168\n",
      "Training Step: 221  | total loss: \u001b[1m\u001b[32m0.15182\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 012 | loss: 0.15182 - R2: 1.0010 -- iter: 0768/1168\n",
      "Training Step: 222  | total loss: \u001b[1m\u001b[32m0.15848\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 012 | loss: 0.15848 - R2: 1.0010 -- iter: 0832/1168\n",
      "Training Step: 223  | total loss: \u001b[1m\u001b[32m0.15848\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 012 | loss: 0.15848 - R2: 1.0010 -- iter: 0896/1168\n",
      "Training Step: 224  | total loss: \u001b[1m\u001b[32m0.15406\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 012 | loss: 0.15406 - R2: 0.9999 -- iter: 0960/1168\n",
      "Training Step: 225  | total loss: \u001b[1m\u001b[32m0.15530\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 012 | loss: 0.15530 - R2: 1.0008 -- iter: 1024/1168\n",
      "Training Step: 226  | total loss: \u001b[1m\u001b[32m0.16206\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 012 | loss: 0.16206 - R2: 1.0011 -- iter: 1088/1168\n",
      "Training Step: 227  | total loss: \u001b[1m\u001b[32m0.16206\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 012 | loss: 0.16206 - R2: 1.0011 -- iter: 1152/1168\n",
      "Training Step: 228  | total loss: \u001b[1m\u001b[32m0.16186\u001b[0m\u001b[0m | time: 1.149s\n",
      "| SGD | epoch: 012 | loss: 0.16186 - R2: 0.9998 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 229  | total loss: \u001b[1m\u001b[32m0.16196\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 013 | loss: 0.16196 - R2: 1.0001 -- iter: 0064/1168\n",
      "Training Step: 230  | total loss: \u001b[1m\u001b[32m0.15824\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 013 | loss: 0.15824 - R2: 0.9996 -- iter: 0128/1168\n",
      "Training Step: 231  | total loss: \u001b[1m\u001b[32m0.16140\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 013 | loss: 0.16140 - R2: 1.0006 -- iter: 0192/1168\n",
      "Training Step: 232  | total loss: \u001b[1m\u001b[32m0.16140\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 013 | loss: 0.16140 - R2: 1.0006 -- iter: 0256/1168\n",
      "Training Step: 233  | total loss: \u001b[1m\u001b[32m0.16145\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 013 | loss: 0.16145 - R2: 1.0004 -- iter: 0320/1168\n",
      "Training Step: 234  | total loss: \u001b[1m\u001b[32m0.16625\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 013 | loss: 0.16625 - R2: 1.0007 -- iter: 0384/1168\n",
      "Training Step: 235  | total loss: \u001b[1m\u001b[32m0.16586\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 013 | loss: 0.16586 - R2: 1.0000 -- iter: 0448/1168\n",
      "Training Step: 236  | total loss: \u001b[1m\u001b[32m0.16169\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 013 | loss: 0.16169 - R2: 0.9998 -- iter: 0512/1168\n",
      "Training Step: 237  | total loss: \u001b[1m\u001b[32m0.16169\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 013 | loss: 0.16169 - R2: 0.9998 -- iter: 0576/1168\n",
      "Training Step: 238  | total loss: \u001b[1m\u001b[32m0.16338\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 013 | loss: 0.16338 - R2: 1.0000 -- iter: 0640/1168\n",
      "Training Step: 239  | total loss: \u001b[1m\u001b[32m0.16338\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 013 | loss: 0.16338 - R2: 1.0000 -- iter: 0704/1168\n",
      "Training Step: 240  | total loss: \u001b[1m\u001b[32m0.20016\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 013 | loss: 0.20016 - R2: 0.9930 -- iter: 0768/1168\n",
      "Training Step: 241  | total loss: \u001b[1m\u001b[32m0.20016\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 013 | loss: 0.20016 - R2: 0.9930 -- iter: 0832/1168\n",
      "Training Step: 242  | total loss: \u001b[1m\u001b[32m0.19717\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 013 | loss: 0.19717 - R2: 0.9949 -- iter: 0896/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 243  | total loss: \u001b[1m\u001b[32m0.18964\u001b[0m\u001b[0m | time: 0.172s\n",
      "| SGD | epoch: 013 | loss: 0.18964 - R2: 0.9959 -- iter: 0960/1168\n",
      "Training Step: 244  | total loss: \u001b[1m\u001b[32m0.18745\u001b[0m\u001b[0m | time: 0.176s\n",
      "| SGD | epoch: 013 | loss: 0.18745 - R2: 0.9971 -- iter: 1024/1168\n",
      "Training Step: 245  | total loss: \u001b[1m\u001b[32m0.17988\u001b[0m\u001b[0m | time: 0.180s\n",
      "| SGD | epoch: 013 | loss: 0.17988 - R2: 0.9971 -- iter: 1088/1168\n",
      "Training Step: 246  | total loss: \u001b[1m\u001b[32m0.17988\u001b[0m\u001b[0m | time: 0.184s\n",
      "| SGD | epoch: 013 | loss: 0.17988 - R2: 0.9971 -- iter: 1152/1168\n",
      "Training Step: 247  | total loss: \u001b[1m\u001b[32m0.17835\u001b[0m\u001b[0m | time: 1.191s\n",
      "| SGD | epoch: 013 | loss: 0.17835 - R2: 0.9984 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 248  | total loss: \u001b[1m\u001b[32m0.17674\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 014 | loss: 0.17674 - R2: 0.9980 -- iter: 0064/1168\n",
      "Training Step: 249  | total loss: \u001b[1m\u001b[32m0.17674\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 014 | loss: 0.17674 - R2: 0.9980 -- iter: 0128/1168\n",
      "Training Step: 250  | total loss: \u001b[1m\u001b[32m0.17468\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 014 | loss: 0.17468 - R2: 0.9984 -- iter: 0192/1168\n",
      "Training Step: 251  | total loss: \u001b[1m\u001b[32m0.17468\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 014 | loss: 0.17468 - R2: 0.9984 -- iter: 0256/1168\n",
      "Training Step: 252  | total loss: \u001b[1m\u001b[32m0.17298\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 014 | loss: 0.17298 - R2: 1.0009 -- iter: 0320/1168\n",
      "Training Step: 253  | total loss: \u001b[1m\u001b[32m0.17298\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 014 | loss: 0.17298 - R2: 1.0009 -- iter: 0384/1168\n",
      "Training Step: 254  | total loss: \u001b[1m\u001b[32m0.18003\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 014 | loss: 0.18003 - R2: 1.0010 -- iter: 0448/1168\n",
      "Training Step: 255  | total loss: \u001b[1m\u001b[32m0.17407\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 014 | loss: 0.17407 - R2: 1.0010 -- iter: 0512/1168\n",
      "Training Step: 256  | total loss: \u001b[1m\u001b[32m0.17407\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 014 | loss: 0.17407 - R2: 1.0010 -- iter: 0576/1168\n",
      "Training Step: 257  | total loss: \u001b[1m\u001b[32m0.16699\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 014 | loss: 0.16699 - R2: 1.0013 -- iter: 0640/1168\n",
      "Training Step: 258  | total loss: \u001b[1m\u001b[32m0.16699\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 014 | loss: 0.16699 - R2: 1.0013 -- iter: 0704/1168\n",
      "Training Step: 259  | total loss: \u001b[1m\u001b[32m0.16939\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 014 | loss: 0.16939 - R2: 0.9985 -- iter: 0768/1168\n",
      "Training Step: 260  | total loss: \u001b[1m\u001b[32m0.16822\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 014 | loss: 0.16822 - R2: 0.9969 -- iter: 0832/1168\n",
      "Training Step: 261  | total loss: \u001b[1m\u001b[32m0.16822\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 014 | loss: 0.16822 - R2: 0.9969 -- iter: 0896/1168\n",
      "Training Step: 262  | total loss: \u001b[1m\u001b[32m0.16828\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 014 | loss: 0.16828 - R2: 0.9953 -- iter: 0960/1168\n",
      "Training Step: 263  | total loss: \u001b[1m\u001b[32m0.16701\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 014 | loss: 0.16701 - R2: 0.9958 -- iter: 1024/1168\n",
      "Training Step: 264  | total loss: \u001b[1m\u001b[32m0.16832\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 014 | loss: 0.16832 - R2: 0.9958 -- iter: 1088/1168\n",
      "Training Step: 265  | total loss: \u001b[1m\u001b[32m0.16798\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 014 | loss: 0.16798 - R2: 0.9960 -- iter: 1152/1168\n",
      "Training Step: 266  | total loss: \u001b[1m\u001b[32m0.16312\u001b[0m\u001b[0m | time: 1.133s\n",
      "| SGD | epoch: 014 | loss: 0.16312 - R2: 0.9967 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 267  | total loss: \u001b[1m\u001b[32m0.15673\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 015 | loss: 0.15673 - R2: 0.9971 -- iter: 0064/1168\n",
      "Training Step: 268  | total loss: \u001b[1m\u001b[32m0.15428\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 015 | loss: 0.15428 - R2: 0.9970 -- iter: 0128/1168\n",
      "Training Step: 269  | total loss: \u001b[1m\u001b[32m0.15296\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 015 | loss: 0.15296 - R2: 0.9979 -- iter: 0192/1168\n",
      "Training Step: 270  | total loss: \u001b[1m\u001b[32m0.15326\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 015 | loss: 0.15326 - R2: 0.9984 -- iter: 0256/1168\n",
      "Training Step: 271  | total loss: \u001b[1m\u001b[32m0.15253\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 015 | loss: 0.15253 - R2: 0.9995 -- iter: 0320/1168\n",
      "Training Step: 272  | total loss: \u001b[1m\u001b[32m0.15755\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 015 | loss: 0.15755 - R2: 0.9995 -- iter: 0384/1168\n",
      "Training Step: 273  | total loss: \u001b[1m\u001b[32m0.15712\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 015 | loss: 0.15712 - R2: 1.0000 -- iter: 0448/1168\n",
      "Training Step: 274  | total loss: \u001b[1m\u001b[32m0.16480\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 015 | loss: 0.16480 - R2: 0.9983 -- iter: 0512/1168\n",
      "Training Step: 275  | total loss: \u001b[1m\u001b[32m0.16738\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 015 | loss: 0.16738 - R2: 0.9983 -- iter: 0576/1168\n",
      "Training Step: 276  | total loss: \u001b[1m\u001b[32m0.16738\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 015 | loss: 0.16738 - R2: 0.9976 -- iter: 0640/1168\n",
      "Training Step: 277  | total loss: \u001b[1m\u001b[32m0.16356\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 015 | loss: 0.16356 - R2: 0.9978 -- iter: 0704/1168\n",
      "Training Step: 278  | total loss: \u001b[1m\u001b[32m0.15888\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 015 | loss: 0.15888 - R2: 0.9983 -- iter: 0768/1168\n",
      "Training Step: 279  | total loss: \u001b[1m\u001b[32m0.15888\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 015 | loss: 0.15888 - R2: 0.9983 -- iter: 0832/1168\n",
      "Training Step: 280  | total loss: \u001b[1m\u001b[32m0.14821\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 015 | loss: 0.14821 - R2: 0.9991 -- iter: 0896/1168\n",
      "Training Step: 281  | total loss: \u001b[1m\u001b[32m0.13861\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 015 | loss: 0.13861 - R2: 0.9998 -- iter: 0960/1168\n",
      "Training Step: 282  | total loss: \u001b[1m\u001b[32m0.14042\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 015 | loss: 0.14042 - R2: 1.0006 -- iter: 1024/1168\n",
      "Training Step: 283  | total loss: \u001b[1m\u001b[32m0.14541\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 015 | loss: 0.14541 - R2: 0.9996 -- iter: 1088/1168\n",
      "Training Step: 284  | total loss: \u001b[1m\u001b[32m0.14541\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 015 | loss: 0.14541 - R2: 0.9996 -- iter: 1152/1168\n",
      "Training Step: 285  | total loss: \u001b[1m\u001b[32m0.14884\u001b[0m\u001b[0m | time: 1.155s\n",
      "| SGD | epoch: 015 | loss: 0.14884 - R2: 1.0003 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 286  | total loss: \u001b[1m\u001b[32m0.15486\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 016 | loss: 0.15486 - R2: 1.0003 -- iter: 0064/1168\n",
      "Training Step: 287  | total loss: \u001b[1m\u001b[32m0.15254\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 016 | loss: 0.15254 - R2: 1.0005 -- iter: 0128/1168\n",
      "Training Step: 288  | total loss: \u001b[1m\u001b[32m0.16051\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 016 | loss: 0.16051 - R2: 0.9986 -- iter: 0192/1168\n",
      "Training Step: 289  | total loss: \u001b[1m\u001b[32m0.16051\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 016 | loss: 0.16051 - R2: 0.9986 -- iter: 0256/1168\n",
      "Training Step: 290  | total loss: \u001b[1m\u001b[32m0.15751\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 016 | loss: 0.15751 - R2: 0.9996 -- iter: 0320/1168\n",
      "Training Step: 291  | total loss: \u001b[1m\u001b[32m0.15834\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 016 | loss: 0.15834 - R2: 0.9992 -- iter: 0384/1168\n",
      "Training Step: 292  | total loss: \u001b[1m\u001b[32m0.16161\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 016 | loss: 0.16161 - R2: 0.9995 -- iter: 0448/1168\n",
      "Training Step: 293  | total loss: \u001b[1m\u001b[32m0.15928\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 016 | loss: 0.15928 - R2: 0.9991 -- iter: 0512/1168\n",
      "Training Step: 294  | total loss: \u001b[1m\u001b[32m0.16146\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 016 | loss: 0.16146 - R2: 1.0003 -- iter: 0576/1168\n",
      "Training Step: 295  | total loss: \u001b[1m\u001b[32m0.16146\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 016 | loss: 0.16146 - R2: 1.0003 -- iter: 0640/1168\n",
      "Training Step: 296  | total loss: \u001b[1m\u001b[32m0.16535\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 016 | loss: 0.16535 - R2: 0.9999 -- iter: 0704/1168\n",
      "Training Step: 297  | total loss: \u001b[1m\u001b[32m0.16116\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 016 | loss: 0.16116 - R2: 0.9987 -- iter: 0768/1168\n",
      "Training Step: 298  | total loss: \u001b[1m\u001b[32m0.16229\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 016 | loss: 0.16229 - R2: 0.9994 -- iter: 0832/1168\n",
      "Training Step: 299  | total loss: \u001b[1m\u001b[32m0.15777\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 016 | loss: 0.15777 - R2: 0.9992 -- iter: 0896/1168\n",
      "Training Step: 300  | total loss: \u001b[1m\u001b[32m0.15364\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 016 | loss: 0.15364 - R2: 0.9991 -- iter: 0960/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 301  | total loss: \u001b[1m\u001b[32m0.15364\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 016 | loss: 0.15364 - R2: 0.9991 -- iter: 1024/1168\n",
      "Training Step: 302  | total loss: \u001b[1m\u001b[32m0.15566\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 016 | loss: 0.15566 - R2: 0.9995 -- iter: 1088/1168\n",
      "Training Step: 303  | total loss: \u001b[1m\u001b[32m0.15571\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 016 | loss: 0.15571 - R2: 1.0004 -- iter: 1152/1168\n",
      "Training Step: 304  | total loss: \u001b[1m\u001b[32m0.15854\u001b[0m\u001b[0m | time: 1.133s\n",
      "| SGD | epoch: 016 | loss: 0.15854 - R2: 1.0008 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 305  | total loss: \u001b[1m\u001b[32m0.15854\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 017 | loss: 0.15854 - R2: 1.0008 -- iter: 0064/1168\n",
      "Training Step: 306  | total loss: \u001b[1m\u001b[32m0.15037\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 017 | loss: 0.15037 - R2: 1.0008 -- iter: 0128/1168\n",
      "Training Step: 307  | total loss: \u001b[1m\u001b[32m0.16013\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 017 | loss: 0.16013 - R2: 1.0000 -- iter: 0192/1168\n",
      "Training Step: 308  | total loss: \u001b[1m\u001b[32m0.16384\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 017 | loss: 0.16384 - R2: 1.0000 -- iter: 0256/1168\n",
      "Training Step: 309  | total loss: \u001b[1m\u001b[32m0.16384\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 017 | loss: 0.16384 - R2: 0.9999 -- iter: 0320/1168\n",
      "Training Step: 310  | total loss: \u001b[1m\u001b[32m0.16353\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 017 | loss: 0.16353 - R2: 0.9999 -- iter: 0384/1168\n",
      "Training Step: 311  | total loss: \u001b[1m\u001b[32m0.16493\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 017 | loss: 0.16493 - R2: 0.9984 -- iter: 0448/1168\n",
      "Training Step: 312  | total loss: \u001b[1m\u001b[32m0.16756\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 017 | loss: 0.16756 - R2: 0.9989 -- iter: 0512/1168\n",
      "Training Step: 313  | total loss: \u001b[1m\u001b[32m0.16400\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 017 | loss: 0.16400 - R2: 1.0000 -- iter: 0576/1168\n",
      "Training Step: 314  | total loss: \u001b[1m\u001b[32m0.16400\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 017 | loss: 0.16400 - R2: 1.0000 -- iter: 0640/1168\n",
      "Training Step: 315  | total loss: \u001b[1m\u001b[32m0.16526\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 017 | loss: 0.16526 - R2: 1.0005 -- iter: 0704/1168\n",
      "Training Step: 316  | total loss: \u001b[1m\u001b[32m0.16828\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 017 | loss: 0.16828 - R2: 0.9996 -- iter: 0768/1168\n",
      "Training Step: 317  | total loss: \u001b[1m\u001b[32m0.16672\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 017 | loss: 0.16672 - R2: 0.9972 -- iter: 0832/1168\n",
      "Training Step: 318  | total loss: \u001b[1m\u001b[32m0.16533\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 017 | loss: 0.16533 - R2: 0.9972 -- iter: 0896/1168\n",
      "Training Step: 319  | total loss: \u001b[1m\u001b[32m0.15891\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 017 | loss: 0.15891 - R2: 0.9979 -- iter: 0960/1168\n",
      "Training Step: 320  | total loss: \u001b[1m\u001b[32m0.15891\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 017 | loss: 0.15891 - R2: 0.9979 -- iter: 1024/1168\n",
      "Training Step: 321  | total loss: \u001b[1m\u001b[32m0.15848\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 017 | loss: 0.15848 - R2: 0.9986 -- iter: 1088/1168\n",
      "Training Step: 322  | total loss: \u001b[1m\u001b[32m0.15919\u001b[0m\u001b[0m | time: 0.174s\n",
      "| SGD | epoch: 017 | loss: 0.15919 - R2: 0.9993 -- iter: 1152/1168\n",
      "Training Step: 323  | total loss: \u001b[1m\u001b[32m0.15828\u001b[0m\u001b[0m | time: 1.184s\n",
      "| SGD | epoch: 017 | loss: 0.15828 - R2: 1.0001 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 324  | total loss: \u001b[1m\u001b[32m0.15828\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 018 | loss: 0.15828 - R2: 1.0001 -- iter: 0064/1168\n",
      "Training Step: 325  | total loss: \u001b[1m\u001b[32m0.15782\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 018 | loss: 0.15782 - R2: 0.9998 -- iter: 0128/1168\n",
      "Training Step: 326  | total loss: \u001b[1m\u001b[32m0.15782\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 018 | loss: 0.15782 - R2: 0.9998 -- iter: 0192/1168\n",
      "Training Step: 327  | total loss: \u001b[1m\u001b[32m0.15357\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 018 | loss: 0.15357 - R2: 1.0007 -- iter: 0256/1168\n",
      "Training Step: 328  | total loss: \u001b[1m\u001b[32m0.15389\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 018 | loss: 0.15389 - R2: 1.0002 -- iter: 0320/1168\n",
      "Training Step: 329  | total loss: \u001b[1m\u001b[32m0.16137\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 018 | loss: 0.16137 - R2: 0.9994 -- iter: 0384/1168\n",
      "Training Step: 330  | total loss: \u001b[1m\u001b[32m0.15669\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 018 | loss: 0.15669 - R2: 0.9995 -- iter: 0448/1168\n",
      "Training Step: 331  | total loss: \u001b[1m\u001b[32m0.15669\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 018 | loss: 0.15669 - R2: 0.9995 -- iter: 0512/1168\n",
      "Training Step: 332  | total loss: \u001b[1m\u001b[32m0.15216\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 018 | loss: 0.15216 - R2: 0.9995 -- iter: 0576/1168\n",
      "Training Step: 333  | total loss: \u001b[1m\u001b[32m0.15579\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 018 | loss: 0.15579 - R2: 1.0003 -- iter: 0640/1168\n",
      "Training Step: 334  | total loss: \u001b[1m\u001b[32m0.16181\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 018 | loss: 0.16181 - R2: 0.9992 -- iter: 0704/1168\n",
      "Training Step: 335  | total loss: \u001b[1m\u001b[32m0.16002\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 018 | loss: 0.16002 - R2: 0.9987 -- iter: 0768/1168\n",
      "Training Step: 336  | total loss: \u001b[1m\u001b[32m0.16002\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 018 | loss: 0.16002 - R2: 0.9987 -- iter: 0832/1168\n",
      "Training Step: 337  | total loss: \u001b[1m\u001b[32m0.16390\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 018 | loss: 0.16390 - R2: 0.9979 -- iter: 0896/1168\n",
      "Training Step: 338  | total loss: \u001b[1m\u001b[32m0.16247\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 018 | loss: 0.16247 - R2: 0.9995 -- iter: 0960/1168\n",
      "Training Step: 339  | total loss: \u001b[1m\u001b[32m0.16577\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 018 | loss: 0.16577 - R2: 0.9998 -- iter: 1024/1168\n",
      "Training Step: 340  | total loss: \u001b[1m\u001b[32m0.17266\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 018 | loss: 0.17266 - R2: 1.0001 -- iter: 1088/1168\n",
      "Training Step: 341  | total loss: \u001b[1m\u001b[32m0.17266\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 018 | loss: 0.17266 - R2: 1.0001 -- iter: 1152/1168\n",
      "Training Step: 342  | total loss: \u001b[1m\u001b[32m0.17259\u001b[0m\u001b[0m | time: 1.152s\n",
      "| SGD | epoch: 018 | loss: 0.17259 - R2: 1.0003 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 343  | total loss: \u001b[1m\u001b[32m0.16489\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 019 | loss: 0.16489 - R2: 1.0027 -- iter: 0064/1168\n",
      "Training Step: 344  | total loss: \u001b[1m\u001b[32m0.16489\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 019 | loss: 0.16489 - R2: 1.0027 -- iter: 0128/1168\n",
      "Training Step: 345  | total loss: \u001b[1m\u001b[32m0.16629\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 019 | loss: 0.16629 - R2: 1.0026 -- iter: 0192/1168\n",
      "Training Step: 346  | total loss: \u001b[1m\u001b[32m0.17132\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 019 | loss: 0.17132 - R2: 1.0023 -- iter: 0256/1168\n",
      "Training Step: 347  | total loss: \u001b[1m\u001b[32m0.17132\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 019 | loss: 0.17132 - R2: 1.0023 -- iter: 0320/1168\n",
      "Training Step: 348  | total loss: \u001b[1m\u001b[32m0.17049\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 019 | loss: 0.17049 - R2: 1.0012 -- iter: 0384/1168\n",
      "Training Step: 349  | total loss: \u001b[1m\u001b[32m0.16968\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 019 | loss: 0.16968 - R2: 1.0020 -- iter: 0448/1168\n",
      "Training Step: 350  | total loss: \u001b[1m\u001b[32m0.16636\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 019 | loss: 0.16636 - R2: 1.0019 -- iter: 0512/1168\n",
      "Training Step: 351  | total loss: \u001b[1m\u001b[32m0.16655\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 019 | loss: 0.16655 - R2: 1.0021 -- iter: 0576/1168\n",
      "Training Step: 352  | total loss: \u001b[1m\u001b[32m0.16655\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 019 | loss: 0.16655 - R2: 1.0021 -- iter: 0640/1168\n",
      "Training Step: 353  | total loss: \u001b[1m\u001b[32m0.16846\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 019 | loss: 0.16846 - R2: 1.0019 -- iter: 0704/1168\n",
      "Training Step: 354  | total loss: \u001b[1m\u001b[32m0.16780\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 019 | loss: 0.16780 - R2: 1.0008 -- iter: 0768/1168\n",
      "Training Step: 355  | total loss: \u001b[1m\u001b[32m0.17106\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 019 | loss: 0.17106 - R2: 1.0003 -- iter: 0832/1168\n",
      "Training Step: 356  | total loss: \u001b[1m\u001b[32m0.17106\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 019 | loss: 0.17106 - R2: 1.0003 -- iter: 0896/1168\n",
      "Training Step: 357  | total loss: \u001b[1m\u001b[32m0.16658\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 019 | loss: 0.16658 - R2: 1.0005 -- iter: 0960/1168\n",
      "Training Step: 358  | total loss: \u001b[1m\u001b[32m0.16993\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 019 | loss: 0.16993 - R2: 1.0012 -- iter: 1024/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 359  | total loss: \u001b[1m\u001b[32m0.16995\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 019 | loss: 0.16995 - R2: 1.0015 -- iter: 1088/1168\n",
      "Training Step: 360  | total loss: \u001b[1m\u001b[32m0.16997\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 019 | loss: 0.16997 - R2: 1.0017 -- iter: 1152/1168\n",
      "Training Step: 361  | total loss: \u001b[1m\u001b[32m0.16997\u001b[0m\u001b[0m | time: 1.155s\n",
      "| SGD | epoch: 019 | loss: 0.16997 - R2: 1.0017 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 362  | total loss: \u001b[1m\u001b[32m0.16655\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 020 | loss: 0.16655 - R2: 1.0019 -- iter: 0064/1168\n",
      "Training Step: 363  | total loss: \u001b[1m\u001b[32m0.16512\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 020 | loss: 0.16512 - R2: 1.0022 -- iter: 0128/1168\n",
      "Training Step: 364  | total loss: \u001b[1m\u001b[32m0.16214\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 020 | loss: 0.16214 - R2: 1.0021 -- iter: 0192/1168\n",
      "Training Step: 365  | total loss: \u001b[1m\u001b[32m0.16128\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 020 | loss: 0.16128 - R2: 1.0019 -- iter: 0256/1168\n",
      "Training Step: 366  | total loss: \u001b[1m\u001b[32m0.16309\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 020 | loss: 0.16309 - R2: 1.0000 -- iter: 0320/1168\n",
      "Training Step: 367  | total loss: \u001b[1m\u001b[32m0.16160\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 020 | loss: 0.16160 - R2: 1.0000 -- iter: 0384/1168\n",
      "Training Step: 368  | total loss: \u001b[1m\u001b[32m0.16177\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 020 | loss: 0.16177 - R2: 0.9991 -- iter: 0448/1168\n",
      "Training Step: 369  | total loss: \u001b[1m\u001b[32m0.15936\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 020 | loss: 0.15936 - R2: 0.9991 -- iter: 0512/1168\n",
      "Training Step: 370  | total loss: \u001b[1m\u001b[32m0.15889\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 020 | loss: 0.15889 - R2: 0.9983 -- iter: 0576/1168\n",
      "Training Step: 371  | total loss: \u001b[1m\u001b[32m0.15775\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 020 | loss: 0.15775 - R2: 0.9986 -- iter: 0640/1168\n",
      "Training Step: 372  | total loss: \u001b[1m\u001b[32m0.15916\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 020 | loss: 0.15916 - R2: 1.0003 -- iter: 0704/1168\n",
      "Training Step: 373  | total loss: \u001b[1m\u001b[32m0.15732\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 020 | loss: 0.15732 - R2: 1.0017 -- iter: 0768/1168\n",
      "Training Step: 374  | total loss: \u001b[1m\u001b[32m0.15732\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 020 | loss: 0.15732 - R2: 1.0017 -- iter: 0832/1168\n",
      "Training Step: 375  | total loss: \u001b[1m\u001b[32m0.15378\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 020 | loss: 0.15378 - R2: 1.0014 -- iter: 0896/1168\n",
      "Training Step: 376  | total loss: \u001b[1m\u001b[32m0.16125\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 020 | loss: 0.16125 - R2: 1.0007 -- iter: 0960/1168\n",
      "Training Step: 377  | total loss: \u001b[1m\u001b[32m0.16488\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 020 | loss: 0.16488 - R2: 1.0002 -- iter: 1024/1168\n",
      "Training Step: 378  | total loss: \u001b[1m\u001b[32m0.16624\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 020 | loss: 0.16624 - R2: 0.9990 -- iter: 1088/1168\n",
      "Training Step: 379  | total loss: \u001b[1m\u001b[32m0.17053\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 020 | loss: 0.17053 - R2: 1.0002 -- iter: 1152/1168\n",
      "Training Step: 380  | total loss: \u001b[1m\u001b[32m0.17053\u001b[0m\u001b[0m | time: 1.156s\n",
      "| SGD | epoch: 020 | loss: 0.17053 - R2: 1.0002 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 381  | total loss: \u001b[1m\u001b[32m0.17439\u001b[0m\u001b[0m | time: 0.017s\n",
      "| SGD | epoch: 021 | loss: 0.17439 - R2: 1.0012 -- iter: 0064/1168\n",
      "Training Step: 382  | total loss: \u001b[1m\u001b[32m0.16792\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 021 | loss: 0.16792 - R2: 1.0005 -- iter: 0128/1168\n",
      "Training Step: 383  | total loss: \u001b[1m\u001b[32m0.16290\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 021 | loss: 0.16290 - R2: 0.9994 -- iter: 0192/1168\n",
      "Training Step: 384  | total loss: \u001b[1m\u001b[32m0.16290\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 021 | loss: 0.16290 - R2: 0.9994 -- iter: 0256/1168\n",
      "Training Step: 385  | total loss: \u001b[1m\u001b[32m0.16667\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 021 | loss: 0.16667 - R2: 0.9997 -- iter: 0320/1168\n",
      "Training Step: 386  | total loss: \u001b[1m\u001b[32m0.16774\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 021 | loss: 0.16774 - R2: 0.9997 -- iter: 0384/1168\n",
      "Training Step: 387  | total loss: \u001b[1m\u001b[32m0.16852\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 021 | loss: 0.16852 - R2: 1.0006 -- iter: 0448/1168\n",
      "Training Step: 388  | total loss: \u001b[1m\u001b[32m0.17409\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 021 | loss: 0.17409 - R2: 0.9987 -- iter: 0512/1168\n",
      "Training Step: 389  | total loss: \u001b[1m\u001b[32m0.17067\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 021 | loss: 0.17067 - R2: 0.9991 -- iter: 0576/1168\n",
      "Training Step: 390  | total loss: \u001b[1m\u001b[32m0.16456\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 021 | loss: 0.16456 - R2: 0.9975 -- iter: 0640/1168\n",
      "Training Step: 391  | total loss: \u001b[1m\u001b[32m0.16456\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 021 | loss: 0.16456 - R2: 0.9975 -- iter: 0704/1168\n",
      "Training Step: 392  | total loss: \u001b[1m\u001b[32m0.16134\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 021 | loss: 0.16134 - R2: 0.9969 -- iter: 0768/1168\n",
      "Training Step: 393  | total loss: \u001b[1m\u001b[32m0.16134\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 021 | loss: 0.16134 - R2: 0.9969 -- iter: 0832/1168\n",
      "Training Step: 394  | total loss: \u001b[1m\u001b[32m0.16832\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 021 | loss: 0.16832 - R2: 0.9952 -- iter: 0896/1168\n",
      "Training Step: 395  | total loss: \u001b[1m\u001b[32m0.16808\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 021 | loss: 0.16808 - R2: 0.9952 -- iter: 0960/1168\n",
      "Training Step: 396  | total loss: \u001b[1m\u001b[32m0.17364\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 021 | loss: 0.17364 - R2: 0.9966 -- iter: 1024/1168\n",
      "Training Step: 397  | total loss: \u001b[1m\u001b[32m0.16781\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 021 | loss: 0.16781 - R2: 0.9984 -- iter: 1088/1168\n",
      "Training Step: 398  | total loss: \u001b[1m\u001b[32m0.16660\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 021 | loss: 0.16660 - R2: 0.9996 -- iter: 1152/1168\n",
      "Training Step: 399  | total loss: \u001b[1m\u001b[32m0.16660\u001b[0m\u001b[0m | time: 1.112s\n",
      "| SGD | epoch: 021 | loss: 0.16660 - R2: 0.9996 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 400  | total loss: \u001b[1m\u001b[32m0.17682\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 022 | loss: 0.17682 - R2: 1.0003 -- iter: 0064/1168\n",
      "Training Step: 401  | total loss: \u001b[1m\u001b[32m0.18602\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 022 | loss: 0.18602 - R2: 1.0008 -- iter: 0128/1168\n",
      "Training Step: 402  | total loss: \u001b[1m\u001b[32m0.17969\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 022 | loss: 0.17969 - R2: 1.0034 -- iter: 0192/1168\n",
      "Training Step: 403  | total loss: \u001b[1m\u001b[32m0.17969\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 022 | loss: 0.17969 - R2: 1.0034 -- iter: 0256/1168\n",
      "Training Step: 404  | total loss: \u001b[1m\u001b[32m0.17386\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 022 | loss: 0.17386 - R2: 1.0027 -- iter: 0320/1168\n",
      "Training Step: 405  | total loss: \u001b[1m\u001b[32m0.17386\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 022 | loss: 0.17386 - R2: 1.0027 -- iter: 0384/1168\n",
      "Training Step: 406  | total loss: \u001b[1m\u001b[32m0.16797\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 022 | loss: 0.16797 - R2: 1.0033 -- iter: 0448/1168\n",
      "Training Step: 407  | total loss: \u001b[1m\u001b[32m0.16797\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 022 | loss: 0.16797 - R2: 1.0012 -- iter: 0512/1168\n",
      "Training Step: 408  | total loss: \u001b[1m\u001b[32m0.17362\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 022 | loss: 0.17362 - R2: 1.0012 -- iter: 0576/1168\n",
      "Training Step: 409  | total loss: \u001b[1m\u001b[32m0.17315\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 022 | loss: 0.17315 - R2: 1.0009 -- iter: 0640/1168\n",
      "Training Step: 410  | total loss: \u001b[1m\u001b[32m0.17317\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 022 | loss: 0.17317 - R2: 1.0021 -- iter: 0704/1168\n",
      "Training Step: 411  | total loss: \u001b[1m\u001b[32m0.17317\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 022 | loss: 0.17317 - R2: 1.0021 -- iter: 0768/1168\n",
      "Training Step: 412  | total loss: \u001b[1m\u001b[32m0.16664\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 022 | loss: 0.16664 - R2: 1.0018 -- iter: 0832/1168\n",
      "Training Step: 413  | total loss: \u001b[1m\u001b[32m0.16482\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 022 | loss: 0.16482 - R2: 1.0019 -- iter: 0896/1168\n",
      "Training Step: 414  | total loss: \u001b[1m\u001b[32m0.16312\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 022 | loss: 0.16312 - R2: 1.0009 -- iter: 0960/1168\n",
      "Training Step: 415  | total loss: \u001b[1m\u001b[32m0.16378\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 022 | loss: 0.16378 - R2: 1.0008 -- iter: 1024/1168\n",
      "Training Step: 416  | total loss: \u001b[1m\u001b[32m0.16263\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 022 | loss: 0.16263 - R2: 1.0017 -- iter: 1088/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 417  | total loss: \u001b[1m\u001b[32m0.16263\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 022 | loss: 0.16263 - R2: 1.0017 -- iter: 1152/1168\n",
      "Training Step: 418  | total loss: \u001b[1m\u001b[32m0.16116\u001b[0m\u001b[0m | time: 1.128s\n",
      "| SGD | epoch: 022 | loss: 0.16116 - R2: 1.0012 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 419  | total loss: \u001b[1m\u001b[32m0.15646\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 023 | loss: 0.15646 - R2: 1.0009 -- iter: 0064/1168\n",
      "Training Step: 420  | total loss: \u001b[1m\u001b[32m0.15646\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 023 | loss: 0.15646 - R2: 1.0002 -- iter: 0128/1168\n",
      "Training Step: 421  | total loss: \u001b[1m\u001b[32m0.15832\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 023 | loss: 0.15832 - R2: 0.9995 -- iter: 0192/1168\n",
      "Training Step: 422  | total loss: \u001b[1m\u001b[32m0.15832\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 023 | loss: 0.15832 - R2: 0.9997 -- iter: 0256/1168\n",
      "Training Step: 423  | total loss: \u001b[1m\u001b[32m0.16352\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 023 | loss: 0.16352 - R2: 0.9992 -- iter: 0320/1168\n",
      "Training Step: 424  | total loss: \u001b[1m\u001b[32m0.16416\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 023 | loss: 0.16416 - R2: 0.9982 -- iter: 0384/1168\n",
      "Training Step: 425  | total loss: \u001b[1m\u001b[32m0.16416\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 023 | loss: 0.16416 - R2: 0.9982 -- iter: 0448/1168\n",
      "Training Step: 426  | total loss: \u001b[1m\u001b[32m0.16683\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 023 | loss: 0.16683 - R2: 0.9993 -- iter: 0512/1168\n",
      "Training Step: 427  | total loss: \u001b[1m\u001b[32m0.16301\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 023 | loss: 0.16301 - R2: 0.9987 -- iter: 0576/1168\n",
      "Training Step: 428  | total loss: \u001b[1m\u001b[32m0.16222\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 023 | loss: 0.16222 - R2: 0.9991 -- iter: 0640/1168\n",
      "Training Step: 429  | total loss: \u001b[1m\u001b[32m0.16401\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 023 | loss: 0.16401 - R2: 0.9981 -- iter: 0704/1168\n",
      "Training Step: 430  | total loss: \u001b[1m\u001b[32m0.16401\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 023 | loss: 0.16401 - R2: 0.9981 -- iter: 0768/1168\n",
      "Training Step: 431  | total loss: \u001b[1m\u001b[32m0.16623\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 023 | loss: 0.16623 - R2: 0.9979 -- iter: 0832/1168\n",
      "Training Step: 432  | total loss: \u001b[1m\u001b[32m0.16454\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 023 | loss: 0.16454 - R2: 0.9976 -- iter: 0896/1168\n",
      "Training Step: 433  | total loss: \u001b[1m\u001b[32m0.16287\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 023 | loss: 0.16287 - R2: 0.9989 -- iter: 0960/1168\n",
      "Training Step: 434  | total loss: \u001b[1m\u001b[32m0.16211\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 023 | loss: 0.16211 - R2: 0.9990 -- iter: 1024/1168\n",
      "Training Step: 435  | total loss: \u001b[1m\u001b[32m0.15963\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 023 | loss: 0.15963 - R2: 1.0004 -- iter: 1088/1168\n",
      "Training Step: 436  | total loss: \u001b[1m\u001b[32m0.15963\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 023 | loss: 0.15963 - R2: 1.0004 -- iter: 1152/1168\n",
      "Training Step: 437  | total loss: \u001b[1m\u001b[32m0.15475\u001b[0m\u001b[0m | time: 1.148s\n",
      "| SGD | epoch: 023 | loss: 0.15475 - R2: 1.0005 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 438  | total loss: \u001b[1m\u001b[32m0.15478\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 024 | loss: 0.15478 - R2: 1.0013 -- iter: 0064/1168\n",
      "Training Step: 439  | total loss: \u001b[1m\u001b[32m0.15664\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 024 | loss: 0.15664 - R2: 1.0012 -- iter: 0128/1168\n",
      "Training Step: 440  | total loss: \u001b[1m\u001b[32m0.14900\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 024 | loss: 0.14900 - R2: 1.0001 -- iter: 0192/1168\n",
      "Training Step: 441  | total loss: \u001b[1m\u001b[32m0.14900\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 024 | loss: 0.14900 - R2: 1.0001 -- iter: 0256/1168\n",
      "Training Step: 442  | total loss: \u001b[1m\u001b[32m0.15646\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 024 | loss: 0.15646 - R2: 1.0003 -- iter: 0320/1168\n",
      "Training Step: 443  | total loss: \u001b[1m\u001b[32m0.15444\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 024 | loss: 0.15444 - R2: 1.0007 -- iter: 0384/1168\n",
      "Training Step: 444  | total loss: \u001b[1m\u001b[32m0.15299\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 024 | loss: 0.15299 - R2: 0.9996 -- iter: 0448/1168\n",
      "Training Step: 445  | total loss: \u001b[1m\u001b[32m0.15990\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 024 | loss: 0.15990 - R2: 0.9987 -- iter: 0512/1168\n",
      "Training Step: 446  | total loss: \u001b[1m\u001b[32m0.15617\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 024 | loss: 0.15617 - R2: 0.9994 -- iter: 0576/1168\n",
      "Training Step: 447  | total loss: \u001b[1m\u001b[32m0.16146\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 024 | loss: 0.16146 - R2: 0.9993 -- iter: 0640/1168\n",
      "Training Step: 448  | total loss: \u001b[1m\u001b[32m0.15466\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 024 | loss: 0.15466 - R2: 1.0002 -- iter: 0704/1168\n",
      "Training Step: 449  | total loss: \u001b[1m\u001b[32m0.16124\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 024 | loss: 0.16124 - R2: 1.0014 -- iter: 0768/1168\n",
      "Training Step: 450  | total loss: \u001b[1m\u001b[32m0.16124\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 024 | loss: 0.16124 - R2: 1.0014 -- iter: 0832/1168\n",
      "Training Step: 451  | total loss: \u001b[1m\u001b[32m0.16346\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 024 | loss: 0.16346 - R2: 1.0004 -- iter: 0896/1168\n",
      "Training Step: 452  | total loss: \u001b[1m\u001b[32m0.16748\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 024 | loss: 0.16748 - R2: 1.0006 -- iter: 0960/1168\n",
      "Training Step: 453  | total loss: \u001b[1m\u001b[32m0.16748\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 024 | loss: 0.16748 - R2: 1.0009 -- iter: 1024/1168\n",
      "Training Step: 454  | total loss: \u001b[1m\u001b[32m0.16338\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 024 | loss: 0.16338 - R2: 1.0007 -- iter: 1088/1168\n",
      "Training Step: 455  | total loss: \u001b[1m\u001b[32m0.16797\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 024 | loss: 0.16797 - R2: 0.9994 -- iter: 1152/1168\n",
      "Training Step: 456  | total loss: \u001b[1m\u001b[32m0.16481\u001b[0m\u001b[0m | time: 1.159s\n",
      "| SGD | epoch: 024 | loss: 0.16481 - R2: 0.9997 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 457  | total loss: \u001b[1m\u001b[32m0.16481\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 025 | loss: 0.16481 - R2: 0.9997 -- iter: 0064/1168\n",
      "Training Step: 458  | total loss: \u001b[1m\u001b[32m0.16291\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 025 | loss: 0.16291 - R2: 0.9996 -- iter: 0128/1168\n",
      "Training Step: 459  | total loss: \u001b[1m\u001b[32m0.15816\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 025 | loss: 0.15816 - R2: 0.9991 -- iter: 0192/1168\n",
      "Training Step: 460  | total loss: \u001b[1m\u001b[32m0.15816\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 025 | loss: 0.15816 - R2: 0.9991 -- iter: 0256/1168\n",
      "Training Step: 461  | total loss: \u001b[1m\u001b[32m0.15791\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 025 | loss: 0.15791 - R2: 0.9985 -- iter: 0320/1168\n",
      "Training Step: 462  | total loss: \u001b[1m\u001b[32m0.15565\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 025 | loss: 0.15565 - R2: 0.9997 -- iter: 0384/1168\n",
      "Training Step: 463  | total loss: \u001b[1m\u001b[32m0.15769\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 025 | loss: 0.15769 - R2: 0.9976 -- iter: 0448/1168\n",
      "Training Step: 464  | total loss: \u001b[1m\u001b[32m0.16256\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 025 | loss: 0.16256 - R2: 0.9987 -- iter: 0512/1168\n",
      "Training Step: 465  | total loss: \u001b[1m\u001b[32m0.16256\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 025 | loss: 0.16256 - R2: 0.9987 -- iter: 0576/1168\n",
      "Training Step: 466  | total loss: \u001b[1m\u001b[32m0.16183\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 025 | loss: 0.16183 - R2: 0.9992 -- iter: 0640/1168\n",
      "Training Step: 467  | total loss: \u001b[1m\u001b[32m0.16183\u001b[0m\u001b[0m | time: 0.183s\n",
      "| SGD | epoch: 025 | loss: 0.16183 - R2: 0.9992 -- iter: 0704/1168\n",
      "Training Step: 468  | total loss: \u001b[1m\u001b[32m0.16002\u001b[0m\u001b[0m | time: 0.187s\n",
      "| SGD | epoch: 025 | loss: 0.16002 - R2: 0.9987 -- iter: 0768/1168\n",
      "Training Step: 469  | total loss: \u001b[1m\u001b[32m0.16002\u001b[0m\u001b[0m | time: 0.189s\n",
      "| SGD | epoch: 025 | loss: 0.16002 - R2: 0.9987 -- iter: 0832/1168\n",
      "Training Step: 470  | total loss: \u001b[1m\u001b[32m0.16013\u001b[0m\u001b[0m | time: 0.192s\n",
      "| SGD | epoch: 025 | loss: 0.16013 - R2: 0.9990 -- iter: 0896/1168\n",
      "Training Step: 471  | total loss: \u001b[1m\u001b[32m0.15934\u001b[0m\u001b[0m | time: 0.195s\n",
      "| SGD | epoch: 025 | loss: 0.15934 - R2: 0.9986 -- iter: 0960/1168\n",
      "Training Step: 472  | total loss: \u001b[1m\u001b[32m0.15608\u001b[0m\u001b[0m | time: 0.205s\n",
      "| SGD | epoch: 025 | loss: 0.15608 - R2: 0.9986 -- iter: 1024/1168\n",
      "Training Step: 473  | total loss: \u001b[1m\u001b[32m0.16057\u001b[0m\u001b[0m | time: 0.211s\n",
      "| SGD | epoch: 025 | loss: 0.16057 - R2: 0.9994 -- iter: 1088/1168\n",
      "Training Step: 474  | total loss: \u001b[1m\u001b[32m0.16779\u001b[0m\u001b[0m | time: 0.215s\n",
      "| SGD | epoch: 025 | loss: 0.16779 - R2: 0.9999 -- iter: 1152/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 475  | total loss: \u001b[1m\u001b[32m0.16728\u001b[0m\u001b[0m | time: 1.221s\n",
      "| SGD | epoch: 025 | loss: 0.16728 - R2: 1.0003 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 476  | total loss: \u001b[1m\u001b[32m0.16240\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 026 | loss: 0.16240 - R2: 1.0003 -- iter: 0064/1168\n",
      "Training Step: 477  | total loss: \u001b[1m\u001b[32m0.16240\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 026 | loss: 0.16240 - R2: 1.0003 -- iter: 0128/1168\n",
      "Training Step: 478  | total loss: \u001b[1m\u001b[32m0.15752\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 026 | loss: 0.15752 - R2: 1.0015 -- iter: 0192/1168\n",
      "Training Step: 479  | total loss: \u001b[1m\u001b[32m0.15611\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 026 | loss: 0.15611 - R2: 1.0015 -- iter: 0256/1168\n",
      "Training Step: 480  | total loss: \u001b[1m\u001b[32m0.16766\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 026 | loss: 0.16766 - R2: 1.0005 -- iter: 0320/1168\n",
      "Training Step: 481  | total loss: \u001b[1m\u001b[32m0.17281\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 026 | loss: 0.17281 - R2: 0.9998 -- iter: 0384/1168\n",
      "Training Step: 482  | total loss: \u001b[1m\u001b[32m0.17767\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 026 | loss: 0.17767 - R2: 0.9998 -- iter: 0448/1168\n",
      "Training Step: 483  | total loss: \u001b[1m\u001b[32m0.17767\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 026 | loss: 0.17767 - R2: 0.9998 -- iter: 0512/1168\n",
      "Training Step: 484  | total loss: \u001b[1m\u001b[32m0.16913\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 026 | loss: 0.16913 - R2: 1.0012 -- iter: 0576/1168\n",
      "Training Step: 485  | total loss: \u001b[1m\u001b[32m0.16913\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 026 | loss: 0.16913 - R2: 1.0012 -- iter: 0640/1168\n",
      "Training Step: 486  | total loss: \u001b[1m\u001b[32m0.16841\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 026 | loss: 0.16841 - R2: 0.9998 -- iter: 0704/1168\n",
      "Training Step: 487  | total loss: \u001b[1m\u001b[32m0.16249\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 026 | loss: 0.16249 - R2: 1.0008 -- iter: 0768/1168\n",
      "Training Step: 488  | total loss: \u001b[1m\u001b[32m0.16249\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 026 | loss: 0.16249 - R2: 1.0008 -- iter: 0832/1168\n",
      "Training Step: 489  | total loss: \u001b[1m\u001b[32m0.15823\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 026 | loss: 0.15823 - R2: 1.0003 -- iter: 0896/1168\n",
      "Training Step: 490  | total loss: \u001b[1m\u001b[32m0.15823\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 026 | loss: 0.15823 - R2: 1.0003 -- iter: 0960/1168\n",
      "Training Step: 491  | total loss: \u001b[1m\u001b[32m0.15644\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 026 | loss: 0.15644 - R2: 0.9993 -- iter: 1024/1168\n",
      "Training Step: 492  | total loss: \u001b[1m\u001b[32m0.15567\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 026 | loss: 0.15567 - R2: 0.9997 -- iter: 1088/1168\n",
      "Training Step: 493  | total loss: \u001b[1m\u001b[32m0.15863\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 026 | loss: 0.15863 - R2: 1.0003 -- iter: 1152/1168\n",
      "Training Step: 494  | total loss: \u001b[1m\u001b[32m0.15657\u001b[0m\u001b[0m | time: 1.113s\n",
      "| SGD | epoch: 026 | loss: 0.15657 - R2: 0.9986 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 495  | total loss: \u001b[1m\u001b[32m0.16211\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 027 | loss: 0.16211 - R2: 0.9986 -- iter: 0064/1168\n",
      "Training Step: 496  | total loss: \u001b[1m\u001b[32m0.16372\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 027 | loss: 0.16372 - R2: 0.9980 -- iter: 0128/1168\n",
      "Training Step: 497  | total loss: \u001b[1m\u001b[32m0.16372\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 027 | loss: 0.16372 - R2: 0.9980 -- iter: 0192/1168\n",
      "Training Step: 498  | total loss: \u001b[1m\u001b[32m0.16424\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 027 | loss: 0.16424 - R2: 0.9986 -- iter: 0256/1168\n",
      "Training Step: 499  | total loss: \u001b[1m\u001b[32m0.16065\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 027 | loss: 0.16065 - R2: 0.9984 -- iter: 0320/1168\n",
      "Training Step: 500  | total loss: \u001b[1m\u001b[32m0.16414\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 027 | loss: 0.16414 - R2: 0.9969 -- iter: 0384/1168\n",
      "Training Step: 501  | total loss: \u001b[1m\u001b[32m0.16727\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 027 | loss: 0.16727 - R2: 0.9955 -- iter: 0448/1168\n",
      "Training Step: 502  | total loss: \u001b[1m\u001b[32m0.16682\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 027 | loss: 0.16682 - R2: 0.9959 -- iter: 0512/1168\n",
      "Training Step: 503  | total loss: \u001b[1m\u001b[32m0.16975\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 027 | loss: 0.16975 - R2: 0.9973 -- iter: 0576/1168\n",
      "Training Step: 504  | total loss: \u001b[1m\u001b[32m0.17198\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 027 | loss: 0.17198 - R2: 0.9973 -- iter: 0640/1168\n",
      "Training Step: 505  | total loss: \u001b[1m\u001b[32m0.17494\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 027 | loss: 0.17494 - R2: 0.9990 -- iter: 0704/1168\n",
      "Training Step: 506  | total loss: \u001b[1m\u001b[32m0.17456\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 027 | loss: 0.17456 - R2: 0.9996 -- iter: 0768/1168\n",
      "Training Step: 507  | total loss: \u001b[1m\u001b[32m0.16978\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 027 | loss: 0.16978 - R2: 0.9996 -- iter: 0832/1168\n",
      "Training Step: 508  | total loss: \u001b[1m\u001b[32m0.16684\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 027 | loss: 0.16684 - R2: 0.9990 -- iter: 0896/1168\n",
      "Training Step: 509  | total loss: \u001b[1m\u001b[32m0.17127\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 027 | loss: 0.17127 - R2: 0.9972 -- iter: 0960/1168\n",
      "Training Step: 510  | total loss: \u001b[1m\u001b[32m0.17127\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 027 | loss: 0.17127 - R2: 0.9972 -- iter: 1024/1168\n",
      "Training Step: 511  | total loss: \u001b[1m\u001b[32m0.16646\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 027 | loss: 0.16646 - R2: 0.9975 -- iter: 1088/1168\n",
      "Training Step: 512  | total loss: \u001b[1m\u001b[32m0.16691\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 027 | loss: 0.16691 - R2: 0.9984 -- iter: 1152/1168\n",
      "Training Step: 513  | total loss: \u001b[1m\u001b[32m0.16418\u001b[0m\u001b[0m | time: 1.128s\n",
      "| SGD | epoch: 027 | loss: 0.16418 - R2: 1.0000 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 514  | total loss: \u001b[1m\u001b[32m0.16418\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 028 | loss: 0.16418 - R2: 1.0000 -- iter: 0064/1168\n",
      "Training Step: 515  | total loss: \u001b[1m\u001b[32m0.16725\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 028 | loss: 0.16725 - R2: 0.9980 -- iter: 0128/1168\n",
      "Training Step: 516  | total loss: \u001b[1m\u001b[32m0.16967\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 028 | loss: 0.16967 - R2: 0.9994 -- iter: 0192/1168\n",
      "Training Step: 517  | total loss: \u001b[1m\u001b[32m0.16967\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 028 | loss: 0.16967 - R2: 0.9994 -- iter: 0256/1168\n",
      "Training Step: 518  | total loss: \u001b[1m\u001b[32m0.16792\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 028 | loss: 0.16792 - R2: 0.9991 -- iter: 0320/1168\n",
      "Training Step: 519  | total loss: \u001b[1m\u001b[32m0.16677\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 028 | loss: 0.16677 - R2: 0.9990 -- iter: 0384/1168\n",
      "Training Step: 520  | total loss: \u001b[1m\u001b[32m0.15736\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 028 | loss: 0.15736 - R2: 1.0002 -- iter: 0448/1168\n",
      "Training Step: 521  | total loss: \u001b[1m\u001b[32m0.15736\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 028 | loss: 0.15736 - R2: 1.0002 -- iter: 0512/1168\n",
      "Training Step: 522  | total loss: \u001b[1m\u001b[32m0.15979\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 028 | loss: 0.15979 - R2: 1.0019 -- iter: 0576/1168\n",
      "Training Step: 523  | total loss: \u001b[1m\u001b[32m0.15701\u001b[0m\u001b[0m | time: 0.171s\n",
      "| SGD | epoch: 028 | loss: 0.15701 - R2: 1.0015 -- iter: 0640/1168\n",
      "Training Step: 524  | total loss: \u001b[1m\u001b[32m0.15701\u001b[0m\u001b[0m | time: 0.173s\n",
      "| SGD | epoch: 028 | loss: 0.15701 - R2: 1.0015 -- iter: 0704/1168\n",
      "Training Step: 525  | total loss: \u001b[1m\u001b[32m0.15366\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 028 | loss: 0.15366 - R2: 1.0017 -- iter: 0768/1168\n",
      "Training Step: 526  | total loss: \u001b[1m\u001b[32m0.15366\u001b[0m\u001b[0m | time: 0.178s\n",
      "| SGD | epoch: 028 | loss: 0.15366 - R2: 1.0017 -- iter: 0832/1168\n",
      "Training Step: 527  | total loss: \u001b[1m\u001b[32m0.15201\u001b[0m\u001b[0m | time: 0.183s\n",
      "| SGD | epoch: 028 | loss: 0.15201 - R2: 1.0009 -- iter: 0896/1168\n",
      "Training Step: 528  | total loss: \u001b[1m\u001b[32m0.15265\u001b[0m\u001b[0m | time: 0.191s\n",
      "| SGD | epoch: 028 | loss: 0.15265 - R2: 1.0006 -- iter: 0960/1168\n",
      "Training Step: 529  | total loss: \u001b[1m\u001b[32m0.15599\u001b[0m\u001b[0m | time: 0.204s\n",
      "| SGD | epoch: 028 | loss: 0.15599 - R2: 1.0004 -- iter: 1024/1168\n",
      "Training Step: 530  | total loss: \u001b[1m\u001b[32m0.15611\u001b[0m\u001b[0m | time: 0.215s\n",
      "| SGD | epoch: 028 | loss: 0.15611 - R2: 1.0004 -- iter: 1088/1168\n",
      "Training Step: 531  | total loss: \u001b[1m\u001b[32m0.15561\u001b[0m\u001b[0m | time: 0.220s\n",
      "| SGD | epoch: 028 | loss: 0.15561 - R2: 1.0001 -- iter: 1152/1168\n",
      "Training Step: 532  | total loss: \u001b[1m\u001b[32m0.15561\u001b[0m\u001b[0m | time: 1.243s\n",
      "| SGD | epoch: 028 | loss: 0.15561 - R2: 1.0001 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 533  | total loss: \u001b[1m\u001b[32m0.16039\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 029 | loss: 0.16039 - R2: 1.0003 -- iter: 0064/1168\n",
      "Training Step: 534  | total loss: \u001b[1m\u001b[32m0.16039\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 029 | loss: 0.16039 - R2: 1.0003 -- iter: 0128/1168\n",
      "Training Step: 535  | total loss: \u001b[1m\u001b[32m0.16902\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 029 | loss: 0.16902 - R2: 1.0002 -- iter: 0192/1168\n",
      "Training Step: 536  | total loss: \u001b[1m\u001b[32m0.16477\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 029 | loss: 0.16477 - R2: 0.9996 -- iter: 0256/1168\n",
      "Training Step: 537  | total loss: \u001b[1m\u001b[32m0.16477\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 029 | loss: 0.16477 - R2: 0.9996 -- iter: 0320/1168\n",
      "Training Step: 538  | total loss: \u001b[1m\u001b[32m0.15862\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 029 | loss: 0.15862 - R2: 1.0005 -- iter: 0384/1168\n",
      "Training Step: 539  | total loss: \u001b[1m\u001b[32m0.16100\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 029 | loss: 0.16100 - R2: 1.0004 -- iter: 0448/1168\n",
      "Training Step: 540  | total loss: \u001b[1m\u001b[32m0.17995\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 029 | loss: 0.17995 - R2: 1.0010 -- iter: 0512/1168\n",
      "Training Step: 541  | total loss: \u001b[1m\u001b[32m0.17918\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 029 | loss: 0.17918 - R2: 0.9998 -- iter: 0576/1168\n",
      "Training Step: 542  | total loss: \u001b[1m\u001b[32m0.17918\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 029 | loss: 0.17918 - R2: 0.9998 -- iter: 0640/1168\n",
      "Training Step: 543  | total loss: \u001b[1m\u001b[32m0.17556\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 029 | loss: 0.17556 - R2: 1.0004 -- iter: 0704/1168\n",
      "Training Step: 544  | total loss: \u001b[1m\u001b[32m0.17488\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 029 | loss: 0.17488 - R2: 1.0015 -- iter: 0768/1168\n",
      "Training Step: 545  | total loss: \u001b[1m\u001b[32m0.17150\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 029 | loss: 0.17150 - R2: 1.0013 -- iter: 0832/1168\n",
      "Training Step: 546  | total loss: \u001b[1m\u001b[32m0.16912\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 029 | loss: 0.16912 - R2: 1.0012 -- iter: 0896/1168\n",
      "Training Step: 547  | total loss: \u001b[1m\u001b[32m0.16912\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 029 | loss: 0.16912 - R2: 1.0012 -- iter: 0960/1168\n",
      "Training Step: 548  | total loss: \u001b[1m\u001b[32m0.16293\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 029 | loss: 0.16293 - R2: 1.0007 -- iter: 1024/1168\n",
      "Training Step: 549  | total loss: \u001b[1m\u001b[32m0.16103\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 029 | loss: 0.16103 - R2: 1.0000 -- iter: 1088/1168\n",
      "Training Step: 550  | total loss: \u001b[1m\u001b[32m0.16480\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 029 | loss: 0.16480 - R2: 0.9987 -- iter: 1152/1168\n",
      "Training Step: 551  | total loss: \u001b[1m\u001b[32m0.16620\u001b[0m\u001b[0m | time: 1.133s\n",
      "| SGD | epoch: 029 | loss: 0.16620 - R2: 1.0002 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 552  | total loss: \u001b[1m\u001b[32m0.16222\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 030 | loss: 0.16222 - R2: 1.0004 -- iter: 0064/1168\n",
      "Training Step: 553  | total loss: \u001b[1m\u001b[32m0.15893\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 030 | loss: 0.15893 - R2: 1.0007 -- iter: 0128/1168\n",
      "Training Step: 554  | total loss: \u001b[1m\u001b[32m0.15811\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 030 | loss: 0.15811 - R2: 1.0023 -- iter: 0192/1168\n",
      "Training Step: 555  | total loss: \u001b[1m\u001b[32m0.16569\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 030 | loss: 0.16569 - R2: 1.0016 -- iter: 0256/1168\n",
      "Training Step: 556  | total loss: \u001b[1m\u001b[32m0.16569\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 030 | loss: 0.16569 - R2: 1.0016 -- iter: 0320/1168\n",
      "Training Step: 557  | total loss: \u001b[1m\u001b[32m0.16896\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 030 | loss: 0.16896 - R2: 0.9999 -- iter: 0384/1168\n",
      "Training Step: 558  | total loss: \u001b[1m\u001b[32m0.17348\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 030 | loss: 0.17348 - R2: 0.9997 -- iter: 0448/1168\n",
      "Training Step: 559  | total loss: \u001b[1m\u001b[32m0.16274\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 030 | loss: 0.16274 - R2: 0.9993 -- iter: 0512/1168\n",
      "Training Step: 560  | total loss: \u001b[1m\u001b[32m0.16274\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 030 | loss: 0.16274 - R2: 0.9993 -- iter: 0576/1168\n",
      "Training Step: 561  | total loss: \u001b[1m\u001b[32m0.15885\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 030 | loss: 0.15885 - R2: 0.9986 -- iter: 0640/1168\n",
      "Training Step: 562  | total loss: \u001b[1m\u001b[32m0.15719\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 030 | loss: 0.15719 - R2: 0.9991 -- iter: 0704/1168\n",
      "Training Step: 563  | total loss: \u001b[1m\u001b[32m0.15719\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 030 | loss: 0.15719 - R2: 0.9991 -- iter: 0768/1168\n",
      "Training Step: 564  | total loss: \u001b[1m\u001b[32m0.15824\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 030 | loss: 0.15824 - R2: 1.0001 -- iter: 0832/1168\n",
      "Training Step: 565  | total loss: \u001b[1m\u001b[32m0.15602\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 030 | loss: 0.15602 - R2: 1.0001 -- iter: 0896/1168\n",
      "Training Step: 566  | total loss: \u001b[1m\u001b[32m0.15783\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 030 | loss: 0.15783 - R2: 1.0001 -- iter: 0960/1168\n",
      "Training Step: 567  | total loss: \u001b[1m\u001b[32m0.15580\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 030 | loss: 0.15580 - R2: 1.0003 -- iter: 1024/1168\n",
      "Training Step: 568  | total loss: \u001b[1m\u001b[32m0.15580\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 030 | loss: 0.15580 - R2: 1.0003 -- iter: 1088/1168\n",
      "Training Step: 569  | total loss: \u001b[1m\u001b[32m0.15571\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 030 | loss: 0.15571 - R2: 0.9992 -- iter: 1152/1168\n",
      "Training Step: 570  | total loss: \u001b[1m\u001b[32m0.15540\u001b[0m\u001b[0m | time: 1.150s\n",
      "| SGD | epoch: 030 | loss: 0.15540 - R2: 0.9992 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 571  | total loss: \u001b[1m\u001b[32m0.16041\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 031 | loss: 0.16041 - R2: 0.9994 -- iter: 0064/1168\n",
      "Training Step: 572  | total loss: \u001b[1m\u001b[32m0.15325\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 031 | loss: 0.15325 - R2: 0.9987 -- iter: 0128/1168\n",
      "Training Step: 573  | total loss: \u001b[1m\u001b[32m0.15325\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 031 | loss: 0.15325 - R2: 0.9987 -- iter: 0192/1168\n",
      "Training Step: 574  | total loss: \u001b[1m\u001b[32m0.15214\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 031 | loss: 0.15214 - R2: 1.0019 -- iter: 0256/1168\n",
      "Training Step: 575  | total loss: \u001b[1m\u001b[32m0.15401\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 031 | loss: 0.15401 - R2: 1.0009 -- iter: 0320/1168\n",
      "Training Step: 576  | total loss: \u001b[1m\u001b[32m0.15196\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 031 | loss: 0.15196 - R2: 1.0017 -- iter: 0384/1168\n",
      "Training Step: 577  | total loss: \u001b[1m\u001b[32m0.15069\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 031 | loss: 0.15069 - R2: 1.0016 -- iter: 0448/1168\n",
      "Training Step: 578  | total loss: \u001b[1m\u001b[32m0.14900\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 031 | loss: 0.14900 - R2: 1.0016 -- iter: 0512/1168\n",
      "Training Step: 579  | total loss: \u001b[1m\u001b[32m0.14295\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 031 | loss: 0.14295 - R2: 1.0020 -- iter: 0576/1168\n",
      "Training Step: 580  | total loss: \u001b[1m\u001b[32m0.14295\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 031 | loss: 0.14295 - R2: 1.0020 -- iter: 0640/1168\n",
      "Training Step: 581  | total loss: \u001b[1m\u001b[32m0.13751\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 031 | loss: 0.13751 - R2: 1.0023 -- iter: 0704/1168\n",
      "Training Step: 582  | total loss: \u001b[1m\u001b[32m0.14829\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 031 | loss: 0.14829 - R2: 1.0021 -- iter: 0768/1168\n",
      "Training Step: 583  | total loss: \u001b[1m\u001b[32m0.14913\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 031 | loss: 0.14913 - R2: 1.0016 -- iter: 0832/1168\n",
      "Training Step: 584  | total loss: \u001b[1m\u001b[32m0.15034\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 031 | loss: 0.15034 - R2: 1.0015 -- iter: 0896/1168\n",
      "Training Step: 585  | total loss: \u001b[1m\u001b[32m0.15034\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 031 | loss: 0.15034 - R2: 1.0015 -- iter: 0960/1168\n",
      "Training Step: 586  | total loss: \u001b[1m\u001b[32m0.15499\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 031 | loss: 0.15499 - R2: 1.0007 -- iter: 1024/1168\n",
      "Training Step: 587  | total loss: \u001b[1m\u001b[32m0.16252\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 031 | loss: 0.16252 - R2: 0.9997 -- iter: 1088/1168\n",
      "Training Step: 588  | total loss: \u001b[1m\u001b[32m0.16252\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 031 | loss: 0.16252 - R2: 0.9997 -- iter: 1152/1168\n",
      "Training Step: 589  | total loss: \u001b[1m\u001b[32m0.16675\u001b[0m\u001b[0m | time: 1.127s\n",
      "| SGD | epoch: 031 | loss: 0.16675 - R2: 0.9994 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 590  | total loss: \u001b[1m\u001b[32m0.17130\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 032 | loss: 0.17130 - R2: 0.9989 -- iter: 0064/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 591  | total loss: \u001b[1m\u001b[32m0.16963\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 032 | loss: 0.16963 - R2: 0.9978 -- iter: 0128/1168\n",
      "Training Step: 592  | total loss: \u001b[1m\u001b[32m0.16588\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 032 | loss: 0.16588 - R2: 0.9969 -- iter: 0192/1168\n",
      "Training Step: 593  | total loss: \u001b[1m\u001b[32m0.16273\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 032 | loss: 0.16273 - R2: 0.9969 -- iter: 0256/1168\n",
      "Training Step: 594  | total loss: \u001b[1m\u001b[32m0.16216\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 032 | loss: 0.16216 - R2: 0.9970 -- iter: 0320/1168\n",
      "Training Step: 595  | total loss: \u001b[1m\u001b[32m0.15920\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 032 | loss: 0.15920 - R2: 0.9993 -- iter: 0384/1168\n",
      "Training Step: 596  | total loss: \u001b[1m\u001b[32m0.16396\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 032 | loss: 0.16396 - R2: 1.0002 -- iter: 0448/1168\n",
      "Training Step: 597  | total loss: \u001b[1m\u001b[32m0.17322\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 032 | loss: 0.17322 - R2: 0.9992 -- iter: 0512/1168\n",
      "Training Step: 598  | total loss: \u001b[1m\u001b[32m0.17180\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 032 | loss: 0.17180 - R2: 0.9987 -- iter: 0576/1168\n",
      "Training Step: 599  | total loss: \u001b[1m\u001b[32m0.16371\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 032 | loss: 0.16371 - R2: 0.9982 -- iter: 0640/1168\n",
      "Training Step: 600  | total loss: \u001b[1m\u001b[32m0.16371\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 032 | loss: 0.16371 - R2: 0.9982 -- iter: 0704/1168\n",
      "Training Step: 601  | total loss: \u001b[1m\u001b[32m0.15137\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 032 | loss: 0.15137 - R2: 0.9987 -- iter: 0768/1168\n",
      "Training Step: 602  | total loss: \u001b[1m\u001b[32m0.15137\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 032 | loss: 0.15137 - R2: 0.9987 -- iter: 0832/1168\n",
      "Training Step: 603  | total loss: \u001b[1m\u001b[32m0.15388\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 032 | loss: 0.15388 - R2: 0.9998 -- iter: 0896/1168\n",
      "Training Step: 604  | total loss: \u001b[1m\u001b[32m0.15246\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 032 | loss: 0.15246 - R2: 0.9998 -- iter: 0960/1168\n",
      "Training Step: 605  | total loss: \u001b[1m\u001b[32m0.14794\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 032 | loss: 0.14794 - R2: 0.9994 -- iter: 1024/1168\n",
      "Training Step: 606  | total loss: \u001b[1m\u001b[32m0.15316\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 032 | loss: 0.15316 - R2: 0.9995 -- iter: 1088/1168\n",
      "Training Step: 607  | total loss: \u001b[1m\u001b[32m0.15168\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 032 | loss: 0.15168 - R2: 1.0006 -- iter: 1152/1168\n",
      "Training Step: 608  | total loss: \u001b[1m\u001b[32m0.16140\u001b[0m\u001b[0m | time: 1.146s\n",
      "| SGD | epoch: 032 | loss: 0.16140 - R2: 0.9999 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 609  | total loss: \u001b[1m\u001b[32m0.16140\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 033 | loss: 0.16140 - R2: 0.9999 -- iter: 0064/1168\n",
      "Training Step: 610  | total loss: \u001b[1m\u001b[32m0.15731\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 033 | loss: 0.15731 - R2: 0.9996 -- iter: 0128/1168\n",
      "Training Step: 611  | total loss: \u001b[1m\u001b[32m0.15935\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 033 | loss: 0.15935 - R2: 0.9992 -- iter: 0192/1168\n",
      "Training Step: 612  | total loss: \u001b[1m\u001b[32m0.16583\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 033 | loss: 0.16583 - R2: 1.0009 -- iter: 0256/1168\n",
      "Training Step: 613  | total loss: \u001b[1m\u001b[32m0.16015\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 033 | loss: 0.16015 - R2: 1.0004 -- iter: 0320/1168\n",
      "Training Step: 614  | total loss: \u001b[1m\u001b[32m0.15706\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 033 | loss: 0.15706 - R2: 0.9998 -- iter: 0384/1168\n",
      "Training Step: 615  | total loss: \u001b[1m\u001b[32m0.15855\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 033 | loss: 0.15855 - R2: 1.0010 -- iter: 0448/1168\n",
      "Training Step: 616  | total loss: \u001b[1m\u001b[32m0.15785\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 033 | loss: 0.15785 - R2: 1.0012 -- iter: 0512/1168\n",
      "Training Step: 617  | total loss: \u001b[1m\u001b[32m0.15785\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 033 | loss: 0.15785 - R2: 1.0012 -- iter: 0576/1168\n",
      "Training Step: 618  | total loss: \u001b[1m\u001b[32m0.15715\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 033 | loss: 0.15715 - R2: 1.0001 -- iter: 0640/1168\n",
      "Training Step: 619  | total loss: \u001b[1m\u001b[32m0.16006\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 033 | loss: 0.16006 - R2: 1.0021 -- iter: 0704/1168\n",
      "Training Step: 620  | total loss: \u001b[1m\u001b[32m0.16267\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 033 | loss: 0.16267 - R2: 1.0038 -- iter: 0768/1168\n",
      "Training Step: 621  | total loss: \u001b[1m\u001b[32m0.16267\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 033 | loss: 0.16267 - R2: 1.0038 -- iter: 0832/1168\n",
      "Training Step: 622  | total loss: \u001b[1m\u001b[32m0.16720\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 033 | loss: 0.16720 - R2: 1.0031 -- iter: 0896/1168\n",
      "Training Step: 623  | total loss: \u001b[1m\u001b[32m0.16395\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 033 | loss: 0.16395 - R2: 1.0035 -- iter: 0960/1168\n",
      "Training Step: 624  | total loss: \u001b[1m\u001b[32m0.16131\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 033 | loss: 0.16131 - R2: 1.0027 -- iter: 1024/1168\n",
      "Training Step: 625  | total loss: \u001b[1m\u001b[32m0.15772\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 033 | loss: 0.15772 - R2: 1.0027 -- iter: 1088/1168\n",
      "Training Step: 626  | total loss: \u001b[1m\u001b[32m0.15680\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 033 | loss: 0.15680 - R2: 1.0022 -- iter: 1152/1168\n",
      "Training Step: 627  | total loss: \u001b[1m\u001b[32m0.15871\u001b[0m\u001b[0m | time: 1.174s\n",
      "| SGD | epoch: 033 | loss: 0.15871 - R2: 1.0016 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 628  | total loss: \u001b[1m\u001b[32m0.15934\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 034 | loss: 0.15934 - R2: 1.0013 -- iter: 0064/1168\n",
      "Training Step: 629  | total loss: \u001b[1m\u001b[32m0.15934\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 034 | loss: 0.15934 - R2: 1.0013 -- iter: 0128/1168\n",
      "Training Step: 630  | total loss: \u001b[1m\u001b[32m0.15667\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 034 | loss: 0.15667 - R2: 1.0012 -- iter: 0192/1168\n",
      "Training Step: 631  | total loss: \u001b[1m\u001b[32m0.15527\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 034 | loss: 0.15527 - R2: 1.0002 -- iter: 0256/1168\n",
      "Training Step: 632  | total loss: \u001b[1m\u001b[32m0.15379\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 034 | loss: 0.15379 - R2: 0.9996 -- iter: 0320/1168\n",
      "Training Step: 633  | total loss: \u001b[1m\u001b[32m0.15379\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 034 | loss: 0.15379 - R2: 0.9996 -- iter: 0384/1168\n",
      "Training Step: 634  | total loss: \u001b[1m\u001b[32m0.15171\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 034 | loss: 0.15171 - R2: 0.9988 -- iter: 0448/1168\n",
      "Training Step: 635  | total loss: \u001b[1m\u001b[32m0.15901\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 034 | loss: 0.15901 - R2: 0.9988 -- iter: 0512/1168\n",
      "Training Step: 636  | total loss: \u001b[1m\u001b[32m0.16323\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 034 | loss: 0.16323 - R2: 1.0004 -- iter: 0576/1168\n",
      "Training Step: 637  | total loss: \u001b[1m\u001b[32m0.16088\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 034 | loss: 0.16088 - R2: 0.9993 -- iter: 0640/1168\n",
      "Training Step: 638  | total loss: \u001b[1m\u001b[32m0.16088\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 034 | loss: 0.16088 - R2: 0.9993 -- iter: 0704/1168\n",
      "Training Step: 639  | total loss: \u001b[1m\u001b[32m0.15715\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 034 | loss: 0.15715 - R2: 0.9997 -- iter: 0768/1168\n",
      "Training Step: 640  | total loss: \u001b[1m\u001b[32m0.15715\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 034 | loss: 0.15715 - R2: 0.9997 -- iter: 0832/1168\n",
      "Training Step: 641  | total loss: \u001b[1m\u001b[32m0.15561\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 034 | loss: 0.15561 - R2: 1.0016 -- iter: 0896/1168\n",
      "Training Step: 642  | total loss: \u001b[1m\u001b[32m0.15492\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 034 | loss: 0.15492 - R2: 1.0014 -- iter: 0960/1168\n",
      "Training Step: 643  | total loss: \u001b[1m\u001b[32m0.15389\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 034 | loss: 0.15389 - R2: 1.0013 -- iter: 1024/1168\n",
      "Training Step: 644  | total loss: \u001b[1m\u001b[32m0.15223\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 034 | loss: 0.15223 - R2: 1.0030 -- iter: 1088/1168\n",
      "Training Step: 645  | total loss: \u001b[1m\u001b[32m0.15390\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 034 | loss: 0.15390 - R2: 1.0023 -- iter: 1152/1168\n",
      "Training Step: 646  | total loss: \u001b[1m\u001b[32m0.15390\u001b[0m\u001b[0m | time: 1.175s\n",
      "| SGD | epoch: 034 | loss: 0.15390 - R2: 1.0023 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 647  | total loss: \u001b[1m\u001b[32m0.15947\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 035 | loss: 0.15947 - R2: 1.0026 -- iter: 0064/1168\n",
      "Training Step: 648  | total loss: \u001b[1m\u001b[32m0.16046\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 035 | loss: 0.16046 - R2: 1.0030 -- iter: 0128/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 649  | total loss: \u001b[1m\u001b[32m0.16000\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 035 | loss: 0.16000 - R2: 1.0019 -- iter: 0192/1168\n",
      "Training Step: 650  | total loss: \u001b[1m\u001b[32m0.16000\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 035 | loss: 0.16000 - R2: 1.0019 -- iter: 0256/1168\n",
      "Training Step: 651  | total loss: \u001b[1m\u001b[32m0.15825\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 035 | loss: 0.15825 - R2: 1.0015 -- iter: 0320/1168\n",
      "Training Step: 652  | total loss: \u001b[1m\u001b[32m0.16632\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 035 | loss: 0.16632 - R2: 1.0017 -- iter: 0384/1168\n",
      "Training Step: 653  | total loss: \u001b[1m\u001b[32m0.16711\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 035 | loss: 0.16711 - R2: 1.0002 -- iter: 0448/1168\n",
      "Training Step: 654  | total loss: \u001b[1m\u001b[32m0.16721\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 035 | loss: 0.16721 - R2: 1.0008 -- iter: 0512/1168\n",
      "Training Step: 655  | total loss: \u001b[1m\u001b[32m0.16362\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 035 | loss: 0.16362 - R2: 1.0006 -- iter: 0576/1168\n",
      "Training Step: 656  | total loss: \u001b[1m\u001b[32m0.16316\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 035 | loss: 0.16316 - R2: 1.0013 -- iter: 0640/1168\n",
      "Training Step: 657  | total loss: \u001b[1m\u001b[32m0.16322\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 035 | loss: 0.16322 - R2: 1.0013 -- iter: 0704/1168\n",
      "Training Step: 658  | total loss: \u001b[1m\u001b[32m0.16578\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 035 | loss: 0.16578 - R2: 0.9999 -- iter: 0768/1168\n",
      "Training Step: 659  | total loss: \u001b[1m\u001b[32m0.16201\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 035 | loss: 0.16201 - R2: 0.9999 -- iter: 0832/1168\n",
      "Training Step: 660  | total loss: \u001b[1m\u001b[32m0.15431\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 035 | loss: 0.15431 - R2: 0.9997 -- iter: 0896/1168\n",
      "Training Step: 661  | total loss: \u001b[1m\u001b[32m0.14708\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 035 | loss: 0.14708 - R2: 1.0002 -- iter: 0960/1168\n",
      "Training Step: 662  | total loss: \u001b[1m\u001b[32m0.14708\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 035 | loss: 0.14708 - R2: 1.0004 -- iter: 1024/1168\n",
      "Training Step: 663  | total loss: \u001b[1m\u001b[32m0.14849\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 035 | loss: 0.14849 - R2: 1.0004 -- iter: 1088/1168\n",
      "Training Step: 664  | total loss: \u001b[1m\u001b[32m0.14785\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 035 | loss: 0.14785 - R2: 0.9999 -- iter: 1152/1168\n",
      "Training Step: 665  | total loss: \u001b[1m\u001b[32m0.14997\u001b[0m\u001b[0m | time: 1.140s\n",
      "| SGD | epoch: 035 | loss: 0.14997 - R2: 1.0002 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 666  | total loss: \u001b[1m\u001b[32m0.15070\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 036 | loss: 0.15070 - R2: 1.0001 -- iter: 0064/1168\n",
      "Training Step: 667  | total loss: \u001b[1m\u001b[32m0.15178\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 036 | loss: 0.15178 - R2: 1.0010 -- iter: 0128/1168\n",
      "Training Step: 668  | total loss: \u001b[1m\u001b[32m0.15259\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 036 | loss: 0.15259 - R2: 0.9997 -- iter: 0192/1168\n",
      "Training Step: 669  | total loss: \u001b[1m\u001b[32m0.14842\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 036 | loss: 0.14842 - R2: 0.9988 -- iter: 0256/1168\n",
      "Training Step: 670  | total loss: \u001b[1m\u001b[32m0.14842\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 036 | loss: 0.14842 - R2: 0.9988 -- iter: 0320/1168\n",
      "Training Step: 671  | total loss: \u001b[1m\u001b[32m0.14736\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 036 | loss: 0.14736 - R2: 0.9990 -- iter: 0384/1168\n",
      "Training Step: 672  | total loss: \u001b[1m\u001b[32m0.14612\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 036 | loss: 0.14612 - R2: 0.9983 -- iter: 0448/1168\n",
      "Training Step: 673  | total loss: \u001b[1m\u001b[32m0.15076\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 036 | loss: 0.15076 - R2: 1.0004 -- iter: 0512/1168\n",
      "Training Step: 674  | total loss: \u001b[1m\u001b[32m0.15319\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 036 | loss: 0.15319 - R2: 1.0005 -- iter: 0576/1168\n",
      "Training Step: 675  | total loss: \u001b[1m\u001b[32m0.15486\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 036 | loss: 0.15486 - R2: 0.9988 -- iter: 0640/1168\n",
      "Training Step: 676  | total loss: \u001b[1m\u001b[32m0.15729\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 036 | loss: 0.15729 - R2: 0.9999 -- iter: 0704/1168\n",
      "Training Step: 677  | total loss: \u001b[1m\u001b[32m0.15729\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 036 | loss: 0.15729 - R2: 0.9999 -- iter: 0768/1168\n",
      "Training Step: 678  | total loss: \u001b[1m\u001b[32m0.15431\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 036 | loss: 0.15431 - R2: 1.0009 -- iter: 0832/1168\n",
      "Training Step: 679  | total loss: \u001b[1m\u001b[32m0.15868\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 036 | loss: 0.15868 - R2: 1.0009 -- iter: 0896/1168\n",
      "Training Step: 680  | total loss: \u001b[1m\u001b[32m0.16743\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 036 | loss: 0.16743 - R2: 1.0058 -- iter: 0960/1168\n",
      "Training Step: 681  | total loss: \u001b[1m\u001b[32m0.16621\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 036 | loss: 0.16621 - R2: 1.0058 -- iter: 1024/1168\n",
      "Training Step: 682  | total loss: \u001b[1m\u001b[32m0.16741\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 036 | loss: 0.16741 - R2: 1.0050 -- iter: 1088/1168\n",
      "Training Step: 683  | total loss: \u001b[1m\u001b[32m0.16809\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 036 | loss: 0.16809 - R2: 1.0026 -- iter: 1152/1168\n",
      "Training Step: 684  | total loss: \u001b[1m\u001b[32m0.16326\u001b[0m\u001b[0m | time: 1.169s\n",
      "| SGD | epoch: 036 | loss: 0.16326 - R2: 1.0035 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 685  | total loss: \u001b[1m\u001b[32m0.16326\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 037 | loss: 0.16326 - R2: 1.0035 -- iter: 0064/1168\n",
      "Training Step: 686  | total loss: \u001b[1m\u001b[32m0.16936\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 037 | loss: 0.16936 - R2: 1.0031 -- iter: 0128/1168\n",
      "Training Step: 687  | total loss: \u001b[1m\u001b[32m0.16905\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 037 | loss: 0.16905 - R2: 1.0032 -- iter: 0192/1168\n",
      "Training Step: 688  | total loss: \u001b[1m\u001b[32m0.17361\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 037 | loss: 0.17361 - R2: 1.0035 -- iter: 0256/1168\n",
      "Training Step: 689  | total loss: \u001b[1m\u001b[32m0.16916\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 037 | loss: 0.16916 - R2: 1.0038 -- iter: 0320/1168\n",
      "Training Step: 690  | total loss: \u001b[1m\u001b[32m0.16943\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 037 | loss: 0.16943 - R2: 1.0031 -- iter: 0384/1168\n",
      "Training Step: 691  | total loss: \u001b[1m\u001b[32m0.16943\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 037 | loss: 0.16943 - R2: 1.0031 -- iter: 0448/1168\n",
      "Training Step: 692  | total loss: \u001b[1m\u001b[32m0.16928\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 037 | loss: 0.16928 - R2: 1.0027 -- iter: 0512/1168\n",
      "Training Step: 693  | total loss: \u001b[1m\u001b[32m0.16874\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 037 | loss: 0.16874 - R2: 1.0016 -- iter: 0576/1168\n",
      "Training Step: 694  | total loss: \u001b[1m\u001b[32m0.16590\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 037 | loss: 0.16590 - R2: 1.0001 -- iter: 0640/1168\n",
      "Training Step: 695  | total loss: \u001b[1m\u001b[32m0.16122\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 037 | loss: 0.16122 - R2: 1.0001 -- iter: 0704/1168\n",
      "Training Step: 696  | total loss: \u001b[1m\u001b[32m0.16638\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 037 | loss: 0.16638 - R2: 1.0005 -- iter: 0768/1168\n",
      "Training Step: 697  | total loss: \u001b[1m\u001b[32m0.16666\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 037 | loss: 0.16666 - R2: 1.0003 -- iter: 0832/1168\n",
      "Training Step: 698  | total loss: \u001b[1m\u001b[32m0.16915\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 037 | loss: 0.16915 - R2: 1.0003 -- iter: 0896/1168\n",
      "Training Step: 699  | total loss: \u001b[1m\u001b[32m0.16650\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 037 | loss: 0.16650 - R2: 0.9996 -- iter: 0960/1168\n",
      "Training Step: 700  | total loss: \u001b[1m\u001b[32m0.16688\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 037 | loss: 0.16688 - R2: 1.0007 -- iter: 1024/1168\n",
      "Training Step: 701  | total loss: \u001b[1m\u001b[32m0.16723\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 037 | loss: 0.16723 - R2: 1.0016 -- iter: 1088/1168\n",
      "Training Step: 702  | total loss: \u001b[1m\u001b[32m0.16429\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 037 | loss: 0.16429 - R2: 1.0019 -- iter: 1152/1168\n",
      "Training Step: 703  | total loss: \u001b[1m\u001b[32m0.16380\u001b[0m\u001b[0m | time: 1.121s\n",
      "| SGD | epoch: 037 | loss: 0.16380 - R2: 1.0011 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 704  | total loss: \u001b[1m\u001b[32m0.16066\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 038 | loss: 0.16066 - R2: 1.0012 -- iter: 0064/1168\n",
      "Training Step: 705  | total loss: \u001b[1m\u001b[32m0.16066\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 038 | loss: 0.16066 - R2: 1.0012 -- iter: 0128/1168\n",
      "Training Step: 706  | total loss: \u001b[1m\u001b[32m0.16112\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 038 | loss: 0.16112 - R2: 1.0019 -- iter: 0192/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 707  | total loss: \u001b[1m\u001b[32m0.16112\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 038 | loss: 0.16112 - R2: 1.0019 -- iter: 0256/1168\n",
      "Training Step: 708  | total loss: \u001b[1m\u001b[32m0.16822\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 038 | loss: 0.16822 - R2: 1.0028 -- iter: 0320/1168\n",
      "Training Step: 709  | total loss: \u001b[1m\u001b[32m0.16822\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 038 | loss: 0.16822 - R2: 1.0028 -- iter: 0384/1168\n",
      "Training Step: 710  | total loss: \u001b[1m\u001b[32m0.17007\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 038 | loss: 0.17007 - R2: 1.0019 -- iter: 0448/1168\n",
      "Training Step: 711  | total loss: \u001b[1m\u001b[32m0.17154\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 038 | loss: 0.17154 - R2: 1.0028 -- iter: 0512/1168\n",
      "Training Step: 712  | total loss: \u001b[1m\u001b[32m0.16921\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 038 | loss: 0.16921 - R2: 1.0023 -- iter: 0576/1168\n",
      "Training Step: 713  | total loss: \u001b[1m\u001b[32m0.16620\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 038 | loss: 0.16620 - R2: 1.0030 -- iter: 0640/1168\n",
      "Training Step: 714  | total loss: \u001b[1m\u001b[32m0.16620\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 038 | loss: 0.16620 - R2: 1.0031 -- iter: 0704/1168\n",
      "Training Step: 715  | total loss: \u001b[1m\u001b[32m0.16297\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 038 | loss: 0.16297 - R2: 1.0036 -- iter: 0768/1168\n",
      "Training Step: 716  | total loss: \u001b[1m\u001b[32m0.16834\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 038 | loss: 0.16834 - R2: 1.0024 -- iter: 0832/1168\n",
      "Training Step: 717  | total loss: \u001b[1m\u001b[32m0.16834\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 038 | loss: 0.16834 - R2: 1.0024 -- iter: 0896/1168\n",
      "Training Step: 718  | total loss: \u001b[1m\u001b[32m0.16803\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 038 | loss: 0.16803 - R2: 1.0006 -- iter: 0960/1168\n",
      "Training Step: 719  | total loss: \u001b[1m\u001b[32m0.16373\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 038 | loss: 0.16373 - R2: 1.0003 -- iter: 1024/1168\n",
      "Training Step: 720  | total loss: \u001b[1m\u001b[32m0.18696\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 038 | loss: 0.18696 - R2: 1.0019 -- iter: 1088/1168\n",
      "Training Step: 721  | total loss: \u001b[1m\u001b[32m0.20788\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 038 | loss: 0.20788 - R2: 1.0033 -- iter: 1152/1168\n",
      "Training Step: 722  | total loss: \u001b[1m\u001b[32m0.20363\u001b[0m\u001b[0m | time: 1.150s\n",
      "| SGD | epoch: 038 | loss: 0.20363 - R2: 1.0034 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 723  | total loss: \u001b[1m\u001b[32m0.19780\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 039 | loss: 0.19780 - R2: 1.0030 -- iter: 0064/1168\n",
      "Training Step: 724  | total loss: \u001b[1m\u001b[32m0.19081\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 039 | loss: 0.19081 - R2: 1.0025 -- iter: 0128/1168\n",
      "Training Step: 725  | total loss: \u001b[1m\u001b[32m0.18516\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 039 | loss: 0.18516 - R2: 1.0025 -- iter: 0192/1168\n",
      "Training Step: 726  | total loss: \u001b[1m\u001b[32m0.18190\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 039 | loss: 0.18190 - R2: 1.0021 -- iter: 0256/1168\n",
      "Training Step: 727  | total loss: \u001b[1m\u001b[32m0.17633\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 039 | loss: 0.17633 - R2: 1.0004 -- iter: 0320/1168\n",
      "Training Step: 728  | total loss: \u001b[1m\u001b[32m0.17633\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 039 | loss: 0.17633 - R2: 1.0004 -- iter: 0384/1168\n",
      "Training Step: 729  | total loss: \u001b[1m\u001b[32m0.17508\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 039 | loss: 0.17508 - R2: 1.0020 -- iter: 0448/1168\n",
      "Training Step: 730  | total loss: \u001b[1m\u001b[32m0.17469\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 039 | loss: 0.17469 - R2: 1.0020 -- iter: 0512/1168\n",
      "Training Step: 731  | total loss: \u001b[1m\u001b[32m0.16807\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 039 | loss: 0.16807 - R2: 1.0035 -- iter: 0576/1168\n",
      "Training Step: 732  | total loss: \u001b[1m\u001b[32m0.16807\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 039 | loss: 0.16807 - R2: 1.0035 -- iter: 0640/1168\n",
      "Training Step: 733  | total loss: \u001b[1m\u001b[32m0.16553\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 039 | loss: 0.16553 - R2: 1.0035 -- iter: 0704/1168\n",
      "Training Step: 734  | total loss: \u001b[1m\u001b[32m0.16686\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 039 | loss: 0.16686 - R2: 1.0028 -- iter: 0768/1168\n",
      "Training Step: 735  | total loss: \u001b[1m\u001b[32m0.16421\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 039 | loss: 0.16421 - R2: 1.0015 -- iter: 0832/1168\n",
      "Training Step: 736  | total loss: \u001b[1m\u001b[32m0.16362\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 039 | loss: 0.16362 - R2: 1.0026 -- iter: 0896/1168\n",
      "Training Step: 737  | total loss: \u001b[1m\u001b[32m0.16620\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 039 | loss: 0.16620 - R2: 1.0015 -- iter: 0960/1168\n",
      "Training Step: 738  | total loss: \u001b[1m\u001b[32m0.16570\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 039 | loss: 0.16570 - R2: 0.9993 -- iter: 1024/1168\n",
      "Training Step: 739  | total loss: \u001b[1m\u001b[32m0.15903\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 039 | loss: 0.15903 - R2: 0.9999 -- iter: 1088/1168\n",
      "Training Step: 740  | total loss: \u001b[1m\u001b[32m0.15903\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 039 | loss: 0.15903 - R2: 0.9999 -- iter: 1152/1168\n",
      "Training Step: 741  | total loss: \u001b[1m\u001b[32m0.15302\u001b[0m\u001b[0m | time: 1.126s\n",
      "| SGD | epoch: 039 | loss: 0.15302 - R2: 1.0006 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 742  | total loss: \u001b[1m\u001b[32m0.15087\u001b[0m\u001b[0m | time: 0.013s\n",
      "| SGD | epoch: 040 | loss: 0.15087 - R2: 1.0002 -- iter: 0064/1168\n",
      "Training Step: 743  | total loss: \u001b[1m\u001b[32m0.16594\u001b[0m\u001b[0m | time: 0.020s\n",
      "| SGD | epoch: 040 | loss: 0.16594 - R2: 0.9991 -- iter: 0128/1168\n",
      "Training Step: 744  | total loss: \u001b[1m\u001b[32m0.16899\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 040 | loss: 0.16899 - R2: 0.9989 -- iter: 0192/1168\n",
      "Training Step: 745  | total loss: \u001b[1m\u001b[32m0.16315\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 040 | loss: 0.16315 - R2: 0.9992 -- iter: 0256/1168\n",
      "Training Step: 746  | total loss: \u001b[1m\u001b[32m0.16315\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 040 | loss: 0.16315 - R2: 0.9992 -- iter: 0320/1168\n",
      "Training Step: 747  | total loss: \u001b[1m\u001b[32m0.15890\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 040 | loss: 0.15890 - R2: 0.9996 -- iter: 0384/1168\n",
      "Training Step: 748  | total loss: \u001b[1m\u001b[32m0.15756\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 040 | loss: 0.15756 - R2: 0.9991 -- iter: 0448/1168\n",
      "Training Step: 749  | total loss: \u001b[1m\u001b[32m0.15634\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 040 | loss: 0.15634 - R2: 0.9994 -- iter: 0512/1168\n",
      "Training Step: 750  | total loss: \u001b[1m\u001b[32m0.16279\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 040 | loss: 0.16279 - R2: 0.9996 -- iter: 0576/1168\n",
      "Training Step: 751  | total loss: \u001b[1m\u001b[32m0.16432\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 040 | loss: 0.16432 - R2: 0.9987 -- iter: 0640/1168\n",
      "Training Step: 752  | total loss: \u001b[1m\u001b[32m0.16432\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 040 | loss: 0.16432 - R2: 0.9987 -- iter: 0704/1168\n",
      "Training Step: 753  | total loss: \u001b[1m\u001b[32m0.16084\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 040 | loss: 0.16084 - R2: 0.9986 -- iter: 0768/1168\n",
      "Training Step: 754  | total loss: \u001b[1m\u001b[32m0.16183\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 040 | loss: 0.16183 - R2: 0.9993 -- iter: 0832/1168\n",
      "Training Step: 755  | total loss: \u001b[1m\u001b[32m0.16272\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 040 | loss: 0.16272 - R2: 0.9983 -- iter: 0896/1168\n",
      "Training Step: 756  | total loss: \u001b[1m\u001b[32m0.16215\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 040 | loss: 0.16215 - R2: 0.9999 -- iter: 0960/1168\n",
      "Training Step: 757  | total loss: \u001b[1m\u001b[32m0.15699\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 040 | loss: 0.15699 - R2: 0.9991 -- iter: 1024/1168\n",
      "Training Step: 758  | total loss: \u001b[1m\u001b[32m0.15699\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 040 | loss: 0.15699 - R2: 0.9991 -- iter: 1088/1168\n",
      "Training Step: 759  | total loss: \u001b[1m\u001b[32m0.17257\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 040 | loss: 0.17257 - R2: 0.9966 -- iter: 1152/1168\n",
      "Training Step: 760  | total loss: \u001b[1m\u001b[32m0.17257\u001b[0m\u001b[0m | time: 1.096s\n",
      "| SGD | epoch: 040 | loss: 0.17257 - R2: 0.9966 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 761  | total loss: \u001b[1m\u001b[32m0.18731\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 041 | loss: 0.18731 - R2: 0.9953 -- iter: 0064/1168\n",
      "Training Step: 762  | total loss: \u001b[1m\u001b[32m0.18731\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 041 | loss: 0.18731 - R2: 0.9953 -- iter: 0128/1168\n",
      "Training Step: 763  | total loss: \u001b[1m\u001b[32m0.18484\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 041 | loss: 0.18484 - R2: 0.9963 -- iter: 0192/1168\n",
      "Training Step: 764  | total loss: \u001b[1m\u001b[32m0.18029\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 041 | loss: 0.18029 - R2: 0.9961 -- iter: 0256/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 765  | total loss: \u001b[1m\u001b[32m0.17855\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 041 | loss: 0.17855 - R2: 0.9970 -- iter: 0320/1168\n",
      "Training Step: 766  | total loss: \u001b[1m\u001b[32m0.17855\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 041 | loss: 0.17855 - R2: 0.9970 -- iter: 0384/1168\n",
      "Training Step: 767  | total loss: \u001b[1m\u001b[32m0.17674\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 041 | loss: 0.17674 - R2: 1.0013 -- iter: 0448/1168\n",
      "Training Step: 768  | total loss: \u001b[1m\u001b[32m0.17615\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 041 | loss: 0.17615 - R2: 1.0013 -- iter: 0512/1168\n",
      "Training Step: 769  | total loss: \u001b[1m\u001b[32m0.17481\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 041 | loss: 0.17481 - R2: 1.0005 -- iter: 0576/1168\n",
      "Training Step: 770  | total loss: \u001b[1m\u001b[32m0.17481\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 041 | loss: 0.17481 - R2: 1.0005 -- iter: 0640/1168\n",
      "Training Step: 771  | total loss: \u001b[1m\u001b[32m0.17651\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 041 | loss: 0.17651 - R2: 0.9994 -- iter: 0704/1168\n",
      "Training Step: 772  | total loss: \u001b[1m\u001b[32m0.17138\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 041 | loss: 0.17138 - R2: 0.9988 -- iter: 0768/1168\n",
      "Training Step: 773  | total loss: \u001b[1m\u001b[32m0.16733\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 041 | loss: 0.16733 - R2: 0.9988 -- iter: 0832/1168\n",
      "Training Step: 774  | total loss: \u001b[1m\u001b[32m0.16268\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 041 | loss: 0.16268 - R2: 1.0001 -- iter: 0896/1168\n",
      "Training Step: 775  | total loss: \u001b[1m\u001b[32m0.16503\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 041 | loss: 0.16503 - R2: 0.9990 -- iter: 0960/1168\n",
      "Training Step: 776  | total loss: \u001b[1m\u001b[32m0.16152\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 041 | loss: 0.16152 - R2: 0.9992 -- iter: 1024/1168\n",
      "Training Step: 777  | total loss: \u001b[1m\u001b[32m0.16152\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 041 | loss: 0.16152 - R2: 0.9992 -- iter: 1088/1168\n",
      "Training Step: 778  | total loss: \u001b[1m\u001b[32m0.16242\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 041 | loss: 0.16242 - R2: 0.9997 -- iter: 1152/1168\n",
      "Training Step: 779  | total loss: \u001b[1m\u001b[32m0.17585\u001b[0m\u001b[0m | time: 1.148s\n",
      "| SGD | epoch: 041 | loss: 0.17585 - R2: 1.0009 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 780  | total loss: \u001b[1m\u001b[32m0.18819\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 042 | loss: 0.18819 - R2: 1.0029 -- iter: 0064/1168\n",
      "Training Step: 781  | total loss: \u001b[1m\u001b[32m0.18819\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 042 | loss: 0.18819 - R2: 1.0029 -- iter: 0128/1168\n",
      "Training Step: 782  | total loss: \u001b[1m\u001b[32m0.18088\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 042 | loss: 0.18088 - R2: 1.0032 -- iter: 0192/1168\n",
      "Training Step: 783  | total loss: \u001b[1m\u001b[32m0.17334\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 042 | loss: 0.17334 - R2: 1.0041 -- iter: 0256/1168\n",
      "Training Step: 784  | total loss: \u001b[1m\u001b[32m0.17334\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 042 | loss: 0.17334 - R2: 1.0041 -- iter: 0320/1168\n",
      "Training Step: 785  | total loss: \u001b[1m\u001b[32m0.17654\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 042 | loss: 0.17654 - R2: 1.0023 -- iter: 0384/1168\n",
      "Training Step: 786  | total loss: \u001b[1m\u001b[32m0.17654\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 042 | loss: 0.17654 - R2: 1.0022 -- iter: 0448/1168\n",
      "Training Step: 787  | total loss: \u001b[1m\u001b[32m0.18047\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 042 | loss: 0.18047 - R2: 1.0008 -- iter: 0512/1168\n",
      "Training Step: 788  | total loss: \u001b[1m\u001b[32m0.17786\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 042 | loss: 0.17786 - R2: 1.0008 -- iter: 0576/1168\n",
      "Training Step: 789  | total loss: \u001b[1m\u001b[32m0.17324\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 042 | loss: 0.17324 - R2: 0.9994 -- iter: 0640/1168\n",
      "Training Step: 790  | total loss: \u001b[1m\u001b[32m0.17660\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 042 | loss: 0.17660 - R2: 0.9994 -- iter: 0704/1168\n",
      "Training Step: 791  | total loss: \u001b[1m\u001b[32m0.17993\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 042 | loss: 0.17993 - R2: 1.0001 -- iter: 0768/1168\n",
      "Training Step: 792  | total loss: \u001b[1m\u001b[32m0.17993\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 042 | loss: 0.17993 - R2: 1.0006 -- iter: 0832/1168\n",
      "Training Step: 793  | total loss: \u001b[1m\u001b[32m0.18098\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 042 | loss: 0.18098 - R2: 1.0009 -- iter: 0896/1168\n",
      "Training Step: 794  | total loss: \u001b[1m\u001b[32m0.17773\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 042 | loss: 0.17773 - R2: 1.0011 -- iter: 0960/1168\n",
      "Training Step: 795  | total loss: \u001b[1m\u001b[32m0.17873\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 042 | loss: 0.17873 - R2: 1.0013 -- iter: 1024/1168\n",
      "Training Step: 796  | total loss: \u001b[1m\u001b[32m0.16784\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 042 | loss: 0.16784 - R2: 1.0021 -- iter: 1088/1168\n",
      "Training Step: 797  | total loss: \u001b[1m\u001b[32m0.17111\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 042 | loss: 0.17111 - R2: 1.0020 -- iter: 1152/1168\n",
      "Training Step: 798  | total loss: \u001b[1m\u001b[32m0.17111\u001b[0m\u001b[0m | time: 1.169s\n",
      "| SGD | epoch: 042 | loss: 0.17111 - R2: 1.0020 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 799  | total loss: \u001b[1m\u001b[32m0.16631\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 043 | loss: 0.16631 - R2: 1.0030 -- iter: 0064/1168\n",
      "Training Step: 800  | total loss: \u001b[1m\u001b[32m0.16631\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 043 | loss: 0.16631 - R2: 1.0030 -- iter: 0128/1168\n",
      "Training Step: 801  | total loss: \u001b[1m\u001b[32m0.16403\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 043 | loss: 0.16403 - R2: 1.0036 -- iter: 0192/1168\n",
      "Training Step: 802  | total loss: \u001b[1m\u001b[32m0.16219\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 043 | loss: 0.16219 - R2: 1.0017 -- iter: 0256/1168\n",
      "Training Step: 803  | total loss: \u001b[1m\u001b[32m0.16018\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 043 | loss: 0.16018 - R2: 1.0014 -- iter: 0320/1168\n",
      "Training Step: 804  | total loss: \u001b[1m\u001b[32m0.16018\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 043 | loss: 0.16018 - R2: 1.0014 -- iter: 0384/1168\n",
      "Training Step: 805  | total loss: \u001b[1m\u001b[32m0.15988\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 043 | loss: 0.15988 - R2: 1.0023 -- iter: 0448/1168\n",
      "Training Step: 806  | total loss: \u001b[1m\u001b[32m0.15988\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 043 | loss: 0.15988 - R2: 1.0023 -- iter: 0512/1168\n",
      "Training Step: 807  | total loss: \u001b[1m\u001b[32m0.15863\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 043 | loss: 0.15863 - R2: 1.0008 -- iter: 0576/1168\n",
      "Training Step: 808  | total loss: \u001b[1m\u001b[32m0.15820\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 043 | loss: 0.15820 - R2: 0.9995 -- iter: 0640/1168\n",
      "Training Step: 809  | total loss: \u001b[1m\u001b[32m0.15922\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 043 | loss: 0.15922 - R2: 0.9994 -- iter: 0704/1168\n",
      "Training Step: 810  | total loss: \u001b[1m\u001b[32m0.15633\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 043 | loss: 0.15633 - R2: 1.0011 -- iter: 0768/1168\n",
      "Training Step: 811  | total loss: \u001b[1m\u001b[32m0.15258\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 043 | loss: 0.15258 - R2: 1.0004 -- iter: 0832/1168\n",
      "Training Step: 812  | total loss: \u001b[1m\u001b[32m0.15104\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 043 | loss: 0.15104 - R2: 1.0004 -- iter: 0896/1168\n",
      "Training Step: 813  | total loss: \u001b[1m\u001b[32m0.15543\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 043 | loss: 0.15543 - R2: 0.9997 -- iter: 0960/1168\n",
      "Training Step: 814  | total loss: \u001b[1m\u001b[32m0.15603\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 043 | loss: 0.15603 - R2: 0.9993 -- iter: 1024/1168\n",
      "Training Step: 815  | total loss: \u001b[1m\u001b[32m0.15967\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 043 | loss: 0.15967 - R2: 0.9977 -- iter: 1088/1168\n",
      "Training Step: 816  | total loss: \u001b[1m\u001b[32m0.15967\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 043 | loss: 0.15967 - R2: 0.9977 -- iter: 1152/1168\n",
      "Training Step: 817  | total loss: \u001b[1m\u001b[32m0.16265\u001b[0m\u001b[0m | time: 1.126s\n",
      "| SGD | epoch: 043 | loss: 0.16265 - R2: 0.9989 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 818  | total loss: \u001b[1m\u001b[32m0.16175\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 044 | loss: 0.16175 - R2: 0.9993 -- iter: 0064/1168\n",
      "Training Step: 819  | total loss: \u001b[1m\u001b[32m0.16387\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 044 | loss: 0.16387 - R2: 0.9983 -- iter: 0128/1168\n",
      "Training Step: 820  | total loss: \u001b[1m\u001b[32m0.16415\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 044 | loss: 0.16415 - R2: 0.9977 -- iter: 0192/1168\n",
      "Training Step: 821  | total loss: \u001b[1m\u001b[32m0.16439\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 044 | loss: 0.16439 - R2: 0.9977 -- iter: 0256/1168\n",
      "Training Step: 822  | total loss: \u001b[1m\u001b[32m0.16523\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 044 | loss: 0.16523 - R2: 0.9973 -- iter: 0320/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 823  | total loss: \u001b[1m\u001b[32m0.15983\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 044 | loss: 0.15983 - R2: 0.9975 -- iter: 0384/1168\n",
      "Training Step: 824  | total loss: \u001b[1m\u001b[32m0.15800\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 044 | loss: 0.15800 - R2: 0.9983 -- iter: 0448/1168\n",
      "Training Step: 825  | total loss: \u001b[1m\u001b[32m0.15910\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 044 | loss: 0.15910 - R2: 0.9996 -- iter: 0512/1168\n",
      "Training Step: 826  | total loss: \u001b[1m\u001b[32m0.16325\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 044 | loss: 0.16325 - R2: 0.9993 -- iter: 0576/1168\n",
      "Training Step: 827  | total loss: \u001b[1m\u001b[32m0.16873\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 044 | loss: 0.16873 - R2: 0.9994 -- iter: 0640/1168\n",
      "Training Step: 828  | total loss: \u001b[1m\u001b[32m0.16873\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 044 | loss: 0.16873 - R2: 0.9994 -- iter: 0704/1168\n",
      "Training Step: 829  | total loss: \u001b[1m\u001b[32m0.16259\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 044 | loss: 0.16259 - R2: 0.9990 -- iter: 0768/1168\n",
      "Training Step: 830  | total loss: \u001b[1m\u001b[32m0.16259\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 044 | loss: 0.16259 - R2: 0.9990 -- iter: 0832/1168\n",
      "Training Step: 831  | total loss: \u001b[1m\u001b[32m0.15866\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 044 | loss: 0.15866 - R2: 0.9986 -- iter: 0896/1168\n",
      "Training Step: 832  | total loss: \u001b[1m\u001b[32m0.16535\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 044 | loss: 0.16535 - R2: 0.9984 -- iter: 0960/1168\n",
      "Training Step: 833  | total loss: \u001b[1m\u001b[32m0.17020\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 044 | loss: 0.17020 - R2: 0.9971 -- iter: 1024/1168\n",
      "Training Step: 834  | total loss: \u001b[1m\u001b[32m0.16700\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 044 | loss: 0.16700 - R2: 0.9969 -- iter: 1088/1168\n",
      "Training Step: 835  | total loss: \u001b[1m\u001b[32m0.16700\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 044 | loss: 0.16700 - R2: 0.9969 -- iter: 1152/1168\n",
      "Training Step: 836  | total loss: \u001b[1m\u001b[32m0.17271\u001b[0m\u001b[0m | time: 1.075s\n",
      "| SGD | epoch: 044 | loss: 0.17271 - R2: 0.9976 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 837  | total loss: \u001b[1m\u001b[32m0.17271\u001b[0m\u001b[0m | time: 0.006s\n",
      "| SGD | epoch: 045 | loss: 0.17271 - R2: 0.9976 -- iter: 0064/1168\n",
      "Training Step: 838  | total loss: \u001b[1m\u001b[32m0.16590\u001b[0m\u001b[0m | time: 0.009s\n",
      "| SGD | epoch: 045 | loss: 0.16590 - R2: 0.9997 -- iter: 0128/1168\n",
      "Training Step: 839  | total loss: \u001b[1m\u001b[32m0.16119\u001b[0m\u001b[0m | time: 0.012s\n",
      "| SGD | epoch: 045 | loss: 0.16119 - R2: 1.0012 -- iter: 0192/1168\n",
      "Training Step: 840  | total loss: \u001b[1m\u001b[32m0.15729\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 045 | loss: 0.15729 - R2: 1.0026 -- iter: 0256/1168\n",
      "Training Step: 841  | total loss: \u001b[1m\u001b[32m0.15729\u001b[0m\u001b[0m | time: 0.019s\n",
      "| SGD | epoch: 045 | loss: 0.15729 - R2: 1.0026 -- iter: 0320/1168\n",
      "Training Step: 842  | total loss: \u001b[1m\u001b[32m0.15845\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 045 | loss: 0.15845 - R2: 1.0037 -- iter: 0384/1168\n",
      "Training Step: 843  | total loss: \u001b[1m\u001b[32m0.15845\u001b[0m\u001b[0m | time: 0.026s\n",
      "| SGD | epoch: 045 | loss: 0.15845 - R2: 1.0037 -- iter: 0448/1168\n",
      "Training Step: 844  | total loss: \u001b[1m\u001b[32m0.16357\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 045 | loss: 0.16357 - R2: 1.0045 -- iter: 0512/1168\n",
      "Training Step: 845  | total loss: \u001b[1m\u001b[32m0.15748\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 045 | loss: 0.15748 - R2: 1.0027 -- iter: 0576/1168\n",
      "Training Step: 846  | total loss: \u001b[1m\u001b[32m0.15748\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 045 | loss: 0.15748 - R2: 1.0027 -- iter: 0640/1168\n",
      "Training Step: 847  | total loss: \u001b[1m\u001b[32m0.15277\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 045 | loss: 0.15277 - R2: 1.0022 -- iter: 0704/1168\n",
      "Training Step: 848  | total loss: \u001b[1m\u001b[32m0.14687\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 045 | loss: 0.14687 - R2: 1.0007 -- iter: 0768/1168\n",
      "Training Step: 849  | total loss: \u001b[1m\u001b[32m0.15060\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 045 | loss: 0.15060 - R2: 1.0019 -- iter: 0832/1168\n",
      "Training Step: 850  | total loss: \u001b[1m\u001b[32m0.15060\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 045 | loss: 0.15060 - R2: 1.0019 -- iter: 0896/1168\n",
      "Training Step: 851  | total loss: \u001b[1m\u001b[32m0.15150\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 045 | loss: 0.15150 - R2: 1.0018 -- iter: 0960/1168\n",
      "Training Step: 852  | total loss: \u001b[1m\u001b[32m0.15023\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 045 | loss: 0.15023 - R2: 1.0029 -- iter: 1024/1168\n",
      "Training Step: 853  | total loss: \u001b[1m\u001b[32m0.15386\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 045 | loss: 0.15386 - R2: 1.0015 -- iter: 1088/1168\n",
      "Training Step: 854  | total loss: \u001b[1m\u001b[32m0.16102\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 045 | loss: 0.16102 - R2: 1.0001 -- iter: 1152/1168\n",
      "Training Step: 855  | total loss: \u001b[1m\u001b[32m0.16304\u001b[0m\u001b[0m | time: 1.069s\n",
      "| SGD | epoch: 045 | loss: 0.16304 - R2: 0.9999 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 856  | total loss: \u001b[1m\u001b[32m0.16304\u001b[0m\u001b[0m | time: 0.013s\n",
      "| SGD | epoch: 046 | loss: 0.16304 - R2: 0.9999 -- iter: 0064/1168\n",
      "Training Step: 857  | total loss: \u001b[1m\u001b[32m0.16362\u001b[0m\u001b[0m | time: 0.016s\n",
      "| SGD | epoch: 046 | loss: 0.16362 - R2: 1.0002 -- iter: 0128/1168\n",
      "Training Step: 858  | total loss: \u001b[1m\u001b[32m0.16411\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 046 | loss: 0.16411 - R2: 1.0008 -- iter: 0192/1168\n",
      "Training Step: 859  | total loss: \u001b[1m\u001b[32m0.16290\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 046 | loss: 0.16290 - R2: 1.0018 -- iter: 0256/1168\n",
      "Training Step: 860  | total loss: \u001b[1m\u001b[32m0.16290\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 046 | loss: 0.16290 - R2: 1.0027 -- iter: 0320/1168\n",
      "Training Step: 861  | total loss: \u001b[1m\u001b[32m0.16182\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 046 | loss: 0.16182 - R2: 1.0027 -- iter: 0384/1168\n",
      "Training Step: 862  | total loss: \u001b[1m\u001b[32m0.16519\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 046 | loss: 0.16519 - R2: 1.0019 -- iter: 0448/1168\n",
      "Training Step: 863  | total loss: \u001b[1m\u001b[32m0.16150\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 046 | loss: 0.16150 - R2: 1.0023 -- iter: 0512/1168\n",
      "Training Step: 864  | total loss: \u001b[1m\u001b[32m0.15928\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 046 | loss: 0.15928 - R2: 1.0022 -- iter: 0576/1168\n",
      "Training Step: 865  | total loss: \u001b[1m\u001b[32m0.15940\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 046 | loss: 0.15940 - R2: 1.0020 -- iter: 0640/1168\n",
      "Training Step: 866  | total loss: \u001b[1m\u001b[32m0.15920\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 046 | loss: 0.15920 - R2: 1.0013 -- iter: 0704/1168\n",
      "Training Step: 867  | total loss: \u001b[1m\u001b[32m0.15504\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 046 | loss: 0.15504 - R2: 0.9993 -- iter: 0768/1168\n",
      "Training Step: 868  | total loss: \u001b[1m\u001b[32m0.15504\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 046 | loss: 0.15504 - R2: 0.9993 -- iter: 0832/1168\n",
      "Training Step: 869  | total loss: \u001b[1m\u001b[32m0.15578\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 046 | loss: 0.15578 - R2: 1.0016 -- iter: 0896/1168\n",
      "Training Step: 870  | total loss: \u001b[1m\u001b[32m0.15544\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 046 | loss: 0.15544 - R2: 1.0021 -- iter: 0960/1168\n",
      "Training Step: 871  | total loss: \u001b[1m\u001b[32m0.16022\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 046 | loss: 0.16022 - R2: 1.0020 -- iter: 1024/1168\n",
      "Training Step: 872  | total loss: \u001b[1m\u001b[32m0.16022\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 046 | loss: 0.16022 - R2: 1.0020 -- iter: 1088/1168\n",
      "Training Step: 873  | total loss: \u001b[1m\u001b[32m0.15753\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 046 | loss: 0.15753 - R2: 1.0013 -- iter: 1152/1168\n",
      "Training Step: 874  | total loss: \u001b[1m\u001b[32m0.15753\u001b[0m\u001b[0m | time: 1.080s\n",
      "| SGD | epoch: 046 | loss: 0.15753 - R2: 1.0027 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 875  | total loss: \u001b[1m\u001b[32m0.16452\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 047 | loss: 0.16452 - R2: 1.0019 -- iter: 0064/1168\n",
      "Training Step: 876  | total loss: \u001b[1m\u001b[32m0.16303\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 047 | loss: 0.16303 - R2: 1.0016 -- iter: 0128/1168\n",
      "Training Step: 877  | total loss: \u001b[1m\u001b[32m0.16303\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 047 | loss: 0.16303 - R2: 1.0016 -- iter: 0192/1168\n",
      "Training Step: 878  | total loss: \u001b[1m\u001b[32m0.16125\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 047 | loss: 0.16125 - R2: 1.0009 -- iter: 0256/1168\n",
      "Training Step: 879  | total loss: \u001b[1m\u001b[32m0.16375\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 047 | loss: 0.16375 - R2: 1.0000 -- iter: 0320/1168\n",
      "Training Step: 880  | total loss: \u001b[1m\u001b[32m0.16375\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 047 | loss: 0.16375 - R2: 1.0000 -- iter: 0384/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 881  | total loss: \u001b[1m\u001b[32m0.16662\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 047 | loss: 0.16662 - R2: 1.0003 -- iter: 0448/1168\n",
      "Training Step: 882  | total loss: \u001b[1m\u001b[32m0.16662\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 047 | loss: 0.16662 - R2: 0.9992 -- iter: 0512/1168\n",
      "Training Step: 883  | total loss: \u001b[1m\u001b[32m0.16595\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 047 | loss: 0.16595 - R2: 0.9992 -- iter: 0576/1168\n",
      "Training Step: 884  | total loss: \u001b[1m\u001b[32m0.16352\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 047 | loss: 0.16352 - R2: 0.9978 -- iter: 0640/1168\n",
      "Training Step: 885  | total loss: \u001b[1m\u001b[32m0.16340\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 047 | loss: 0.16340 - R2: 0.9978 -- iter: 0704/1168\n",
      "Training Step: 886  | total loss: \u001b[1m\u001b[32m0.16061\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 047 | loss: 0.16061 - R2: 0.9984 -- iter: 0768/1168\n",
      "Training Step: 887  | total loss: \u001b[1m\u001b[32m0.16271\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 047 | loss: 0.16271 - R2: 0.9987 -- iter: 0832/1168\n",
      "Training Step: 888  | total loss: \u001b[1m\u001b[32m0.15829\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 047 | loss: 0.15829 - R2: 0.9990 -- iter: 0896/1168\n",
      "Training Step: 889  | total loss: \u001b[1m\u001b[32m0.15942\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 047 | loss: 0.15942 - R2: 0.9988 -- iter: 0960/1168\n",
      "Training Step: 890  | total loss: \u001b[1m\u001b[32m0.16108\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 047 | loss: 0.16108 - R2: 1.0013 -- iter: 1024/1168\n",
      "Training Step: 891  | total loss: \u001b[1m\u001b[32m0.16104\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 047 | loss: 0.16104 - R2: 1.0014 -- iter: 1088/1168\n",
      "Training Step: 892  | total loss: \u001b[1m\u001b[32m0.16314\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 047 | loss: 0.16314 - R2: 0.9993 -- iter: 1152/1168\n",
      "Training Step: 893  | total loss: \u001b[1m\u001b[32m0.16770\u001b[0m\u001b[0m | time: 1.175s\n",
      "| SGD | epoch: 047 | loss: 0.16770 - R2: 0.9993 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 894  | total loss: \u001b[1m\u001b[32m0.16770\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 048 | loss: 0.16770 - R2: 0.9993 -- iter: 0064/1168\n",
      "Training Step: 895  | total loss: \u001b[1m\u001b[32m0.17050\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 048 | loss: 0.17050 - R2: 1.0012 -- iter: 0128/1168\n",
      "Training Step: 896  | total loss: \u001b[1m\u001b[32m0.16879\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 048 | loss: 0.16879 - R2: 1.0024 -- iter: 0192/1168\n",
      "Training Step: 897  | total loss: \u001b[1m\u001b[32m0.17076\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 048 | loss: 0.17076 - R2: 1.0016 -- iter: 0256/1168\n",
      "Training Step: 898  | total loss: \u001b[1m\u001b[32m0.17346\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 048 | loss: 0.17346 - R2: 1.0007 -- iter: 0320/1168\n",
      "Training Step: 899  | total loss: \u001b[1m\u001b[32m0.16942\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 048 | loss: 0.16942 - R2: 1.0008 -- iter: 0384/1168\n",
      "Training Step: 900  | total loss: \u001b[1m\u001b[32m0.16340\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 048 | loss: 0.16340 - R2: 0.9977 -- iter: 0448/1168\n",
      "Training Step: 901  | total loss: \u001b[1m\u001b[32m0.15798\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 048 | loss: 0.15798 - R2: 0.9949 -- iter: 0512/1168\n",
      "Training Step: 902  | total loss: \u001b[1m\u001b[32m0.15756\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 048 | loss: 0.15756 - R2: 0.9953 -- iter: 0576/1168\n",
      "Training Step: 903  | total loss: \u001b[1m\u001b[32m0.15676\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 048 | loss: 0.15676 - R2: 0.9936 -- iter: 0640/1168\n",
      "Training Step: 904  | total loss: \u001b[1m\u001b[32m0.15676\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 048 | loss: 0.15676 - R2: 0.9936 -- iter: 0704/1168\n",
      "Training Step: 905  | total loss: \u001b[1m\u001b[32m0.15729\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 048 | loss: 0.15729 - R2: 0.9955 -- iter: 0768/1168\n",
      "Training Step: 906  | total loss: \u001b[1m\u001b[32m0.15185\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 048 | loss: 0.15185 - R2: 0.9967 -- iter: 0832/1168\n",
      "Training Step: 907  | total loss: \u001b[1m\u001b[32m0.15390\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 048 | loss: 0.15390 - R2: 0.9964 -- iter: 0896/1168\n",
      "Training Step: 908  | total loss: \u001b[1m\u001b[32m0.15784\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 048 | loss: 0.15784 - R2: 0.9964 -- iter: 0960/1168\n",
      "Training Step: 909  | total loss: \u001b[1m\u001b[32m0.15784\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 048 | loss: 0.15784 - R2: 0.9971 -- iter: 1024/1168\n",
      "Training Step: 910  | total loss: \u001b[1m\u001b[32m0.15403\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 048 | loss: 0.15403 - R2: 0.9978 -- iter: 1088/1168\n",
      "Training Step: 911  | total loss: \u001b[1m\u001b[32m0.15363\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 048 | loss: 0.15363 - R2: 0.9987 -- iter: 1152/1168\n",
      "Training Step: 912  | total loss: \u001b[1m\u001b[32m0.15567\u001b[0m\u001b[0m | time: 1.146s\n",
      "| SGD | epoch: 048 | loss: 0.15567 - R2: 0.9998 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 913  | total loss: \u001b[1m\u001b[32m0.15567\u001b[0m\u001b[0m | time: 0.017s\n",
      "| SGD | epoch: 049 | loss: 0.15567 - R2: 0.9998 -- iter: 0064/1168\n",
      "Training Step: 914  | total loss: \u001b[1m\u001b[32m0.16203\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 049 | loss: 0.16203 - R2: 0.9986 -- iter: 0128/1168\n",
      "Training Step: 915  | total loss: \u001b[1m\u001b[32m0.16203\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 049 | loss: 0.16203 - R2: 0.9986 -- iter: 0192/1168\n",
      "Training Step: 916  | total loss: \u001b[1m\u001b[32m0.16251\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 049 | loss: 0.16251 - R2: 0.9991 -- iter: 0256/1168\n",
      "Training Step: 917  | total loss: \u001b[1m\u001b[32m0.16434\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 049 | loss: 0.16434 - R2: 1.0001 -- iter: 0320/1168\n",
      "Training Step: 918  | total loss: \u001b[1m\u001b[32m0.16434\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 049 | loss: 0.16434 - R2: 1.0001 -- iter: 0384/1168\n",
      "Training Step: 919  | total loss: \u001b[1m\u001b[32m0.16236\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 049 | loss: 0.16236 - R2: 1.0002 -- iter: 0448/1168\n",
      "Training Step: 920  | total loss: \u001b[1m\u001b[32m0.15844\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 049 | loss: 0.15844 - R2: 0.9998 -- iter: 0512/1168\n",
      "Training Step: 921  | total loss: \u001b[1m\u001b[32m0.15695\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 049 | loss: 0.15695 - R2: 0.9994 -- iter: 0576/1168\n",
      "Training Step: 922  | total loss: \u001b[1m\u001b[32m0.15695\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 049 | loss: 0.15695 - R2: 0.9994 -- iter: 0640/1168\n",
      "Training Step: 923  | total loss: \u001b[1m\u001b[32m0.15884\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 049 | loss: 0.15884 - R2: 0.9995 -- iter: 0704/1168\n",
      "Training Step: 924  | total loss: \u001b[1m\u001b[32m0.15884\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 049 | loss: 0.15884 - R2: 0.9995 -- iter: 0768/1168\n",
      "Training Step: 925  | total loss: \u001b[1m\u001b[32m0.15658\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 049 | loss: 0.15658 - R2: 0.9993 -- iter: 0832/1168\n",
      "Training Step: 926  | total loss: \u001b[1m\u001b[32m0.15242\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 049 | loss: 0.15242 - R2: 0.9986 -- iter: 0896/1168\n",
      "Training Step: 927  | total loss: \u001b[1m\u001b[32m0.15400\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 049 | loss: 0.15400 - R2: 0.9986 -- iter: 0960/1168\n",
      "Training Step: 928  | total loss: \u001b[1m\u001b[32m0.15552\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 049 | loss: 0.15552 - R2: 0.9987 -- iter: 1024/1168\n",
      "Training Step: 929  | total loss: \u001b[1m\u001b[32m0.15720\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 049 | loss: 0.15720 - R2: 0.9978 -- iter: 1088/1168\n",
      "Training Step: 930  | total loss: \u001b[1m\u001b[32m0.15720\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 049 | loss: 0.15720 - R2: 0.9978 -- iter: 1152/1168\n",
      "Training Step: 931  | total loss: \u001b[1m\u001b[32m0.15376\u001b[0m\u001b[0m | time: 1.112s\n",
      "| SGD | epoch: 049 | loss: 0.15376 - R2: 0.9990 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 932  | total loss: \u001b[1m\u001b[32m0.15376\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 050 | loss: 0.15376 - R2: 0.9990 -- iter: 0064/1168\n",
      "Training Step: 933  | total loss: \u001b[1m\u001b[32m0.15262\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 050 | loss: 0.15262 - R2: 0.9998 -- iter: 0128/1168\n",
      "Training Step: 934  | total loss: \u001b[1m\u001b[32m0.15588\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 050 | loss: 0.15588 - R2: 1.0008 -- iter: 0192/1168\n",
      "Training Step: 935  | total loss: \u001b[1m\u001b[32m0.15588\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 050 | loss: 0.15588 - R2: 1.0008 -- iter: 0256/1168\n",
      "Training Step: 936  | total loss: \u001b[1m\u001b[32m0.15553\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 050 | loss: 0.15553 - R2: 1.0006 -- iter: 0320/1168\n",
      "Training Step: 937  | total loss: \u001b[1m\u001b[32m0.15553\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 050 | loss: 0.15553 - R2: 1.0006 -- iter: 0384/1168\n",
      "Training Step: 938  | total loss: \u001b[1m\u001b[32m0.15854\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 050 | loss: 0.15854 - R2: 1.0004 -- iter: 0448/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 939  | total loss: \u001b[1m\u001b[32m0.15899\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 050 | loss: 0.15899 - R2: 0.9992 -- iter: 0512/1168\n",
      "Training Step: 940  | total loss: \u001b[1m\u001b[32m0.15261\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 050 | loss: 0.15261 - R2: 1.0005 -- iter: 0576/1168\n",
      "Training Step: 941  | total loss: \u001b[1m\u001b[32m0.14686\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 050 | loss: 0.14686 - R2: 1.0017 -- iter: 0640/1168\n",
      "Training Step: 942  | total loss: \u001b[1m\u001b[32m0.14873\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 050 | loss: 0.14873 - R2: 1.0008 -- iter: 0704/1168\n",
      "Training Step: 943  | total loss: \u001b[1m\u001b[32m0.15587\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 050 | loss: 0.15587 - R2: 1.0011 -- iter: 0768/1168\n",
      "Training Step: 944  | total loss: \u001b[1m\u001b[32m0.15587\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 050 | loss: 0.15587 - R2: 1.0011 -- iter: 0832/1168\n",
      "Training Step: 945  | total loss: \u001b[1m\u001b[32m0.15708\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 050 | loss: 0.15708 - R2: 1.0033 -- iter: 0896/1168\n",
      "Training Step: 946  | total loss: \u001b[1m\u001b[32m0.15334\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 050 | loss: 0.15334 - R2: 1.0020 -- iter: 0960/1168\n",
      "Training Step: 947  | total loss: \u001b[1m\u001b[32m0.15574\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 050 | loss: 0.15574 - R2: 1.0006 -- iter: 1024/1168\n",
      "Training Step: 948  | total loss: \u001b[1m\u001b[32m0.15701\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 050 | loss: 0.15701 - R2: 1.0004 -- iter: 1088/1168\n",
      "Training Step: 949  | total loss: \u001b[1m\u001b[32m0.15628\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 050 | loss: 0.15628 - R2: 0.9997 -- iter: 1152/1168\n",
      "Training Step: 950  | total loss: \u001b[1m\u001b[32m0.15824\u001b[0m\u001b[0m | time: 1.167s\n",
      "| SGD | epoch: 050 | loss: 0.15824 - R2: 1.0018 | val_loss: 0.15090 - val_acc: 1.0040 -- iter: 1168/1168\n",
      "--\n",
      "---------------------------------\n",
      "Run id: D6E48R\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "INFO:tensorflow:Summary name StandardError/ (raw) is illegal; using StandardError/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 1168\n",
      "Validation samples: 292\n",
      "--\n",
      "Training Step: 1  | total loss: \u001b[1m\u001b[32m132.59520\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 001 | loss: 132.59520 - R2: 0.0000 -- iter: 0064/1168\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m132.59520\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 001 | loss: 132.59520 - R2: 0.0000 -- iter: 0128/1168\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m131.77692\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 001 | loss: 131.77692 - R2: 0.0014 -- iter: 0192/1168\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m125.34692\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 001 | loss: 125.34692 - R2: 0.0053 -- iter: 0256/1168\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m107.19267\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 001 | loss: 107.19267 - R2: 0.0198 -- iter: 0320/1168\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m99.84528\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 001 | loss: 99.84528 - R2: 0.0297 -- iter: 0384/1168\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m94.11037\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 001 | loss: 94.11037 - R2: 0.0406 -- iter: 0448/1168\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m94.11037\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 001 | loss: 94.11037 - R2: 0.0406 -- iter: 0512/1168\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m86.40395\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 001 | loss: 86.40395 - R2: 0.0540 -- iter: 0576/1168\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m80.71526\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 001 | loss: 80.71526 - R2: 0.0678 -- iter: 0640/1168\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m74.36945\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 001 | loss: 74.36945 - R2: 0.0838 -- iter: 0704/1168\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m62.50574\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 001 | loss: 62.50574 - R2: 0.1210 -- iter: 0768/1168\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m56.97623\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 001 | loss: 56.97623 - R2: 0.1427 -- iter: 0832/1168\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m56.97623\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 001 | loss: 56.97623 - R2: 0.1427 -- iter: 0896/1168\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m46.41069\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 001 | loss: 46.41069 - R2: 0.1942 -- iter: 0960/1168\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m46.41069\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 001 | loss: 46.41069 - R2: 0.2249 -- iter: 1024/1168\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m41.29496\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 001 | loss: 41.29496 - R2: 0.2590 -- iter: 1088/1168\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m36.40916\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 001 | loss: 36.40916 - R2: 0.2590 -- iter: 1152/1168\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m26.77243\u001b[0m\u001b[0m | time: 1.127s\n",
      "| SGD | epoch: 001 | loss: 26.77243 - R2: 0.3421 | val_loss: 12.55253 - val_acc: 0.4989 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m22.41369\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 002 | loss: 22.41369 - R2: 0.3901 -- iter: 0064/1168\n",
      "Training Step: 21  | total loss: \u001b[1m\u001b[32m18.30548\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 002 | loss: 18.30548 - R2: 0.4441 -- iter: 0128/1168\n",
      "Training Step: 22  | total loss: \u001b[1m\u001b[32m14.74264\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 002 | loss: 14.74264 - R2: 0.5005 -- iter: 0192/1168\n",
      "Training Step: 23  | total loss: \u001b[1m\u001b[32m14.74264\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 002 | loss: 14.74264 - R2: 0.5005 -- iter: 0256/1168\n",
      "Training Step: 24  | total loss: \u001b[1m\u001b[32m11.46016\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 002 | loss: 11.46016 - R2: 0.5660 -- iter: 0320/1168\n",
      "Training Step: 25  | total loss: \u001b[1m\u001b[32m6.58471\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 002 | loss: 6.58471 - R2: 0.7043 -- iter: 0384/1168\n",
      "Training Step: 26  | total loss: \u001b[1m\u001b[32m4.93291\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 002 | loss: 4.93291 - R2: 0.7783 -- iter: 0448/1168\n",
      "Training Step: 27  | total loss: \u001b[1m\u001b[32m4.93291\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 002 | loss: 4.93291 - R2: 0.7783 -- iter: 0512/1168\n",
      "Training Step: 28  | total loss: \u001b[1m\u001b[32m3.73452\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 002 | loss: 3.73452 - R2: 0.8717 -- iter: 0576/1168\n",
      "Training Step: 29  | total loss: \u001b[1m\u001b[32m2.23417\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 002 | loss: 2.23417 - R2: 0.9038 -- iter: 0640/1168\n",
      "Training Step: 30  | total loss: \u001b[1m\u001b[32m2.23417\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 002 | loss: 2.23417 - R2: 0.9250 -- iter: 0704/1168\n",
      "Training Step: 31  | total loss: \u001b[1m\u001b[32m1.74911\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 002 | loss: 1.74911 - R2: 0.9250 -- iter: 0768/1168\n",
      "Training Step: 32  | total loss: \u001b[1m\u001b[32m1.12237\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 002 | loss: 1.12237 - R2: 0.9578 -- iter: 0832/1168\n",
      "Training Step: 33  | total loss: \u001b[1m\u001b[32m0.92357\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 002 | loss: 0.92357 - R2: 0.9681 -- iter: 0896/1168\n",
      "Training Step: 34  | total loss: \u001b[1m\u001b[32m0.76882\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 002 | loss: 0.76882 - R2: 0.9709 -- iter: 0960/1168\n",
      "Training Step: 35  | total loss: \u001b[1m\u001b[32m0.65026\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 002 | loss: 0.65026 - R2: 0.9797 -- iter: 1024/1168\n",
      "Training Step: 36  | total loss: \u001b[1m\u001b[32m0.54816\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 002 | loss: 0.54816 - R2: 0.9799 -- iter: 1088/1168\n",
      "Training Step: 37  | total loss: \u001b[1m\u001b[32m0.47303\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 002 | loss: 0.47303 - R2: 0.9852 -- iter: 1152/1168\n",
      "Training Step: 38  | total loss: \u001b[1m\u001b[32m0.41549\u001b[0m\u001b[0m | time: 1.111s\n",
      "| SGD | epoch: 002 | loss: 0.41549 - R2: 0.9867 | val_loss: 0.14172 - val_acc: 1.0006 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 39  | total loss: \u001b[1m\u001b[32m0.41549\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 003 | loss: 0.41549 - R2: 0.9867 -- iter: 0064/1168\n",
      "Training Step: 40  | total loss: \u001b[1m\u001b[32m0.34992\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 003 | loss: 0.34992 - R2: 0.9863 -- iter: 0128/1168\n",
      "Training Step: 41  | total loss: \u001b[1m\u001b[32m0.31091\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 003 | loss: 0.31091 - R2: 0.9889 -- iter: 0192/1168\n",
      "Training Step: 42  | total loss: \u001b[1m\u001b[32m0.28759\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 003 | loss: 0.28759 - R2: 0.9939 -- iter: 0256/1168\n",
      "Training Step: 43  | total loss: \u001b[1m\u001b[32m0.28759\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 003 | loss: 0.28759 - R2: 0.9939 -- iter: 0320/1168\n",
      "Training Step: 44  | total loss: \u001b[1m\u001b[32m0.24341\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 003 | loss: 0.24341 - R2: 0.9978 -- iter: 0384/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 45  | total loss: \u001b[1m\u001b[32m0.24341\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 003 | loss: 0.24341 - R2: 0.9965 -- iter: 0448/1168\n",
      "Training Step: 46  | total loss: \u001b[1m\u001b[32m0.23544\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 003 | loss: 0.23544 - R2: 0.9965 -- iter: 0512/1168\n",
      "Training Step: 47  | total loss: \u001b[1m\u001b[32m0.21038\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 003 | loss: 0.21038 - R2: 0.9981 -- iter: 0576/1168\n",
      "Training Step: 48  | total loss: \u001b[1m\u001b[32m0.21038\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 003 | loss: 0.21038 - R2: 0.9981 -- iter: 0640/1168\n",
      "Training Step: 49  | total loss: \u001b[1m\u001b[32m0.20412\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 003 | loss: 0.20412 - R2: 0.9977 -- iter: 0704/1168\n",
      "Training Step: 50  | total loss: \u001b[1m\u001b[32m0.20128\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 003 | loss: 0.20128 - R2: 0.9975 -- iter: 0768/1168\n",
      "Training Step: 51  | total loss: \u001b[1m\u001b[32m0.20128\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 003 | loss: 0.20128 - R2: 0.9975 -- iter: 0832/1168\n",
      "Training Step: 52  | total loss: \u001b[1m\u001b[32m0.19011\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 003 | loss: 0.19011 - R2: 0.9983 -- iter: 0896/1168\n",
      "Training Step: 53  | total loss: \u001b[1m\u001b[32m0.18965\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 003 | loss: 0.18965 - R2: 1.0002 -- iter: 0960/1168\n",
      "Training Step: 54  | total loss: \u001b[1m\u001b[32m0.18257\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 003 | loss: 0.18257 - R2: 0.9990 -- iter: 1024/1168\n",
      "Training Step: 55  | total loss: \u001b[1m\u001b[32m0.18257\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 003 | loss: 0.18257 - R2: 0.9990 -- iter: 1088/1168\n",
      "Training Step: 56  | total loss: \u001b[1m\u001b[32m0.18917\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 003 | loss: 0.18917 - R2: 0.9992 -- iter: 1152/1168\n",
      "Training Step: 57  | total loss: \u001b[1m\u001b[32m0.18917\u001b[0m\u001b[0m | time: 1.171s\n",
      "| SGD | epoch: 003 | loss: 0.18917 - R2: 0.9992 | val_loss: 0.14173 - val_acc: 1.0007 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 58  | total loss: \u001b[1m\u001b[32m0.18232\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 004 | loss: 0.18232 - R2: 0.9989 -- iter: 0064/1168\n",
      "Training Step: 59  | total loss: \u001b[1m\u001b[32m0.17806\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 004 | loss: 0.17806 - R2: 0.9986 -- iter: 0128/1168\n",
      "Training Step: 60  | total loss: \u001b[1m\u001b[32m0.17326\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 004 | loss: 0.17326 - R2: 0.9978 -- iter: 0192/1168\n",
      "Training Step: 61  | total loss: \u001b[1m\u001b[32m0.16908\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 004 | loss: 0.16908 - R2: 0.9973 -- iter: 0256/1168\n",
      "Training Step: 62  | total loss: \u001b[1m\u001b[32m0.16205\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 004 | loss: 0.16205 - R2: 0.9989 -- iter: 0320/1168\n",
      "Training Step: 63  | total loss: \u001b[1m\u001b[32m0.16657\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 004 | loss: 0.16657 - R2: 0.9986 -- iter: 0384/1168\n",
      "Training Step: 64  | total loss: \u001b[1m\u001b[32m0.16884\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 004 | loss: 0.16884 - R2: 0.9985 -- iter: 0448/1168\n",
      "Training Step: 65  | total loss: \u001b[1m\u001b[32m0.16815\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 004 | loss: 0.16815 - R2: 0.9980 -- iter: 0512/1168\n",
      "Training Step: 66  | total loss: \u001b[1m\u001b[32m0.16230\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 004 | loss: 0.16230 - R2: 0.9982 -- iter: 0576/1168\n",
      "Training Step: 67  | total loss: \u001b[1m\u001b[32m0.15659\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 004 | loss: 0.15659 - R2: 0.9982 -- iter: 0640/1168\n",
      "Training Step: 68  | total loss: \u001b[1m\u001b[32m0.15607\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 004 | loss: 0.15607 - R2: 0.9990 -- iter: 0704/1168\n",
      "Training Step: 69  | total loss: \u001b[1m\u001b[32m0.15240\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 004 | loss: 0.15240 - R2: 0.9991 -- iter: 0768/1168\n",
      "Training Step: 70  | total loss: \u001b[1m\u001b[32m0.15240\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 004 | loss: 0.15240 - R2: 0.9991 -- iter: 0832/1168\n",
      "Training Step: 71  | total loss: \u001b[1m\u001b[32m0.15587\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 004 | loss: 0.15587 - R2: 1.0001 -- iter: 0896/1168\n",
      "Training Step: 72  | total loss: \u001b[1m\u001b[32m0.16023\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 004 | loss: 0.16023 - R2: 1.0006 -- iter: 0960/1168\n",
      "Training Step: 73  | total loss: \u001b[1m\u001b[32m0.16065\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 004 | loss: 0.16065 - R2: 1.0010 -- iter: 1024/1168\n",
      "Training Step: 74  | total loss: \u001b[1m\u001b[32m0.15958\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 004 | loss: 0.15958 - R2: 1.0001 -- iter: 1088/1168\n",
      "Training Step: 75  | total loss: \u001b[1m\u001b[32m0.16058\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 004 | loss: 0.16058 - R2: 0.9990 -- iter: 1152/1168\n",
      "Training Step: 76  | total loss: \u001b[1m\u001b[32m0.16058\u001b[0m\u001b[0m | time: 1.162s\n",
      "| SGD | epoch: 004 | loss: 0.16058 - R2: 0.9990 | val_loss: 0.14196 - val_acc: 1.0021 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 77  | total loss: \u001b[1m\u001b[32m0.16233\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 005 | loss: 0.16233 - R2: 0.9983 -- iter: 0064/1168\n",
      "Training Step: 78  | total loss: \u001b[1m\u001b[32m0.15792\u001b[0m\u001b[0m | time: 0.007s\n",
      "| SGD | epoch: 005 | loss: 0.15792 - R2: 0.9994 -- iter: 0128/1168\n",
      "Training Step: 79  | total loss: \u001b[1m\u001b[32m0.15724\u001b[0m\u001b[0m | time: 0.009s\n",
      "| SGD | epoch: 005 | loss: 0.15724 - R2: 0.9991 -- iter: 0192/1168\n",
      "Training Step: 80  | total loss: \u001b[1m\u001b[32m0.16756\u001b[0m\u001b[0m | time: 0.013s\n",
      "| SGD | epoch: 005 | loss: 0.16756 - R2: 0.9922 -- iter: 0256/1168\n",
      "Training Step: 81  | total loss: \u001b[1m\u001b[32m0.16756\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 005 | loss: 0.16756 - R2: 0.9922 -- iter: 0320/1168\n",
      "Training Step: 82  | total loss: \u001b[1m\u001b[32m0.16600\u001b[0m\u001b[0m | time: 0.018s\n",
      "| SGD | epoch: 005 | loss: 0.16600 - R2: 0.9937 -- iter: 0384/1168\n",
      "Training Step: 83  | total loss: \u001b[1m\u001b[32m0.16329\u001b[0m\u001b[0m | time: 0.020s\n",
      "| SGD | epoch: 005 | loss: 0.16329 - R2: 0.9944 -- iter: 0448/1168\n",
      "Training Step: 84  | total loss: \u001b[1m\u001b[32m0.16866\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 005 | loss: 0.16866 - R2: 0.9946 -- iter: 0512/1168\n",
      "Training Step: 85  | total loss: \u001b[1m\u001b[32m0.16590\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 005 | loss: 0.16590 - R2: 0.9953 -- iter: 0576/1168\n",
      "Training Step: 86  | total loss: \u001b[1m\u001b[32m0.16900\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 005 | loss: 0.16900 - R2: 0.9966 -- iter: 0640/1168\n",
      "Training Step: 87  | total loss: \u001b[1m\u001b[32m0.17154\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 005 | loss: 0.17154 - R2: 0.9966 -- iter: 0704/1168\n",
      "Training Step: 88  | total loss: \u001b[1m\u001b[32m0.17584\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 005 | loss: 0.17584 - R2: 0.9965 -- iter: 0768/1168\n",
      "Training Step: 89  | total loss: \u001b[1m\u001b[32m0.17432\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 005 | loss: 0.17432 - R2: 0.9981 -- iter: 0832/1168\n",
      "Training Step: 90  | total loss: \u001b[1m\u001b[32m0.17257\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 005 | loss: 0.17257 - R2: 0.9992 -- iter: 0896/1168\n",
      "Training Step: 91  | total loss: \u001b[1m\u001b[32m0.17257\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 005 | loss: 0.17257 - R2: 0.9992 -- iter: 0960/1168\n",
      "Training Step: 92  | total loss: \u001b[1m\u001b[32m0.17770\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 005 | loss: 0.17770 - R2: 0.9992 -- iter: 1024/1168\n",
      "Training Step: 93  | total loss: \u001b[1m\u001b[32m0.17675\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 005 | loss: 0.17675 - R2: 0.9996 -- iter: 1088/1168\n",
      "Training Step: 94  | total loss: \u001b[1m\u001b[32m0.16664\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 005 | loss: 0.16664 - R2: 1.0005 -- iter: 1152/1168\n",
      "Training Step: 95  | total loss: \u001b[1m\u001b[32m0.16629\u001b[0m\u001b[0m | time: 1.058s\n",
      "| SGD | epoch: 005 | loss: 0.16629 - R2: 1.0002 | val_loss: 0.14269 - val_acc: 1.0045 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 96  | total loss: \u001b[1m\u001b[32m0.16629\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 006 | loss: 0.16629 - R2: 1.0002 -- iter: 0064/1168\n",
      "Training Step: 97  | total loss: \u001b[1m\u001b[32m0.16172\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 006 | loss: 0.16172 - R2: 0.9997 -- iter: 0128/1168\n",
      "Training Step: 98  | total loss: \u001b[1m\u001b[32m0.16013\u001b[0m\u001b[0m | time: 0.006s\n",
      "| SGD | epoch: 006 | loss: 0.16013 - R2: 0.9996 -- iter: 0192/1168\n",
      "Training Step: 99  | total loss: \u001b[1m\u001b[32m0.16363\u001b[0m\u001b[0m | time: 0.009s\n",
      "| SGD | epoch: 006 | loss: 0.16363 - R2: 1.0007 -- iter: 0256/1168\n",
      "Training Step: 100  | total loss: \u001b[1m\u001b[32m0.15059\u001b[0m\u001b[0m | time: 0.011s\n",
      "| SGD | epoch: 006 | loss: 0.15059 - R2: 0.9998 -- iter: 0320/1168\n",
      "Training Step: 101  | total loss: \u001b[1m\u001b[32m0.15059\u001b[0m\u001b[0m | time: 0.013s\n",
      "| SGD | epoch: 006 | loss: 0.15059 - R2: 0.9998 -- iter: 0384/1168\n",
      "Training Step: 102  | total loss: \u001b[1m\u001b[32m0.15271\u001b[0m\u001b[0m | time: 0.016s\n",
      "| SGD | epoch: 006 | loss: 0.15271 - R2: 0.9983 -- iter: 0448/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 103  | total loss: \u001b[1m\u001b[32m0.15408\u001b[0m\u001b[0m | time: 0.018s\n",
      "| SGD | epoch: 006 | loss: 0.15408 - R2: 0.9999 -- iter: 0512/1168\n",
      "Training Step: 104  | total loss: \u001b[1m\u001b[32m0.15633\u001b[0m\u001b[0m | time: 0.019s\n",
      "| SGD | epoch: 006 | loss: 0.15633 - R2: 0.9993 -- iter: 0576/1168\n",
      "Training Step: 105  | total loss: \u001b[1m\u001b[32m0.15620\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 006 | loss: 0.15620 - R2: 0.9989 -- iter: 0640/1168\n",
      "Training Step: 106  | total loss: \u001b[1m\u001b[32m0.15742\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 006 | loss: 0.15742 - R2: 0.9995 -- iter: 0704/1168\n",
      "Training Step: 107  | total loss: \u001b[1m\u001b[32m0.15840\u001b[0m\u001b[0m | time: 0.026s\n",
      "| SGD | epoch: 006 | loss: 0.15840 - R2: 1.0008 -- iter: 0768/1168\n",
      "Training Step: 108  | total loss: \u001b[1m\u001b[32m0.15769\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 006 | loss: 0.15769 - R2: 0.9991 -- iter: 0832/1168\n",
      "Training Step: 109  | total loss: \u001b[1m\u001b[32m0.15726\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 006 | loss: 0.15726 - R2: 0.9991 -- iter: 0896/1168\n",
      "Training Step: 110  | total loss: \u001b[1m\u001b[32m0.15128\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 006 | loss: 0.15128 - R2: 1.0000 -- iter: 0960/1168\n",
      "Training Step: 111  | total loss: \u001b[1m\u001b[32m0.15193\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 006 | loss: 0.15193 - R2: 1.0011 -- iter: 1024/1168\n",
      "Training Step: 112  | total loss: \u001b[1m\u001b[32m0.14814\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 006 | loss: 0.14814 - R2: 1.0015 -- iter: 1088/1168\n",
      "Training Step: 113  | total loss: \u001b[1m\u001b[32m0.14814\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 006 | loss: 0.14814 - R2: 1.0015 -- iter: 1152/1168\n",
      "Training Step: 114  | total loss: \u001b[1m\u001b[32m0.14801\u001b[0m\u001b[0m | time: 1.044s\n",
      "| SGD | epoch: 006 | loss: 0.14801 - R2: 1.0023 | val_loss: 0.14246 - val_acc: 1.0038 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 115  | total loss: \u001b[1m\u001b[32m0.15311\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 007 | loss: 0.15311 - R2: 1.0017 -- iter: 0064/1168\n",
      "Training Step: 116  | total loss: \u001b[1m\u001b[32m0.15357\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 007 | loss: 0.15357 - R2: 1.0029 -- iter: 0128/1168\n",
      "Training Step: 117  | total loss: \u001b[1m\u001b[32m0.15199\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 007 | loss: 0.15199 - R2: 1.0029 -- iter: 0192/1168\n",
      "Training Step: 118  | total loss: \u001b[1m\u001b[32m0.15148\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 007 | loss: 0.15148 - R2: 1.0039 -- iter: 0256/1168\n",
      "Training Step: 119  | total loss: \u001b[1m\u001b[32m0.15553\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 007 | loss: 0.15553 - R2: 1.0030 -- iter: 0320/1168\n",
      "Training Step: 120  | total loss: \u001b[1m\u001b[32m0.15931\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 007 | loss: 0.15931 - R2: 1.0023 -- iter: 0384/1168\n",
      "Training Step: 121  | total loss: \u001b[1m\u001b[32m0.16270\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 007 | loss: 0.16270 - R2: 1.0017 -- iter: 0448/1168\n",
      "Training Step: 122  | total loss: \u001b[1m\u001b[32m0.16758\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 007 | loss: 0.16758 - R2: 1.0021 -- iter: 0512/1168\n",
      "Training Step: 123  | total loss: \u001b[1m\u001b[32m0.16982\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 007 | loss: 0.16982 - R2: 1.0026 -- iter: 0576/1168\n",
      "Training Step: 124  | total loss: \u001b[1m\u001b[32m0.16773\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 007 | loss: 0.16773 - R2: 1.0024 -- iter: 0640/1168\n",
      "Training Step: 125  | total loss: \u001b[1m\u001b[32m0.17415\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 007 | loss: 0.17415 - R2: 1.0007 -- iter: 0704/1168\n",
      "Training Step: 126  | total loss: \u001b[1m\u001b[32m0.17501\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 007 | loss: 0.17501 - R2: 0.9995 -- iter: 0768/1168\n",
      "Training Step: 127  | total loss: \u001b[1m\u001b[32m0.17485\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 007 | loss: 0.17485 - R2: 0.9997 -- iter: 0832/1168\n",
      "Training Step: 128  | total loss: \u001b[1m\u001b[32m0.17423\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 007 | loss: 0.17423 - R2: 0.9997 -- iter: 0896/1168\n",
      "Training Step: 129  | total loss: \u001b[1m\u001b[32m0.16705\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 007 | loss: 0.16705 - R2: 0.9994 -- iter: 0960/1168\n",
      "Training Step: 130  | total loss: \u001b[1m\u001b[32m0.16584\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 007 | loss: 0.16584 - R2: 0.9988 -- iter: 1024/1168\n",
      "Training Step: 131  | total loss: \u001b[1m\u001b[32m0.16380\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 007 | loss: 0.16380 - R2: 1.0000 -- iter: 1088/1168\n",
      "Training Step: 132  | total loss: \u001b[1m\u001b[32m0.16522\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 007 | loss: 0.16522 - R2: 1.0007 -- iter: 1152/1168\n",
      "Training Step: 133  | total loss: \u001b[1m\u001b[32m0.16284\u001b[0m\u001b[0m | time: 1.093s\n",
      "| SGD | epoch: 007 | loss: 0.16284 - R2: 1.0018 | val_loss: 0.14231 - val_acc: 1.0034 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 134  | total loss: \u001b[1m\u001b[32m0.16621\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 008 | loss: 0.16621 - R2: 1.0013 -- iter: 0064/1168\n",
      "Training Step: 135  | total loss: \u001b[1m\u001b[32m0.16919\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 008 | loss: 0.16919 - R2: 0.9999 -- iter: 0128/1168\n",
      "Training Step: 136  | total loss: \u001b[1m\u001b[32m0.16655\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 008 | loss: 0.16655 - R2: 1.0002 -- iter: 0192/1168\n",
      "Training Step: 137  | total loss: \u001b[1m\u001b[32m0.16716\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 008 | loss: 0.16716 - R2: 0.9996 -- iter: 0256/1168\n",
      "Training Step: 138  | total loss: \u001b[1m\u001b[32m0.16666\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 008 | loss: 0.16666 - R2: 0.9989 -- iter: 0320/1168\n",
      "Training Step: 139  | total loss: \u001b[1m\u001b[32m0.16666\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 008 | loss: 0.16666 - R2: 0.9990 -- iter: 0384/1168\n",
      "Training Step: 140  | total loss: \u001b[1m\u001b[32m0.16556\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 008 | loss: 0.16556 - R2: 0.9959 -- iter: 0448/1168\n",
      "Training Step: 141  | total loss: \u001b[1m\u001b[32m0.16453\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 008 | loss: 0.16453 - R2: 0.9931 -- iter: 0512/1168\n",
      "Training Step: 142  | total loss: \u001b[1m\u001b[32m0.16638\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 008 | loss: 0.16638 - R2: 0.9942 -- iter: 0576/1168\n",
      "Training Step: 143  | total loss: \u001b[1m\u001b[32m0.16733\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 008 | loss: 0.16733 - R2: 0.9955 -- iter: 0640/1168\n",
      "Training Step: 144  | total loss: \u001b[1m\u001b[32m0.16865\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 008 | loss: 0.16865 - R2: 0.9970 -- iter: 0704/1168\n",
      "Training Step: 145  | total loss: \u001b[1m\u001b[32m0.16391\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 008 | loss: 0.16391 - R2: 0.9975 -- iter: 0768/1168\n",
      "Training Step: 146  | total loss: \u001b[1m\u001b[32m0.17185\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 008 | loss: 0.17185 - R2: 0.9975 -- iter: 0832/1168\n",
      "Training Step: 147  | total loss: \u001b[1m\u001b[32m0.16710\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 008 | loss: 0.16710 - R2: 0.9974 -- iter: 0896/1168\n",
      "Training Step: 148  | total loss: \u001b[1m\u001b[32m0.16103\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 008 | loss: 0.16103 - R2: 0.9981 -- iter: 0960/1168\n",
      "Training Step: 149  | total loss: \u001b[1m\u001b[32m0.15731\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 008 | loss: 0.15731 - R2: 0.9980 -- iter: 1024/1168\n",
      "Training Step: 150  | total loss: \u001b[1m\u001b[32m0.15787\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 008 | loss: 0.15787 - R2: 0.9989 -- iter: 1088/1168\n",
      "Training Step: 151  | total loss: \u001b[1m\u001b[32m0.15802\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 008 | loss: 0.15802 - R2: 0.9998 -- iter: 1152/1168\n",
      "Training Step: 152  | total loss: \u001b[1m\u001b[32m0.15263\u001b[0m\u001b[0m | time: 1.152s\n",
      "| SGD | epoch: 008 | loss: 0.15263 - R2: 1.0001 | val_loss: 0.14239 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 153  | total loss: \u001b[1m\u001b[32m0.15708\u001b[0m\u001b[0m | time: 0.013s\n",
      "| SGD | epoch: 009 | loss: 0.15708 - R2: 0.9993 -- iter: 0064/1168\n",
      "Training Step: 154  | total loss: \u001b[1m\u001b[32m0.15835\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 009 | loss: 0.15835 - R2: 0.9993 -- iter: 0128/1168\n",
      "Training Step: 155  | total loss: \u001b[1m\u001b[32m0.15384\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 009 | loss: 0.15384 - R2: 0.9999 -- iter: 0192/1168\n",
      "Training Step: 156  | total loss: \u001b[1m\u001b[32m0.15060\u001b[0m\u001b[0m | time: 0.026s\n",
      "| SGD | epoch: 009 | loss: 0.15060 - R2: 0.9989 -- iter: 0256/1168\n",
      "Training Step: 157  | total loss: \u001b[1m\u001b[32m0.15208\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 009 | loss: 0.15208 - R2: 0.9997 -- iter: 0320/1168\n",
      "Training Step: 158  | total loss: \u001b[1m\u001b[32m0.15141\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 009 | loss: 0.15141 - R2: 1.0008 -- iter: 0384/1168\n",
      "Training Step: 159  | total loss: \u001b[1m\u001b[32m0.15453\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 009 | loss: 0.15453 - R2: 1.0002 -- iter: 0448/1168\n",
      "Training Step: 160  | total loss: \u001b[1m\u001b[32m0.16506\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 009 | loss: 0.16506 - R2: 0.9992 -- iter: 0512/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 161  | total loss: \u001b[1m\u001b[32m0.17268\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 009 | loss: 0.17268 - R2: 0.9992 -- iter: 0576/1168\n",
      "Training Step: 162  | total loss: \u001b[1m\u001b[32m0.17779\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 009 | loss: 0.17779 - R2: 0.9990 -- iter: 0640/1168\n",
      "Training Step: 163  | total loss: \u001b[1m\u001b[32m0.17647\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 009 | loss: 0.17647 - R2: 1.0000 -- iter: 0704/1168\n",
      "Training Step: 164  | total loss: \u001b[1m\u001b[32m0.17881\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 009 | loss: 0.17881 - R2: 0.9994 -- iter: 0768/1168\n",
      "Training Step: 165  | total loss: \u001b[1m\u001b[32m0.18637\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 009 | loss: 0.18637 - R2: 0.9990 -- iter: 0832/1168\n",
      "Training Step: 166  | total loss: \u001b[1m\u001b[32m0.18637\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 009 | loss: 0.18637 - R2: 0.9990 -- iter: 0896/1168\n",
      "Training Step: 167  | total loss: \u001b[1m\u001b[32m0.18544\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 009 | loss: 0.18544 - R2: 0.9983 -- iter: 0960/1168\n",
      "Training Step: 168  | total loss: \u001b[1m\u001b[32m0.17753\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 009 | loss: 0.17753 - R2: 0.9981 -- iter: 1024/1168\n",
      "Training Step: 169  | total loss: \u001b[1m\u001b[32m0.16890\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 009 | loss: 0.16890 - R2: 0.9995 -- iter: 1088/1168\n",
      "Training Step: 170  | total loss: \u001b[1m\u001b[32m0.16890\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 009 | loss: 0.16890 - R2: 0.9995 -- iter: 1152/1168\n",
      "Training Step: 171  | total loss: \u001b[1m\u001b[32m0.17317\u001b[0m\u001b[0m | time: 1.085s\n",
      "| SGD | epoch: 009 | loss: 0.17317 - R2: 1.0003 | val_loss: 0.14238 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 172  | total loss: \u001b[1m\u001b[32m0.17716\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 010 | loss: 0.17716 - R2: 0.9992 -- iter: 0064/1168\n",
      "Training Step: 173  | total loss: \u001b[1m\u001b[32m0.17342\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 010 | loss: 0.17342 - R2: 0.9993 -- iter: 0128/1168\n",
      "Training Step: 174  | total loss: \u001b[1m\u001b[32m0.16820\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 010 | loss: 0.16820 - R2: 1.0000 -- iter: 0192/1168\n",
      "Training Step: 175  | total loss: \u001b[1m\u001b[32m0.16839\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 010 | loss: 0.16839 - R2: 0.9992 -- iter: 0256/1168\n",
      "Training Step: 176  | total loss: \u001b[1m\u001b[32m0.16839\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 010 | loss: 0.16839 - R2: 0.9994 -- iter: 0320/1168\n",
      "Training Step: 177  | total loss: \u001b[1m\u001b[32m0.16652\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 010 | loss: 0.16652 - R2: 1.0008 -- iter: 0384/1168\n",
      "Training Step: 178  | total loss: \u001b[1m\u001b[32m0.16652\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 010 | loss: 0.16652 - R2: 1.0003 -- iter: 0448/1168\n",
      "Training Step: 179  | total loss: \u001b[1m\u001b[32m0.16679\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 010 | loss: 0.16679 - R2: 1.0003 -- iter: 0512/1168\n",
      "Training Step: 180  | total loss: \u001b[1m\u001b[32m0.16980\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 010 | loss: 0.16980 - R2: 0.9995 -- iter: 0576/1168\n",
      "Training Step: 181  | total loss: \u001b[1m\u001b[32m0.17251\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 010 | loss: 0.17251 - R2: 0.9995 -- iter: 0640/1168\n",
      "Training Step: 182  | total loss: \u001b[1m\u001b[32m0.16961\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 010 | loss: 0.16961 - R2: 0.9997 -- iter: 0704/1168\n",
      "Training Step: 183  | total loss: \u001b[1m\u001b[32m0.16488\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 010 | loss: 0.16488 - R2: 1.0001 -- iter: 0768/1168\n",
      "Training Step: 184  | total loss: \u001b[1m\u001b[32m0.16484\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 010 | loss: 0.16484 - R2: 1.0006 -- iter: 0832/1168\n",
      "Training Step: 185  | total loss: \u001b[1m\u001b[32m0.16165\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 010 | loss: 0.16165 - R2: 1.0007 -- iter: 0896/1168\n",
      "Training Step: 186  | total loss: \u001b[1m\u001b[32m0.17078\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 010 | loss: 0.17078 - R2: 1.0005 -- iter: 0960/1168\n",
      "Training Step: 187  | total loss: \u001b[1m\u001b[32m0.16464\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 010 | loss: 0.16464 - R2: 0.9996 -- iter: 1024/1168\n",
      "Training Step: 188  | total loss: \u001b[1m\u001b[32m0.16661\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 010 | loss: 0.16661 - R2: 0.9991 -- iter: 1088/1168\n",
      "Training Step: 189  | total loss: \u001b[1m\u001b[32m0.17831\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 010 | loss: 0.17831 - R2: 0.9991 -- iter: 1152/1168\n",
      "Training Step: 190  | total loss: \u001b[1m\u001b[32m0.17062\u001b[0m\u001b[0m | time: 1.146s\n",
      "| SGD | epoch: 010 | loss: 0.17062 - R2: 0.9981 | val_loss: 0.14238 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 191  | total loss: \u001b[1m\u001b[32m0.17062\u001b[0m\u001b[0m | time: 0.016s\n",
      "| SGD | epoch: 011 | loss: 0.17062 - R2: 0.9981 -- iter: 0064/1168\n",
      "Training Step: 192  | total loss: \u001b[1m\u001b[32m0.16782\u001b[0m\u001b[0m | time: 0.018s\n",
      "| SGD | epoch: 011 | loss: 0.16782 - R2: 0.9983 -- iter: 0128/1168\n",
      "Training Step: 193  | total loss: \u001b[1m\u001b[32m0.16258\u001b[0m\u001b[0m | time: 0.020s\n",
      "| SGD | epoch: 011 | loss: 0.16258 - R2: 0.9992 -- iter: 0192/1168\n",
      "Training Step: 194  | total loss: \u001b[1m\u001b[32m0.16070\u001b[0m\u001b[0m | time: 0.023s\n",
      "| SGD | epoch: 011 | loss: 0.16070 - R2: 1.0018 -- iter: 0256/1168\n",
      "Training Step: 195  | total loss: \u001b[1m\u001b[32m0.16070\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 011 | loss: 0.16070 - R2: 1.0018 -- iter: 0320/1168\n",
      "Training Step: 196  | total loss: \u001b[1m\u001b[32m0.16201\u001b[0m\u001b[0m | time: 0.026s\n",
      "| SGD | epoch: 011 | loss: 0.16201 - R2: 1.0005 -- iter: 0384/1168\n",
      "Training Step: 197  | total loss: \u001b[1m\u001b[32m0.16643\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 011 | loss: 0.16643 - R2: 1.0005 -- iter: 0448/1168\n",
      "Training Step: 198  | total loss: \u001b[1m\u001b[32m0.16128\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 011 | loss: 0.16128 - R2: 1.0020 -- iter: 0512/1168\n",
      "Training Step: 199  | total loss: \u001b[1m\u001b[32m0.16572\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 011 | loss: 0.16572 - R2: 1.0014 -- iter: 0576/1168\n",
      "Training Step: 200  | total loss: \u001b[1m\u001b[32m0.14713\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 011 | loss: 0.14713 - R2: 1.0004 -- iter: 0640/1168\n",
      "Training Step: 201  | total loss: \u001b[1m\u001b[32m0.14713\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 011 | loss: 0.14713 - R2: 1.0004 -- iter: 0704/1168\n",
      "Training Step: 202  | total loss: \u001b[1m\u001b[32m0.14712\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 011 | loss: 0.14712 - R2: 0.9998 -- iter: 0768/1168\n",
      "Training Step: 203  | total loss: \u001b[1m\u001b[32m0.15151\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 011 | loss: 0.15151 - R2: 1.0005 -- iter: 0832/1168\n",
      "Training Step: 204  | total loss: \u001b[1m\u001b[32m0.15155\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 011 | loss: 0.15155 - R2: 1.0014 -- iter: 0896/1168\n",
      "Training Step: 205  | total loss: \u001b[1m\u001b[32m0.14814\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 011 | loss: 0.14814 - R2: 1.0014 -- iter: 0960/1168\n",
      "Training Step: 206  | total loss: \u001b[1m\u001b[32m0.15031\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 011 | loss: 0.15031 - R2: 1.0009 -- iter: 1024/1168\n",
      "Training Step: 207  | total loss: \u001b[1m\u001b[32m0.15526\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 011 | loss: 0.15526 - R2: 1.0000 -- iter: 1088/1168\n",
      "Training Step: 208  | total loss: \u001b[1m\u001b[32m0.16195\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 011 | loss: 0.16195 - R2: 1.0000 -- iter: 1152/1168\n",
      "Training Step: 209  | total loss: \u001b[1m\u001b[32m0.15640\u001b[0m\u001b[0m | time: 1.057s\n",
      "| SGD | epoch: 011 | loss: 0.15640 - R2: 0.9990 | val_loss: 0.14238 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 210  | total loss: \u001b[1m\u001b[32m0.15666\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 012 | loss: 0.15666 - R2: 0.9990 -- iter: 0064/1168\n",
      "Training Step: 211  | total loss: \u001b[1m\u001b[32m0.15297\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 012 | loss: 0.15297 - R2: 1.0006 -- iter: 0128/1168\n",
      "Training Step: 212  | total loss: \u001b[1m\u001b[32m0.15147\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 012 | loss: 0.15147 - R2: 1.0023 -- iter: 0192/1168\n",
      "Training Step: 213  | total loss: \u001b[1m\u001b[32m0.14949\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 012 | loss: 0.14949 - R2: 1.0036 -- iter: 0256/1168\n",
      "Training Step: 214  | total loss: \u001b[1m\u001b[32m0.15191\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 012 | loss: 0.15191 - R2: 1.0019 -- iter: 0320/1168\n",
      "Training Step: 215  | total loss: \u001b[1m\u001b[32m0.15712\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 012 | loss: 0.15712 - R2: 0.9992 -- iter: 0384/1168\n",
      "Training Step: 216  | total loss: \u001b[1m\u001b[32m0.15705\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 012 | loss: 0.15705 - R2: 1.0001 -- iter: 0448/1168\n",
      "Training Step: 217  | total loss: \u001b[1m\u001b[32m0.15705\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 012 | loss: 0.15705 - R2: 0.9986 -- iter: 0512/1168\n",
      "Training Step: 218  | total loss: \u001b[1m\u001b[32m0.16209\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 012 | loss: 0.16209 - R2: 0.9989 -- iter: 0576/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 219  | total loss: \u001b[1m\u001b[32m0.16209\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 012 | loss: 0.16209 - R2: 0.9989 -- iter: 0640/1168\n",
      "Training Step: 220  | total loss: \u001b[1m\u001b[32m0.16660\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 012 | loss: 0.16660 - R2: 0.9990 -- iter: 0704/1168\n",
      "Training Step: 221  | total loss: \u001b[1m\u001b[32m0.17066\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 012 | loss: 0.17066 - R2: 0.9990 -- iter: 0768/1168\n",
      "Training Step: 222  | total loss: \u001b[1m\u001b[32m0.16604\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 012 | loss: 0.16604 - R2: 0.9997 -- iter: 0832/1168\n",
      "Training Step: 223  | total loss: \u001b[1m\u001b[32m0.16604\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 012 | loss: 0.16604 - R2: 0.9997 -- iter: 0896/1168\n",
      "Training Step: 224  | total loss: \u001b[1m\u001b[32m0.16722\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 012 | loss: 0.16722 - R2: 0.9999 -- iter: 0960/1168\n",
      "Training Step: 225  | total loss: \u001b[1m\u001b[32m0.17348\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 012 | loss: 0.17348 - R2: 1.0007 -- iter: 1024/1168\n",
      "Training Step: 226  | total loss: \u001b[1m\u001b[32m0.17035\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 012 | loss: 0.17035 - R2: 1.0004 -- iter: 1088/1168\n",
      "Training Step: 227  | total loss: \u001b[1m\u001b[32m0.17035\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 012 | loss: 0.17035 - R2: 1.0004 -- iter: 1152/1168\n",
      "Training Step: 228  | total loss: \u001b[1m\u001b[32m0.16979\u001b[0m\u001b[0m | time: 1.123s\n",
      "| SGD | epoch: 012 | loss: 0.16979 - R2: 1.0004 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 229  | total loss: \u001b[1m\u001b[32m0.16786\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 013 | loss: 0.16786 - R2: 1.0026 -- iter: 0064/1168\n",
      "Training Step: 230  | total loss: \u001b[1m\u001b[32m0.16623\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 013 | loss: 0.16623 - R2: 1.0031 -- iter: 0128/1168\n",
      "Training Step: 231  | total loss: \u001b[1m\u001b[32m0.16475\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 013 | loss: 0.16475 - R2: 1.0023 -- iter: 0192/1168\n",
      "Training Step: 232  | total loss: \u001b[1m\u001b[32m0.16599\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 013 | loss: 0.16599 - R2: 1.0014 -- iter: 0256/1168\n",
      "Training Step: 233  | total loss: \u001b[1m\u001b[32m0.16319\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 013 | loss: 0.16319 - R2: 1.0008 -- iter: 0320/1168\n",
      "Training Step: 234  | total loss: \u001b[1m\u001b[32m0.16496\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 013 | loss: 0.16496 - R2: 1.0009 -- iter: 0384/1168\n",
      "Training Step: 235  | total loss: \u001b[1m\u001b[32m0.16496\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 013 | loss: 0.16496 - R2: 1.0009 -- iter: 0448/1168\n",
      "Training Step: 236  | total loss: \u001b[1m\u001b[32m0.15830\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 013 | loss: 0.15830 - R2: 1.0024 -- iter: 0512/1168\n",
      "Training Step: 237  | total loss: \u001b[1m\u001b[32m0.15869\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 013 | loss: 0.15869 - R2: 1.0003 -- iter: 0576/1168\n",
      "Training Step: 238  | total loss: \u001b[1m\u001b[32m0.15869\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 013 | loss: 0.15869 - R2: 1.0003 -- iter: 0640/1168\n",
      "Training Step: 239  | total loss: \u001b[1m\u001b[32m0.17351\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 013 | loss: 0.17351 - R2: 0.9990 -- iter: 0704/1168\n",
      "Training Step: 240  | total loss: \u001b[1m\u001b[32m0.18757\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 013 | loss: 0.18757 - R2: 0.9974 -- iter: 0768/1168\n",
      "Training Step: 241  | total loss: \u001b[1m\u001b[32m0.18757\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 013 | loss: 0.18757 - R2: 0.9970 -- iter: 0832/1168\n",
      "Training Step: 242  | total loss: \u001b[1m\u001b[32m0.18479\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 013 | loss: 0.18479 - R2: 0.9970 -- iter: 0896/1168\n",
      "Training Step: 243  | total loss: \u001b[1m\u001b[32m0.18937\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 013 | loss: 0.18937 - R2: 0.9989 -- iter: 0960/1168\n",
      "Training Step: 244  | total loss: \u001b[1m\u001b[32m0.18638\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 013 | loss: 0.18638 - R2: 0.9996 -- iter: 1024/1168\n",
      "Training Step: 245  | total loss: \u001b[1m\u001b[32m0.18285\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 013 | loss: 0.18285 - R2: 0.9988 -- iter: 1088/1168\n",
      "Training Step: 246  | total loss: \u001b[1m\u001b[32m0.18138\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 013 | loss: 0.18138 - R2: 0.9980 -- iter: 1152/1168\n",
      "Training Step: 247  | total loss: \u001b[1m\u001b[32m0.18145\u001b[0m\u001b[0m | time: 1.154s\n",
      "| SGD | epoch: 013 | loss: 0.18145 - R2: 0.9977 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 248  | total loss: \u001b[1m\u001b[32m0.18315\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 014 | loss: 0.18315 - R2: 0.9977 -- iter: 0064/1168\n",
      "Training Step: 249  | total loss: \u001b[1m\u001b[32m0.18471\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 014 | loss: 0.18471 - R2: 0.9982 -- iter: 0128/1168\n",
      "Training Step: 250  | total loss: \u001b[1m\u001b[32m0.18572\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 014 | loss: 0.18572 - R2: 0.9980 -- iter: 0192/1168\n",
      "Training Step: 251  | total loss: \u001b[1m\u001b[32m0.18572\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 014 | loss: 0.18572 - R2: 0.9980 -- iter: 0256/1168\n",
      "Training Step: 252  | total loss: \u001b[1m\u001b[32m0.17755\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 014 | loss: 0.17755 - R2: 0.9972 -- iter: 0320/1168\n",
      "Training Step: 253  | total loss: \u001b[1m\u001b[32m0.17897\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 014 | loss: 0.17897 - R2: 0.9982 -- iter: 0384/1168\n",
      "Training Step: 254  | total loss: \u001b[1m\u001b[32m0.17438\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 014 | loss: 0.17438 - R2: 0.9980 -- iter: 0448/1168\n",
      "Training Step: 255  | total loss: \u001b[1m\u001b[32m0.17176\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 014 | loss: 0.17176 - R2: 0.9986 -- iter: 0512/1168\n",
      "Training Step: 256  | total loss: \u001b[1m\u001b[32m0.17176\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 014 | loss: 0.17176 - R2: 0.9986 -- iter: 0576/1168\n",
      "Training Step: 257  | total loss: \u001b[1m\u001b[32m0.16992\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 014 | loss: 0.16992 - R2: 0.9991 -- iter: 0640/1168\n",
      "Training Step: 258  | total loss: \u001b[1m\u001b[32m0.16992\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 014 | loss: 0.16992 - R2: 0.9991 -- iter: 0704/1168\n",
      "Training Step: 259  | total loss: \u001b[1m\u001b[32m0.16678\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 014 | loss: 0.16678 - R2: 0.9996 -- iter: 0768/1168\n",
      "Training Step: 260  | total loss: \u001b[1m\u001b[32m0.16592\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 014 | loss: 0.16592 - R2: 0.9985 -- iter: 0832/1168\n",
      "Training Step: 261  | total loss: \u001b[1m\u001b[32m0.16515\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 014 | loss: 0.16515 - R2: 0.9975 -- iter: 0896/1168\n",
      "Training Step: 262  | total loss: \u001b[1m\u001b[32m0.16288\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 014 | loss: 0.16288 - R2: 0.9971 -- iter: 0960/1168\n",
      "Training Step: 263  | total loss: \u001b[1m\u001b[32m0.16372\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 014 | loss: 0.16372 - R2: 0.9986 -- iter: 1024/1168\n",
      "Training Step: 264  | total loss: \u001b[1m\u001b[32m0.16648\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 014 | loss: 0.16648 - R2: 0.9994 -- iter: 1088/1168\n",
      "Training Step: 265  | total loss: \u001b[1m\u001b[32m0.16214\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 014 | loss: 0.16214 - R2: 0.9994 -- iter: 1152/1168\n",
      "Training Step: 266  | total loss: \u001b[1m\u001b[32m0.16214\u001b[0m\u001b[0m | time: 1.165s\n",
      "| SGD | epoch: 014 | loss: 0.16214 - R2: 0.9994 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 267  | total loss: \u001b[1m\u001b[32m0.15959\u001b[0m\u001b[0m | time: 0.008s\n",
      "| SGD | epoch: 015 | loss: 0.15959 - R2: 0.9988 -- iter: 0064/1168\n",
      "Training Step: 268  | total loss: \u001b[1m\u001b[32m0.16543\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 015 | loss: 0.16543 - R2: 1.0001 -- iter: 0128/1168\n",
      "Training Step: 269  | total loss: \u001b[1m\u001b[32m0.16340\u001b[0m\u001b[0m | time: 0.017s\n",
      "| SGD | epoch: 015 | loss: 0.16340 - R2: 1.0008 -- iter: 0192/1168\n",
      "Training Step: 270  | total loss: \u001b[1m\u001b[32m0.16074\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 015 | loss: 0.16074 - R2: 1.0009 -- iter: 0256/1168\n",
      "Training Step: 271  | total loss: \u001b[1m\u001b[32m0.15477\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 015 | loss: 0.15477 - R2: 1.0017 -- iter: 0320/1168\n",
      "Training Step: 272  | total loss: \u001b[1m\u001b[32m0.15228\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 015 | loss: 0.15228 - R2: 1.0011 -- iter: 0384/1168\n",
      "Training Step: 273  | total loss: \u001b[1m\u001b[32m0.15509\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 015 | loss: 0.15509 - R2: 1.0021 -- iter: 0448/1168\n",
      "Training Step: 274  | total loss: \u001b[1m\u001b[32m0.15509\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 015 | loss: 0.15509 - R2: 1.0021 -- iter: 0512/1168\n",
      "Training Step: 275  | total loss: \u001b[1m\u001b[32m0.16141\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 015 | loss: 0.16141 - R2: 1.0006 -- iter: 0576/1168\n",
      "Training Step: 276  | total loss: \u001b[1m\u001b[32m0.16840\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 015 | loss: 0.16840 - R2: 1.0013 -- iter: 0640/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 277  | total loss: \u001b[1m\u001b[32m0.16244\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 015 | loss: 0.16244 - R2: 1.0007 -- iter: 0704/1168\n",
      "Training Step: 278  | total loss: \u001b[1m\u001b[32m0.15913\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 015 | loss: 0.15913 - R2: 1.0014 -- iter: 0768/1168\n",
      "Training Step: 279  | total loss: \u001b[1m\u001b[32m0.15676\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 015 | loss: 0.15676 - R2: 1.0024 -- iter: 0832/1168\n",
      "Training Step: 280  | total loss: \u001b[1m\u001b[32m0.15462\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 015 | loss: 0.15462 - R2: 1.0033 -- iter: 0896/1168\n",
      "Training Step: 281  | total loss: \u001b[1m\u001b[32m0.15750\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 015 | loss: 0.15750 - R2: 1.0028 -- iter: 0960/1168\n",
      "Training Step: 282  | total loss: \u001b[1m\u001b[32m0.15509\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 015 | loss: 0.15509 - R2: 1.0025 -- iter: 1024/1168\n",
      "Training Step: 283  | total loss: \u001b[1m\u001b[32m0.15703\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 015 | loss: 0.15703 - R2: 1.0014 -- iter: 1088/1168\n",
      "Training Step: 284  | total loss: \u001b[1m\u001b[32m0.16022\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 015 | loss: 0.16022 - R2: 1.0023 -- iter: 1152/1168\n",
      "Training Step: 285  | total loss: \u001b[1m\u001b[32m0.15769\u001b[0m\u001b[0m | time: 1.111s\n",
      "| SGD | epoch: 015 | loss: 0.15769 - R2: 1.0017 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 286  | total loss: \u001b[1m\u001b[32m0.15769\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 016 | loss: 0.15769 - R2: 1.0017 -- iter: 0064/1168\n",
      "Training Step: 287  | total loss: \u001b[1m\u001b[32m0.16793\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 016 | loss: 0.16793 - R2: 0.9997 -- iter: 0128/1168\n",
      "Training Step: 288  | total loss: \u001b[1m\u001b[32m0.16793\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 016 | loss: 0.16793 - R2: 0.9997 -- iter: 0192/1168\n",
      "Training Step: 289  | total loss: \u001b[1m\u001b[32m0.17028\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 016 | loss: 0.17028 - R2: 1.0002 -- iter: 0256/1168\n",
      "Training Step: 290  | total loss: \u001b[1m\u001b[32m0.17028\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 016 | loss: 0.17028 - R2: 1.0002 -- iter: 0320/1168\n",
      "Training Step: 291  | total loss: \u001b[1m\u001b[32m0.17373\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 016 | loss: 0.17373 - R2: 1.0000 -- iter: 0384/1168\n",
      "Training Step: 292  | total loss: \u001b[1m\u001b[32m0.16920\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 016 | loss: 0.16920 - R2: 1.0008 -- iter: 0448/1168\n",
      "Training Step: 293  | total loss: \u001b[1m\u001b[32m0.16901\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 016 | loss: 0.16901 - R2: 1.0001 -- iter: 0512/1168\n",
      "Training Step: 294  | total loss: \u001b[1m\u001b[32m0.16866\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 016 | loss: 0.16866 - R2: 1.0000 -- iter: 0576/1168\n",
      "Training Step: 295  | total loss: \u001b[1m\u001b[32m0.16781\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 016 | loss: 0.16781 - R2: 1.0007 -- iter: 0640/1168\n",
      "Training Step: 296  | total loss: \u001b[1m\u001b[32m0.16737\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 016 | loss: 0.16737 - R2: 1.0013 -- iter: 0704/1168\n",
      "Training Step: 297  | total loss: \u001b[1m\u001b[32m0.16601\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 016 | loss: 0.16601 - R2: 1.0020 -- iter: 0768/1168\n",
      "Training Step: 298  | total loss: \u001b[1m\u001b[32m0.16303\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 016 | loss: 0.16303 - R2: 1.0017 -- iter: 0832/1168\n",
      "Training Step: 299  | total loss: \u001b[1m\u001b[32m0.16088\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 016 | loss: 0.16088 - R2: 1.0030 -- iter: 0896/1168\n",
      "Training Step: 300  | total loss: \u001b[1m\u001b[32m0.16077\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 016 | loss: 0.16077 - R2: 1.0030 -- iter: 0960/1168\n",
      "Training Step: 301  | total loss: \u001b[1m\u001b[32m0.16172\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 016 | loss: 0.16172 - R2: 1.0051 -- iter: 1024/1168\n",
      "Training Step: 302  | total loss: \u001b[1m\u001b[32m0.16172\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 016 | loss: 0.16172 - R2: 1.0051 -- iter: 1088/1168\n",
      "Training Step: 303  | total loss: \u001b[1m\u001b[32m0.16435\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 016 | loss: 0.16435 - R2: 1.0043 -- iter: 1152/1168\n",
      "Training Step: 304  | total loss: \u001b[1m\u001b[32m0.16496\u001b[0m\u001b[0m | time: 1.130s\n",
      "| SGD | epoch: 016 | loss: 0.16496 - R2: 1.0041 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 305  | total loss: \u001b[1m\u001b[32m0.16349\u001b[0m\u001b[0m | time: 0.011s\n",
      "| SGD | epoch: 017 | loss: 0.16349 - R2: 1.0040 -- iter: 0064/1168\n",
      "Training Step: 306  | total loss: \u001b[1m\u001b[32m0.16329\u001b[0m\u001b[0m | time: 0.019s\n",
      "| SGD | epoch: 017 | loss: 0.16329 - R2: 1.0014 -- iter: 0128/1168\n",
      "Training Step: 307  | total loss: \u001b[1m\u001b[32m0.16329\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 017 | loss: 0.16329 - R2: 1.0014 -- iter: 0192/1168\n",
      "Training Step: 308  | total loss: \u001b[1m\u001b[32m0.16349\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 017 | loss: 0.16349 - R2: 0.9998 -- iter: 0256/1168\n",
      "Training Step: 309  | total loss: \u001b[1m\u001b[32m0.16601\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 017 | loss: 0.16601 - R2: 0.9992 -- iter: 0320/1168\n",
      "Training Step: 310  | total loss: \u001b[1m\u001b[32m0.16539\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 017 | loss: 0.16539 - R2: 1.0017 -- iter: 0384/1168\n",
      "Training Step: 311  | total loss: \u001b[1m\u001b[32m0.16530\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 017 | loss: 0.16530 - R2: 1.0015 -- iter: 0448/1168\n",
      "Training Step: 312  | total loss: \u001b[1m\u001b[32m0.16647\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 017 | loss: 0.16647 - R2: 1.0006 -- iter: 0512/1168\n",
      "Training Step: 313  | total loss: \u001b[1m\u001b[32m0.16360\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 017 | loss: 0.16360 - R2: 0.9990 -- iter: 0576/1168\n",
      "Training Step: 314  | total loss: \u001b[1m\u001b[32m0.15902\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 017 | loss: 0.15902 - R2: 0.9995 -- iter: 0640/1168\n",
      "Training Step: 315  | total loss: \u001b[1m\u001b[32m0.16243\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 017 | loss: 0.16243 - R2: 0.9988 -- iter: 0704/1168\n",
      "Training Step: 316  | total loss: \u001b[1m\u001b[32m0.16392\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 017 | loss: 0.16392 - R2: 0.9975 -- iter: 0768/1168\n",
      "Training Step: 317  | total loss: \u001b[1m\u001b[32m0.16392\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 017 | loss: 0.16392 - R2: 0.9975 -- iter: 0832/1168\n",
      "Training Step: 318  | total loss: \u001b[1m\u001b[32m0.15765\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 017 | loss: 0.15765 - R2: 0.9996 -- iter: 0896/1168\n",
      "Training Step: 319  | total loss: \u001b[1m\u001b[32m0.15785\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 017 | loss: 0.15785 - R2: 1.0001 -- iter: 0960/1168\n",
      "Training Step: 320  | total loss: \u001b[1m\u001b[32m0.15804\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 017 | loss: 0.15804 - R2: 1.0005 -- iter: 1024/1168\n",
      "Training Step: 321  | total loss: \u001b[1m\u001b[32m0.15904\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 017 | loss: 0.15904 - R2: 1.0012 -- iter: 1088/1168\n",
      "Training Step: 322  | total loss: \u001b[1m\u001b[32m0.15904\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 017 | loss: 0.15904 - R2: 1.0012 -- iter: 1152/1168\n",
      "Training Step: 323  | total loss: \u001b[1m\u001b[32m0.16049\u001b[0m\u001b[0m | time: 1.089s\n",
      "| SGD | epoch: 017 | loss: 0.16049 - R2: 1.0002 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 324  | total loss: \u001b[1m\u001b[32m0.15721\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 018 | loss: 0.15721 - R2: 1.0002 -- iter: 0064/1168\n",
      "Training Step: 325  | total loss: \u001b[1m\u001b[32m0.15721\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 018 | loss: 0.15721 - R2: 1.0002 -- iter: 0128/1168\n",
      "Training Step: 326  | total loss: \u001b[1m\u001b[32m0.15947\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 018 | loss: 0.15947 - R2: 1.0001 -- iter: 0192/1168\n",
      "Training Step: 327  | total loss: \u001b[1m\u001b[32m0.16065\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 018 | loss: 0.16065 - R2: 1.0018 -- iter: 0256/1168\n",
      "Training Step: 328  | total loss: \u001b[1m\u001b[32m0.16763\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 018 | loss: 0.16763 - R2: 1.0021 -- iter: 0320/1168\n",
      "Training Step: 329  | total loss: \u001b[1m\u001b[32m0.16175\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 018 | loss: 0.16175 - R2: 1.0021 -- iter: 0384/1168\n",
      "Training Step: 330  | total loss: \u001b[1m\u001b[32m0.16107\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 018 | loss: 0.16107 - R2: 1.0008 -- iter: 0448/1168\n",
      "Training Step: 331  | total loss: \u001b[1m\u001b[32m0.16774\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 018 | loss: 0.16774 - R2: 1.0010 -- iter: 0512/1168\n",
      "Training Step: 332  | total loss: \u001b[1m\u001b[32m0.16883\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 018 | loss: 0.16883 - R2: 1.0005 -- iter: 0576/1168\n",
      "Training Step: 333  | total loss: \u001b[1m\u001b[32m0.16883\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 018 | loss: 0.16883 - R2: 1.0005 -- iter: 0640/1168\n",
      "Training Step: 334  | total loss: \u001b[1m\u001b[32m0.16731\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 018 | loss: 0.16731 - R2: 1.0004 -- iter: 0704/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 335  | total loss: \u001b[1m\u001b[32m0.16731\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 018 | loss: 0.16731 - R2: 1.0004 -- iter: 0768/1168\n",
      "Training Step: 336  | total loss: \u001b[1m\u001b[32m0.16940\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 018 | loss: 0.16940 - R2: 1.0005 -- iter: 0832/1168\n",
      "Training Step: 337  | total loss: \u001b[1m\u001b[32m0.16947\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 018 | loss: 0.16947 - R2: 1.0002 -- iter: 0896/1168\n",
      "Training Step: 338  | total loss: \u001b[1m\u001b[32m0.17246\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 018 | loss: 0.17246 - R2: 1.0002 -- iter: 0960/1168\n",
      "Training Step: 339  | total loss: \u001b[1m\u001b[32m0.16547\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 018 | loss: 0.16547 - R2: 1.0005 -- iter: 1024/1168\n",
      "Training Step: 340  | total loss: \u001b[1m\u001b[32m0.15841\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 018 | loss: 0.15841 - R2: 1.0002 -- iter: 1088/1168\n",
      "Training Step: 341  | total loss: \u001b[1m\u001b[32m0.15131\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 018 | loss: 0.15131 - R2: 0.9983 -- iter: 1152/1168\n",
      "Training Step: 342  | total loss: \u001b[1m\u001b[32m0.15131\u001b[0m\u001b[0m | time: 1.144s\n",
      "| SGD | epoch: 018 | loss: 0.15131 - R2: 0.9983 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 343  | total loss: \u001b[1m\u001b[32m0.14894\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 019 | loss: 0.14894 - R2: 0.9999 -- iter: 0064/1168\n",
      "Training Step: 344  | total loss: \u001b[1m\u001b[32m0.14894\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 019 | loss: 0.14894 - R2: 1.0006 -- iter: 0128/1168\n",
      "Training Step: 345  | total loss: \u001b[1m\u001b[32m0.15067\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 019 | loss: 0.15067 - R2: 1.0015 -- iter: 0192/1168\n",
      "Training Step: 346  | total loss: \u001b[1m\u001b[32m0.15812\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 019 | loss: 0.15812 - R2: 1.0017 -- iter: 0256/1168\n",
      "Training Step: 347  | total loss: \u001b[1m\u001b[32m0.15812\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 019 | loss: 0.15812 - R2: 1.0017 -- iter: 0320/1168\n",
      "Training Step: 348  | total loss: \u001b[1m\u001b[32m0.14958\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 019 | loss: 0.14958 - R2: 0.9993 -- iter: 0384/1168\n",
      "Training Step: 349  | total loss: \u001b[1m\u001b[32m0.15285\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 019 | loss: 0.15285 - R2: 0.9998 -- iter: 0448/1168\n",
      "Training Step: 350  | total loss: \u001b[1m\u001b[32m0.15285\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 019 | loss: 0.15285 - R2: 0.9998 -- iter: 0512/1168\n",
      "Training Step: 351  | total loss: \u001b[1m\u001b[32m0.15307\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 019 | loss: 0.15307 - R2: 0.9998 -- iter: 0576/1168\n",
      "Training Step: 352  | total loss: \u001b[1m\u001b[32m0.15224\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 019 | loss: 0.15224 - R2: 0.9998 -- iter: 0640/1168\n",
      "Training Step: 353  | total loss: \u001b[1m\u001b[32m0.14950\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 019 | loss: 0.14950 - R2: 1.0004 -- iter: 0704/1168\n",
      "Training Step: 354  | total loss: \u001b[1m\u001b[32m0.14912\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 019 | loss: 0.14912 - R2: 1.0019 -- iter: 0768/1168\n",
      "Training Step: 355  | total loss: \u001b[1m\u001b[32m0.14840\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 019 | loss: 0.14840 - R2: 1.0022 -- iter: 0832/1168\n",
      "Training Step: 356  | total loss: \u001b[1m\u001b[32m0.14840\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 019 | loss: 0.14840 - R2: 1.0022 -- iter: 0896/1168\n",
      "Training Step: 357  | total loss: \u001b[1m\u001b[32m0.15469\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 019 | loss: 0.15469 - R2: 1.0024 -- iter: 0960/1168\n",
      "Training Step: 358  | total loss: \u001b[1m\u001b[32m0.15475\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 019 | loss: 0.15475 - R2: 1.0018 -- iter: 1024/1168\n",
      "Training Step: 359  | total loss: \u001b[1m\u001b[32m0.15475\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 019 | loss: 0.15475 - R2: 1.0018 -- iter: 1088/1168\n",
      "Training Step: 360  | total loss: \u001b[1m\u001b[32m0.15101\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 019 | loss: 0.15101 - R2: 1.0013 -- iter: 1152/1168\n",
      "Training Step: 361  | total loss: \u001b[1m\u001b[32m0.14765\u001b[0m\u001b[0m | time: 1.099s\n",
      "| SGD | epoch: 019 | loss: 0.14765 - R2: 1.0008 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 362  | total loss: \u001b[1m\u001b[32m0.15217\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 020 | loss: 0.15217 - R2: 1.0018 -- iter: 0064/1168\n",
      "Training Step: 363  | total loss: \u001b[1m\u001b[32m0.15812\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 020 | loss: 0.15812 - R2: 1.0003 -- iter: 0128/1168\n",
      "Training Step: 364  | total loss: \u001b[1m\u001b[32m0.16672\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 020 | loss: 0.16672 - R2: 1.0004 -- iter: 0192/1168\n",
      "Training Step: 365  | total loss: \u001b[1m\u001b[32m0.16672\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 020 | loss: 0.16672 - R2: 1.0004 -- iter: 0256/1168\n",
      "Training Step: 366  | total loss: \u001b[1m\u001b[32m0.16980\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 020 | loss: 0.16980 - R2: 1.0003 -- iter: 0320/1168\n",
      "Training Step: 367  | total loss: \u001b[1m\u001b[32m0.16674\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 020 | loss: 0.16674 - R2: 0.9992 -- iter: 0384/1168\n",
      "Training Step: 368  | total loss: \u001b[1m\u001b[32m0.16715\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 020 | loss: 0.16715 - R2: 0.9990 -- iter: 0448/1168\n",
      "Training Step: 369  | total loss: \u001b[1m\u001b[32m0.17637\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 020 | loss: 0.17637 - R2: 1.0001 -- iter: 0512/1168\n",
      "Training Step: 370  | total loss: \u001b[1m\u001b[32m0.17637\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 020 | loss: 0.17637 - R2: 1.0007 -- iter: 0576/1168\n",
      "Training Step: 371  | total loss: \u001b[1m\u001b[32m0.17511\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 020 | loss: 0.17511 - R2: 1.0023 -- iter: 0640/1168\n",
      "Training Step: 372  | total loss: \u001b[1m\u001b[32m0.17511\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 020 | loss: 0.17511 - R2: 1.0023 -- iter: 0704/1168\n",
      "Training Step: 373  | total loss: \u001b[1m\u001b[32m0.16824\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 020 | loss: 0.16824 - R2: 1.0014 -- iter: 0768/1168\n",
      "Training Step: 374  | total loss: \u001b[1m\u001b[32m0.16991\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 020 | loss: 0.16991 - R2: 1.0004 -- iter: 0832/1168\n",
      "Training Step: 375  | total loss: \u001b[1m\u001b[32m0.16638\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 020 | loss: 0.16638 - R2: 1.0009 -- iter: 0896/1168\n",
      "Training Step: 376  | total loss: \u001b[1m\u001b[32m0.16638\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 020 | loss: 0.16638 - R2: 1.0002 -- iter: 0960/1168\n",
      "Training Step: 377  | total loss: \u001b[1m\u001b[32m0.16745\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 020 | loss: 0.16745 - R2: 1.0002 -- iter: 1024/1168\n",
      "Training Step: 378  | total loss: \u001b[1m\u001b[32m0.16379\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 020 | loss: 0.16379 - R2: 1.0005 -- iter: 1088/1168\n",
      "Training Step: 379  | total loss: \u001b[1m\u001b[32m0.16309\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 020 | loss: 0.16309 - R2: 1.0009 -- iter: 1152/1168\n",
      "Training Step: 380  | total loss: \u001b[1m\u001b[32m0.14865\u001b[0m\u001b[0m | time: 1.156s\n",
      "| SGD | epoch: 020 | loss: 0.14865 - R2: 1.0005 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 381  | total loss: \u001b[1m\u001b[32m0.14865\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 021 | loss: 0.14865 - R2: 1.0005 -- iter: 0064/1168\n",
      "Training Step: 382  | total loss: \u001b[1m\u001b[32m0.14939\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 021 | loss: 0.14939 - R2: 1.0009 -- iter: 0128/1168\n",
      "Training Step: 383  | total loss: \u001b[1m\u001b[32m0.14911\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 021 | loss: 0.14911 - R2: 0.9994 -- iter: 0192/1168\n",
      "Training Step: 384  | total loss: \u001b[1m\u001b[32m0.14911\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 021 | loss: 0.14911 - R2: 0.9994 -- iter: 0256/1168\n",
      "Training Step: 385  | total loss: \u001b[1m\u001b[32m0.14755\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 021 | loss: 0.14755 - R2: 0.9991 -- iter: 0320/1168\n",
      "Training Step: 386  | total loss: \u001b[1m\u001b[32m0.14856\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 021 | loss: 0.14856 - R2: 0.9998 -- iter: 0384/1168\n",
      "Training Step: 387  | total loss: \u001b[1m\u001b[32m0.15823\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 021 | loss: 0.15823 - R2: 0.9990 -- iter: 0448/1168\n",
      "Training Step: 388  | total loss: \u001b[1m\u001b[32m0.15823\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 021 | loss: 0.15823 - R2: 0.9990 -- iter: 0512/1168\n",
      "Training Step: 389  | total loss: \u001b[1m\u001b[32m0.15788\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 021 | loss: 0.15788 - R2: 0.9990 -- iter: 0576/1168\n",
      "Training Step: 390  | total loss: \u001b[1m\u001b[32m0.16059\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 021 | loss: 0.16059 - R2: 0.9988 -- iter: 0640/1168\n",
      "Training Step: 391  | total loss: \u001b[1m\u001b[32m0.16639\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 021 | loss: 0.16639 - R2: 0.9983 -- iter: 0704/1168\n",
      "Training Step: 392  | total loss: \u001b[1m\u001b[32m0.16192\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 021 | loss: 0.16192 - R2: 0.9990 -- iter: 0768/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 393  | total loss: \u001b[1m\u001b[32m0.16192\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 021 | loss: 0.16192 - R2: 0.9990 -- iter: 0832/1168\n",
      "Training Step: 394  | total loss: \u001b[1m\u001b[32m0.16136\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 021 | loss: 0.16136 - R2: 1.0000 -- iter: 0896/1168\n",
      "Training Step: 395  | total loss: \u001b[1m\u001b[32m0.16465\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 021 | loss: 0.16465 - R2: 1.0023 -- iter: 0960/1168\n",
      "Training Step: 396  | total loss: \u001b[1m\u001b[32m0.16510\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 021 | loss: 0.16510 - R2: 1.0034 -- iter: 1024/1168\n",
      "Training Step: 397  | total loss: \u001b[1m\u001b[32m0.16510\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 021 | loss: 0.16510 - R2: 1.0041 -- iter: 1088/1168\n",
      "Training Step: 398  | total loss: \u001b[1m\u001b[32m0.16754\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 021 | loss: 0.16754 - R2: 1.0041 -- iter: 1152/1168\n",
      "Training Step: 399  | total loss: \u001b[1m\u001b[32m0.16797\u001b[0m\u001b[0m | time: 1.145s\n",
      "| SGD | epoch: 021 | loss: 0.16797 - R2: 0.9995 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 400  | total loss: \u001b[1m\u001b[32m0.17021\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 022 | loss: 0.17021 - R2: 0.9970 -- iter: 0064/1168\n",
      "Training Step: 401  | total loss: \u001b[1m\u001b[32m0.16475\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 022 | loss: 0.16475 - R2: 0.9974 -- iter: 0128/1168\n",
      "Training Step: 402  | total loss: \u001b[1m\u001b[32m0.16481\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 022 | loss: 0.16481 - R2: 0.9962 -- iter: 0192/1168\n",
      "Training Step: 403  | total loss: \u001b[1m\u001b[32m0.16179\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 022 | loss: 0.16179 - R2: 0.9976 -- iter: 0256/1168\n",
      "Training Step: 404  | total loss: \u001b[1m\u001b[32m0.16030\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 022 | loss: 0.16030 - R2: 0.9986 -- iter: 0320/1168\n",
      "Training Step: 405  | total loss: \u001b[1m\u001b[32m0.16030\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 022 | loss: 0.16030 - R2: 0.9986 -- iter: 0384/1168\n",
      "Training Step: 406  | total loss: \u001b[1m\u001b[32m0.15995\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 022 | loss: 0.15995 - R2: 0.9977 -- iter: 0448/1168\n",
      "Training Step: 407  | total loss: \u001b[1m\u001b[32m0.15995\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 022 | loss: 0.15995 - R2: 0.9971 -- iter: 0512/1168\n",
      "Training Step: 408  | total loss: \u001b[1m\u001b[32m0.16442\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 022 | loss: 0.16442 - R2: 0.9972 -- iter: 0576/1168\n",
      "Training Step: 409  | total loss: \u001b[1m\u001b[32m0.16442\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 022 | loss: 0.16442 - R2: 0.9972 -- iter: 0640/1168\n",
      "Training Step: 410  | total loss: \u001b[1m\u001b[32m0.16247\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 022 | loss: 0.16247 - R2: 0.9960 -- iter: 0704/1168\n",
      "Training Step: 411  | total loss: \u001b[1m\u001b[32m0.16565\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 022 | loss: 0.16565 - R2: 0.9960 -- iter: 0768/1168\n",
      "Training Step: 412  | total loss: \u001b[1m\u001b[32m0.16890\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 022 | loss: 0.16890 - R2: 0.9963 -- iter: 0832/1168\n",
      "Training Step: 413  | total loss: \u001b[1m\u001b[32m0.16890\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 022 | loss: 0.16890 - R2: 0.9963 -- iter: 0896/1168\n",
      "Training Step: 414  | total loss: \u001b[1m\u001b[32m0.17197\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 022 | loss: 0.17197 - R2: 0.9979 -- iter: 0960/1168\n",
      "Training Step: 415  | total loss: \u001b[1m\u001b[32m0.16502\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 022 | loss: 0.16502 - R2: 0.9994 -- iter: 1024/1168\n",
      "Training Step: 416  | total loss: \u001b[1m\u001b[32m0.16098\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 022 | loss: 0.16098 - R2: 1.0007 -- iter: 1088/1168\n",
      "Training Step: 417  | total loss: \u001b[1m\u001b[32m0.16377\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 022 | loss: 0.16377 - R2: 1.0006 -- iter: 1152/1168\n",
      "Training Step: 418  | total loss: \u001b[1m\u001b[32m0.16352\u001b[0m\u001b[0m | time: 1.175s\n",
      "| SGD | epoch: 022 | loss: 0.16352 - R2: 1.0008 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 419  | total loss: \u001b[1m\u001b[32m0.16352\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 023 | loss: 0.16352 - R2: 1.0008 -- iter: 0064/1168\n",
      "Training Step: 420  | total loss: \u001b[1m\u001b[32m0.17165\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 023 | loss: 0.17165 - R2: 1.0053 -- iter: 0128/1168\n",
      "Training Step: 421  | total loss: \u001b[1m\u001b[32m0.17739\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 023 | loss: 0.17739 - R2: 1.0088 -- iter: 0192/1168\n",
      "Training Step: 422  | total loss: \u001b[1m\u001b[32m0.17567\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 023 | loss: 0.17567 - R2: 1.0061 -- iter: 0256/1168\n",
      "Training Step: 423  | total loss: \u001b[1m\u001b[32m0.17567\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 023 | loss: 0.17567 - R2: 1.0061 -- iter: 0320/1168\n",
      "Training Step: 424  | total loss: \u001b[1m\u001b[32m0.17325\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 023 | loss: 0.17325 - R2: 1.0045 -- iter: 0384/1168\n",
      "Training Step: 425  | total loss: \u001b[1m\u001b[32m0.17251\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 023 | loss: 0.17251 - R2: 1.0045 -- iter: 0448/1168\n",
      "Training Step: 426  | total loss: \u001b[1m\u001b[32m0.17382\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 023 | loss: 0.17382 - R2: 1.0037 -- iter: 0512/1168\n",
      "Training Step: 427  | total loss: \u001b[1m\u001b[32m0.16963\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 023 | loss: 0.16963 - R2: 1.0043 -- iter: 0576/1168\n",
      "Training Step: 428  | total loss: \u001b[1m\u001b[32m0.17209\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 023 | loss: 0.17209 - R2: 1.0043 -- iter: 0640/1168\n",
      "Training Step: 429  | total loss: \u001b[1m\u001b[32m0.17209\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 023 | loss: 0.17209 - R2: 1.0044 -- iter: 0704/1168\n",
      "Training Step: 430  | total loss: \u001b[1m\u001b[32m0.17097\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 023 | loss: 0.17097 - R2: 1.0014 -- iter: 0768/1168\n",
      "Training Step: 431  | total loss: \u001b[1m\u001b[32m0.17097\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 023 | loss: 0.17097 - R2: 1.0022 -- iter: 0832/1168\n",
      "Training Step: 432  | total loss: \u001b[1m\u001b[32m0.16392\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 023 | loss: 0.16392 - R2: 1.0022 -- iter: 0896/1168\n",
      "Training Step: 433  | total loss: \u001b[1m\u001b[32m0.16434\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 023 | loss: 0.16434 - R2: 1.0019 -- iter: 0960/1168\n",
      "Training Step: 434  | total loss: \u001b[1m\u001b[32m0.16706\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 023 | loss: 0.16706 - R2: 1.0009 -- iter: 1024/1168\n",
      "Training Step: 435  | total loss: \u001b[1m\u001b[32m0.17227\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 023 | loss: 0.17227 - R2: 1.0008 -- iter: 1088/1168\n",
      "Training Step: 436  | total loss: \u001b[1m\u001b[32m0.17155\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 023 | loss: 0.17155 - R2: 1.0007 -- iter: 1152/1168\n",
      "Training Step: 437  | total loss: \u001b[1m\u001b[32m0.16983\u001b[0m\u001b[0m | time: 1.153s\n",
      "| SGD | epoch: 023 | loss: 0.16983 - R2: 1.0000 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 438  | total loss: \u001b[1m\u001b[32m0.16983\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 024 | loss: 0.16983 - R2: 1.0000 -- iter: 0064/1168\n",
      "Training Step: 439  | total loss: \u001b[1m\u001b[32m0.15940\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 024 | loss: 0.15940 - R2: 1.0010 -- iter: 0128/1168\n",
      "Training Step: 440  | total loss: \u001b[1m\u001b[32m0.15414\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 024 | loss: 0.15414 - R2: 1.0018 -- iter: 0192/1168\n",
      "Training Step: 441  | total loss: \u001b[1m\u001b[32m0.15980\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 024 | loss: 0.15980 - R2: 1.0009 -- iter: 0256/1168\n",
      "Training Step: 442  | total loss: \u001b[1m\u001b[32m0.15795\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 024 | loss: 0.15795 - R2: 1.0018 -- iter: 0320/1168\n",
      "Training Step: 443  | total loss: \u001b[1m\u001b[32m0.15722\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 024 | loss: 0.15722 - R2: 1.0021 -- iter: 0384/1168\n",
      "Training Step: 444  | total loss: \u001b[1m\u001b[32m0.15722\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 024 | loss: 0.15722 - R2: 1.0021 -- iter: 0448/1168\n",
      "Training Step: 445  | total loss: \u001b[1m\u001b[32m0.15535\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 024 | loss: 0.15535 - R2: 1.0021 -- iter: 0512/1168\n",
      "Training Step: 446  | total loss: \u001b[1m\u001b[32m0.15726\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 024 | loss: 0.15726 - R2: 1.0021 -- iter: 0576/1168\n",
      "Training Step: 447  | total loss: \u001b[1m\u001b[32m0.15720\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 024 | loss: 0.15720 - R2: 1.0025 -- iter: 0640/1168\n",
      "Training Step: 448  | total loss: \u001b[1m\u001b[32m0.16009\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 024 | loss: 0.16009 - R2: 1.0023 -- iter: 0704/1168\n",
      "Training Step: 449  | total loss: \u001b[1m\u001b[32m0.16232\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 024 | loss: 0.16232 - R2: 1.0026 -- iter: 0768/1168\n",
      "Training Step: 450  | total loss: \u001b[1m\u001b[32m0.16232\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 024 | loss: 0.16232 - R2: 1.0026 -- iter: 0832/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 451  | total loss: \u001b[1m\u001b[32m0.16103\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 024 | loss: 0.16103 - R2: 1.0027 -- iter: 0896/1168\n",
      "Training Step: 452  | total loss: \u001b[1m\u001b[32m0.15864\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 024 | loss: 0.15864 - R2: 1.0011 -- iter: 0960/1168\n",
      "Training Step: 453  | total loss: \u001b[1m\u001b[32m0.15751\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 024 | loss: 0.15751 - R2: 1.0012 -- iter: 1024/1168\n",
      "Training Step: 454  | total loss: \u001b[1m\u001b[32m0.16226\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 024 | loss: 0.16226 - R2: 1.0009 -- iter: 1088/1168\n",
      "Training Step: 455  | total loss: \u001b[1m\u001b[32m0.16226\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 024 | loss: 0.16226 - R2: 1.0009 -- iter: 1152/1168\n",
      "Training Step: 456  | total loss: \u001b[1m\u001b[32m0.16388\u001b[0m\u001b[0m | time: 1.143s\n",
      "| SGD | epoch: 024 | loss: 0.16388 - R2: 1.0002 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 457  | total loss: \u001b[1m\u001b[32m0.16389\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 025 | loss: 0.16389 - R2: 0.9996 -- iter: 0064/1168\n",
      "Training Step: 458  | total loss: \u001b[1m\u001b[32m0.16389\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 025 | loss: 0.16389 - R2: 0.9996 -- iter: 0128/1168\n",
      "Training Step: 459  | total loss: \u001b[1m\u001b[32m0.17617\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 025 | loss: 0.17617 - R2: 1.0003 -- iter: 0192/1168\n",
      "Training Step: 460  | total loss: \u001b[1m\u001b[32m0.17617\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 025 | loss: 0.17617 - R2: 1.0003 -- iter: 0256/1168\n",
      "Training Step: 461  | total loss: \u001b[1m\u001b[32m0.18468\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 025 | loss: 0.18468 - R2: 1.0017 -- iter: 0320/1168\n",
      "Training Step: 462  | total loss: \u001b[1m\u001b[32m0.18305\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 025 | loss: 0.18305 - R2: 1.0007 -- iter: 0384/1168\n",
      "Training Step: 463  | total loss: \u001b[1m\u001b[32m0.17121\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 025 | loss: 0.17121 - R2: 1.0017 -- iter: 0448/1168\n",
      "Training Step: 464  | total loss: \u001b[1m\u001b[32m0.17121\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 025 | loss: 0.17121 - R2: 1.0017 -- iter: 0512/1168\n",
      "Training Step: 465  | total loss: \u001b[1m\u001b[32m0.17082\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 025 | loss: 0.17082 - R2: 1.0024 -- iter: 0576/1168\n",
      "Training Step: 466  | total loss: \u001b[1m\u001b[32m0.16935\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 025 | loss: 0.16935 - R2: 1.0028 -- iter: 0640/1168\n",
      "Training Step: 467  | total loss: \u001b[1m\u001b[32m0.16959\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 025 | loss: 0.16959 - R2: 1.0033 -- iter: 0704/1168\n",
      "Training Step: 468  | total loss: \u001b[1m\u001b[32m0.16716\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 025 | loss: 0.16716 - R2: 1.0027 -- iter: 0768/1168\n",
      "Training Step: 469  | total loss: \u001b[1m\u001b[32m0.16716\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 025 | loss: 0.16716 - R2: 1.0027 -- iter: 0832/1168\n",
      "Training Step: 470  | total loss: \u001b[1m\u001b[32m0.16490\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 025 | loss: 0.16490 - R2: 1.0024 -- iter: 0896/1168\n",
      "Training Step: 471  | total loss: \u001b[1m\u001b[32m0.16288\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 025 | loss: 0.16288 - R2: 1.0025 -- iter: 0960/1168\n",
      "Training Step: 472  | total loss: \u001b[1m\u001b[32m0.16510\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 025 | loss: 0.16510 - R2: 1.0008 -- iter: 1024/1168\n",
      "Training Step: 473  | total loss: \u001b[1m\u001b[32m0.16244\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 025 | loss: 0.16244 - R2: 1.0022 -- iter: 1088/1168\n",
      "Training Step: 474  | total loss: \u001b[1m\u001b[32m0.16244\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 025 | loss: 0.16244 - R2: 1.0022 -- iter: 1152/1168\n",
      "Training Step: 475  | total loss: \u001b[1m\u001b[32m0.16046\u001b[0m\u001b[0m | time: 1.122s\n",
      "| SGD | epoch: 025 | loss: 0.16046 - R2: 1.0024 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 476  | total loss: \u001b[1m\u001b[32m0.16038\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 026 | loss: 0.16038 - R2: 1.0015 -- iter: 0064/1168\n",
      "Training Step: 477  | total loss: \u001b[1m\u001b[32m0.15610\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 026 | loss: 0.15610 - R2: 1.0022 -- iter: 0128/1168\n",
      "Training Step: 478  | total loss: \u001b[1m\u001b[32m0.16058\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 026 | loss: 0.16058 - R2: 1.0034 -- iter: 0192/1168\n",
      "Training Step: 479  | total loss: \u001b[1m\u001b[32m0.16058\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 026 | loss: 0.16058 - R2: 1.0034 -- iter: 0256/1168\n",
      "Training Step: 480  | total loss: \u001b[1m\u001b[32m0.15853\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 026 | loss: 0.15853 - R2: 1.0021 -- iter: 0320/1168\n",
      "Training Step: 481  | total loss: \u001b[1m\u001b[32m0.16309\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 026 | loss: 0.16309 - R2: 1.0019 -- iter: 0384/1168\n",
      "Training Step: 482  | total loss: \u001b[1m\u001b[32m0.16387\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 026 | loss: 0.16387 - R2: 1.0026 -- iter: 0448/1168\n",
      "Training Step: 483  | total loss: \u001b[1m\u001b[32m0.16387\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 026 | loss: 0.16387 - R2: 1.0026 -- iter: 0512/1168\n",
      "Training Step: 484  | total loss: \u001b[1m\u001b[32m0.16870\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 026 | loss: 0.16870 - R2: 1.0022 -- iter: 0576/1168\n",
      "Training Step: 485  | total loss: \u001b[1m\u001b[32m0.16974\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 026 | loss: 0.16974 - R2: 1.0000 -- iter: 0640/1168\n",
      "Training Step: 486  | total loss: \u001b[1m\u001b[32m0.16823\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 026 | loss: 0.16823 - R2: 0.9992 -- iter: 0704/1168\n",
      "Training Step: 487  | total loss: \u001b[1m\u001b[32m0.16486\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 026 | loss: 0.16486 - R2: 0.9988 -- iter: 0768/1168\n",
      "Training Step: 488  | total loss: \u001b[1m\u001b[32m0.16486\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 026 | loss: 0.16486 - R2: 0.9998 -- iter: 0832/1168\n",
      "Training Step: 489  | total loss: \u001b[1m\u001b[32m0.16645\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 026 | loss: 0.16645 - R2: 0.9998 -- iter: 0896/1168\n",
      "Training Step: 490  | total loss: \u001b[1m\u001b[32m0.16282\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 026 | loss: 0.16282 - R2: 1.0003 -- iter: 0960/1168\n",
      "Training Step: 491  | total loss: \u001b[1m\u001b[32m0.16282\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 026 | loss: 0.16282 - R2: 1.0003 -- iter: 1024/1168\n",
      "Training Step: 492  | total loss: \u001b[1m\u001b[32m0.16387\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 026 | loss: 0.16387 - R2: 1.0012 -- iter: 1088/1168\n",
      "Training Step: 493  | total loss: \u001b[1m\u001b[32m0.16212\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 026 | loss: 0.16212 - R2: 1.0026 -- iter: 1152/1168\n",
      "Training Step: 494  | total loss: \u001b[1m\u001b[32m0.16399\u001b[0m\u001b[0m | time: 1.162s\n",
      "| SGD | epoch: 026 | loss: 0.16399 - R2: 1.0021 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 495  | total loss: \u001b[1m\u001b[32m0.16850\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 027 | loss: 0.16850 - R2: 1.0016 -- iter: 0064/1168\n",
      "Training Step: 496  | total loss: \u001b[1m\u001b[32m0.16733\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 027 | loss: 0.16733 - R2: 1.0022 -- iter: 0128/1168\n",
      "Training Step: 497  | total loss: \u001b[1m\u001b[32m0.16724\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 027 | loss: 0.16724 - R2: 1.0022 -- iter: 0192/1168\n",
      "Training Step: 498  | total loss: \u001b[1m\u001b[32m0.16481\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 027 | loss: 0.16481 - R2: 1.0016 -- iter: 0256/1168\n",
      "Training Step: 499  | total loss: \u001b[1m\u001b[32m0.16478\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 027 | loss: 0.16478 - R2: 0.9995 -- iter: 0320/1168\n",
      "Training Step: 500  | total loss: \u001b[1m\u001b[32m0.16478\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 027 | loss: 0.16478 - R2: 0.9983 -- iter: 0384/1168\n",
      "Training Step: 501  | total loss: \u001b[1m\u001b[32m0.16158\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 027 | loss: 0.16158 - R2: 0.9983 -- iter: 0448/1168\n",
      "Training Step: 502  | total loss: \u001b[1m\u001b[32m0.16350\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 027 | loss: 0.16350 - R2: 0.9999 -- iter: 0512/1168\n",
      "Training Step: 503  | total loss: \u001b[1m\u001b[32m0.15889\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 027 | loss: 0.15889 - R2: 1.0009 -- iter: 0576/1168\n",
      "Training Step: 504  | total loss: \u001b[1m\u001b[32m0.15846\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 027 | loss: 0.15846 - R2: 1.0000 -- iter: 0640/1168\n",
      "Training Step: 505  | total loss: \u001b[1m\u001b[32m0.15875\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 027 | loss: 0.15875 - R2: 0.9986 -- iter: 0704/1168\n",
      "Training Step: 506  | total loss: \u001b[1m\u001b[32m0.15516\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 027 | loss: 0.15516 - R2: 0.9977 -- iter: 0768/1168\n",
      "Training Step: 507  | total loss: \u001b[1m\u001b[32m0.16446\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 027 | loss: 0.16446 - R2: 0.9980 -- iter: 0832/1168\n",
      "Training Step: 508  | total loss: \u001b[1m\u001b[32m0.16415\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 027 | loss: 0.16415 - R2: 0.9988 -- iter: 0896/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 509  | total loss: \u001b[1m\u001b[32m0.16415\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 027 | loss: 0.16415 - R2: 0.9988 -- iter: 0960/1168\n",
      "Training Step: 510  | total loss: \u001b[1m\u001b[32m0.16966\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 027 | loss: 0.16966 - R2: 0.9980 -- iter: 1024/1168\n",
      "Training Step: 511  | total loss: \u001b[1m\u001b[32m0.16689\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 027 | loss: 0.16689 - R2: 0.9980 -- iter: 1088/1168\n",
      "Training Step: 512  | total loss: \u001b[1m\u001b[32m0.16502\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 027 | loss: 0.16502 - R2: 0.9987 -- iter: 1152/1168\n",
      "Training Step: 513  | total loss: \u001b[1m\u001b[32m0.16409\u001b[0m\u001b[0m | time: 1.119s\n",
      "| SGD | epoch: 027 | loss: 0.16409 - R2: 0.9990 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 514  | total loss: \u001b[1m\u001b[32m0.17187\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 028 | loss: 0.17187 - R2: 0.9988 -- iter: 0064/1168\n",
      "Training Step: 515  | total loss: \u001b[1m\u001b[32m0.17137\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 028 | loss: 0.17137 - R2: 0.9995 -- iter: 0128/1168\n",
      "Training Step: 516  | total loss: \u001b[1m\u001b[32m0.17044\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 028 | loss: 0.17044 - R2: 1.0004 -- iter: 0192/1168\n",
      "Training Step: 517  | total loss: \u001b[1m\u001b[32m0.17044\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 028 | loss: 0.17044 - R2: 0.9999 -- iter: 0256/1168\n",
      "Training Step: 518  | total loss: \u001b[1m\u001b[32m0.16994\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 028 | loss: 0.16994 - R2: 0.9999 -- iter: 0320/1168\n",
      "Training Step: 519  | total loss: \u001b[1m\u001b[32m0.16440\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 028 | loss: 0.16440 - R2: 0.9990 -- iter: 0384/1168\n",
      "Training Step: 520  | total loss: \u001b[1m\u001b[32m0.16043\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 028 | loss: 0.16043 - R2: 0.9988 -- iter: 0448/1168\n",
      "Training Step: 521  | total loss: \u001b[1m\u001b[32m0.15978\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 028 | loss: 0.15978 - R2: 0.9992 -- iter: 0512/1168\n",
      "Training Step: 522  | total loss: \u001b[1m\u001b[32m0.16055\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 028 | loss: 0.16055 - R2: 0.9989 -- iter: 0576/1168\n",
      "Training Step: 523  | total loss: \u001b[1m\u001b[32m0.15357\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 028 | loss: 0.15357 - R2: 0.9997 -- iter: 0640/1168\n",
      "Training Step: 524  | total loss: \u001b[1m\u001b[32m0.15357\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 028 | loss: 0.15357 - R2: 0.9997 -- iter: 0704/1168\n",
      "Training Step: 525  | total loss: \u001b[1m\u001b[32m0.15350\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 028 | loss: 0.15350 - R2: 1.0007 -- iter: 0768/1168\n",
      "Training Step: 526  | total loss: \u001b[1m\u001b[32m0.15497\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 028 | loss: 0.15497 - R2: 0.9995 -- iter: 0832/1168\n",
      "Training Step: 527  | total loss: \u001b[1m\u001b[32m0.15789\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 028 | loss: 0.15789 - R2: 1.0003 -- iter: 0896/1168\n",
      "Training Step: 528  | total loss: \u001b[1m\u001b[32m0.15789\u001b[0m\u001b[0m | time: 0.172s\n",
      "| SGD | epoch: 028 | loss: 0.15789 - R2: 1.0003 -- iter: 0960/1168\n",
      "Training Step: 529  | total loss: \u001b[1m\u001b[32m0.15933\u001b[0m\u001b[0m | time: 0.176s\n",
      "| SGD | epoch: 028 | loss: 0.15933 - R2: 0.9995 -- iter: 1024/1168\n",
      "Training Step: 530  | total loss: \u001b[1m\u001b[32m0.15933\u001b[0m\u001b[0m | time: 0.179s\n",
      "| SGD | epoch: 028 | loss: 0.15933 - R2: 0.9995 -- iter: 1088/1168\n",
      "Training Step: 531  | total loss: \u001b[1m\u001b[32m0.15773\u001b[0m\u001b[0m | time: 0.181s\n",
      "| SGD | epoch: 028 | loss: 0.15773 - R2: 1.0002 -- iter: 1152/1168\n",
      "Training Step: 532  | total loss: \u001b[1m\u001b[32m0.15778\u001b[0m\u001b[0m | time: 1.189s\n",
      "| SGD | epoch: 028 | loss: 0.15778 - R2: 1.0018 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 533  | total loss: \u001b[1m\u001b[32m0.16337\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 029 | loss: 0.16337 - R2: 1.0021 -- iter: 0064/1168\n",
      "Training Step: 534  | total loss: \u001b[1m\u001b[32m0.15861\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 029 | loss: 0.15861 - R2: 1.0027 -- iter: 0128/1168\n",
      "Training Step: 535  | total loss: \u001b[1m\u001b[32m0.16115\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 029 | loss: 0.16115 - R2: 1.0022 -- iter: 0192/1168\n",
      "Training Step: 536  | total loss: \u001b[1m\u001b[32m0.16115\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 029 | loss: 0.16115 - R2: 1.0022 -- iter: 0256/1168\n",
      "Training Step: 537  | total loss: \u001b[1m\u001b[32m0.16190\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 029 | loss: 0.16190 - R2: 1.0009 -- iter: 0320/1168\n",
      "Training Step: 538  | total loss: \u001b[1m\u001b[32m0.17085\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 029 | loss: 0.17085 - R2: 1.0018 -- iter: 0384/1168\n",
      "Training Step: 539  | total loss: \u001b[1m\u001b[32m0.17321\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 029 | loss: 0.17321 - R2: 0.9997 -- iter: 0448/1168\n",
      "Training Step: 540  | total loss: \u001b[1m\u001b[32m0.16130\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 029 | loss: 0.16130 - R2: 1.0023 -- iter: 0512/1168\n",
      "Training Step: 541  | total loss: \u001b[1m\u001b[32m0.16130\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 029 | loss: 0.16130 - R2: 1.0023 -- iter: 0576/1168\n",
      "Training Step: 542  | total loss: \u001b[1m\u001b[32m0.15796\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 029 | loss: 0.15796 - R2: 1.0028 -- iter: 0640/1168\n",
      "Training Step: 543  | total loss: \u001b[1m\u001b[32m0.15826\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 029 | loss: 0.15826 - R2: 1.0020 -- iter: 0704/1168\n",
      "Training Step: 544  | total loss: \u001b[1m\u001b[32m0.15720\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 029 | loss: 0.15720 - R2: 1.0014 -- iter: 0768/1168\n",
      "Training Step: 545  | total loss: \u001b[1m\u001b[32m0.15509\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 029 | loss: 0.15509 - R2: 1.0012 -- iter: 0832/1168\n",
      "Training Step: 546  | total loss: \u001b[1m\u001b[32m0.15256\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 029 | loss: 0.15256 - R2: 1.0020 -- iter: 0896/1168\n",
      "Training Step: 547  | total loss: \u001b[1m\u001b[32m0.15645\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 029 | loss: 0.15645 - R2: 1.0010 -- iter: 0960/1168\n",
      "Training Step: 548  | total loss: \u001b[1m\u001b[32m0.15749\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 029 | loss: 0.15749 - R2: 1.0006 -- iter: 1024/1168\n",
      "Training Step: 549  | total loss: \u001b[1m\u001b[32m0.15687\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 029 | loss: 0.15687 - R2: 1.0026 -- iter: 1088/1168\n",
      "Training Step: 550  | total loss: \u001b[1m\u001b[32m0.15527\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 029 | loss: 0.15527 - R2: 1.0014 -- iter: 1152/1168\n",
      "Training Step: 551  | total loss: \u001b[1m\u001b[32m0.15527\u001b[0m\u001b[0m | time: 1.145s\n",
      "| SGD | epoch: 029 | loss: 0.15527 - R2: 1.0014 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 552  | total loss: \u001b[1m\u001b[32m0.15325\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 030 | loss: 0.15325 - R2: 1.0001 -- iter: 0064/1168\n",
      "Training Step: 553  | total loss: \u001b[1m\u001b[32m0.15101\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 030 | loss: 0.15101 - R2: 1.0000 -- iter: 0128/1168\n",
      "Training Step: 554  | total loss: \u001b[1m\u001b[32m0.15101\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 030 | loss: 0.15101 - R2: 1.0000 -- iter: 0192/1168\n",
      "Training Step: 555  | total loss: \u001b[1m\u001b[32m0.14885\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 030 | loss: 0.14885 - R2: 1.0008 -- iter: 0256/1168\n",
      "Training Step: 556  | total loss: \u001b[1m\u001b[32m0.14799\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 030 | loss: 0.14799 - R2: 1.0000 -- iter: 0320/1168\n",
      "Training Step: 557  | total loss: \u001b[1m\u001b[32m0.14799\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 030 | loss: 0.14799 - R2: 1.0000 -- iter: 0384/1168\n",
      "Training Step: 558  | total loss: \u001b[1m\u001b[32m0.15794\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 030 | loss: 0.15794 - R2: 0.9988 -- iter: 0448/1168\n",
      "Training Step: 559  | total loss: \u001b[1m\u001b[32m0.15763\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 030 | loss: 0.15763 - R2: 1.0008 -- iter: 0512/1168\n",
      "Training Step: 560  | total loss: \u001b[1m\u001b[32m0.15736\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 030 | loss: 0.15736 - R2: 1.0026 -- iter: 0576/1168\n",
      "Training Step: 561  | total loss: \u001b[1m\u001b[32m0.15736\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 030 | loss: 0.15736 - R2: 1.0026 -- iter: 0640/1168\n",
      "Training Step: 562  | total loss: \u001b[1m\u001b[32m0.16009\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 030 | loss: 0.16009 - R2: 1.0019 -- iter: 0704/1168\n",
      "Training Step: 563  | total loss: \u001b[1m\u001b[32m0.16026\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 030 | loss: 0.16026 - R2: 1.0026 -- iter: 0768/1168\n",
      "Training Step: 564  | total loss: \u001b[1m\u001b[32m0.16395\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 030 | loss: 0.16395 - R2: 1.0026 -- iter: 0832/1168\n",
      "Training Step: 565  | total loss: \u001b[1m\u001b[32m0.16518\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 030 | loss: 0.16518 - R2: 1.0027 -- iter: 0896/1168\n",
      "Training Step: 566  | total loss: \u001b[1m\u001b[32m0.16516\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 030 | loss: 0.16516 - R2: 1.0032 -- iter: 0960/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 567  | total loss: \u001b[1m\u001b[32m0.16882\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 030 | loss: 0.16882 - R2: 1.0031 -- iter: 1024/1168\n",
      "Training Step: 568  | total loss: \u001b[1m\u001b[32m0.16882\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 030 | loss: 0.16882 - R2: 1.0031 -- iter: 1088/1168\n",
      "Training Step: 569  | total loss: \u001b[1m\u001b[32m0.17087\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 030 | loss: 0.17087 - R2: 1.0034 -- iter: 1152/1168\n",
      "Training Step: 570  | total loss: \u001b[1m\u001b[32m0.17087\u001b[0m\u001b[0m | time: 1.100s\n",
      "| SGD | epoch: 030 | loss: 0.17087 - R2: 1.0034 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 571  | total loss: \u001b[1m\u001b[32m0.16816\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 031 | loss: 0.16816 - R2: 1.0024 -- iter: 0064/1168\n",
      "Training Step: 572  | total loss: \u001b[1m\u001b[32m0.16786\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 031 | loss: 0.16786 - R2: 1.0024 -- iter: 0128/1168\n",
      "Training Step: 573  | total loss: \u001b[1m\u001b[32m0.16719\u001b[0m\u001b[0m | time: 0.008s\n",
      "| SGD | epoch: 031 | loss: 0.16719 - R2: 1.0022 -- iter: 0192/1168\n",
      "Training Step: 574  | total loss: \u001b[1m\u001b[32m0.17150\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 031 | loss: 0.17150 - R2: 1.0022 -- iter: 0256/1168\n",
      "Training Step: 575  | total loss: \u001b[1m\u001b[32m0.16965\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 031 | loss: 0.16965 - R2: 1.0014 -- iter: 0320/1168\n",
      "Training Step: 576  | total loss: \u001b[1m\u001b[32m0.16965\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 031 | loss: 0.16965 - R2: 1.0014 -- iter: 0384/1168\n",
      "Training Step: 577  | total loss: \u001b[1m\u001b[32m0.17051\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 031 | loss: 0.17051 - R2: 1.0004 -- iter: 0448/1168\n",
      "Training Step: 578  | total loss: \u001b[1m\u001b[32m0.17059\u001b[0m\u001b[0m | time: 0.030s\n",
      "| SGD | epoch: 031 | loss: 0.17059 - R2: 1.0004 -- iter: 0512/1168\n",
      "Training Step: 579  | total loss: \u001b[1m\u001b[32m0.16173\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 031 | loss: 0.16173 - R2: 0.9994 -- iter: 0576/1168\n",
      "Training Step: 580  | total loss: \u001b[1m\u001b[32m0.16173\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 031 | loss: 0.16173 - R2: 0.9994 -- iter: 0640/1168\n",
      "Training Step: 581  | total loss: \u001b[1m\u001b[32m0.15374\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 031 | loss: 0.15374 - R2: 0.9987 -- iter: 0704/1168\n",
      "Training Step: 582  | total loss: \u001b[1m\u001b[32m0.15312\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 031 | loss: 0.15312 - R2: 0.9995 -- iter: 0768/1168\n",
      "Training Step: 583  | total loss: \u001b[1m\u001b[32m0.15674\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 031 | loss: 0.15674 - R2: 1.0001 -- iter: 0832/1168\n",
      "Training Step: 584  | total loss: \u001b[1m\u001b[32m0.15805\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 031 | loss: 0.15805 - R2: 1.0010 -- iter: 0896/1168\n",
      "Training Step: 585  | total loss: \u001b[1m\u001b[32m0.15805\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 031 | loss: 0.15805 - R2: 1.0010 -- iter: 0960/1168\n",
      "Training Step: 586  | total loss: \u001b[1m\u001b[32m0.15020\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 031 | loss: 0.15020 - R2: 1.0004 -- iter: 1024/1168\n",
      "Training Step: 587  | total loss: \u001b[1m\u001b[32m0.15343\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 031 | loss: 0.15343 - R2: 1.0008 -- iter: 1088/1168\n",
      "Training Step: 588  | total loss: \u001b[1m\u001b[32m0.15258\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 031 | loss: 0.15258 - R2: 0.9993 -- iter: 1152/1168\n",
      "Training Step: 589  | total loss: \u001b[1m\u001b[32m0.14989\u001b[0m\u001b[0m | time: 1.074s\n",
      "| SGD | epoch: 031 | loss: 0.14989 - R2: 1.0003 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 590  | total loss: \u001b[1m\u001b[32m0.14989\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 032 | loss: 0.14989 - R2: 1.0003 -- iter: 0064/1168\n",
      "Training Step: 591  | total loss: \u001b[1m\u001b[32m0.15859\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 032 | loss: 0.15859 - R2: 0.9994 -- iter: 0128/1168\n",
      "Training Step: 592  | total loss: \u001b[1m\u001b[32m0.16484\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 032 | loss: 0.16484 - R2: 0.9987 -- iter: 0192/1168\n",
      "Training Step: 593  | total loss: \u001b[1m\u001b[32m0.16643\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 032 | loss: 0.16643 - R2: 0.9995 -- iter: 0256/1168\n",
      "Training Step: 594  | total loss: \u001b[1m\u001b[32m0.16088\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 032 | loss: 0.16088 - R2: 0.9999 -- iter: 0320/1168\n",
      "Training Step: 595  | total loss: \u001b[1m\u001b[32m0.16244\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 032 | loss: 0.16244 - R2: 0.9987 -- iter: 0384/1168\n",
      "Training Step: 596  | total loss: \u001b[1m\u001b[32m0.16362\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 032 | loss: 0.16362 - R2: 0.9985 -- iter: 0448/1168\n",
      "Training Step: 597  | total loss: \u001b[1m\u001b[32m0.16362\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 032 | loss: 0.16362 - R2: 0.9985 -- iter: 0512/1168\n",
      "Training Step: 598  | total loss: \u001b[1m\u001b[32m0.16346\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 032 | loss: 0.16346 - R2: 0.9987 -- iter: 0576/1168\n",
      "Training Step: 599  | total loss: \u001b[1m\u001b[32m0.16245\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 032 | loss: 0.16245 - R2: 0.9991 -- iter: 0640/1168\n",
      "Training Step: 600  | total loss: \u001b[1m\u001b[32m0.16101\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 032 | loss: 0.16101 - R2: 1.0023 -- iter: 0704/1168\n",
      "Training Step: 601  | total loss: \u001b[1m\u001b[32m0.15971\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 032 | loss: 0.15971 - R2: 1.0052 -- iter: 0768/1168\n",
      "Training Step: 602  | total loss: \u001b[1m\u001b[32m0.15655\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 032 | loss: 0.15655 - R2: 1.0055 -- iter: 0832/1168\n",
      "Training Step: 603  | total loss: \u001b[1m\u001b[32m0.15864\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 032 | loss: 0.15864 - R2: 1.0053 -- iter: 0896/1168\n",
      "Training Step: 604  | total loss: \u001b[1m\u001b[32m0.15864\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 032 | loss: 0.15864 - R2: 1.0053 -- iter: 0960/1168\n",
      "Training Step: 605  | total loss: \u001b[1m\u001b[32m0.16138\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 032 | loss: 0.16138 - R2: 1.0039 -- iter: 1024/1168\n",
      "Training Step: 606  | total loss: \u001b[1m\u001b[32m0.16255\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 032 | loss: 0.16255 - R2: 1.0057 -- iter: 1088/1168\n",
      "Training Step: 607  | total loss: \u001b[1m\u001b[32m0.15959\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 032 | loss: 0.15959 - R2: 1.0048 -- iter: 1152/1168\n",
      "Training Step: 608  | total loss: \u001b[1m\u001b[32m0.15677\u001b[0m\u001b[0m | time: 1.167s\n",
      "| SGD | epoch: 032 | loss: 0.15677 - R2: 1.0042 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 609  | total loss: \u001b[1m\u001b[32m0.16267\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 033 | loss: 0.16267 - R2: 1.0033 -- iter: 0064/1168\n",
      "Training Step: 610  | total loss: \u001b[1m\u001b[32m0.16267\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 033 | loss: 0.16267 - R2: 1.0033 -- iter: 0128/1168\n",
      "Training Step: 611  | total loss: \u001b[1m\u001b[32m0.16320\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 033 | loss: 0.16320 - R2: 1.0041 -- iter: 0192/1168\n",
      "Training Step: 612  | total loss: \u001b[1m\u001b[32m0.16188\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 033 | loss: 0.16188 - R2: 1.0035 -- iter: 0256/1168\n",
      "Training Step: 613  | total loss: \u001b[1m\u001b[32m0.16175\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 033 | loss: 0.16175 - R2: 1.0037 -- iter: 0320/1168\n",
      "Training Step: 614  | total loss: \u001b[1m\u001b[32m0.16175\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 033 | loss: 0.16175 - R2: 1.0037 -- iter: 0384/1168\n",
      "Training Step: 615  | total loss: \u001b[1m\u001b[32m0.16352\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 033 | loss: 0.16352 - R2: 1.0035 -- iter: 0448/1168\n",
      "Training Step: 616  | total loss: \u001b[1m\u001b[32m0.16308\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 033 | loss: 0.16308 - R2: 1.0027 -- iter: 0512/1168\n",
      "Training Step: 617  | total loss: \u001b[1m\u001b[32m0.15823\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 033 | loss: 0.15823 - R2: 1.0038 -- iter: 0576/1168\n",
      "Training Step: 618  | total loss: \u001b[1m\u001b[32m0.15823\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 033 | loss: 0.15823 - R2: 1.0038 -- iter: 0640/1168\n",
      "Training Step: 619  | total loss: \u001b[1m\u001b[32m0.16949\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 033 | loss: 0.16949 - R2: 1.0063 -- iter: 0704/1168\n",
      "Training Step: 620  | total loss: \u001b[1m\u001b[32m0.16949\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 033 | loss: 0.16949 - R2: 1.0063 -- iter: 0768/1168\n",
      "Training Step: 621  | total loss: \u001b[1m\u001b[32m0.17365\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 033 | loss: 0.17365 - R2: 1.0076 -- iter: 0832/1168\n",
      "Training Step: 622  | total loss: \u001b[1m\u001b[32m0.17365\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 033 | loss: 0.17365 - R2: 1.0071 -- iter: 0896/1168\n",
      "Training Step: 623  | total loss: \u001b[1m\u001b[32m0.17244\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 033 | loss: 0.17244 - R2: 1.0055 -- iter: 0960/1168\n",
      "Training Step: 624  | total loss: \u001b[1m\u001b[32m0.17244\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 033 | loss: 0.17244 - R2: 1.0055 -- iter: 1024/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 625  | total loss: \u001b[1m\u001b[32m0.16619\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 033 | loss: 0.16619 - R2: 1.0040 -- iter: 1088/1168\n",
      "Training Step: 626  | total loss: \u001b[1m\u001b[32m0.16619\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 033 | loss: 0.16619 - R2: 1.0040 -- iter: 1152/1168\n",
      "Training Step: 627  | total loss: \u001b[1m\u001b[32m0.16463\u001b[0m\u001b[0m | time: 1.158s\n",
      "| SGD | epoch: 033 | loss: 0.16463 - R2: 1.0039 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 628  | total loss: \u001b[1m\u001b[32m0.16616\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 034 | loss: 0.16616 - R2: 1.0062 -- iter: 0064/1168\n",
      "Training Step: 629  | total loss: \u001b[1m\u001b[32m0.16164\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 034 | loss: 0.16164 - R2: 1.0067 -- iter: 0128/1168\n",
      "Training Step: 630  | total loss: \u001b[1m\u001b[32m0.16164\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 034 | loss: 0.16164 - R2: 1.0067 -- iter: 0192/1168\n",
      "Training Step: 631  | total loss: \u001b[1m\u001b[32m0.16329\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 034 | loss: 0.16329 - R2: 1.0051 -- iter: 0256/1168\n",
      "Training Step: 632  | total loss: \u001b[1m\u001b[32m0.15940\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 034 | loss: 0.15940 - R2: 1.0036 -- iter: 0320/1168\n",
      "Training Step: 633  | total loss: \u001b[1m\u001b[32m0.16156\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 034 | loss: 0.16156 - R2: 1.0030 -- iter: 0384/1168\n",
      "Training Step: 634  | total loss: \u001b[1m\u001b[32m0.15855\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 034 | loss: 0.15855 - R2: 1.0022 -- iter: 0448/1168\n",
      "Training Step: 635  | total loss: \u001b[1m\u001b[32m0.16242\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 034 | loss: 0.16242 - R2: 1.0032 -- iter: 0512/1168\n",
      "Training Step: 636  | total loss: \u001b[1m\u001b[32m0.17211\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 034 | loss: 0.17211 - R2: 1.0017 -- iter: 0576/1168\n",
      "Training Step: 637  | total loss: \u001b[1m\u001b[32m0.17471\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 034 | loss: 0.17471 - R2: 1.0013 -- iter: 0640/1168\n",
      "Training Step: 638  | total loss: \u001b[1m\u001b[32m0.17428\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 034 | loss: 0.17428 - R2: 1.0017 -- iter: 0704/1168\n",
      "Training Step: 639  | total loss: \u001b[1m\u001b[32m0.17428\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 034 | loss: 0.17428 - R2: 1.0017 -- iter: 0768/1168\n",
      "Training Step: 640  | total loss: \u001b[1m\u001b[32m0.16975\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 034 | loss: 0.16975 - R2: 1.0013 -- iter: 0832/1168\n",
      "Training Step: 641  | total loss: \u001b[1m\u001b[32m0.16809\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 034 | loss: 0.16809 - R2: 1.0001 -- iter: 0896/1168\n",
      "Training Step: 642  | total loss: \u001b[1m\u001b[32m0.16809\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 034 | loss: 0.16809 - R2: 1.0001 -- iter: 0960/1168\n",
      "Training Step: 643  | total loss: \u001b[1m\u001b[32m0.16423\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 034 | loss: 0.16423 - R2: 1.0002 -- iter: 1024/1168\n",
      "Training Step: 644  | total loss: \u001b[1m\u001b[32m0.15835\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 034 | loss: 0.15835 - R2: 1.0006 -- iter: 1088/1168\n",
      "Training Step: 645  | total loss: \u001b[1m\u001b[32m0.15596\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 034 | loss: 0.15596 - R2: 0.9994 -- iter: 1152/1168\n",
      "Training Step: 646  | total loss: \u001b[1m\u001b[32m0.15683\u001b[0m\u001b[0m | time: 1.146s\n",
      "| SGD | epoch: 034 | loss: 0.15683 - R2: 0.9997 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 647  | total loss: \u001b[1m\u001b[32m0.15683\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 035 | loss: 0.15683 - R2: 0.9997 -- iter: 0064/1168\n",
      "Training Step: 648  | total loss: \u001b[1m\u001b[32m0.15969\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 035 | loss: 0.15969 - R2: 0.9983 -- iter: 0128/1168\n",
      "Training Step: 649  | total loss: \u001b[1m\u001b[32m0.15799\u001b[0m\u001b[0m | time: 0.010s\n",
      "| SGD | epoch: 035 | loss: 0.15799 - R2: 0.9983 -- iter: 0192/1168\n",
      "Training Step: 650  | total loss: \u001b[1m\u001b[32m0.15609\u001b[0m\u001b[0m | time: 0.013s\n",
      "| SGD | epoch: 035 | loss: 0.15609 - R2: 0.9976 -- iter: 0256/1168\n",
      "Training Step: 651  | total loss: \u001b[1m\u001b[32m0.15563\u001b[0m\u001b[0m | time: 0.019s\n",
      "| SGD | epoch: 035 | loss: 0.15563 - R2: 1.0000 -- iter: 0320/1168\n",
      "Training Step: 652  | total loss: \u001b[1m\u001b[32m0.15563\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 035 | loss: 0.15563 - R2: 1.0000 -- iter: 0384/1168\n",
      "Training Step: 653  | total loss: \u001b[1m\u001b[32m0.16120\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 035 | loss: 0.16120 - R2: 1.0030 -- iter: 0448/1168\n",
      "Training Step: 654  | total loss: \u001b[1m\u001b[32m0.16120\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 035 | loss: 0.16120 - R2: 1.0030 -- iter: 0512/1168\n",
      "Training Step: 655  | total loss: \u001b[1m\u001b[32m0.15569\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 035 | loss: 0.15569 - R2: 1.0030 -- iter: 0576/1168\n",
      "Training Step: 656  | total loss: \u001b[1m\u001b[32m0.15569\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 035 | loss: 0.15569 - R2: 1.0030 -- iter: 0640/1168\n",
      "Training Step: 657  | total loss: \u001b[1m\u001b[32m0.16111\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 035 | loss: 0.16111 - R2: 1.0034 -- iter: 0704/1168\n",
      "Training Step: 658  | total loss: \u001b[1m\u001b[32m0.16445\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 035 | loss: 0.16445 - R2: 1.0025 -- iter: 0768/1168\n",
      "Training Step: 659  | total loss: \u001b[1m\u001b[32m0.16914\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 035 | loss: 0.16914 - R2: 1.0042 -- iter: 0832/1168\n",
      "Training Step: 660  | total loss: \u001b[1m\u001b[32m0.16914\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 035 | loss: 0.16914 - R2: 1.0042 -- iter: 0896/1168\n",
      "Training Step: 661  | total loss: \u001b[1m\u001b[32m0.17337\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 035 | loss: 0.17337 - R2: 1.0058 -- iter: 0960/1168\n",
      "Training Step: 662  | total loss: \u001b[1m\u001b[32m0.17638\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 035 | loss: 0.17638 - R2: 1.0041 -- iter: 1024/1168\n",
      "Training Step: 663  | total loss: \u001b[1m\u001b[32m0.17784\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 035 | loss: 0.17784 - R2: 1.0041 -- iter: 1088/1168\n",
      "Training Step: 664  | total loss: \u001b[1m\u001b[32m0.17331\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 035 | loss: 0.17331 - R2: 1.0031 -- iter: 1152/1168\n",
      "Training Step: 665  | total loss: \u001b[1m\u001b[32m0.17040\u001b[0m\u001b[0m | time: 1.085s\n",
      "| SGD | epoch: 035 | loss: 0.17040 - R2: 1.0015 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 666  | total loss: \u001b[1m\u001b[32m0.16603\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 036 | loss: 0.16603 - R2: 1.0005 -- iter: 0064/1168\n",
      "Training Step: 667  | total loss: \u001b[1m\u001b[32m0.16603\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 036 | loss: 0.16603 - R2: 1.0005 -- iter: 0128/1168\n",
      "Training Step: 668  | total loss: \u001b[1m\u001b[32m0.17138\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 036 | loss: 0.17138 - R2: 0.9994 -- iter: 0192/1168\n",
      "Training Step: 669  | total loss: \u001b[1m\u001b[32m0.17208\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 036 | loss: 0.17208 - R2: 0.9988 -- iter: 0256/1168\n",
      "Training Step: 670  | total loss: \u001b[1m\u001b[32m0.17208\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 036 | loss: 0.17208 - R2: 0.9984 -- iter: 0320/1168\n",
      "Training Step: 671  | total loss: \u001b[1m\u001b[32m0.16880\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 036 | loss: 0.16880 - R2: 0.9984 -- iter: 0384/1168\n",
      "Training Step: 672  | total loss: \u001b[1m\u001b[32m0.16810\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 036 | loss: 0.16810 - R2: 0.9990 -- iter: 0448/1168\n",
      "Training Step: 673  | total loss: \u001b[1m\u001b[32m0.16835\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 036 | loss: 0.16835 - R2: 0.9990 -- iter: 0512/1168\n",
      "Training Step: 674  | total loss: \u001b[1m\u001b[32m0.16527\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 036 | loss: 0.16527 - R2: 1.0005 -- iter: 0576/1168\n",
      "Training Step: 675  | total loss: \u001b[1m\u001b[32m0.16409\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 036 | loss: 0.16409 - R2: 0.9999 -- iter: 0640/1168\n",
      "Training Step: 676  | total loss: \u001b[1m\u001b[32m0.16057\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 036 | loss: 0.16057 - R2: 1.0006 -- iter: 0704/1168\n",
      "Training Step: 677  | total loss: \u001b[1m\u001b[32m0.16057\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 036 | loss: 0.16057 - R2: 1.0006 -- iter: 0768/1168\n",
      "Training Step: 678  | total loss: \u001b[1m\u001b[32m0.15934\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 036 | loss: 0.15934 - R2: 0.9998 -- iter: 0832/1168\n",
      "Training Step: 679  | total loss: \u001b[1m\u001b[32m0.16455\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 036 | loss: 0.16455 - R2: 1.0022 -- iter: 0896/1168\n",
      "Training Step: 680  | total loss: \u001b[1m\u001b[32m0.16455\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 036 | loss: 0.16455 - R2: 1.0022 -- iter: 0960/1168\n",
      "Training Step: 681  | total loss: \u001b[1m\u001b[32m0.16649\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 036 | loss: 0.16649 - R2: 1.0023 -- iter: 1024/1168\n",
      "Training Step: 682  | total loss: \u001b[1m\u001b[32m0.16790\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 036 | loss: 0.16790 - R2: 1.0014 -- iter: 1088/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 683  | total loss: \u001b[1m\u001b[32m0.16525\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 036 | loss: 0.16525 - R2: 1.0019 -- iter: 1152/1168\n",
      "Training Step: 684  | total loss: \u001b[1m\u001b[32m0.16525\u001b[0m\u001b[0m | time: 1.138s\n",
      "| SGD | epoch: 036 | loss: 0.16525 - R2: 1.0019 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 685  | total loss: \u001b[1m\u001b[32m0.16571\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 037 | loss: 0.16571 - R2: 1.0021 -- iter: 0064/1168\n",
      "Training Step: 686  | total loss: \u001b[1m\u001b[32m0.16565\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 037 | loss: 0.16565 - R2: 1.0015 -- iter: 0128/1168\n",
      "Training Step: 687  | total loss: \u001b[1m\u001b[32m0.16878\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 037 | loss: 0.16878 - R2: 1.0023 -- iter: 0192/1168\n",
      "Training Step: 688  | total loss: \u001b[1m\u001b[32m0.16687\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 037 | loss: 0.16687 - R2: 1.0025 -- iter: 0256/1168\n",
      "Training Step: 689  | total loss: \u001b[1m\u001b[32m0.16490\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 037 | loss: 0.16490 - R2: 1.0016 -- iter: 0320/1168\n",
      "Training Step: 690  | total loss: \u001b[1m\u001b[32m0.15777\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 037 | loss: 0.15777 - R2: 1.0020 -- iter: 0384/1168\n",
      "Training Step: 691  | total loss: \u001b[1m\u001b[32m0.16401\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 037 | loss: 0.16401 - R2: 1.0034 -- iter: 0448/1168\n",
      "Training Step: 692  | total loss: \u001b[1m\u001b[32m0.16401\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 037 | loss: 0.16401 - R2: 1.0034 -- iter: 0512/1168\n",
      "Training Step: 693  | total loss: \u001b[1m\u001b[32m0.16430\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 037 | loss: 0.16430 - R2: 1.0029 -- iter: 0576/1168\n",
      "Training Step: 694  | total loss: \u001b[1m\u001b[32m0.16469\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 037 | loss: 0.16469 - R2: 1.0030 -- iter: 0640/1168\n",
      "Training Step: 695  | total loss: \u001b[1m\u001b[32m0.16649\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 037 | loss: 0.16649 - R2: 1.0016 -- iter: 0704/1168\n",
      "Training Step: 696  | total loss: \u001b[1m\u001b[32m0.16434\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 037 | loss: 0.16434 - R2: 1.0017 -- iter: 0768/1168\n",
      "Training Step: 697  | total loss: \u001b[1m\u001b[32m0.16434\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 037 | loss: 0.16434 - R2: 1.0017 -- iter: 0832/1168\n",
      "Training Step: 698  | total loss: \u001b[1m\u001b[32m0.15642\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 037 | loss: 0.15642 - R2: 1.0024 -- iter: 0896/1168\n",
      "Training Step: 699  | total loss: \u001b[1m\u001b[32m0.15456\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 037 | loss: 0.15456 - R2: 1.0009 -- iter: 0960/1168\n",
      "Training Step: 700  | total loss: \u001b[1m\u001b[32m0.15456\u001b[0m\u001b[0m | time: 0.169s\n",
      "| SGD | epoch: 037 | loss: 0.15456 - R2: 1.0009 -- iter: 1024/1168\n",
      "Training Step: 701  | total loss: \u001b[1m\u001b[32m0.15606\u001b[0m\u001b[0m | time: 0.174s\n",
      "| SGD | epoch: 037 | loss: 0.15606 - R2: 0.9982 -- iter: 1088/1168\n",
      "Training Step: 702  | total loss: \u001b[1m\u001b[32m0.16374\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 037 | loss: 0.16374 - R2: 0.9979 -- iter: 1152/1168\n",
      "Training Step: 703  | total loss: \u001b[1m\u001b[32m0.16499\u001b[0m\u001b[0m | time: 1.184s\n",
      "| SGD | epoch: 037 | loss: 0.16499 - R2: 0.9981 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 704  | total loss: \u001b[1m\u001b[32m0.16659\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 038 | loss: 0.16659 - R2: 0.9965 -- iter: 0064/1168\n",
      "Training Step: 705  | total loss: \u001b[1m\u001b[32m0.16659\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 038 | loss: 0.16659 - R2: 0.9965 -- iter: 0128/1168\n",
      "Training Step: 706  | total loss: \u001b[1m\u001b[32m0.16721\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 038 | loss: 0.16721 - R2: 0.9993 -- iter: 0192/1168\n",
      "Training Step: 707  | total loss: \u001b[1m\u001b[32m0.16721\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 038 | loss: 0.16721 - R2: 0.9993 -- iter: 0256/1168\n",
      "Training Step: 708  | total loss: \u001b[1m\u001b[32m0.17544\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 038 | loss: 0.17544 - R2: 0.9993 -- iter: 0320/1168\n",
      "Training Step: 709  | total loss: \u001b[1m\u001b[32m0.17544\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 038 | loss: 0.17544 - R2: 0.9993 -- iter: 0384/1168\n",
      "Training Step: 710  | total loss: \u001b[1m\u001b[32m0.17751\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 038 | loss: 0.17751 - R2: 0.9985 -- iter: 0448/1168\n",
      "Training Step: 711  | total loss: \u001b[1m\u001b[32m0.17773\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 038 | loss: 0.17773 - R2: 0.9985 -- iter: 0512/1168\n",
      "Training Step: 712  | total loss: \u001b[1m\u001b[32m0.17186\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 038 | loss: 0.17186 - R2: 0.9983 -- iter: 0576/1168\n",
      "Training Step: 713  | total loss: \u001b[1m\u001b[32m0.17417\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 038 | loss: 0.17417 - R2: 0.9982 -- iter: 0640/1168\n",
      "Training Step: 714  | total loss: \u001b[1m\u001b[32m0.17505\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 038 | loss: 0.17505 - R2: 0.9981 -- iter: 0704/1168\n",
      "Training Step: 715  | total loss: \u001b[1m\u001b[32m0.17135\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 038 | loss: 0.17135 - R2: 0.9998 -- iter: 0768/1168\n",
      "Training Step: 716  | total loss: \u001b[1m\u001b[32m0.16964\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 038 | loss: 0.16964 - R2: 0.9998 -- iter: 0832/1168\n",
      "Training Step: 717  | total loss: \u001b[1m\u001b[32m0.16558\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 038 | loss: 0.16558 - R2: 1.0008 -- iter: 0896/1168\n",
      "Training Step: 718  | total loss: \u001b[1m\u001b[32m0.16331\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 038 | loss: 0.16331 - R2: 1.0002 -- iter: 0960/1168\n",
      "Training Step: 719  | total loss: \u001b[1m\u001b[32m0.16010\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 038 | loss: 0.16010 - R2: 1.0013 -- iter: 1024/1168\n",
      "Training Step: 720  | total loss: \u001b[1m\u001b[32m0.17004\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 038 | loss: 0.17004 - R2: 0.9988 -- iter: 1088/1168\n",
      "Training Step: 721  | total loss: \u001b[1m\u001b[32m0.17004\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 038 | loss: 0.17004 - R2: 0.9988 -- iter: 1152/1168\n",
      "Training Step: 722  | total loss: \u001b[1m\u001b[32m0.16949\u001b[0m\u001b[0m | time: 1.120s\n",
      "| SGD | epoch: 038 | loss: 0.16949 - R2: 0.9997 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 723  | total loss: \u001b[1m\u001b[32m0.16552\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 039 | loss: 0.16552 - R2: 0.9997 -- iter: 0064/1168\n",
      "Training Step: 724  | total loss: \u001b[1m\u001b[32m0.16939\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 039 | loss: 0.16939 - R2: 0.9987 -- iter: 0128/1168\n",
      "Training Step: 725  | total loss: \u001b[1m\u001b[32m0.16939\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 039 | loss: 0.16939 - R2: 0.9987 -- iter: 0192/1168\n",
      "Training Step: 726  | total loss: \u001b[1m\u001b[32m0.16630\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 039 | loss: 0.16630 - R2: 0.9982 -- iter: 0256/1168\n",
      "Training Step: 727  | total loss: \u001b[1m\u001b[32m0.15963\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 039 | loss: 0.15963 - R2: 0.9994 -- iter: 0320/1168\n",
      "Training Step: 728  | total loss: \u001b[1m\u001b[32m0.15963\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 039 | loss: 0.15963 - R2: 0.9994 -- iter: 0384/1168\n",
      "Training Step: 729  | total loss: \u001b[1m\u001b[32m0.15871\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 039 | loss: 0.15871 - R2: 0.9987 -- iter: 0448/1168\n",
      "Training Step: 730  | total loss: \u001b[1m\u001b[32m0.15549\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 039 | loss: 0.15549 - R2: 0.9990 -- iter: 0512/1168\n",
      "Training Step: 731  | total loss: \u001b[1m\u001b[32m0.16094\u001b[0m\u001b[0m | time: 0.137s\n",
      "| SGD | epoch: 039 | loss: 0.16094 - R2: 0.9993 -- iter: 0576/1168\n",
      "Training Step: 732  | total loss: \u001b[1m\u001b[32m0.16039\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 039 | loss: 0.16039 - R2: 0.9992 -- iter: 0640/1168\n",
      "Training Step: 733  | total loss: \u001b[1m\u001b[32m0.16615\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 039 | loss: 0.16615 - R2: 1.0005 -- iter: 0704/1168\n",
      "Training Step: 734  | total loss: \u001b[1m\u001b[32m0.16046\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 039 | loss: 0.16046 - R2: 1.0003 -- iter: 0768/1168\n",
      "Training Step: 735  | total loss: \u001b[1m\u001b[32m0.15966\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 039 | loss: 0.15966 - R2: 1.0011 -- iter: 0832/1168\n",
      "Training Step: 736  | total loss: \u001b[1m\u001b[32m0.16277\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 039 | loss: 0.16277 - R2: 0.9998 -- iter: 0896/1168\n",
      "Training Step: 737  | total loss: \u001b[1m\u001b[32m0.16262\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 039 | loss: 0.16262 - R2: 1.0004 -- iter: 0960/1168\n",
      "Training Step: 738  | total loss: \u001b[1m\u001b[32m0.16492\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 039 | loss: 0.16492 - R2: 1.0012 -- iter: 1024/1168\n",
      "Training Step: 739  | total loss: \u001b[1m\u001b[32m0.16330\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 039 | loss: 0.16330 - R2: 1.0022 -- iter: 1088/1168\n",
      "Training Step: 740  | total loss: \u001b[1m\u001b[32m0.16330\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 039 | loss: 0.16330 - R2: 1.0022 -- iter: 1152/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 741  | total loss: \u001b[1m\u001b[32m0.16185\u001b[0m\u001b[0m | time: 1.164s\n",
      "| SGD | epoch: 039 | loss: 0.16185 - R2: 1.0032 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 742  | total loss: \u001b[1m\u001b[32m0.16173\u001b[0m\u001b[0m | time: 0.007s\n",
      "| SGD | epoch: 040 | loss: 0.16173 - R2: 1.0019 -- iter: 0064/1168\n",
      "Training Step: 743  | total loss: \u001b[1m\u001b[32m0.15819\u001b[0m\u001b[0m | time: 0.009s\n",
      "| SGD | epoch: 040 | loss: 0.15819 - R2: 1.0027 -- iter: 0128/1168\n",
      "Training Step: 744  | total loss: \u001b[1m\u001b[32m0.16238\u001b[0m\u001b[0m | time: 0.011s\n",
      "| SGD | epoch: 040 | loss: 0.16238 - R2: 1.0038 -- iter: 0192/1168\n",
      "Training Step: 745  | total loss: \u001b[1m\u001b[32m0.15985\u001b[0m\u001b[0m | time: 0.013s\n",
      "| SGD | epoch: 040 | loss: 0.15985 - R2: 1.0026 -- iter: 0256/1168\n",
      "Training Step: 746  | total loss: \u001b[1m\u001b[32m0.16961\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 040 | loss: 0.16961 - R2: 1.0009 -- iter: 0320/1168\n",
      "Training Step: 747  | total loss: \u001b[1m\u001b[32m0.16604\u001b[0m\u001b[0m | time: 0.017s\n",
      "| SGD | epoch: 040 | loss: 0.16604 - R2: 1.0014 -- iter: 0384/1168\n",
      "Training Step: 748  | total loss: \u001b[1m\u001b[32m0.16570\u001b[0m\u001b[0m | time: 0.019s\n",
      "| SGD | epoch: 040 | loss: 0.16570 - R2: 1.0014 -- iter: 0448/1168\n",
      "Training Step: 749  | total loss: \u001b[1m\u001b[32m0.16553\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 040 | loss: 0.16553 - R2: 1.0014 -- iter: 0512/1168\n",
      "Training Step: 750  | total loss: \u001b[1m\u001b[32m0.16131\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 040 | loss: 0.16131 - R2: 1.0027 -- iter: 0576/1168\n",
      "Training Step: 751  | total loss: \u001b[1m\u001b[32m0.16443\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 040 | loss: 0.16443 - R2: 1.0027 -- iter: 0640/1168\n",
      "Training Step: 752  | total loss: \u001b[1m\u001b[32m0.16138\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 040 | loss: 0.16138 - R2: 1.0021 -- iter: 0704/1168\n",
      "Training Step: 753  | total loss: \u001b[1m\u001b[32m0.16149\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 040 | loss: 0.16149 - R2: 1.0028 -- iter: 0768/1168\n",
      "Training Step: 754  | total loss: \u001b[1m\u001b[32m0.16127\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 040 | loss: 0.16127 - R2: 1.0027 -- iter: 0832/1168\n",
      "Training Step: 755  | total loss: \u001b[1m\u001b[32m0.15904\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 040 | loss: 0.15904 - R2: 1.0005 -- iter: 0896/1168\n",
      "Training Step: 756  | total loss: \u001b[1m\u001b[32m0.16634\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 040 | loss: 0.16634 - R2: 1.0005 -- iter: 0960/1168\n",
      "Training Step: 757  | total loss: \u001b[1m\u001b[32m0.16930\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 040 | loss: 0.16930 - R2: 0.9984 -- iter: 1024/1168\n",
      "Training Step: 758  | total loss: \u001b[1m\u001b[32m0.16930\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 040 | loss: 0.16930 - R2: 0.9984 -- iter: 1088/1168\n",
      "Training Step: 759  | total loss: \u001b[1m\u001b[32m0.16415\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 040 | loss: 0.16415 - R2: 1.0003 -- iter: 1152/1168\n",
      "Training Step: 760  | total loss: \u001b[1m\u001b[32m0.16897\u001b[0m\u001b[0m | time: 1.057s\n",
      "| SGD | epoch: 040 | loss: 0.16897 - R2: 1.0022 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 761  | total loss: \u001b[1m\u001b[32m0.17032\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 041 | loss: 0.17032 - R2: 1.0038 -- iter: 0064/1168\n",
      "Training Step: 762  | total loss: \u001b[1m\u001b[32m0.17032\u001b[0m\u001b[0m | time: 0.023s\n",
      "| SGD | epoch: 041 | loss: 0.17032 - R2: 1.0038 -- iter: 0128/1168\n",
      "Training Step: 763  | total loss: \u001b[1m\u001b[32m0.16468\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 041 | loss: 0.16468 - R2: 1.0028 -- iter: 0192/1168\n",
      "Training Step: 764  | total loss: \u001b[1m\u001b[32m0.16850\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 041 | loss: 0.16850 - R2: 1.0043 -- iter: 0256/1168\n",
      "Training Step: 765  | total loss: \u001b[1m\u001b[32m0.16850\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 041 | loss: 0.16850 - R2: 1.0043 -- iter: 0320/1168\n",
      "Training Step: 766  | total loss: \u001b[1m\u001b[32m0.17134\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 041 | loss: 0.17134 - R2: 1.0019 -- iter: 0384/1168\n",
      "Training Step: 767  | total loss: \u001b[1m\u001b[32m0.17047\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 041 | loss: 0.17047 - R2: 1.0018 -- iter: 0448/1168\n",
      "Training Step: 768  | total loss: \u001b[1m\u001b[32m0.16998\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 041 | loss: 0.16998 - R2: 1.0021 -- iter: 0512/1168\n",
      "Training Step: 769  | total loss: \u001b[1m\u001b[32m0.16998\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 041 | loss: 0.16998 - R2: 1.0021 -- iter: 0576/1168\n",
      "Training Step: 770  | total loss: \u001b[1m\u001b[32m0.16593\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 041 | loss: 0.16593 - R2: 1.0021 -- iter: 0640/1168\n",
      "Training Step: 771  | total loss: \u001b[1m\u001b[32m0.16555\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 041 | loss: 0.16555 - R2: 1.0030 -- iter: 0704/1168\n",
      "Training Step: 772  | total loss: \u001b[1m\u001b[32m0.16639\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 041 | loss: 0.16639 - R2: 1.0031 -- iter: 0768/1168\n",
      "Training Step: 773  | total loss: \u001b[1m\u001b[32m0.16269\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 041 | loss: 0.16269 - R2: 1.0032 -- iter: 0832/1168\n",
      "Training Step: 774  | total loss: \u001b[1m\u001b[32m0.16509\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 041 | loss: 0.16509 - R2: 1.0027 -- iter: 0896/1168\n",
      "Training Step: 775  | total loss: \u001b[1m\u001b[32m0.16477\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 041 | loss: 0.16477 - R2: 1.0004 -- iter: 0960/1168\n",
      "Training Step: 776  | total loss: \u001b[1m\u001b[32m0.16477\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 041 | loss: 0.16477 - R2: 1.0004 -- iter: 1024/1168\n",
      "Training Step: 777  | total loss: \u001b[1m\u001b[32m0.16381\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 041 | loss: 0.16381 - R2: 1.0024 -- iter: 1088/1168\n",
      "Training Step: 778  | total loss: \u001b[1m\u001b[32m0.16200\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 041 | loss: 0.16200 - R2: 1.0025 -- iter: 1152/1168\n",
      "Training Step: 779  | total loss: \u001b[1m\u001b[32m0.16709\u001b[0m\u001b[0m | time: 1.109s\n",
      "| SGD | epoch: 041 | loss: 0.16709 - R2: 1.0024 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 780  | total loss: \u001b[1m\u001b[32m0.17582\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 042 | loss: 0.17582 - R2: 1.0068 -- iter: 0064/1168\n",
      "Training Step: 781  | total loss: \u001b[1m\u001b[32m0.17582\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 042 | loss: 0.17582 - R2: 1.0068 -- iter: 0128/1168\n",
      "Training Step: 782  | total loss: \u001b[1m\u001b[32m0.17121\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 042 | loss: 0.17121 - R2: 1.0045 -- iter: 0192/1168\n",
      "Training Step: 783  | total loss: \u001b[1m\u001b[32m0.17121\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 042 | loss: 0.17121 - R2: 1.0045 -- iter: 0256/1168\n",
      "Training Step: 784  | total loss: \u001b[1m\u001b[32m0.17200\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 042 | loss: 0.17200 - R2: 1.0032 -- iter: 0320/1168\n",
      "Training Step: 785  | total loss: \u001b[1m\u001b[32m0.17200\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 042 | loss: 0.17200 - R2: 1.0032 -- iter: 0384/1168\n",
      "Training Step: 786  | total loss: \u001b[1m\u001b[32m0.17840\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 042 | loss: 0.17840 - R2: 1.0033 -- iter: 0448/1168\n",
      "Training Step: 787  | total loss: \u001b[1m\u001b[32m0.17812\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 042 | loss: 0.17812 - R2: 1.0022 -- iter: 0512/1168\n",
      "Training Step: 788  | total loss: \u001b[1m\u001b[32m0.17681\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 042 | loss: 0.17681 - R2: 1.0026 -- iter: 0576/1168\n",
      "Training Step: 789  | total loss: \u001b[1m\u001b[32m0.17426\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 042 | loss: 0.17426 - R2: 1.0010 -- iter: 0640/1168\n",
      "Training Step: 790  | total loss: \u001b[1m\u001b[32m0.17378\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 042 | loss: 0.17378 - R2: 1.0018 -- iter: 0704/1168\n",
      "Training Step: 791  | total loss: \u001b[1m\u001b[32m0.17378\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 042 | loss: 0.17378 - R2: 1.0018 -- iter: 0768/1168\n",
      "Training Step: 792  | total loss: \u001b[1m\u001b[32m0.16940\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 042 | loss: 0.16940 - R2: 1.0007 -- iter: 0832/1168\n",
      "Training Step: 793  | total loss: \u001b[1m\u001b[32m0.17236\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 042 | loss: 0.17236 - R2: 1.0010 -- iter: 0896/1168\n",
      "Training Step: 794  | total loss: \u001b[1m\u001b[32m0.16788\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 042 | loss: 0.16788 - R2: 1.0010 -- iter: 0960/1168\n",
      "Training Step: 795  | total loss: \u001b[1m\u001b[32m0.16758\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 042 | loss: 0.16758 - R2: 1.0018 -- iter: 1024/1168\n",
      "Training Step: 796  | total loss: \u001b[1m\u001b[32m0.17624\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 042 | loss: 0.17624 - R2: 0.9994 -- iter: 1088/1168\n",
      "Training Step: 797  | total loss: \u001b[1m\u001b[32m0.17046\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 042 | loss: 0.17046 - R2: 0.9997 -- iter: 1152/1168\n",
      "Training Step: 798  | total loss: \u001b[1m\u001b[32m0.17094\u001b[0m\u001b[0m | time: 1.131s\n",
      "| SGD | epoch: 042 | loss: 0.17094 - R2: 1.0009 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 799  | total loss: \u001b[1m\u001b[32m0.17168\u001b[0m\u001b[0m | time: 0.017s\n",
      "| SGD | epoch: 043 | loss: 0.17168 - R2: 0.9997 -- iter: 0064/1168\n",
      "Training Step: 800  | total loss: \u001b[1m\u001b[32m0.17934\u001b[0m\u001b[0m | time: 0.020s\n",
      "| SGD | epoch: 043 | loss: 0.17934 - R2: 0.9971 -- iter: 0128/1168\n",
      "Training Step: 801  | total loss: \u001b[1m\u001b[32m0.18624\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 043 | loss: 0.18624 - R2: 0.9948 -- iter: 0192/1168\n",
      "Training Step: 802  | total loss: \u001b[1m\u001b[32m0.18438\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 043 | loss: 0.18438 - R2: 0.9960 -- iter: 0256/1168\n",
      "Training Step: 803  | total loss: \u001b[1m\u001b[32m0.18438\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 043 | loss: 0.18438 - R2: 0.9958 -- iter: 0320/1168\n",
      "Training Step: 804  | total loss: \u001b[1m\u001b[32m0.17353\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 043 | loss: 0.17353 - R2: 0.9970 -- iter: 0384/1168\n",
      "Training Step: 805  | total loss: \u001b[1m\u001b[32m0.17104\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 043 | loss: 0.17104 - R2: 0.9992 -- iter: 0448/1168\n",
      "Training Step: 806  | total loss: \u001b[1m\u001b[32m0.17104\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 043 | loss: 0.17104 - R2: 0.9992 -- iter: 0512/1168\n",
      "Training Step: 807  | total loss: \u001b[1m\u001b[32m0.17105\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 043 | loss: 0.17105 - R2: 0.9999 -- iter: 0576/1168\n",
      "Training Step: 808  | total loss: \u001b[1m\u001b[32m0.17044\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 043 | loss: 0.17044 - R2: 1.0005 -- iter: 0640/1168\n",
      "Training Step: 809  | total loss: \u001b[1m\u001b[32m0.16405\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 043 | loss: 0.16405 - R2: 1.0009 -- iter: 0704/1168\n",
      "Training Step: 810  | total loss: \u001b[1m\u001b[32m0.16405\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 043 | loss: 0.16405 - R2: 1.0009 -- iter: 0768/1168\n",
      "Training Step: 811  | total loss: \u001b[1m\u001b[32m0.16874\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 043 | loss: 0.16874 - R2: 0.9997 -- iter: 0832/1168\n",
      "Training Step: 812  | total loss: \u001b[1m\u001b[32m0.16874\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 043 | loss: 0.16874 - R2: 0.9999 -- iter: 0896/1168\n",
      "Training Step: 813  | total loss: \u001b[1m\u001b[32m0.17273\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 043 | loss: 0.17273 - R2: 0.9994 -- iter: 0960/1168\n",
      "Training Step: 814  | total loss: \u001b[1m\u001b[32m0.17623\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 043 | loss: 0.17623 - R2: 1.0006 -- iter: 1024/1168\n",
      "Training Step: 815  | total loss: \u001b[1m\u001b[32m0.17623\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 043 | loss: 0.17623 - R2: 1.0006 -- iter: 1088/1168\n",
      "Training Step: 816  | total loss: \u001b[1m\u001b[32m0.17882\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 043 | loss: 0.17882 - R2: 1.0015 -- iter: 1152/1168\n",
      "Training Step: 817  | total loss: \u001b[1m\u001b[32m0.17475\u001b[0m\u001b[0m | time: 1.089s\n",
      "| SGD | epoch: 043 | loss: 0.17475 - R2: 1.0003 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 818  | total loss: \u001b[1m\u001b[32m0.17475\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 044 | loss: 0.17475 - R2: 1.0003 -- iter: 0064/1168\n",
      "Training Step: 819  | total loss: \u001b[1m\u001b[32m0.17076\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 044 | loss: 0.17076 - R2: 0.9998 -- iter: 0128/1168\n",
      "Training Step: 820  | total loss: \u001b[1m\u001b[32m0.17532\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 044 | loss: 0.17532 - R2: 1.0004 -- iter: 0192/1168\n",
      "Training Step: 821  | total loss: \u001b[1m\u001b[32m0.17942\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 044 | loss: 0.17942 - R2: 1.0009 -- iter: 0256/1168\n",
      "Training Step: 822  | total loss: \u001b[1m\u001b[32m0.17354\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 044 | loss: 0.17354 - R2: 1.0012 -- iter: 0320/1168\n",
      "Training Step: 823  | total loss: \u001b[1m\u001b[32m0.17354\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 044 | loss: 0.17354 - R2: 1.0012 -- iter: 0384/1168\n",
      "Training Step: 824  | total loss: \u001b[1m\u001b[32m0.16927\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 044 | loss: 0.16927 - R2: 1.0014 -- iter: 0448/1168\n",
      "Training Step: 825  | total loss: \u001b[1m\u001b[32m0.17011\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 044 | loss: 0.17011 - R2: 1.0024 -- iter: 0512/1168\n",
      "Training Step: 826  | total loss: \u001b[1m\u001b[32m0.16897\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 044 | loss: 0.16897 - R2: 1.0010 -- iter: 0576/1168\n",
      "Training Step: 827  | total loss: \u001b[1m\u001b[32m0.16981\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 044 | loss: 0.16981 - R2: 1.0008 -- iter: 0640/1168\n",
      "Training Step: 828  | total loss: \u001b[1m\u001b[32m0.16536\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 044 | loss: 0.16536 - R2: 1.0008 -- iter: 0704/1168\n",
      "Training Step: 829  | total loss: \u001b[1m\u001b[32m0.16474\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 044 | loss: 0.16474 - R2: 1.0015 -- iter: 0768/1168\n",
      "Training Step: 830  | total loss: \u001b[1m\u001b[32m0.16474\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 044 | loss: 0.16474 - R2: 1.0015 -- iter: 0832/1168\n",
      "Training Step: 831  | total loss: \u001b[1m\u001b[32m0.16703\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 044 | loss: 0.16703 - R2: 1.0034 -- iter: 0896/1168\n",
      "Training Step: 832  | total loss: \u001b[1m\u001b[32m0.17348\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 044 | loss: 0.17348 - R2: 1.0019 -- iter: 0960/1168\n",
      "Training Step: 833  | total loss: \u001b[1m\u001b[32m0.17348\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 044 | loss: 0.17348 - R2: 1.0019 -- iter: 1024/1168\n",
      "Training Step: 834  | total loss: \u001b[1m\u001b[32m0.17568\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 044 | loss: 0.17568 - R2: 1.0005 -- iter: 1088/1168\n",
      "Training Step: 835  | total loss: \u001b[1m\u001b[32m0.17568\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 044 | loss: 0.17568 - R2: 1.0005 -- iter: 1152/1168\n",
      "Training Step: 836  | total loss: \u001b[1m\u001b[32m0.17305\u001b[0m\u001b[0m | time: 1.146s\n",
      "| SGD | epoch: 044 | loss: 0.17305 - R2: 0.9998 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 837  | total loss: \u001b[1m\u001b[32m0.17044\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 045 | loss: 0.17044 - R2: 0.9983 -- iter: 0064/1168\n",
      "Training Step: 838  | total loss: \u001b[1m\u001b[32m0.16725\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 045 | loss: 0.16725 - R2: 0.9987 -- iter: 0128/1168\n",
      "Training Step: 839  | total loss: \u001b[1m\u001b[32m0.16725\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 045 | loss: 0.16725 - R2: 0.9987 -- iter: 0192/1168\n",
      "Training Step: 840  | total loss: \u001b[1m\u001b[32m0.18254\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 045 | loss: 0.18254 - R2: 1.0009 -- iter: 0256/1168\n",
      "Training Step: 841  | total loss: \u001b[1m\u001b[32m0.18582\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 045 | loss: 0.18582 - R2: 1.0010 -- iter: 0320/1168\n",
      "Training Step: 842  | total loss: \u001b[1m\u001b[32m0.18582\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 045 | loss: 0.18582 - R2: 1.0010 -- iter: 0384/1168\n",
      "Training Step: 843  | total loss: \u001b[1m\u001b[32m0.18403\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 045 | loss: 0.18403 - R2: 1.0022 -- iter: 0448/1168\n",
      "Training Step: 844  | total loss: \u001b[1m\u001b[32m0.17970\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 045 | loss: 0.17970 - R2: 1.0015 -- iter: 0512/1168\n",
      "Training Step: 845  | total loss: \u001b[1m\u001b[32m0.17970\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 045 | loss: 0.17970 - R2: 1.0015 -- iter: 0576/1168\n",
      "Training Step: 846  | total loss: \u001b[1m\u001b[32m0.17489\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 045 | loss: 0.17489 - R2: 1.0017 -- iter: 0640/1168\n",
      "Training Step: 847  | total loss: \u001b[1m\u001b[32m0.17536\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 045 | loss: 0.17536 - R2: 1.0017 -- iter: 0704/1168\n",
      "Training Step: 848  | total loss: \u001b[1m\u001b[32m0.17557\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 045 | loss: 0.17557 - R2: 1.0013 -- iter: 0768/1168\n",
      "Training Step: 849  | total loss: \u001b[1m\u001b[32m0.16832\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 045 | loss: 0.16832 - R2: 1.0005 -- iter: 0832/1168\n",
      "Training Step: 850  | total loss: \u001b[1m\u001b[32m0.16961\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 045 | loss: 0.16961 - R2: 0.9989 -- iter: 0896/1168\n",
      "Training Step: 851  | total loss: \u001b[1m\u001b[32m0.17816\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 045 | loss: 0.17816 - R2: 1.0004 -- iter: 0960/1168\n",
      "Training Step: 852  | total loss: \u001b[1m\u001b[32m0.17816\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 045 | loss: 0.17816 - R2: 1.0007 -- iter: 1024/1168\n",
      "Training Step: 853  | total loss: \u001b[1m\u001b[32m0.17710\u001b[0m\u001b[0m | time: 0.174s\n",
      "| SGD | epoch: 045 | loss: 0.17710 - R2: 1.0007 -- iter: 1088/1168\n",
      "Training Step: 854  | total loss: \u001b[1m\u001b[32m0.17111\u001b[0m\u001b[0m | time: 0.178s\n",
      "| SGD | epoch: 045 | loss: 0.17111 - R2: 1.0015 -- iter: 1152/1168\n",
      "Training Step: 855  | total loss: \u001b[1m\u001b[32m0.16780\u001b[0m\u001b[0m | time: 1.183s\n",
      "| SGD | epoch: 045 | loss: 0.16780 - R2: 1.0015 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 856  | total loss: \u001b[1m\u001b[32m0.16640\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 046 | loss: 0.16640 - R2: 1.0010 -- iter: 0064/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 857  | total loss: \u001b[1m\u001b[32m0.16426\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 046 | loss: 0.16426 - R2: 1.0004 -- iter: 0128/1168\n",
      "Training Step: 858  | total loss: \u001b[1m\u001b[32m0.16426\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 046 | loss: 0.16426 - R2: 1.0004 -- iter: 0192/1168\n",
      "Training Step: 859  | total loss: \u001b[1m\u001b[32m0.16104\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 046 | loss: 0.16104 - R2: 1.0002 -- iter: 0256/1168\n",
      "Training Step: 860  | total loss: \u001b[1m\u001b[32m0.15882\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 046 | loss: 0.15882 - R2: 0.9991 -- iter: 0320/1168\n",
      "Training Step: 861  | total loss: \u001b[1m\u001b[32m0.15570\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 046 | loss: 0.15570 - R2: 0.9988 -- iter: 0384/1168\n",
      "Training Step: 862  | total loss: \u001b[1m\u001b[32m0.15570\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 046 | loss: 0.15570 - R2: 0.9981 -- iter: 0448/1168\n",
      "Training Step: 863  | total loss: \u001b[1m\u001b[32m0.15408\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 046 | loss: 0.15408 - R2: 0.9981 -- iter: 0512/1168\n",
      "Training Step: 864  | total loss: \u001b[1m\u001b[32m0.15885\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 046 | loss: 0.15885 - R2: 0.9986 -- iter: 0576/1168\n",
      "Training Step: 865  | total loss: \u001b[1m\u001b[32m0.15863\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 046 | loss: 0.15863 - R2: 1.0001 -- iter: 0640/1168\n",
      "Training Step: 866  | total loss: \u001b[1m\u001b[32m0.16084\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 046 | loss: 0.16084 - R2: 0.9989 -- iter: 0704/1168\n",
      "Training Step: 867  | total loss: \u001b[1m\u001b[32m0.16695\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 046 | loss: 0.16695 - R2: 1.0007 -- iter: 0768/1168\n",
      "Training Step: 868  | total loss: \u001b[1m\u001b[32m0.16546\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 046 | loss: 0.16546 - R2: 1.0002 -- iter: 0832/1168\n",
      "Training Step: 869  | total loss: \u001b[1m\u001b[32m0.16290\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 046 | loss: 0.16290 - R2: 1.0016 -- iter: 0896/1168\n",
      "Training Step: 870  | total loss: \u001b[1m\u001b[32m0.16290\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 046 | loss: 0.16290 - R2: 1.0016 -- iter: 0960/1168\n",
      "Training Step: 871  | total loss: \u001b[1m\u001b[32m0.15939\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 046 | loss: 0.15939 - R2: 1.0006 -- iter: 1024/1168\n",
      "Training Step: 872  | total loss: \u001b[1m\u001b[32m0.16086\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 046 | loss: 0.16086 - R2: 1.0006 -- iter: 1088/1168\n",
      "Training Step: 873  | total loss: \u001b[1m\u001b[32m0.15755\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 046 | loss: 0.15755 - R2: 1.0011 -- iter: 1152/1168\n",
      "Training Step: 874  | total loss: \u001b[1m\u001b[32m0.15550\u001b[0m\u001b[0m | time: 1.147s\n",
      "| SGD | epoch: 046 | loss: 0.15550 - R2: 0.9984 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 875  | total loss: \u001b[1m\u001b[32m0.15288\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 047 | loss: 0.15288 - R2: 0.9988 -- iter: 0064/1168\n",
      "Training Step: 876  | total loss: \u001b[1m\u001b[32m0.15350\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 047 | loss: 0.15350 - R2: 0.9983 -- iter: 0128/1168\n",
      "Training Step: 877  | total loss: \u001b[1m\u001b[32m0.15350\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 047 | loss: 0.15350 - R2: 0.9983 -- iter: 0192/1168\n",
      "Training Step: 878  | total loss: \u001b[1m\u001b[32m0.16873\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 047 | loss: 0.16873 - R2: 0.9988 -- iter: 0256/1168\n",
      "Training Step: 879  | total loss: \u001b[1m\u001b[32m0.16519\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 047 | loss: 0.16519 - R2: 1.0012 -- iter: 0320/1168\n",
      "Training Step: 880  | total loss: \u001b[1m\u001b[32m0.16201\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 047 | loss: 0.16201 - R2: 1.0034 -- iter: 0384/1168\n",
      "Training Step: 881  | total loss: \u001b[1m\u001b[32m0.16582\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 047 | loss: 0.16582 - R2: 1.0042 -- iter: 0448/1168\n",
      "Training Step: 882  | total loss: \u001b[1m\u001b[32m0.16261\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 047 | loss: 0.16261 - R2: 1.0032 -- iter: 0512/1168\n",
      "Training Step: 883  | total loss: \u001b[1m\u001b[32m0.16308\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 047 | loss: 0.16308 - R2: 1.0041 -- iter: 0576/1168\n",
      "Training Step: 884  | total loss: \u001b[1m\u001b[32m0.16308\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 047 | loss: 0.16308 - R2: 1.0041 -- iter: 0640/1168\n",
      "Training Step: 885  | total loss: \u001b[1m\u001b[32m0.16660\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 047 | loss: 0.16660 - R2: 1.0040 -- iter: 0704/1168\n",
      "Training Step: 886  | total loss: \u001b[1m\u001b[32m0.16667\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 047 | loss: 0.16667 - R2: 1.0035 -- iter: 0768/1168\n",
      "Training Step: 887  | total loss: \u001b[1m\u001b[32m0.16535\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 047 | loss: 0.16535 - R2: 1.0029 -- iter: 0832/1168\n",
      "Training Step: 888  | total loss: \u001b[1m\u001b[32m0.15896\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 047 | loss: 0.15896 - R2: 1.0020 -- iter: 0896/1168\n",
      "Training Step: 889  | total loss: \u001b[1m\u001b[32m0.15942\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 047 | loss: 0.15942 - R2: 1.0004 -- iter: 0960/1168\n",
      "Training Step: 890  | total loss: \u001b[1m\u001b[32m0.16213\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 047 | loss: 0.16213 - R2: 1.0008 -- iter: 1024/1168\n",
      "Training Step: 891  | total loss: \u001b[1m\u001b[32m0.16272\u001b[0m\u001b[0m | time: 0.172s\n",
      "| SGD | epoch: 047 | loss: 0.16272 - R2: 0.9992 -- iter: 1088/1168\n",
      "Training Step: 892  | total loss: \u001b[1m\u001b[32m0.15832\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 047 | loss: 0.15832 - R2: 0.9990 -- iter: 1152/1168\n",
      "Training Step: 893  | total loss: \u001b[1m\u001b[32m0.15832\u001b[0m\u001b[0m | time: 1.185s\n",
      "| SGD | epoch: 047 | loss: 0.15832 - R2: 0.9990 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 894  | total loss: \u001b[1m\u001b[32m0.16087\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 048 | loss: 0.16087 - R2: 0.9990 -- iter: 0064/1168\n",
      "Training Step: 895  | total loss: \u001b[1m\u001b[32m0.16651\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 048 | loss: 0.16651 - R2: 1.0012 -- iter: 0128/1168\n",
      "Training Step: 896  | total loss: \u001b[1m\u001b[32m0.17303\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 048 | loss: 0.17303 - R2: 1.0015 -- iter: 0192/1168\n",
      "Training Step: 897  | total loss: \u001b[1m\u001b[32m0.17005\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 048 | loss: 0.17005 - R2: 1.0013 -- iter: 0256/1168\n",
      "Training Step: 898  | total loss: \u001b[1m\u001b[32m0.16321\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 048 | loss: 0.16321 - R2: 1.0026 -- iter: 0320/1168\n",
      "Training Step: 899  | total loss: \u001b[1m\u001b[32m0.16321\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 048 | loss: 0.16321 - R2: 1.0026 -- iter: 0384/1168\n",
      "Training Step: 900  | total loss: \u001b[1m\u001b[32m0.15587\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 048 | loss: 0.15587 - R2: 1.0049 -- iter: 0448/1168\n",
      "Training Step: 901  | total loss: \u001b[1m\u001b[32m0.15206\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 048 | loss: 0.15206 - R2: 1.0036 -- iter: 0512/1168\n",
      "Training Step: 902  | total loss: \u001b[1m\u001b[32m0.15206\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 048 | loss: 0.15206 - R2: 1.0036 -- iter: 0576/1168\n",
      "Training Step: 903  | total loss: \u001b[1m\u001b[32m0.15586\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 048 | loss: 0.15586 - R2: 1.0025 -- iter: 0640/1168\n",
      "Training Step: 904  | total loss: \u001b[1m\u001b[32m0.15586\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 048 | loss: 0.15586 - R2: 1.0025 -- iter: 0704/1168\n",
      "Training Step: 905  | total loss: \u001b[1m\u001b[32m0.16719\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 048 | loss: 0.16719 - R2: 1.0025 -- iter: 0768/1168\n",
      "Training Step: 906  | total loss: \u001b[1m\u001b[32m0.16389\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 048 | loss: 0.16389 - R2: 1.0036 -- iter: 0832/1168\n",
      "Training Step: 907  | total loss: \u001b[1m\u001b[32m0.16285\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 048 | loss: 0.16285 - R2: 1.0029 -- iter: 0896/1168\n",
      "Training Step: 908  | total loss: \u001b[1m\u001b[32m0.15905\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 048 | loss: 0.15905 - R2: 1.0026 -- iter: 0960/1168\n",
      "Training Step: 909  | total loss: \u001b[1m\u001b[32m0.15905\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 048 | loss: 0.15905 - R2: 1.0026 -- iter: 1024/1168\n",
      "Training Step: 910  | total loss: \u001b[1m\u001b[32m0.16202\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 048 | loss: 0.16202 - R2: 1.0029 -- iter: 1088/1168\n",
      "Training Step: 911  | total loss: \u001b[1m\u001b[32m0.16735\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 048 | loss: 0.16735 - R2: 1.0034 -- iter: 1152/1168\n",
      "Training Step: 912  | total loss: \u001b[1m\u001b[32m0.17000\u001b[0m\u001b[0m | time: 1.147s\n",
      "| SGD | epoch: 048 | loss: 0.17000 - R2: 1.0024 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 913  | total loss: \u001b[1m\u001b[32m0.16976\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 049 | loss: 0.16976 - R2: 1.0020 -- iter: 0064/1168\n",
      "Training Step: 914  | total loss: \u001b[1m\u001b[32m0.16171\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 049 | loss: 0.16171 - R2: 1.0019 -- iter: 0128/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 915  | total loss: \u001b[1m\u001b[32m0.16171\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 049 | loss: 0.16171 - R2: 1.0019 -- iter: 0192/1168\n",
      "Training Step: 916  | total loss: \u001b[1m\u001b[32m0.16719\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 049 | loss: 0.16719 - R2: 1.0025 -- iter: 0256/1168\n",
      "Training Step: 917  | total loss: \u001b[1m\u001b[32m0.16735\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 049 | loss: 0.16735 - R2: 1.0028 -- iter: 0320/1168\n",
      "Training Step: 918  | total loss: \u001b[1m\u001b[32m0.16971\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 049 | loss: 0.16971 - R2: 1.0018 -- iter: 0384/1168\n",
      "Training Step: 919  | total loss: \u001b[1m\u001b[32m0.17299\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 049 | loss: 0.17299 - R2: 1.0001 -- iter: 0448/1168\n",
      "Training Step: 920  | total loss: \u001b[1m\u001b[32m0.17299\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 049 | loss: 0.17299 - R2: 1.0001 -- iter: 0512/1168\n",
      "Training Step: 921  | total loss: \u001b[1m\u001b[32m0.17689\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 049 | loss: 0.17689 - R2: 0.9980 -- iter: 0576/1168\n",
      "Training Step: 922  | total loss: \u001b[1m\u001b[32m0.17367\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 049 | loss: 0.17367 - R2: 0.9982 -- iter: 0640/1168\n",
      "Training Step: 923  | total loss: \u001b[1m\u001b[32m0.17321\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 049 | loss: 0.17321 - R2: 0.9975 -- iter: 0704/1168\n",
      "Training Step: 924  | total loss: \u001b[1m\u001b[32m0.17170\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 049 | loss: 0.17170 - R2: 0.9980 -- iter: 0768/1168\n",
      "Training Step: 925  | total loss: \u001b[1m\u001b[32m0.16385\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 049 | loss: 0.16385 - R2: 0.9984 -- iter: 0832/1168\n",
      "Training Step: 926  | total loss: \u001b[1m\u001b[32m0.16517\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 049 | loss: 0.16517 - R2: 1.0011 -- iter: 0896/1168\n",
      "Training Step: 927  | total loss: \u001b[1m\u001b[32m0.16517\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 049 | loss: 0.16517 - R2: 1.0011 -- iter: 0960/1168\n",
      "Training Step: 928  | total loss: \u001b[1m\u001b[32m0.16736\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 049 | loss: 0.16736 - R2: 1.0002 -- iter: 1024/1168\n",
      "Training Step: 929  | total loss: \u001b[1m\u001b[32m0.17263\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 049 | loss: 0.17263 - R2: 0.9995 -- iter: 1088/1168\n",
      "Training Step: 930  | total loss: \u001b[1m\u001b[32m0.17263\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 049 | loss: 0.17263 - R2: 0.9995 -- iter: 1152/1168\n",
      "Training Step: 931  | total loss: \u001b[1m\u001b[32m0.17254\u001b[0m\u001b[0m | time: 1.144s\n",
      "| SGD | epoch: 049 | loss: 0.17254 - R2: 0.9997 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 932  | total loss: \u001b[1m\u001b[32m0.16774\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 050 | loss: 0.16774 - R2: 0.9993 -- iter: 0064/1168\n",
      "Training Step: 933  | total loss: \u001b[1m\u001b[32m0.16774\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 050 | loss: 0.16774 - R2: 0.9993 -- iter: 0128/1168\n",
      "Training Step: 934  | total loss: \u001b[1m\u001b[32m0.16864\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 050 | loss: 0.16864 - R2: 0.9988 -- iter: 0192/1168\n",
      "Training Step: 935  | total loss: \u001b[1m\u001b[32m0.17012\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 050 | loss: 0.17012 - R2: 0.9977 -- iter: 0256/1168\n",
      "Training Step: 936  | total loss: \u001b[1m\u001b[32m0.17147\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 050 | loss: 0.17147 - R2: 0.9994 -- iter: 0320/1168\n",
      "Training Step: 937  | total loss: \u001b[1m\u001b[32m0.17209\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 050 | loss: 0.17209 - R2: 1.0001 -- iter: 0384/1168\n",
      "Training Step: 938  | total loss: \u001b[1m\u001b[32m0.17705\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 050 | loss: 0.17705 - R2: 1.0013 -- iter: 0448/1168\n",
      "Training Step: 939  | total loss: \u001b[1m\u001b[32m0.17705\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 050 | loss: 0.17705 - R2: 1.0020 -- iter: 0512/1168\n",
      "Training Step: 940  | total loss: \u001b[1m\u001b[32m0.15886\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 050 | loss: 0.15886 - R2: 1.0027 -- iter: 0576/1168\n",
      "Training Step: 941  | total loss: \u001b[1m\u001b[32m0.15749\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 050 | loss: 0.15749 - R2: 1.0030 -- iter: 0640/1168\n",
      "Training Step: 942  | total loss: \u001b[1m\u001b[32m0.15254\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 050 | loss: 0.15254 - R2: 1.0031 -- iter: 0704/1168\n",
      "Training Step: 943  | total loss: \u001b[1m\u001b[32m0.15254\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 050 | loss: 0.15254 - R2: 1.0031 -- iter: 0768/1168\n",
      "Training Step: 944  | total loss: \u001b[1m\u001b[32m0.15434\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 050 | loss: 0.15434 - R2: 1.0034 -- iter: 0832/1168\n",
      "Training Step: 945  | total loss: \u001b[1m\u001b[32m0.15254\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 050 | loss: 0.15254 - R2: 1.0027 -- iter: 0896/1168\n",
      "Training Step: 946  | total loss: \u001b[1m\u001b[32m0.14947\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 050 | loss: 0.14947 - R2: 1.0018 -- iter: 0960/1168\n",
      "Training Step: 947  | total loss: \u001b[1m\u001b[32m0.15200\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 050 | loss: 0.15200 - R2: 1.0015 -- iter: 1024/1168\n",
      "Training Step: 948  | total loss: \u001b[1m\u001b[32m0.15200\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 050 | loss: 0.15200 - R2: 1.0015 -- iter: 1088/1168\n",
      "Training Step: 949  | total loss: \u001b[1m\u001b[32m0.14748\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 050 | loss: 0.14748 - R2: 1.0020 -- iter: 1152/1168\n",
      "Training Step: 950  | total loss: \u001b[1m\u001b[32m0.14748\u001b[0m\u001b[0m | time: 1.168s\n",
      "| SGD | epoch: 050 | loss: 0.14748 - R2: 1.0020 | val_loss: 0.14237 - val_acc: 1.0036 -- iter: 1168/1168\n",
      "--\n",
      "---------------------------------\n",
      "Run id: Y5W11W\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "INFO:tensorflow:Summary name StandardError/ (raw) is illegal; using StandardError/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 1168\n",
      "Validation samples: 292\n",
      "--\n",
      "Training Step: 1  | total loss: \u001b[1m\u001b[32m128.67592\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 001 | loss: 128.67592 - R2: 0.0000 -- iter: 0064/1168\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m131.93672\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 001 | loss: 131.93672 - R2: 0.0014 -- iter: 0128/1168\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m124.38027\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 001 | loss: 124.38027 - R2: 0.0053 -- iter: 0192/1168\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m124.38027\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 001 | loss: 124.38027 - R2: 0.0053 -- iter: 0256/1168\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m107.09724\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 001 | loss: 107.09724 - R2: 0.0198 -- iter: 0320/1168\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m107.09724\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 001 | loss: 107.09724 - R2: 0.0297 -- iter: 0384/1168\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m92.06257\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 001 | loss: 92.06257 - R2: 0.0417 -- iter: 0448/1168\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m92.06257\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 001 | loss: 92.06257 - R2: 0.0417 -- iter: 0512/1168\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m86.53778\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 001 | loss: 86.53778 - R2: 0.0543 -- iter: 0576/1168\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m80.28102\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 001 | loss: 80.28102 - R2: 0.0690 -- iter: 0640/1168\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m67.10748\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 001 | loss: 67.10748 - R2: 0.1055 -- iter: 0704/1168\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m67.10748\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 001 | loss: 67.10748 - R2: 0.1055 -- iter: 0768/1168\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m61.29385\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 001 | loss: 61.29385 - R2: 0.1268 -- iter: 0832/1168\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m49.71037\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 001 | loss: 49.71037 - R2: 0.1793 -- iter: 0896/1168\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m43.56657\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 001 | loss: 43.56657 - R2: 0.2132 -- iter: 0960/1168\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m37.81746\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 001 | loss: 37.81746 - R2: 0.2510 -- iter: 1024/1168\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m32.20396\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 001 | loss: 32.20396 - R2: 0.2944 -- iter: 1088/1168\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m26.89221\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 001 | loss: 26.89221 - R2: 0.3434 -- iter: 1152/1168\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m22.60257\u001b[0m\u001b[0m | time: 1.157s\n",
      "| SGD | epoch: 001 | loss: 22.60257 - R2: 0.3910 | val_loss: 8.11739 - val_acc: 0.5858 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m18.47350\u001b[0m\u001b[0m | time: 0.009s\n",
      "| SGD | epoch: 002 | loss: 18.47350 - R2: 0.4456 -- iter: 0064/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 21  | total loss: \u001b[1m\u001b[32m18.47350\u001b[0m\u001b[0m | time: 0.012s\n",
      "| SGD | epoch: 002 | loss: 18.47350 - R2: 0.4456 -- iter: 0128/1168\n",
      "Training Step: 22  | total loss: \u001b[1m\u001b[32m10.79813\u001b[0m\u001b[0m | time: 0.023s\n",
      "| SGD | epoch: 002 | loss: 10.79813 - R2: 0.5911 -- iter: 0192/1168\n",
      "Training Step: 23  | total loss: \u001b[1m\u001b[32m10.79813\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 002 | loss: 10.79813 - R2: 0.5911 -- iter: 0256/1168\n",
      "Training Step: 24  | total loss: \u001b[1m\u001b[32m7.98193\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 002 | loss: 7.98193 - R2: 0.6694 -- iter: 0320/1168\n",
      "Training Step: 25  | total loss: \u001b[1m\u001b[32m5.83977\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 002 | loss: 5.83977 - R2: 0.7544 -- iter: 0384/1168\n",
      "Training Step: 26  | total loss: \u001b[1m\u001b[32m4.33254\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 002 | loss: 4.33254 - R2: 0.8207 -- iter: 0448/1168\n",
      "Training Step: 27  | total loss: \u001b[1m\u001b[32m3.26186\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 002 | loss: 3.26186 - R2: 0.8608 -- iter: 0512/1168\n",
      "Training Step: 28  | total loss: \u001b[1m\u001b[32m1.91105\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 002 | loss: 1.91105 - R2: 0.9219 -- iter: 0576/1168\n",
      "Training Step: 29  | total loss: \u001b[1m\u001b[32m1.91105\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 002 | loss: 1.91105 - R2: 0.9219 -- iter: 0640/1168\n",
      "Training Step: 30  | total loss: \u001b[1m\u001b[32m1.49539\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 002 | loss: 1.49539 - R2: 0.9396 -- iter: 0704/1168\n",
      "Training Step: 31  | total loss: \u001b[1m\u001b[32m1.18092\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 002 | loss: 1.18092 - R2: 0.9520 -- iter: 0768/1168\n",
      "Training Step: 32  | total loss: \u001b[1m\u001b[32m0.95454\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 002 | loss: 0.95454 - R2: 0.9628 -- iter: 0832/1168\n",
      "Training Step: 33  | total loss: \u001b[1m\u001b[32m0.77200\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 002 | loss: 0.77200 - R2: 0.9702 -- iter: 0896/1168\n",
      "Training Step: 34  | total loss: \u001b[1m\u001b[32m0.63772\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 002 | loss: 0.63772 - R2: 0.9805 -- iter: 0960/1168\n",
      "Training Step: 35  | total loss: \u001b[1m\u001b[32m0.55469\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 002 | loss: 0.55469 - R2: 0.9823 -- iter: 1024/1168\n",
      "Training Step: 36  | total loss: \u001b[1m\u001b[32m0.48662\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 002 | loss: 0.48662 - R2: 0.9838 -- iter: 1088/1168\n",
      "Training Step: 37  | total loss: \u001b[1m\u001b[32m0.41709\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 002 | loss: 0.41709 - R2: 0.9885 -- iter: 1152/1168\n",
      "Training Step: 38  | total loss: \u001b[1m\u001b[32m0.37087\u001b[0m\u001b[0m | time: 1.098s\n",
      "| SGD | epoch: 002 | loss: 0.37087 - R2: 0.9946 | val_loss: 0.16239 - val_acc: 0.9907 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 39  | total loss: \u001b[1m\u001b[32m0.33889\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 003 | loss: 0.33889 - R2: 0.9946 -- iter: 0064/1168\n",
      "Training Step: 40  | total loss: \u001b[1m\u001b[32m0.30635\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 003 | loss: 0.30635 - R2: 0.9940 -- iter: 0128/1168\n",
      "Training Step: 41  | total loss: \u001b[1m\u001b[32m0.28010\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 003 | loss: 0.28010 - R2: 0.9940 -- iter: 0192/1168\n",
      "Training Step: 42  | total loss: \u001b[1m\u001b[32m0.24360\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 003 | loss: 0.24360 - R2: 0.9974 -- iter: 0256/1168\n",
      "Training Step: 43  | total loss: \u001b[1m\u001b[32m0.22111\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 003 | loss: 0.22111 - R2: 0.9981 -- iter: 0320/1168\n",
      "Training Step: 44  | total loss: \u001b[1m\u001b[32m0.20921\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 003 | loss: 0.20921 - R2: 0.9974 -- iter: 0384/1168\n",
      "Training Step: 45  | total loss: \u001b[1m\u001b[32m0.19493\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 003 | loss: 0.19493 - R2: 0.9987 -- iter: 0448/1168\n",
      "Training Step: 46  | total loss: \u001b[1m\u001b[32m0.19493\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 003 | loss: 0.19493 - R2: 0.9987 -- iter: 0512/1168\n",
      "Training Step: 47  | total loss: \u001b[1m\u001b[32m0.19941\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 003 | loss: 0.19941 - R2: 0.9991 -- iter: 0576/1168\n",
      "Training Step: 48  | total loss: \u001b[1m\u001b[32m0.18985\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 003 | loss: 0.18985 - R2: 0.9976 -- iter: 0640/1168\n",
      "Training Step: 49  | total loss: \u001b[1m\u001b[32m0.18985\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 003 | loss: 0.18985 - R2: 0.9976 -- iter: 0704/1168\n",
      "Training Step: 50  | total loss: \u001b[1m\u001b[32m0.17854\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 003 | loss: 0.17854 - R2: 0.9982 -- iter: 0768/1168\n",
      "Training Step: 51  | total loss: \u001b[1m\u001b[32m0.17854\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 003 | loss: 0.17854 - R2: 0.9970 -- iter: 0832/1168\n",
      "Training Step: 52  | total loss: \u001b[1m\u001b[32m0.17572\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 003 | loss: 0.17572 - R2: 0.9970 -- iter: 0896/1168\n",
      "Training Step: 53  | total loss: \u001b[1m\u001b[32m0.16754\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 003 | loss: 0.16754 - R2: 0.9978 -- iter: 0960/1168\n",
      "Training Step: 54  | total loss: \u001b[1m\u001b[32m0.16472\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 003 | loss: 0.16472 - R2: 0.9983 -- iter: 1024/1168\n",
      "Training Step: 55  | total loss: \u001b[1m\u001b[32m0.16472\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 003 | loss: 0.16472 - R2: 0.9983 -- iter: 1088/1168\n",
      "Training Step: 56  | total loss: \u001b[1m\u001b[32m0.15800\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 003 | loss: 0.15800 - R2: 0.9971 -- iter: 1152/1168\n",
      "Training Step: 57  | total loss: \u001b[1m\u001b[32m0.15870\u001b[0m\u001b[0m | time: 1.166s\n",
      "| SGD | epoch: 003 | loss: 0.15870 - R2: 0.9960 | val_loss: 0.15995 - val_acc: 0.9999 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 58  | total loss: \u001b[1m\u001b[32m0.16050\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 004 | loss: 0.16050 - R2: 0.9954 -- iter: 0064/1168\n",
      "Training Step: 59  | total loss: \u001b[1m\u001b[32m0.16186\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 004 | loss: 0.16186 - R2: 0.9960 -- iter: 0128/1168\n",
      "Training Step: 60  | total loss: \u001b[1m\u001b[32m0.15722\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 004 | loss: 0.15722 - R2: 0.9953 -- iter: 0192/1168\n",
      "Training Step: 61  | total loss: \u001b[1m\u001b[32m0.15312\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 004 | loss: 0.15312 - R2: 0.9949 -- iter: 0256/1168\n",
      "Training Step: 62  | total loss: \u001b[1m\u001b[32m0.15858\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 004 | loss: 0.15858 - R2: 0.9981 -- iter: 0320/1168\n",
      "Training Step: 63  | total loss: \u001b[1m\u001b[32m0.16314\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 004 | loss: 0.16314 - R2: 1.0004 -- iter: 0384/1168\n",
      "Training Step: 64  | total loss: \u001b[1m\u001b[32m0.16548\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 004 | loss: 0.16548 - R2: 1.0016 -- iter: 0448/1168\n",
      "Training Step: 65  | total loss: \u001b[1m\u001b[32m0.16548\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 004 | loss: 0.16548 - R2: 1.0016 -- iter: 0512/1168\n",
      "Training Step: 66  | total loss: \u001b[1m\u001b[32m0.15513\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 004 | loss: 0.15513 - R2: 1.0006 -- iter: 0576/1168\n",
      "Training Step: 67  | total loss: \u001b[1m\u001b[32m0.16758\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 004 | loss: 0.16758 - R2: 1.0009 -- iter: 0640/1168\n",
      "Training Step: 68  | total loss: \u001b[1m\u001b[32m0.16758\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 004 | loss: 0.16758 - R2: 1.0009 -- iter: 0704/1168\n",
      "Training Step: 69  | total loss: \u001b[1m\u001b[32m0.16389\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 004 | loss: 0.16389 - R2: 1.0010 -- iter: 0768/1168\n",
      "Training Step: 70  | total loss: \u001b[1m\u001b[32m0.16300\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 004 | loss: 0.16300 - R2: 1.0010 -- iter: 0832/1168\n",
      "Training Step: 71  | total loss: \u001b[1m\u001b[32m0.16254\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 004 | loss: 0.16254 - R2: 0.9991 -- iter: 0896/1168\n",
      "Training Step: 72  | total loss: \u001b[1m\u001b[32m0.16496\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 004 | loss: 0.16496 - R2: 0.9987 -- iter: 0960/1168\n",
      "Training Step: 73  | total loss: \u001b[1m\u001b[32m0.16496\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 004 | loss: 0.16496 - R2: 0.9987 -- iter: 1024/1168\n",
      "Training Step: 74  | total loss: \u001b[1m\u001b[32m0.16099\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 004 | loss: 0.16099 - R2: 0.9982 -- iter: 1088/1168\n",
      "Training Step: 75  | total loss: \u001b[1m\u001b[32m0.15736\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 004 | loss: 0.15736 - R2: 0.9982 -- iter: 1152/1168\n",
      "Training Step: 76  | total loss: \u001b[1m\u001b[32m0.15934\u001b[0m\u001b[0m | time: 1.147s\n",
      "| SGD | epoch: 004 | loss: 0.15934 - R2: 0.9965 | val_loss: 0.15994 - val_acc: 0.9982 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 77  | total loss: \u001b[1m\u001b[32m0.16038\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 005 | loss: 0.16038 - R2: 0.9965 -- iter: 0064/1168\n",
      "Training Step: 78  | total loss: \u001b[1m\u001b[32m0.15464\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 005 | loss: 0.15464 - R2: 0.9994 -- iter: 0128/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 79  | total loss: \u001b[1m\u001b[32m0.15464\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 005 | loss: 0.15464 - R2: 0.9994 -- iter: 0192/1168\n",
      "Training Step: 80  | total loss: \u001b[1m\u001b[32m0.13859\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 005 | loss: 0.13859 - R2: 1.0004 -- iter: 0256/1168\n",
      "Training Step: 81  | total loss: \u001b[1m\u001b[32m0.14730\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 005 | loss: 0.14730 - R2: 1.0015 -- iter: 0320/1168\n",
      "Training Step: 82  | total loss: \u001b[1m\u001b[32m0.15296\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 005 | loss: 0.15296 - R2: 1.0000 -- iter: 0384/1168\n",
      "Training Step: 83  | total loss: \u001b[1m\u001b[32m0.15708\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 005 | loss: 0.15708 - R2: 1.0009 -- iter: 0448/1168\n",
      "Training Step: 84  | total loss: \u001b[1m\u001b[32m0.15460\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 005 | loss: 0.15460 - R2: 1.0006 -- iter: 0512/1168\n",
      "Training Step: 85  | total loss: \u001b[1m\u001b[32m0.15634\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 005 | loss: 0.15634 - R2: 1.0005 -- iter: 0576/1168\n",
      "Training Step: 86  | total loss: \u001b[1m\u001b[32m0.15657\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 005 | loss: 0.15657 - R2: 1.0002 -- iter: 0640/1168\n",
      "Training Step: 87  | total loss: \u001b[1m\u001b[32m0.15657\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 005 | loss: 0.15657 - R2: 1.0002 -- iter: 0704/1168\n",
      "Training Step: 88  | total loss: \u001b[1m\u001b[32m0.15280\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 005 | loss: 0.15280 - R2: 1.0005 -- iter: 0768/1168\n",
      "Training Step: 89  | total loss: \u001b[1m\u001b[32m0.15340\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 005 | loss: 0.15340 - R2: 1.0005 -- iter: 0832/1168\n",
      "Training Step: 90  | total loss: \u001b[1m\u001b[32m0.15015\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 005 | loss: 0.15015 - R2: 1.0013 -- iter: 0896/1168\n",
      "Training Step: 91  | total loss: \u001b[1m\u001b[32m0.14730\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 005 | loss: 0.14730 - R2: 1.0003 -- iter: 0960/1168\n",
      "Training Step: 92  | total loss: \u001b[1m\u001b[32m0.14520\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 005 | loss: 0.14520 - R2: 0.9996 -- iter: 1024/1168\n",
      "Training Step: 93  | total loss: \u001b[1m\u001b[32m0.15022\u001b[0m\u001b[0m | time: 0.174s\n",
      "| SGD | epoch: 005 | loss: 0.15022 - R2: 0.9995 -- iter: 1088/1168\n",
      "Training Step: 94  | total loss: \u001b[1m\u001b[32m0.15257\u001b[0m\u001b[0m | time: 0.180s\n",
      "| SGD | epoch: 005 | loss: 0.15257 - R2: 1.0001 -- iter: 1152/1168\n",
      "Training Step: 95  | total loss: \u001b[1m\u001b[32m0.14854\u001b[0m\u001b[0m | time: 1.187s\n",
      "| SGD | epoch: 005 | loss: 0.14854 - R2: 1.0005 | val_loss: 0.16083 - val_acc: 0.9940 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 96  | total loss: \u001b[1m\u001b[32m0.15077\u001b[0m\u001b[0m | time: 0.026s\n",
      "| SGD | epoch: 006 | loss: 0.15077 - R2: 0.9992 -- iter: 0064/1168\n",
      "Training Step: 97  | total loss: \u001b[1m\u001b[32m0.15073\u001b[0m\u001b[0m | time: 0.031s\n",
      "| SGD | epoch: 006 | loss: 0.15073 - R2: 0.9991 -- iter: 0128/1168\n",
      "Training Step: 98  | total loss: \u001b[1m\u001b[32m0.15073\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 006 | loss: 0.15073 - R2: 0.9991 -- iter: 0192/1168\n",
      "Training Step: 99  | total loss: \u001b[1m\u001b[32m0.15134\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 006 | loss: 0.15134 - R2: 0.9977 -- iter: 0256/1168\n",
      "Training Step: 100  | total loss: \u001b[1m\u001b[32m0.16212\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 006 | loss: 0.16212 - R2: 0.9972 -- iter: 0320/1168\n",
      "Training Step: 101  | total loss: \u001b[1m\u001b[32m0.17182\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 006 | loss: 0.17182 - R2: 0.9968 -- iter: 0384/1168\n",
      "Training Step: 102  | total loss: \u001b[1m\u001b[32m0.18218\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 006 | loss: 0.18218 - R2: 0.9967 -- iter: 0448/1168\n",
      "Training Step: 103  | total loss: \u001b[1m\u001b[32m0.17618\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 006 | loss: 0.17618 - R2: 0.9978 -- iter: 0512/1168\n",
      "Training Step: 104  | total loss: \u001b[1m\u001b[32m0.17618\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 006 | loss: 0.17618 - R2: 0.9978 -- iter: 0576/1168\n",
      "Training Step: 105  | total loss: \u001b[1m\u001b[32m0.17219\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 006 | loss: 0.17219 - R2: 0.9983 -- iter: 0640/1168\n",
      "Training Step: 106  | total loss: \u001b[1m\u001b[32m0.17574\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 006 | loss: 0.17574 - R2: 0.9978 -- iter: 0704/1168\n",
      "Training Step: 107  | total loss: \u001b[1m\u001b[32m0.17089\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 006 | loss: 0.17089 - R2: 0.9982 -- iter: 0768/1168\n",
      "Training Step: 108  | total loss: \u001b[1m\u001b[32m0.16482\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 006 | loss: 0.16482 - R2: 0.9981 -- iter: 0832/1168\n",
      "Training Step: 109  | total loss: \u001b[1m\u001b[32m0.16250\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 006 | loss: 0.16250 - R2: 0.9990 -- iter: 0896/1168\n",
      "Training Step: 110  | total loss: \u001b[1m\u001b[32m0.16250\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 006 | loss: 0.16250 - R2: 0.9990 -- iter: 0960/1168\n",
      "Training Step: 111  | total loss: \u001b[1m\u001b[32m0.16526\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 006 | loss: 0.16526 - R2: 0.9993 -- iter: 1024/1168\n",
      "Training Step: 112  | total loss: \u001b[1m\u001b[32m0.16302\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 006 | loss: 0.16302 - R2: 0.9983 -- iter: 1088/1168\n",
      "Training Step: 113  | total loss: \u001b[1m\u001b[32m0.15666\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 006 | loss: 0.15666 - R2: 0.9979 -- iter: 1152/1168\n",
      "Training Step: 114  | total loss: \u001b[1m\u001b[32m0.16003\u001b[0m\u001b[0m | time: 1.114s\n",
      "| SGD | epoch: 006 | loss: 0.16003 - R2: 0.9986 | val_loss: 0.16049 - val_acc: 0.9950 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 115  | total loss: \u001b[1m\u001b[32m0.16249\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 007 | loss: 0.16249 - R2: 0.9993 -- iter: 0064/1168\n",
      "Training Step: 116  | total loss: \u001b[1m\u001b[32m0.16204\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 007 | loss: 0.16204 - R2: 0.9984 -- iter: 0128/1168\n",
      "Training Step: 117  | total loss: \u001b[1m\u001b[32m0.16191\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 007 | loss: 0.16191 - R2: 0.9987 -- iter: 0192/1168\n",
      "Training Step: 118  | total loss: \u001b[1m\u001b[32m0.15904\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 007 | loss: 0.15904 - R2: 0.9981 -- iter: 0256/1168\n",
      "Training Step: 119  | total loss: \u001b[1m\u001b[32m0.15716\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 007 | loss: 0.15716 - R2: 0.9974 -- iter: 0320/1168\n",
      "Training Step: 120  | total loss: \u001b[1m\u001b[32m0.15830\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 007 | loss: 0.15830 - R2: 0.9998 -- iter: 0384/1168\n",
      "Training Step: 121  | total loss: \u001b[1m\u001b[32m0.15954\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 007 | loss: 0.15954 - R2: 0.9999 -- iter: 0448/1168\n",
      "Training Step: 122  | total loss: \u001b[1m\u001b[32m0.15954\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 007 | loss: 0.15954 - R2: 0.9991 -- iter: 0512/1168\n",
      "Training Step: 123  | total loss: \u001b[1m\u001b[32m0.16006\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 007 | loss: 0.16006 - R2: 1.0001 -- iter: 0576/1168\n",
      "Training Step: 124  | total loss: \u001b[1m\u001b[32m0.16085\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 007 | loss: 0.16085 - R2: 0.9981 -- iter: 0640/1168\n",
      "Training Step: 125  | total loss: \u001b[1m\u001b[32m0.16085\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 007 | loss: 0.16085 - R2: 0.9981 -- iter: 0704/1168\n",
      "Training Step: 126  | total loss: \u001b[1m\u001b[32m0.16466\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 007 | loss: 0.16466 - R2: 1.0003 -- iter: 0768/1168\n",
      "Training Step: 127  | total loss: \u001b[1m\u001b[32m0.16776\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 007 | loss: 0.16776 - R2: 1.0014 -- iter: 0832/1168\n",
      "Training Step: 128  | total loss: \u001b[1m\u001b[32m0.16776\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 007 | loss: 0.16776 - R2: 1.0014 -- iter: 0896/1168\n",
      "Training Step: 129  | total loss: \u001b[1m\u001b[32m0.15820\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 007 | loss: 0.15820 - R2: 1.0021 -- iter: 0960/1168\n",
      "Training Step: 130  | total loss: \u001b[1m\u001b[32m0.16366\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 007 | loss: 0.16366 - R2: 1.0012 -- iter: 1024/1168\n",
      "Training Step: 131  | total loss: \u001b[1m\u001b[32m0.16485\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 007 | loss: 0.16485 - R2: 0.9997 -- iter: 1088/1168\n",
      "Training Step: 132  | total loss: \u001b[1m\u001b[32m0.16492\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 007 | loss: 0.16492 - R2: 0.9999 -- iter: 1152/1168\n",
      "Training Step: 133  | total loss: \u001b[1m\u001b[32m0.16492\u001b[0m\u001b[0m | time: 1.157s\n",
      "| SGD | epoch: 007 | loss: 0.16492 - R2: 0.9999 | val_loss: 0.16051 - val_acc: 0.9949 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 134  | total loss: \u001b[1m\u001b[32m0.16369\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 008 | loss: 0.16369 - R2: 0.9984 -- iter: 0064/1168\n",
      "Training Step: 135  | total loss: \u001b[1m\u001b[32m0.16369\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 008 | loss: 0.16369 - R2: 0.9984 -- iter: 0128/1168\n",
      "Training Step: 136  | total loss: \u001b[1m\u001b[32m0.15819\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 008 | loss: 0.15819 - R2: 0.9991 -- iter: 0192/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 137  | total loss: \u001b[1m\u001b[32m0.15745\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 008 | loss: 0.15745 - R2: 0.9986 -- iter: 0256/1168\n",
      "Training Step: 138  | total loss: \u001b[1m\u001b[32m0.16009\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 008 | loss: 0.16009 - R2: 0.9987 -- iter: 0320/1168\n",
      "Training Step: 139  | total loss: \u001b[1m\u001b[32m0.15798\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 008 | loss: 0.15798 - R2: 0.9990 -- iter: 0384/1168\n",
      "Training Step: 140  | total loss: \u001b[1m\u001b[32m0.15779\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 008 | loss: 0.15779 - R2: 1.0077 -- iter: 0448/1168\n",
      "Training Step: 141  | total loss: \u001b[1m\u001b[32m0.15779\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 008 | loss: 0.15779 - R2: 1.0077 -- iter: 0512/1168\n",
      "Training Step: 142  | total loss: \u001b[1m\u001b[32m0.16467\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 008 | loss: 0.16467 - R2: 1.0043 -- iter: 0576/1168\n",
      "Training Step: 143  | total loss: \u001b[1m\u001b[32m0.16467\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 008 | loss: 0.16467 - R2: 1.0037 -- iter: 0640/1168\n",
      "Training Step: 144  | total loss: \u001b[1m\u001b[32m0.15971\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 008 | loss: 0.15971 - R2: 1.0037 -- iter: 0704/1168\n",
      "Training Step: 145  | total loss: \u001b[1m\u001b[32m0.16208\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 008 | loss: 0.16208 - R2: 1.0036 -- iter: 0768/1168\n",
      "Training Step: 146  | total loss: \u001b[1m\u001b[32m0.16208\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 008 | loss: 0.16208 - R2: 1.0036 -- iter: 0832/1168\n",
      "Training Step: 147  | total loss: \u001b[1m\u001b[32m0.16567\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 008 | loss: 0.16567 - R2: 1.0039 -- iter: 0896/1168\n",
      "Training Step: 148  | total loss: \u001b[1m\u001b[32m0.16567\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 008 | loss: 0.16567 - R2: 1.0039 -- iter: 0960/1168\n",
      "Training Step: 149  | total loss: \u001b[1m\u001b[32m0.16745\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 008 | loss: 0.16745 - R2: 1.0010 -- iter: 1024/1168\n",
      "Training Step: 150  | total loss: \u001b[1m\u001b[32m0.17010\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 008 | loss: 0.17010 - R2: 1.0017 -- iter: 1088/1168\n",
      "Training Step: 151  | total loss: \u001b[1m\u001b[32m0.16795\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 008 | loss: 0.16795 - R2: 1.0021 -- iter: 1152/1168\n",
      "Training Step: 152  | total loss: \u001b[1m\u001b[32m0.16742\u001b[0m\u001b[0m | time: 1.162s\n",
      "| SGD | epoch: 008 | loss: 0.16742 - R2: 1.0017 | val_loss: 0.16066 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 153  | total loss: \u001b[1m\u001b[32m0.16821\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 009 | loss: 0.16821 - R2: 1.0015 -- iter: 0064/1168\n",
      "Training Step: 154  | total loss: \u001b[1m\u001b[32m0.16821\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 009 | loss: 0.16821 - R2: 1.0015 -- iter: 0128/1168\n",
      "Training Step: 155  | total loss: \u001b[1m\u001b[32m0.16399\u001b[0m\u001b[0m | time: 0.045s\n",
      "| SGD | epoch: 009 | loss: 0.16399 - R2: 0.9997 -- iter: 0192/1168\n",
      "Training Step: 156  | total loss: \u001b[1m\u001b[32m0.16412\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 009 | loss: 0.16412 - R2: 1.0007 -- iter: 0256/1168\n",
      "Training Step: 157  | total loss: \u001b[1m\u001b[32m0.16107\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 009 | loss: 0.16107 - R2: 1.0003 -- iter: 0320/1168\n",
      "Training Step: 158  | total loss: \u001b[1m\u001b[32m0.15878\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 009 | loss: 0.15878 - R2: 0.9990 -- iter: 0384/1168\n",
      "Training Step: 159  | total loss: \u001b[1m\u001b[32m0.15618\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 009 | loss: 0.15618 - R2: 0.9996 -- iter: 0448/1168\n",
      "Training Step: 160  | total loss: \u001b[1m\u001b[32m0.15610\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 009 | loss: 0.15610 - R2: 0.9997 -- iter: 0512/1168\n",
      "Training Step: 161  | total loss: \u001b[1m\u001b[32m0.15603\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 009 | loss: 0.15603 - R2: 0.9997 -- iter: 0576/1168\n",
      "Training Step: 162  | total loss: \u001b[1m\u001b[32m0.15443\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 009 | loss: 0.15443 - R2: 0.9982 -- iter: 0640/1168\n",
      "Training Step: 163  | total loss: \u001b[1m\u001b[32m0.15357\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 009 | loss: 0.15357 - R2: 0.9975 -- iter: 0704/1168\n",
      "Training Step: 164  | total loss: \u001b[1m\u001b[32m0.15693\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 009 | loss: 0.15693 - R2: 0.9976 -- iter: 0768/1168\n",
      "Training Step: 165  | total loss: \u001b[1m\u001b[32m0.15537\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 009 | loss: 0.15537 - R2: 0.9978 -- iter: 0832/1168\n",
      "Training Step: 166  | total loss: \u001b[1m\u001b[32m0.15683\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 009 | loss: 0.15683 - R2: 0.9983 -- iter: 0896/1168\n",
      "Training Step: 167  | total loss: \u001b[1m\u001b[32m0.16165\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 009 | loss: 0.16165 - R2: 1.0000 -- iter: 0960/1168\n",
      "Training Step: 168  | total loss: \u001b[1m\u001b[32m0.16165\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 009 | loss: 0.16165 - R2: 1.0000 -- iter: 1024/1168\n",
      "Training Step: 169  | total loss: \u001b[1m\u001b[32m0.15520\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 009 | loss: 0.15520 - R2: 0.9996 -- iter: 1088/1168\n",
      "Training Step: 170  | total loss: \u001b[1m\u001b[32m0.15550\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 009 | loss: 0.15550 - R2: 0.9996 -- iter: 1152/1168\n",
      "Training Step: 171  | total loss: \u001b[1m\u001b[32m0.15400\u001b[0m\u001b[0m | time: 1.100s\n",
      "| SGD | epoch: 009 | loss: 0.15400 - R2: 0.9993 | val_loss: 0.16064 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 172  | total loss: \u001b[1m\u001b[32m0.15400\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 010 | loss: 0.15400 - R2: 0.9993 -- iter: 0064/1168\n",
      "Training Step: 173  | total loss: \u001b[1m\u001b[32m0.15355\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 010 | loss: 0.15355 - R2: 0.9990 -- iter: 0128/1168\n",
      "Training Step: 174  | total loss: \u001b[1m\u001b[32m0.15460\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 010 | loss: 0.15460 - R2: 0.9987 -- iter: 0192/1168\n",
      "Training Step: 175  | total loss: \u001b[1m\u001b[32m0.15315\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 010 | loss: 0.15315 - R2: 0.9982 -- iter: 0256/1168\n",
      "Training Step: 176  | total loss: \u001b[1m\u001b[32m0.15760\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 010 | loss: 0.15760 - R2: 0.9984 -- iter: 0320/1168\n",
      "Training Step: 177  | total loss: \u001b[1m\u001b[32m0.15960\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 010 | loss: 0.15960 - R2: 0.9983 -- iter: 0384/1168\n",
      "Training Step: 178  | total loss: \u001b[1m\u001b[32m0.16014\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 010 | loss: 0.16014 - R2: 0.9986 -- iter: 0448/1168\n",
      "Training Step: 179  | total loss: \u001b[1m\u001b[32m0.16110\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 010 | loss: 0.16110 - R2: 0.9990 -- iter: 0512/1168\n",
      "Training Step: 180  | total loss: \u001b[1m\u001b[32m0.16197\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 010 | loss: 0.16197 - R2: 0.9994 -- iter: 0576/1168\n",
      "Training Step: 181  | total loss: \u001b[1m\u001b[32m0.16218\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 010 | loss: 0.16218 - R2: 0.9980 -- iter: 0640/1168\n",
      "Training Step: 182  | total loss: \u001b[1m\u001b[32m0.16218\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 010 | loss: 0.16218 - R2: 0.9980 -- iter: 0704/1168\n",
      "Training Step: 183  | total loss: \u001b[1m\u001b[32m0.16278\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 010 | loss: 0.16278 - R2: 0.9977 -- iter: 0768/1168\n",
      "Training Step: 184  | total loss: \u001b[1m\u001b[32m0.16278\u001b[0m\u001b[0m | time: 0.157s\n",
      "| SGD | epoch: 010 | loss: 0.16278 - R2: 0.9984 -- iter: 0832/1168\n",
      "Training Step: 185  | total loss: \u001b[1m\u001b[32m0.15922\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 010 | loss: 0.15922 - R2: 0.9983 -- iter: 0896/1168\n",
      "Training Step: 186  | total loss: \u001b[1m\u001b[32m0.16297\u001b[0m\u001b[0m | time: 0.165s\n",
      "| SGD | epoch: 010 | loss: 0.16297 - R2: 0.9988 -- iter: 0960/1168\n",
      "Training Step: 187  | total loss: \u001b[1m\u001b[32m0.16297\u001b[0m\u001b[0m | time: 0.167s\n",
      "| SGD | epoch: 010 | loss: 0.16297 - R2: 0.9988 -- iter: 1024/1168\n",
      "Training Step: 188  | total loss: \u001b[1m\u001b[32m0.16488\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 010 | loss: 0.16488 - R2: 0.9995 -- iter: 1088/1168\n",
      "Training Step: 189  | total loss: \u001b[1m\u001b[32m0.16228\u001b[0m\u001b[0m | time: 0.172s\n",
      "| SGD | epoch: 010 | loss: 0.16228 - R2: 1.0000 -- iter: 1152/1168\n",
      "Training Step: 190  | total loss: \u001b[1m\u001b[32m0.16608\u001b[0m\u001b[0m | time: 1.179s\n",
      "| SGD | epoch: 010 | loss: 0.16608 - R2: 0.9986 | val_loss: 0.16064 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 191  | total loss: \u001b[1m\u001b[32m0.16608\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 011 | loss: 0.16608 - R2: 0.9986 -- iter: 0064/1168\n",
      "Training Step: 192  | total loss: \u001b[1m\u001b[32m0.16661\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 011 | loss: 0.16661 - R2: 1.0005 -- iter: 0128/1168\n",
      "Training Step: 193  | total loss: \u001b[1m\u001b[32m0.16675\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 011 | loss: 0.16675 - R2: 1.0000 -- iter: 0192/1168\n",
      "Training Step: 194  | total loss: \u001b[1m\u001b[32m0.16199\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 011 | loss: 0.16199 - R2: 0.9996 -- iter: 0256/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 195  | total loss: \u001b[1m\u001b[32m0.16022\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 011 | loss: 0.16022 - R2: 0.9989 -- iter: 0320/1168\n",
      "Training Step: 196  | total loss: \u001b[1m\u001b[32m0.15945\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 011 | loss: 0.15945 - R2: 0.9984 -- iter: 0384/1168\n",
      "Training Step: 197  | total loss: \u001b[1m\u001b[32m0.15600\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 011 | loss: 0.15600 - R2: 0.9977 -- iter: 0448/1168\n",
      "Training Step: 198  | total loss: \u001b[1m\u001b[32m0.15920\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 011 | loss: 0.15920 - R2: 0.9966 -- iter: 0512/1168\n",
      "Training Step: 199  | total loss: \u001b[1m\u001b[32m0.15920\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 011 | loss: 0.15920 - R2: 0.9966 -- iter: 0576/1168\n",
      "Training Step: 200  | total loss: \u001b[1m\u001b[32m0.15555\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 011 | loss: 0.15555 - R2: 0.9974 -- iter: 0640/1168\n",
      "Training Step: 201  | total loss: \u001b[1m\u001b[32m0.15555\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 011 | loss: 0.15555 - R2: 0.9974 -- iter: 0704/1168\n",
      "Training Step: 202  | total loss: \u001b[1m\u001b[32m0.15367\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 011 | loss: 0.15367 - R2: 0.9992 -- iter: 0768/1168\n",
      "Training Step: 203  | total loss: \u001b[1m\u001b[32m0.15367\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 011 | loss: 0.15367 - R2: 0.9992 -- iter: 0832/1168\n",
      "Training Step: 204  | total loss: \u001b[1m\u001b[32m0.15515\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 011 | loss: 0.15515 - R2: 0.9997 -- iter: 0896/1168\n",
      "Training Step: 205  | total loss: \u001b[1m\u001b[32m0.15515\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 011 | loss: 0.15515 - R2: 0.9997 -- iter: 0960/1168\n",
      "Training Step: 206  | total loss: \u001b[1m\u001b[32m0.15931\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 011 | loss: 0.15931 - R2: 0.9994 -- iter: 1024/1168\n",
      "Training Step: 207  | total loss: \u001b[1m\u001b[32m0.16033\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 011 | loss: 0.16033 - R2: 0.9984 -- iter: 1088/1168\n",
      "Training Step: 208  | total loss: \u001b[1m\u001b[32m0.15875\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 011 | loss: 0.15875 - R2: 0.9991 -- iter: 1152/1168\n",
      "Training Step: 209  | total loss: \u001b[1m\u001b[32m0.15596\u001b[0m\u001b[0m | time: 1.128s\n",
      "| SGD | epoch: 011 | loss: 0.15596 - R2: 1.0004 | val_loss: 0.16064 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 210  | total loss: \u001b[1m\u001b[32m0.15326\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 012 | loss: 0.15326 - R2: 1.0016 -- iter: 0064/1168\n",
      "Training Step: 211  | total loss: \u001b[1m\u001b[32m0.15326\u001b[0m\u001b[0m | time: 0.032s\n",
      "| SGD | epoch: 012 | loss: 0.15326 - R2: 1.0016 -- iter: 0128/1168\n",
      "Training Step: 212  | total loss: \u001b[1m\u001b[32m0.15879\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 012 | loss: 0.15879 - R2: 1.0007 -- iter: 0192/1168\n",
      "Training Step: 213  | total loss: \u001b[1m\u001b[32m0.16281\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 012 | loss: 0.16281 - R2: 0.9995 -- iter: 0256/1168\n",
      "Training Step: 214  | total loss: \u001b[1m\u001b[32m0.16205\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 012 | loss: 0.16205 - R2: 0.9981 -- iter: 0320/1168\n",
      "Training Step: 215  | total loss: \u001b[1m\u001b[32m0.16827\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 012 | loss: 0.16827 - R2: 0.9981 -- iter: 0384/1168\n",
      "Training Step: 216  | total loss: \u001b[1m\u001b[32m0.17090\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 012 | loss: 0.17090 - R2: 0.9976 -- iter: 0448/1168\n",
      "Training Step: 217  | total loss: \u001b[1m\u001b[32m0.17212\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 012 | loss: 0.17212 - R2: 0.9974 -- iter: 0512/1168\n",
      "Training Step: 218  | total loss: \u001b[1m\u001b[32m0.17212\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 012 | loss: 0.17212 - R2: 0.9974 -- iter: 0576/1168\n",
      "Training Step: 219  | total loss: \u001b[1m\u001b[32m0.18356\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 012 | loss: 0.18356 - R2: 0.9926 -- iter: 0640/1168\n",
      "Training Step: 220  | total loss: \u001b[1m\u001b[32m0.18356\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 012 | loss: 0.18356 - R2: 0.9882 -- iter: 0704/1168\n",
      "Training Step: 221  | total loss: \u001b[1m\u001b[32m0.19651\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 012 | loss: 0.19651 - R2: 0.9882 -- iter: 0768/1168\n",
      "Training Step: 222  | total loss: \u001b[1m\u001b[32m0.19063\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 012 | loss: 0.19063 - R2: 0.9895 -- iter: 0832/1168\n",
      "Training Step: 223  | total loss: \u001b[1m\u001b[32m0.17716\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 012 | loss: 0.17716 - R2: 0.9916 -- iter: 0896/1168\n",
      "Training Step: 224  | total loss: \u001b[1m\u001b[32m0.17621\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 012 | loss: 0.17621 - R2: 0.9919 -- iter: 0960/1168\n",
      "Training Step: 225  | total loss: \u001b[1m\u001b[32m0.17621\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 012 | loss: 0.17621 - R2: 0.9919 -- iter: 1024/1168\n",
      "Training Step: 226  | total loss: \u001b[1m\u001b[32m0.16758\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 012 | loss: 0.16758 - R2: 0.9946 -- iter: 1088/1168\n",
      "Training Step: 227  | total loss: \u001b[1m\u001b[32m0.16655\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 012 | loss: 0.16655 - R2: 0.9941 -- iter: 1152/1168\n",
      "Training Step: 228  | total loss: \u001b[1m\u001b[32m0.16734\u001b[0m\u001b[0m | time: 1.140s\n",
      "| SGD | epoch: 012 | loss: 0.16734 - R2: 0.9939 | val_loss: 0.16064 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 229  | total loss: \u001b[1m\u001b[32m0.16546\u001b[0m\u001b[0m | time: 0.037s\n",
      "| SGD | epoch: 013 | loss: 0.16546 - R2: 0.9924 -- iter: 0064/1168\n",
      "Training Step: 230  | total loss: \u001b[1m\u001b[32m0.16321\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 013 | loss: 0.16321 - R2: 0.9939 -- iter: 0128/1168\n",
      "Training Step: 231  | total loss: \u001b[1m\u001b[32m0.16703\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 013 | loss: 0.16703 - R2: 0.9939 -- iter: 0192/1168\n",
      "Training Step: 232  | total loss: \u001b[1m\u001b[32m0.16459\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 013 | loss: 0.16459 - R2: 0.9932 -- iter: 0256/1168\n",
      "Training Step: 233  | total loss: \u001b[1m\u001b[32m0.16216\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 013 | loss: 0.16216 - R2: 0.9941 -- iter: 0320/1168\n",
      "Training Step: 234  | total loss: \u001b[1m\u001b[32m0.16216\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 013 | loss: 0.16216 - R2: 0.9941 -- iter: 0384/1168\n",
      "Training Step: 235  | total loss: \u001b[1m\u001b[32m0.16479\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 013 | loss: 0.16479 - R2: 0.9950 -- iter: 0448/1168\n",
      "Training Step: 236  | total loss: \u001b[1m\u001b[32m0.15829\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 013 | loss: 0.15829 - R2: 0.9960 -- iter: 0512/1168\n",
      "Training Step: 237  | total loss: \u001b[1m\u001b[32m0.15777\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 013 | loss: 0.15777 - R2: 0.9958 -- iter: 0576/1168\n",
      "Training Step: 238  | total loss: \u001b[1m\u001b[32m0.16475\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 013 | loss: 0.16475 - R2: 0.9971 -- iter: 0640/1168\n",
      "Training Step: 239  | total loss: \u001b[1m\u001b[32m0.16475\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 013 | loss: 0.16475 - R2: 0.9971 -- iter: 0704/1168\n",
      "Training Step: 240  | total loss: \u001b[1m\u001b[32m0.19629\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 013 | loss: 0.19629 - R2: 0.9938 -- iter: 0768/1168\n",
      "Training Step: 241  | total loss: \u001b[1m\u001b[32m0.19319\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 013 | loss: 0.19319 - R2: 0.9950 -- iter: 0832/1168\n",
      "Training Step: 242  | total loss: \u001b[1m\u001b[32m0.18542\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 013 | loss: 0.18542 - R2: 0.9947 -- iter: 0896/1168\n",
      "Training Step: 243  | total loss: \u001b[1m\u001b[32m0.18542\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 013 | loss: 0.18542 - R2: 0.9947 -- iter: 0960/1168\n",
      "Training Step: 244  | total loss: \u001b[1m\u001b[32m0.18146\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 013 | loss: 0.18146 - R2: 0.9977 -- iter: 1024/1168\n",
      "Training Step: 245  | total loss: \u001b[1m\u001b[32m0.17757\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 013 | loss: 0.17757 - R2: 0.9977 -- iter: 1088/1168\n",
      "Training Step: 246  | total loss: \u001b[1m\u001b[32m0.17392\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 013 | loss: 0.17392 - R2: 0.9980 -- iter: 1152/1168\n",
      "Training Step: 247  | total loss: \u001b[1m\u001b[32m0.17392\u001b[0m\u001b[0m | time: 1.135s\n",
      "| SGD | epoch: 013 | loss: 0.17392 - R2: 0.9980 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 248  | total loss: \u001b[1m\u001b[32m0.17225\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 014 | loss: 0.17225 - R2: 0.9972 -- iter: 0064/1168\n",
      "Training Step: 249  | total loss: \u001b[1m\u001b[32m0.17375\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 014 | loss: 0.17375 - R2: 0.9984 -- iter: 0128/1168\n",
      "Training Step: 250  | total loss: \u001b[1m\u001b[32m0.17405\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 014 | loss: 0.17405 - R2: 0.9988 -- iter: 0192/1168\n",
      "Training Step: 251  | total loss: \u001b[1m\u001b[32m0.17336\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 014 | loss: 0.17336 - R2: 0.9983 -- iter: 0256/1168\n",
      "Training Step: 252  | total loss: \u001b[1m\u001b[32m0.16999\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 014 | loss: 0.16999 - R2: 0.9968 -- iter: 0320/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 253  | total loss: \u001b[1m\u001b[32m0.16604\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 014 | loss: 0.16604 - R2: 0.9968 -- iter: 0384/1168\n",
      "Training Step: 254  | total loss: \u001b[1m\u001b[32m0.16378\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 014 | loss: 0.16378 - R2: 0.9979 -- iter: 0448/1168\n",
      "Training Step: 255  | total loss: \u001b[1m\u001b[32m0.16238\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 014 | loss: 0.16238 - R2: 0.9965 -- iter: 0512/1168\n",
      "Training Step: 256  | total loss: \u001b[1m\u001b[32m0.16388\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 014 | loss: 0.16388 - R2: 0.9978 -- iter: 0576/1168\n",
      "Training Step: 257  | total loss: \u001b[1m\u001b[32m0.16096\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 014 | loss: 0.16096 - R2: 0.9971 -- iter: 0640/1168\n",
      "Training Step: 258  | total loss: \u001b[1m\u001b[32m0.16535\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 014 | loss: 0.16535 - R2: 0.9982 -- iter: 0704/1168\n",
      "Training Step: 259  | total loss: \u001b[1m\u001b[32m0.16686\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 014 | loss: 0.16686 - R2: 0.9979 -- iter: 0768/1168\n",
      "Training Step: 260  | total loss: \u001b[1m\u001b[32m0.16823\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 014 | loss: 0.16823 - R2: 0.9977 -- iter: 0832/1168\n",
      "Training Step: 261  | total loss: \u001b[1m\u001b[32m0.16823\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 014 | loss: 0.16823 - R2: 0.9977 -- iter: 0896/1168\n",
      "Training Step: 262  | total loss: \u001b[1m\u001b[32m0.16405\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 014 | loss: 0.16405 - R2: 0.9968 -- iter: 0960/1168\n",
      "Training Step: 263  | total loss: \u001b[1m\u001b[32m0.15981\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 014 | loss: 0.15981 - R2: 0.9976 -- iter: 1024/1168\n",
      "Training Step: 264  | total loss: \u001b[1m\u001b[32m0.15901\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 014 | loss: 0.15901 - R2: 0.9973 -- iter: 1088/1168\n",
      "Training Step: 265  | total loss: \u001b[1m\u001b[32m0.15901\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 014 | loss: 0.15901 - R2: 0.9969 -- iter: 1152/1168\n",
      "Training Step: 266  | total loss: \u001b[1m\u001b[32m0.16225\u001b[0m\u001b[0m | time: 1.135s\n",
      "| SGD | epoch: 014 | loss: 0.16225 - R2: 0.9978 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 267  | total loss: \u001b[1m\u001b[32m0.16225\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 015 | loss: 0.16225 - R2: 0.9978 -- iter: 0064/1168\n",
      "Training Step: 268  | total loss: \u001b[1m\u001b[32m0.16151\u001b[0m\u001b[0m | time: 0.040s\n",
      "| SGD | epoch: 015 | loss: 0.16151 - R2: 0.9982 -- iter: 0128/1168\n",
      "Training Step: 269  | total loss: \u001b[1m\u001b[32m0.16284\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 015 | loss: 0.16284 - R2: 0.9994 -- iter: 0192/1168\n",
      "Training Step: 270  | total loss: \u001b[1m\u001b[32m0.16284\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 015 | loss: 0.16284 - R2: 0.9994 -- iter: 0256/1168\n",
      "Training Step: 271  | total loss: \u001b[1m\u001b[32m0.16917\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 015 | loss: 0.16917 - R2: 0.9995 -- iter: 0320/1168\n",
      "Training Step: 272  | total loss: \u001b[1m\u001b[32m0.16488\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 015 | loss: 0.16488 - R2: 0.9994 -- iter: 0384/1168\n",
      "Training Step: 273  | total loss: \u001b[1m\u001b[32m0.16114\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 015 | loss: 0.16114 - R2: 0.9986 -- iter: 0448/1168\n",
      "Training Step: 274  | total loss: \u001b[1m\u001b[32m0.15538\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 015 | loss: 0.15538 - R2: 0.9989 -- iter: 0512/1168\n",
      "Training Step: 275  | total loss: \u001b[1m\u001b[32m0.15538\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 015 | loss: 0.15538 - R2: 0.9989 -- iter: 0576/1168\n",
      "Training Step: 276  | total loss: \u001b[1m\u001b[32m0.15400\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 015 | loss: 0.15400 - R2: 0.9980 -- iter: 0640/1168\n",
      "Training Step: 277  | total loss: \u001b[1m\u001b[32m0.15174\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 015 | loss: 0.15174 - R2: 0.9992 -- iter: 0704/1168\n",
      "Training Step: 278  | total loss: \u001b[1m\u001b[32m0.15967\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 015 | loss: 0.15967 - R2: 0.9983 -- iter: 0768/1168\n",
      "Training Step: 279  | total loss: \u001b[1m\u001b[32m0.15967\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 015 | loss: 0.15967 - R2: 0.9983 -- iter: 0832/1168\n",
      "Training Step: 280  | total loss: \u001b[1m\u001b[32m0.15446\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 015 | loss: 0.15446 - R2: 1.0008 -- iter: 0896/1168\n",
      "Training Step: 281  | total loss: \u001b[1m\u001b[32m0.14977\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 015 | loss: 0.14977 - R2: 1.0030 -- iter: 0960/1168\n",
      "Training Step: 282  | total loss: \u001b[1m\u001b[32m0.15722\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 015 | loss: 0.15722 - R2: 0.9989 -- iter: 1024/1168\n",
      "Training Step: 283  | total loss: \u001b[1m\u001b[32m0.15722\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 015 | loss: 0.15722 - R2: 0.9989 -- iter: 1088/1168\n",
      "Training Step: 284  | total loss: \u001b[1m\u001b[32m0.15701\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 015 | loss: 0.15701 - R2: 0.9987 -- iter: 1152/1168\n",
      "Training Step: 285  | total loss: \u001b[1m\u001b[32m0.15197\u001b[0m\u001b[0m | time: 1.114s\n",
      "| SGD | epoch: 015 | loss: 0.15197 - R2: 0.9999 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 286  | total loss: \u001b[1m\u001b[32m0.15029\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 016 | loss: 0.15029 - R2: 0.9992 -- iter: 0064/1168\n",
      "Training Step: 287  | total loss: \u001b[1m\u001b[32m0.15029\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 016 | loss: 0.15029 - R2: 0.9992 -- iter: 0128/1168\n",
      "Training Step: 288  | total loss: \u001b[1m\u001b[32m0.15802\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 016 | loss: 0.15802 - R2: 0.9999 -- iter: 0192/1168\n",
      "Training Step: 289  | total loss: \u001b[1m\u001b[32m0.15564\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 016 | loss: 0.15564 - R2: 0.9985 -- iter: 0256/1168\n",
      "Training Step: 290  | total loss: \u001b[1m\u001b[32m0.15643\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 016 | loss: 0.15643 - R2: 0.9999 -- iter: 0320/1168\n",
      "Training Step: 291  | total loss: \u001b[1m\u001b[32m0.15599\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 016 | loss: 0.15599 - R2: 1.0001 -- iter: 0384/1168\n",
      "Training Step: 292  | total loss: \u001b[1m\u001b[32m0.15599\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 016 | loss: 0.15599 - R2: 1.0001 -- iter: 0448/1168\n",
      "Training Step: 293  | total loss: \u001b[1m\u001b[32m0.16078\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 016 | loss: 0.16078 - R2: 0.9996 -- iter: 0512/1168\n",
      "Training Step: 294  | total loss: \u001b[1m\u001b[32m0.16174\u001b[0m\u001b[0m | time: 0.125s\n",
      "| SGD | epoch: 016 | loss: 0.16174 - R2: 0.9998 -- iter: 0576/1168\n",
      "Training Step: 295  | total loss: \u001b[1m\u001b[32m0.15846\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 016 | loss: 0.15846 - R2: 0.9992 -- iter: 0640/1168\n",
      "Training Step: 296  | total loss: \u001b[1m\u001b[32m0.15616\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 016 | loss: 0.15616 - R2: 0.9990 -- iter: 0704/1168\n",
      "Training Step: 297  | total loss: \u001b[1m\u001b[32m0.14934\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 016 | loss: 0.14934 - R2: 0.9988 -- iter: 0768/1168\n",
      "Training Step: 298  | total loss: \u001b[1m\u001b[32m0.14934\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 016 | loss: 0.14934 - R2: 0.9988 -- iter: 0832/1168\n",
      "Training Step: 299  | total loss: \u001b[1m\u001b[32m0.14893\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 016 | loss: 0.14893 - R2: 0.9986 -- iter: 0896/1168\n",
      "Training Step: 300  | total loss: \u001b[1m\u001b[32m0.14830\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 016 | loss: 0.14830 - R2: 1.0013 -- iter: 0960/1168\n",
      "Training Step: 301  | total loss: \u001b[1m\u001b[32m0.14773\u001b[0m\u001b[0m | time: 0.161s\n",
      "| SGD | epoch: 016 | loss: 0.14773 - R2: 1.0031 -- iter: 1024/1168\n",
      "Training Step: 302  | total loss: \u001b[1m\u001b[32m0.15751\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 016 | loss: 0.15751 - R2: 1.0021 -- iter: 1088/1168\n",
      "Training Step: 303  | total loss: \u001b[1m\u001b[32m0.16254\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 016 | loss: 0.16254 - R2: 1.0017 -- iter: 1152/1168\n",
      "Training Step: 304  | total loss: \u001b[1m\u001b[32m0.15793\u001b[0m\u001b[0m | time: 1.175s\n",
      "| SGD | epoch: 016 | loss: 0.15793 - R2: 1.0014 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 305  | total loss: \u001b[1m\u001b[32m0.15691\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 017 | loss: 0.15691 - R2: 1.0022 -- iter: 0064/1168\n",
      "Training Step: 306  | total loss: \u001b[1m\u001b[32m0.15943\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 017 | loss: 0.15943 - R2: 1.0010 -- iter: 0128/1168\n",
      "Training Step: 307  | total loss: \u001b[1m\u001b[32m0.15974\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 017 | loss: 0.15974 - R2: 1.0008 -- iter: 0192/1168\n",
      "Training Step: 308  | total loss: \u001b[1m\u001b[32m0.16170\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 017 | loss: 0.16170 - R2: 1.0002 -- iter: 0256/1168\n",
      "Training Step: 309  | total loss: \u001b[1m\u001b[32m0.16350\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 017 | loss: 0.16350 - R2: 0.9994 -- iter: 0320/1168\n",
      "Training Step: 310  | total loss: \u001b[1m\u001b[32m0.16350\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 017 | loss: 0.16350 - R2: 0.9997 -- iter: 0384/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 311  | total loss: \u001b[1m\u001b[32m0.16506\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 017 | loss: 0.16506 - R2: 0.9997 -- iter: 0448/1168\n",
      "Training Step: 312  | total loss: \u001b[1m\u001b[32m0.16581\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 017 | loss: 0.16581 - R2: 0.9988 -- iter: 0512/1168\n",
      "Training Step: 313  | total loss: \u001b[1m\u001b[32m0.16291\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 017 | loss: 0.16291 - R2: 0.9990 -- iter: 0576/1168\n",
      "Training Step: 314  | total loss: \u001b[1m\u001b[32m0.16291\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 017 | loss: 0.16291 - R2: 0.9990 -- iter: 0640/1168\n",
      "Training Step: 315  | total loss: \u001b[1m\u001b[32m0.15673\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 017 | loss: 0.15673 - R2: 0.9983 -- iter: 0704/1168\n",
      "Training Step: 316  | total loss: \u001b[1m\u001b[32m0.15597\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 017 | loss: 0.15597 - R2: 0.9980 -- iter: 0768/1168\n",
      "Training Step: 317  | total loss: \u001b[1m\u001b[32m0.15597\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 017 | loss: 0.15597 - R2: 0.9980 -- iter: 0832/1168\n",
      "Training Step: 318  | total loss: \u001b[1m\u001b[32m0.16427\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 017 | loss: 0.16427 - R2: 0.9985 -- iter: 0896/1168\n",
      "Training Step: 319  | total loss: \u001b[1m\u001b[32m0.15840\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 017 | loss: 0.15840 - R2: 0.9984 -- iter: 0960/1168\n",
      "Training Step: 320  | total loss: \u001b[1m\u001b[32m0.16092\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 017 | loss: 0.16092 - R2: 0.9976 -- iter: 1024/1168\n",
      "Training Step: 321  | total loss: \u001b[1m\u001b[32m0.16319\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 017 | loss: 0.16319 - R2: 0.9968 -- iter: 1088/1168\n",
      "Training Step: 322  | total loss: \u001b[1m\u001b[32m0.16393\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 017 | loss: 0.16393 - R2: 0.9972 -- iter: 1152/1168\n",
      "Training Step: 323  | total loss: \u001b[1m\u001b[32m0.16434\u001b[0m\u001b[0m | time: 1.121s\n",
      "| SGD | epoch: 017 | loss: 0.16434 - R2: 0.9967 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 324  | total loss: \u001b[1m\u001b[32m0.16638\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 018 | loss: 0.16638 - R2: 0.9987 -- iter: 0064/1168\n",
      "Training Step: 325  | total loss: \u001b[1m\u001b[32m0.16098\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 018 | loss: 0.16098 - R2: 0.9988 -- iter: 0128/1168\n",
      "Training Step: 326  | total loss: \u001b[1m\u001b[32m0.16139\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 018 | loss: 0.16139 - R2: 0.9986 -- iter: 0192/1168\n",
      "Training Step: 327  | total loss: \u001b[1m\u001b[32m0.16139\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 018 | loss: 0.16139 - R2: 0.9986 -- iter: 0256/1168\n",
      "Training Step: 328  | total loss: \u001b[1m\u001b[32m0.16521\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 018 | loss: 0.16521 - R2: 0.9982 -- iter: 0320/1168\n",
      "Training Step: 329  | total loss: \u001b[1m\u001b[32m0.16521\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 018 | loss: 0.16521 - R2: 0.9971 -- iter: 0384/1168\n",
      "Training Step: 330  | total loss: \u001b[1m\u001b[32m0.16436\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 018 | loss: 0.16436 - R2: 0.9953 -- iter: 0448/1168\n",
      "Training Step: 331  | total loss: \u001b[1m\u001b[32m0.16386\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 018 | loss: 0.16386 - R2: 0.9960 -- iter: 0512/1168\n",
      "Training Step: 332  | total loss: \u001b[1m\u001b[32m0.16373\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 018 | loss: 0.16373 - R2: 0.9978 -- iter: 0576/1168\n",
      "Training Step: 333  | total loss: \u001b[1m\u001b[32m0.15977\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 018 | loss: 0.15977 - R2: 0.9965 -- iter: 0640/1168\n",
      "Training Step: 334  | total loss: \u001b[1m\u001b[32m0.15509\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 018 | loss: 0.15509 - R2: 0.9975 -- iter: 0704/1168\n",
      "Training Step: 335  | total loss: \u001b[1m\u001b[32m0.15509\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 018 | loss: 0.15509 - R2: 0.9975 -- iter: 0768/1168\n",
      "Training Step: 336  | total loss: \u001b[1m\u001b[32m0.15173\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 018 | loss: 0.15173 - R2: 0.9963 -- iter: 0832/1168\n",
      "Training Step: 337  | total loss: \u001b[1m\u001b[32m0.15204\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 018 | loss: 0.15204 - R2: 0.9961 -- iter: 0896/1168\n",
      "Training Step: 338  | total loss: \u001b[1m\u001b[32m0.15159\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 018 | loss: 0.15159 - R2: 0.9973 -- iter: 0960/1168\n",
      "Training Step: 339  | total loss: \u001b[1m\u001b[32m0.14908\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 018 | loss: 0.14908 - R2: 0.9976 -- iter: 1024/1168\n",
      "Training Step: 340  | total loss: \u001b[1m\u001b[32m0.14682\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 018 | loss: 0.14682 - R2: 0.9978 -- iter: 1088/1168\n",
      "Training Step: 341  | total loss: \u001b[1m\u001b[32m0.15259\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 018 | loss: 0.15259 - R2: 0.9993 -- iter: 1152/1168\n",
      "Training Step: 342  | total loss: \u001b[1m\u001b[32m0.15259\u001b[0m\u001b[0m | time: 1.154s\n",
      "| SGD | epoch: 018 | loss: 0.15259 - R2: 0.9993 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 343  | total loss: \u001b[1m\u001b[32m0.15413\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 019 | loss: 0.15413 - R2: 1.0000 -- iter: 0064/1168\n",
      "Training Step: 344  | total loss: \u001b[1m\u001b[32m0.15444\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 019 | loss: 0.15444 - R2: 1.0000 -- iter: 0128/1168\n",
      "Training Step: 345  | total loss: \u001b[1m\u001b[32m0.16453\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 019 | loss: 0.16453 - R2: 1.0005 -- iter: 0192/1168\n",
      "Training Step: 346  | total loss: \u001b[1m\u001b[32m0.15977\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 019 | loss: 0.15977 - R2: 0.9999 -- iter: 0256/1168\n",
      "Training Step: 347  | total loss: \u001b[1m\u001b[32m0.15977\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 019 | loss: 0.15977 - R2: 0.9999 -- iter: 0320/1168\n",
      "Training Step: 348  | total loss: \u001b[1m\u001b[32m0.17185\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 019 | loss: 0.17185 - R2: 1.0008 -- iter: 0384/1168\n",
      "Training Step: 349  | total loss: \u001b[1m\u001b[32m0.16793\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 019 | loss: 0.16793 - R2: 1.0002 -- iter: 0448/1168\n",
      "Training Step: 350  | total loss: \u001b[1m\u001b[32m0.16485\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 019 | loss: 0.16485 - R2: 0.9998 -- iter: 0512/1168\n",
      "Training Step: 351  | total loss: \u001b[1m\u001b[32m0.16091\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 019 | loss: 0.16091 - R2: 1.0011 -- iter: 0576/1168\n",
      "Training Step: 352  | total loss: \u001b[1m\u001b[32m0.15825\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 019 | loss: 0.15825 - R2: 0.9995 -- iter: 0640/1168\n",
      "Training Step: 353  | total loss: \u001b[1m\u001b[32m0.15825\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 019 | loss: 0.15825 - R2: 0.9995 -- iter: 0704/1168\n",
      "Training Step: 354  | total loss: \u001b[1m\u001b[32m0.16158\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 019 | loss: 0.16158 - R2: 0.9984 -- iter: 0768/1168\n",
      "Training Step: 355  | total loss: \u001b[1m\u001b[32m0.15863\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 019 | loss: 0.15863 - R2: 0.9973 -- iter: 0832/1168\n",
      "Training Step: 356  | total loss: \u001b[1m\u001b[32m0.15651\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 019 | loss: 0.15651 - R2: 0.9974 -- iter: 0896/1168\n",
      "Training Step: 357  | total loss: \u001b[1m\u001b[32m0.15148\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 019 | loss: 0.15148 - R2: 0.9980 -- iter: 0960/1168\n",
      "Training Step: 358  | total loss: \u001b[1m\u001b[32m0.16255\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 019 | loss: 0.16255 - R2: 0.9988 -- iter: 1024/1168\n",
      "Training Step: 359  | total loss: \u001b[1m\u001b[32m0.15917\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 019 | loss: 0.15917 - R2: 0.9991 -- iter: 1088/1168\n",
      "Training Step: 360  | total loss: \u001b[1m\u001b[32m0.15917\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 019 | loss: 0.15917 - R2: 0.9991 -- iter: 1152/1168\n",
      "Training Step: 361  | total loss: \u001b[1m\u001b[32m0.15612\u001b[0m\u001b[0m | time: 1.136s\n",
      "| SGD | epoch: 019 | loss: 0.15612 - R2: 0.9994 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 362  | total loss: \u001b[1m\u001b[32m0.16277\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 020 | loss: 0.16277 - R2: 0.9994 -- iter: 0064/1168\n",
      "Training Step: 363  | total loss: \u001b[1m\u001b[32m0.16102\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 020 | loss: 0.16102 - R2: 0.9999 -- iter: 0128/1168\n",
      "Training Step: 364  | total loss: \u001b[1m\u001b[32m0.16102\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 020 | loss: 0.16102 - R2: 0.9999 -- iter: 0192/1168\n",
      "Training Step: 365  | total loss: \u001b[1m\u001b[32m0.15445\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 020 | loss: 0.15445 - R2: 0.9980 -- iter: 0256/1168\n",
      "Training Step: 366  | total loss: \u001b[1m\u001b[32m0.15181\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 020 | loss: 0.15181 - R2: 0.9985 -- iter: 0320/1168\n",
      "Training Step: 367  | total loss: \u001b[1m\u001b[32m0.15624\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 020 | loss: 0.15624 - R2: 0.9980 -- iter: 0384/1168\n",
      "Training Step: 368  | total loss: \u001b[1m\u001b[32m0.15529\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 020 | loss: 0.15529 - R2: 0.9979 -- iter: 0448/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 369  | total loss: \u001b[1m\u001b[32m0.15529\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 020 | loss: 0.15529 - R2: 0.9979 -- iter: 0512/1168\n",
      "Training Step: 370  | total loss: \u001b[1m\u001b[32m0.16211\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 020 | loss: 0.16211 - R2: 0.9982 -- iter: 0576/1168\n",
      "Training Step: 371  | total loss: \u001b[1m\u001b[32m0.16103\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 020 | loss: 0.16103 - R2: 0.9992 -- iter: 0640/1168\n",
      "Training Step: 372  | total loss: \u001b[1m\u001b[32m0.15996\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 020 | loss: 0.15996 - R2: 0.9992 -- iter: 0704/1168\n",
      "Training Step: 373  | total loss: \u001b[1m\u001b[32m0.16071\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 020 | loss: 0.16071 - R2: 0.9988 -- iter: 0768/1168\n",
      "Training Step: 374  | total loss: \u001b[1m\u001b[32m0.16071\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 020 | loss: 0.16071 - R2: 0.9988 -- iter: 0832/1168\n",
      "Training Step: 375  | total loss: \u001b[1m\u001b[32m0.15760\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 020 | loss: 0.15760 - R2: 0.9983 -- iter: 0896/1168\n",
      "Training Step: 376  | total loss: \u001b[1m\u001b[32m0.15760\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 020 | loss: 0.15760 - R2: 0.9983 -- iter: 0960/1168\n",
      "Training Step: 377  | total loss: \u001b[1m\u001b[32m0.15725\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 020 | loss: 0.15725 - R2: 0.9993 -- iter: 1024/1168\n",
      "Training Step: 378  | total loss: \u001b[1m\u001b[32m0.15728\u001b[0m\u001b[0m | time: 0.165s\n",
      "| SGD | epoch: 020 | loss: 0.15728 - R2: 0.9990 -- iter: 1088/1168\n",
      "Training Step: 379  | total loss: \u001b[1m\u001b[32m0.15053\u001b[0m\u001b[0m | time: 0.170s\n",
      "| SGD | epoch: 020 | loss: 0.15053 - R2: 0.9988 -- iter: 1152/1168\n",
      "Training Step: 380  | total loss: \u001b[1m\u001b[32m0.14445\u001b[0m\u001b[0m | time: 1.178s\n",
      "| SGD | epoch: 020 | loss: 0.14445 - R2: 0.9985 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 381  | total loss: \u001b[1m\u001b[32m0.14468\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 021 | loss: 0.14468 - R2: 0.9990 -- iter: 0064/1168\n",
      "Training Step: 382  | total loss: \u001b[1m\u001b[32m0.14468\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 021 | loss: 0.14468 - R2: 0.9988 -- iter: 0128/1168\n",
      "Training Step: 383  | total loss: \u001b[1m\u001b[32m0.14734\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 021 | loss: 0.14734 - R2: 0.9988 -- iter: 0192/1168\n",
      "Training Step: 384  | total loss: \u001b[1m\u001b[32m0.15138\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 021 | loss: 0.15138 - R2: 0.9975 -- iter: 0256/1168\n",
      "Training Step: 385  | total loss: \u001b[1m\u001b[32m0.14794\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 021 | loss: 0.14794 - R2: 0.9973 -- iter: 0320/1168\n",
      "Training Step: 386  | total loss: \u001b[1m\u001b[32m0.14990\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 021 | loss: 0.14990 - R2: 0.9978 -- iter: 0384/1168\n",
      "Training Step: 387  | total loss: \u001b[1m\u001b[32m0.15452\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 021 | loss: 0.15452 - R2: 0.9974 -- iter: 0448/1168\n",
      "Training Step: 388  | total loss: \u001b[1m\u001b[32m0.15897\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 021 | loss: 0.15897 - R2: 0.9982 -- iter: 0512/1168\n",
      "Training Step: 389  | total loss: \u001b[1m\u001b[32m0.16149\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 021 | loss: 0.16149 - R2: 0.9987 -- iter: 0576/1168\n",
      "Training Step: 390  | total loss: \u001b[1m\u001b[32m0.16149\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 021 | loss: 0.16149 - R2: 0.9987 -- iter: 0640/1168\n",
      "Training Step: 391  | total loss: \u001b[1m\u001b[32m0.15614\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 021 | loss: 0.15614 - R2: 0.9996 -- iter: 0704/1168\n",
      "Training Step: 392  | total loss: \u001b[1m\u001b[32m0.15742\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 021 | loss: 0.15742 - R2: 0.9996 -- iter: 0768/1168\n",
      "Training Step: 393  | total loss: \u001b[1m\u001b[32m0.16054\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 021 | loss: 0.16054 - R2: 0.9980 -- iter: 0832/1168\n",
      "Training Step: 394  | total loss: \u001b[1m\u001b[32m0.16054\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 021 | loss: 0.16054 - R2: 0.9980 -- iter: 0896/1168\n",
      "Training Step: 395  | total loss: \u001b[1m\u001b[32m0.15329\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 021 | loss: 0.15329 - R2: 0.9974 -- iter: 0960/1168\n",
      "Training Step: 396  | total loss: \u001b[1m\u001b[32m0.15155\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 021 | loss: 0.15155 - R2: 0.9966 -- iter: 1024/1168\n",
      "Training Step: 397  | total loss: \u001b[1m\u001b[32m0.15389\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 021 | loss: 0.15389 - R2: 0.9973 -- iter: 1088/1168\n",
      "Training Step: 398  | total loss: \u001b[1m\u001b[32m0.15293\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 021 | loss: 0.15293 - R2: 0.9985 -- iter: 1152/1168\n",
      "Training Step: 399  | total loss: \u001b[1m\u001b[32m0.15293\u001b[0m\u001b[0m | time: 1.165s\n",
      "| SGD | epoch: 021 | loss: 0.15293 - R2: 0.9985 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 400  | total loss: \u001b[1m\u001b[32m0.15947\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 022 | loss: 0.15947 - R2: 1.0005 -- iter: 0064/1168\n",
      "Training Step: 401  | total loss: \u001b[1m\u001b[32m0.15947\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 022 | loss: 0.15947 - R2: 1.0005 -- iter: 0128/1168\n",
      "Training Step: 402  | total loss: \u001b[1m\u001b[32m0.16186\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 022 | loss: 0.16186 - R2: 0.9992 -- iter: 0192/1168\n",
      "Training Step: 403  | total loss: \u001b[1m\u001b[32m0.16004\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 022 | loss: 0.16004 - R2: 0.9972 -- iter: 0256/1168\n",
      "Training Step: 404  | total loss: \u001b[1m\u001b[32m0.16004\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 022 | loss: 0.16004 - R2: 0.9972 -- iter: 0320/1168\n",
      "Training Step: 405  | total loss: \u001b[1m\u001b[32m0.16517\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 022 | loss: 0.16517 - R2: 0.9986 -- iter: 0384/1168\n",
      "Training Step: 406  | total loss: \u001b[1m\u001b[32m0.16197\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 022 | loss: 0.16197 - R2: 0.9983 -- iter: 0448/1168\n",
      "Training Step: 407  | total loss: \u001b[1m\u001b[32m0.15587\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 022 | loss: 0.15587 - R2: 0.9977 -- iter: 0512/1168\n",
      "Training Step: 408  | total loss: \u001b[1m\u001b[32m0.15587\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 022 | loss: 0.15587 - R2: 0.9977 -- iter: 0576/1168\n",
      "Training Step: 409  | total loss: \u001b[1m\u001b[32m0.15366\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 022 | loss: 0.15366 - R2: 0.9965 -- iter: 0640/1168\n",
      "Training Step: 410  | total loss: \u001b[1m\u001b[32m0.15854\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 022 | loss: 0.15854 - R2: 0.9975 -- iter: 0704/1168\n",
      "Training Step: 411  | total loss: \u001b[1m\u001b[32m0.17172\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 022 | loss: 0.17172 - R2: 0.9971 -- iter: 0768/1168\n",
      "Training Step: 412  | total loss: \u001b[1m\u001b[32m0.17002\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 022 | loss: 0.17002 - R2: 0.9967 -- iter: 0832/1168\n",
      "Training Step: 413  | total loss: \u001b[1m\u001b[32m0.16843\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 022 | loss: 0.16843 - R2: 0.9970 -- iter: 0896/1168\n",
      "Training Step: 414  | total loss: \u001b[1m\u001b[32m0.16843\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 022 | loss: 0.16843 - R2: 0.9974 -- iter: 0960/1168\n",
      "Training Step: 415  | total loss: \u001b[1m\u001b[32m0.16789\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 022 | loss: 0.16789 - R2: 0.9979 -- iter: 1024/1168\n",
      "Training Step: 416  | total loss: \u001b[1m\u001b[32m0.17370\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 022 | loss: 0.17370 - R2: 0.9994 -- iter: 1088/1168\n",
      "Training Step: 417  | total loss: \u001b[1m\u001b[32m0.17370\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 022 | loss: 0.17370 - R2: 0.9994 -- iter: 1152/1168\n",
      "Training Step: 418  | total loss: \u001b[1m\u001b[32m0.16908\u001b[0m\u001b[0m | time: 1.150s\n",
      "| SGD | epoch: 022 | loss: 0.16908 - R2: 0.9994 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 419  | total loss: \u001b[1m\u001b[32m0.16194\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 023 | loss: 0.16194 - R2: 1.0009 -- iter: 0064/1168\n",
      "Training Step: 420  | total loss: \u001b[1m\u001b[32m0.16145\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 023 | loss: 0.16145 - R2: 1.0017 -- iter: 0128/1168\n",
      "Training Step: 421  | total loss: \u001b[1m\u001b[32m0.15751\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 023 | loss: 0.15751 - R2: 1.0025 -- iter: 0192/1168\n",
      "Training Step: 422  | total loss: \u001b[1m\u001b[32m0.15751\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 023 | loss: 0.15751 - R2: 1.0025 -- iter: 0256/1168\n",
      "Training Step: 423  | total loss: \u001b[1m\u001b[32m0.15536\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 023 | loss: 0.15536 - R2: 1.0012 -- iter: 0320/1168\n",
      "Training Step: 424  | total loss: \u001b[1m\u001b[32m0.15278\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 023 | loss: 0.15278 - R2: 1.0013 -- iter: 0384/1168\n",
      "Training Step: 425  | total loss: \u001b[1m\u001b[32m0.15363\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 023 | loss: 0.15363 - R2: 0.9996 -- iter: 0448/1168\n",
      "Training Step: 426  | total loss: \u001b[1m\u001b[32m0.15354\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 023 | loss: 0.15354 - R2: 0.9996 -- iter: 0512/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 427  | total loss: \u001b[1m\u001b[32m0.15649\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 023 | loss: 0.15649 - R2: 0.9988 -- iter: 0576/1168\n",
      "Training Step: 428  | total loss: \u001b[1m\u001b[32m0.14983\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 023 | loss: 0.14983 - R2: 0.9976 -- iter: 0640/1168\n",
      "Training Step: 429  | total loss: \u001b[1m\u001b[32m0.14983\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 023 | loss: 0.14983 - R2: 0.9976 -- iter: 0704/1168\n",
      "Training Step: 430  | total loss: \u001b[1m\u001b[32m0.15439\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 023 | loss: 0.15439 - R2: 0.9968 -- iter: 0768/1168\n",
      "Training Step: 431  | total loss: \u001b[1m\u001b[32m0.15140\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 023 | loss: 0.15140 - R2: 0.9967 -- iter: 0832/1168\n",
      "Training Step: 432  | total loss: \u001b[1m\u001b[32m0.14952\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 023 | loss: 0.14952 - R2: 0.9976 -- iter: 0896/1168\n",
      "Training Step: 433  | total loss: \u001b[1m\u001b[32m0.14996\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 023 | loss: 0.14996 - R2: 0.9980 -- iter: 0960/1168\n",
      "Training Step: 434  | total loss: \u001b[1m\u001b[32m0.15214\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 023 | loss: 0.15214 - R2: 0.9980 -- iter: 1024/1168\n",
      "Training Step: 435  | total loss: \u001b[1m\u001b[32m0.15390\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 023 | loss: 0.15390 - R2: 0.9974 -- iter: 1088/1168\n",
      "Training Step: 436  | total loss: \u001b[1m\u001b[32m0.15897\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 023 | loss: 0.15897 - R2: 0.9977 -- iter: 1152/1168\n",
      "Training Step: 437  | total loss: \u001b[1m\u001b[32m0.15897\u001b[0m\u001b[0m | time: 1.131s\n",
      "| SGD | epoch: 023 | loss: 0.15897 - R2: 0.9977 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 438  | total loss: \u001b[1m\u001b[32m0.16428\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 024 | loss: 0.16428 - R2: 0.9995 -- iter: 0064/1168\n",
      "Training Step: 439  | total loss: \u001b[1m\u001b[32m0.16595\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 024 | loss: 0.16595 - R2: 0.9987 -- iter: 0128/1168\n",
      "Training Step: 440  | total loss: \u001b[1m\u001b[32m0.16409\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 024 | loss: 0.16409 - R2: 1.0012 -- iter: 0192/1168\n",
      "Training Step: 441  | total loss: \u001b[1m\u001b[32m0.16241\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 024 | loss: 0.16241 - R2: 1.0035 -- iter: 0256/1168\n",
      "Training Step: 442  | total loss: \u001b[1m\u001b[32m0.15701\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 024 | loss: 0.15701 - R2: 1.0035 -- iter: 0320/1168\n",
      "Training Step: 443  | total loss: \u001b[1m\u001b[32m0.15701\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 024 | loss: 0.15701 - R2: 1.0035 -- iter: 0384/1168\n",
      "Training Step: 444  | total loss: \u001b[1m\u001b[32m0.15179\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 024 | loss: 0.15179 - R2: 1.0013 -- iter: 0448/1168\n",
      "Training Step: 445  | total loss: \u001b[1m\u001b[32m0.15179\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 024 | loss: 0.15179 - R2: 1.0013 -- iter: 0512/1168\n",
      "Training Step: 446  | total loss: \u001b[1m\u001b[32m0.15475\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 024 | loss: 0.15475 - R2: 1.0009 -- iter: 0576/1168\n",
      "Training Step: 447  | total loss: \u001b[1m\u001b[32m0.15790\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 024 | loss: 0.15790 - R2: 1.0000 -- iter: 0640/1168\n",
      "Training Step: 448  | total loss: \u001b[1m\u001b[32m0.15562\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 024 | loss: 0.15562 - R2: 0.9999 -- iter: 0704/1168\n",
      "Training Step: 449  | total loss: \u001b[1m\u001b[32m0.15509\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 024 | loss: 0.15509 - R2: 0.9987 -- iter: 0768/1168\n",
      "Training Step: 450  | total loss: \u001b[1m\u001b[32m0.15857\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 024 | loss: 0.15857 - R2: 0.9978 -- iter: 0832/1168\n",
      "Training Step: 451  | total loss: \u001b[1m\u001b[32m0.16354\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 024 | loss: 0.16354 - R2: 0.9978 -- iter: 0896/1168\n",
      "Training Step: 452  | total loss: \u001b[1m\u001b[32m0.16139\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 024 | loss: 0.16139 - R2: 0.9965 -- iter: 0960/1168\n",
      "Training Step: 453  | total loss: \u001b[1m\u001b[32m0.16382\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 024 | loss: 0.16382 - R2: 0.9960 -- iter: 1024/1168\n",
      "Training Step: 454  | total loss: \u001b[1m\u001b[32m0.16502\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 024 | loss: 0.16502 - R2: 0.9958 -- iter: 1088/1168\n",
      "Training Step: 455  | total loss: \u001b[1m\u001b[32m0.16418\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 024 | loss: 0.16418 - R2: 0.9964 -- iter: 1152/1168\n",
      "Training Step: 456  | total loss: \u001b[1m\u001b[32m0.17048\u001b[0m\u001b[0m | time: 1.120s\n",
      "| SGD | epoch: 024 | loss: 0.17048 - R2: 0.9956 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 457  | total loss: \u001b[1m\u001b[32m0.16159\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 025 | loss: 0.16159 - R2: 0.9972 -- iter: 0064/1168\n",
      "Training Step: 458  | total loss: \u001b[1m\u001b[32m0.16009\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 025 | loss: 0.16009 - R2: 0.9965 -- iter: 0128/1168\n",
      "Training Step: 459  | total loss: \u001b[1m\u001b[32m0.16516\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 025 | loss: 0.16516 - R2: 0.9982 -- iter: 0192/1168\n",
      "Training Step: 460  | total loss: \u001b[1m\u001b[32m0.16971\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 025 | loss: 0.16971 - R2: 0.9998 -- iter: 0256/1168\n",
      "Training Step: 461  | total loss: \u001b[1m\u001b[32m0.16455\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 025 | loss: 0.16455 - R2: 1.0005 -- iter: 0320/1168\n",
      "Training Step: 462  | total loss: \u001b[1m\u001b[32m0.16455\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 025 | loss: 0.16455 - R2: 1.0005 -- iter: 0384/1168\n",
      "Training Step: 463  | total loss: \u001b[1m\u001b[32m0.16653\u001b[0m\u001b[0m | time: 0.141s\n",
      "| SGD | epoch: 025 | loss: 0.16653 - R2: 1.0004 -- iter: 0448/1168\n",
      "Training Step: 464  | total loss: \u001b[1m\u001b[32m0.16395\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 025 | loss: 0.16395 - R2: 1.0011 -- iter: 0512/1168\n",
      "Training Step: 465  | total loss: \u001b[1m\u001b[32m0.16467\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 025 | loss: 0.16467 - R2: 1.0012 -- iter: 0576/1168\n",
      "Training Step: 466  | total loss: \u001b[1m\u001b[32m0.15902\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 025 | loss: 0.15902 - R2: 1.0014 -- iter: 0640/1168\n",
      "Training Step: 467  | total loss: \u001b[1m\u001b[32m0.15674\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 025 | loss: 0.15674 - R2: 1.0022 -- iter: 0704/1168\n",
      "Training Step: 468  | total loss: \u001b[1m\u001b[32m0.15674\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 025 | loss: 0.15674 - R2: 1.0014 -- iter: 0768/1168\n",
      "Training Step: 469  | total loss: \u001b[1m\u001b[32m0.15791\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 025 | loss: 0.15791 - R2: 1.0004 -- iter: 0832/1168\n",
      "Training Step: 470  | total loss: \u001b[1m\u001b[32m0.15791\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 025 | loss: 0.15791 - R2: 1.0004 -- iter: 0896/1168\n",
      "Training Step: 471  | total loss: \u001b[1m\u001b[32m0.15782\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 025 | loss: 0.15782 - R2: 1.0000 -- iter: 0960/1168\n",
      "Training Step: 472  | total loss: \u001b[1m\u001b[32m0.16192\u001b[0m\u001b[0m | time: 0.171s\n",
      "| SGD | epoch: 025 | loss: 0.16192 - R2: 0.9994 -- iter: 1024/1168\n",
      "Training Step: 473  | total loss: \u001b[1m\u001b[32m0.16540\u001b[0m\u001b[0m | time: 0.176s\n",
      "| SGD | epoch: 025 | loss: 0.16540 - R2: 0.9988 -- iter: 1088/1168\n",
      "Training Step: 474  | total loss: \u001b[1m\u001b[32m0.16688\u001b[0m\u001b[0m | time: 0.184s\n",
      "| SGD | epoch: 025 | loss: 0.16688 - R2: 0.9990 -- iter: 1152/1168\n",
      "Training Step: 475  | total loss: \u001b[1m\u001b[32m0.16688\u001b[0m\u001b[0m | time: 1.189s\n",
      "| SGD | epoch: 025 | loss: 0.16688 - R2: 0.9990 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 476  | total loss: \u001b[1m\u001b[32m0.16406\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 026 | loss: 0.16406 - R2: 0.9979 -- iter: 0064/1168\n",
      "Training Step: 477  | total loss: \u001b[1m\u001b[32m0.16406\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 026 | loss: 0.16406 - R2: 0.9990 -- iter: 0128/1168\n",
      "Training Step: 478  | total loss: \u001b[1m\u001b[32m0.16347\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 026 | loss: 0.16347 - R2: 0.9990 -- iter: 0192/1168\n",
      "Training Step: 479  | total loss: \u001b[1m\u001b[32m0.16010\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 026 | loss: 0.16010 - R2: 0.9997 -- iter: 0256/1168\n",
      "Training Step: 480  | total loss: \u001b[1m\u001b[32m0.15962\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 026 | loss: 0.15962 - R2: 0.9954 -- iter: 0320/1168\n",
      "Training Step: 481  | total loss: \u001b[1m\u001b[32m0.15988\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 026 | loss: 0.15988 - R2: 0.9950 -- iter: 0384/1168\n",
      "Training Step: 482  | total loss: \u001b[1m\u001b[32m0.15697\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 026 | loss: 0.15697 - R2: 0.9946 -- iter: 0448/1168\n",
      "Training Step: 483  | total loss: \u001b[1m\u001b[32m0.15697\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 026 | loss: 0.15697 - R2: 0.9946 -- iter: 0512/1168\n",
      "Training Step: 484  | total loss: \u001b[1m\u001b[32m0.15478\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 026 | loss: 0.15478 - R2: 0.9961 -- iter: 0576/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 485  | total loss: \u001b[1m\u001b[32m0.16075\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 026 | loss: 0.16075 - R2: 0.9988 -- iter: 0640/1168\n",
      "Training Step: 486  | total loss: \u001b[1m\u001b[32m0.16075\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 026 | loss: 0.16075 - R2: 0.9982 -- iter: 0704/1168\n",
      "Training Step: 487  | total loss: \u001b[1m\u001b[32m0.15350\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 026 | loss: 0.15350 - R2: 0.9982 -- iter: 0768/1168\n",
      "Training Step: 488  | total loss: \u001b[1m\u001b[32m0.14898\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 026 | loss: 0.14898 - R2: 0.9996 -- iter: 0832/1168\n",
      "Training Step: 489  | total loss: \u001b[1m\u001b[32m0.15722\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 026 | loss: 0.15722 - R2: 0.9996 -- iter: 0896/1168\n",
      "Training Step: 490  | total loss: \u001b[1m\u001b[32m0.15616\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 026 | loss: 0.15616 - R2: 0.9989 -- iter: 0960/1168\n",
      "Training Step: 491  | total loss: \u001b[1m\u001b[32m0.15555\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 026 | loss: 0.15555 - R2: 0.9986 -- iter: 1024/1168\n",
      "Training Step: 492  | total loss: \u001b[1m\u001b[32m0.15457\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 026 | loss: 0.15457 - R2: 0.9988 -- iter: 1088/1168\n",
      "Training Step: 493  | total loss: \u001b[1m\u001b[32m0.15457\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 026 | loss: 0.15457 - R2: 0.9988 -- iter: 1152/1168\n",
      "Training Step: 494  | total loss: \u001b[1m\u001b[32m0.15658\u001b[0m\u001b[0m | time: 1.167s\n",
      "| SGD | epoch: 026 | loss: 0.15658 - R2: 0.9974 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 495  | total loss: \u001b[1m\u001b[32m0.15655\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 027 | loss: 0.15655 - R2: 0.9986 -- iter: 0064/1168\n",
      "Training Step: 496  | total loss: \u001b[1m\u001b[32m0.15999\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 027 | loss: 0.15999 - R2: 0.9998 -- iter: 0128/1168\n",
      "Training Step: 497  | total loss: \u001b[1m\u001b[32m0.15532\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 027 | loss: 0.15532 - R2: 0.9998 -- iter: 0192/1168\n",
      "Training Step: 498  | total loss: \u001b[1m\u001b[32m0.15536\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 027 | loss: 0.15536 - R2: 0.9996 -- iter: 0256/1168\n",
      "Training Step: 499  | total loss: \u001b[1m\u001b[32m0.15196\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 027 | loss: 0.15196 - R2: 0.9986 -- iter: 0320/1168\n",
      "Training Step: 500  | total loss: \u001b[1m\u001b[32m0.15570\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 027 | loss: 0.15570 - R2: 0.9956 -- iter: 0384/1168\n",
      "Training Step: 501  | total loss: \u001b[1m\u001b[32m0.15906\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 027 | loss: 0.15906 - R2: 0.9929 -- iter: 0448/1168\n",
      "Training Step: 502  | total loss: \u001b[1m\u001b[32m0.16108\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 027 | loss: 0.16108 - R2: 0.9926 -- iter: 0512/1168\n",
      "Training Step: 503  | total loss: \u001b[1m\u001b[32m0.15903\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 027 | loss: 0.15903 - R2: 0.9925 -- iter: 0576/1168\n",
      "Training Step: 504  | total loss: \u001b[1m\u001b[32m0.16371\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 027 | loss: 0.16371 - R2: 0.9945 -- iter: 0640/1168\n",
      "Training Step: 505  | total loss: \u001b[1m\u001b[32m0.16905\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 027 | loss: 0.16905 - R2: 0.9957 -- iter: 0704/1168\n",
      "Training Step: 506  | total loss: \u001b[1m\u001b[32m0.16236\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 027 | loss: 0.16236 - R2: 0.9967 -- iter: 0768/1168\n",
      "Training Step: 507  | total loss: \u001b[1m\u001b[32m0.16195\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 027 | loss: 0.16195 - R2: 0.9954 -- iter: 0832/1168\n",
      "Training Step: 508  | total loss: \u001b[1m\u001b[32m0.16461\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 027 | loss: 0.16461 - R2: 0.9954 -- iter: 0896/1168\n",
      "Training Step: 509  | total loss: \u001b[1m\u001b[32m0.16563\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 027 | loss: 0.16563 - R2: 0.9951 -- iter: 0960/1168\n",
      "Training Step: 510  | total loss: \u001b[1m\u001b[32m0.16383\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 027 | loss: 0.16383 - R2: 0.9968 -- iter: 1024/1168\n",
      "Training Step: 511  | total loss: \u001b[1m\u001b[32m0.16383\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 027 | loss: 0.16383 - R2: 0.9968 -- iter: 1088/1168\n",
      "Training Step: 512  | total loss: \u001b[1m\u001b[32m0.16325\u001b[0m\u001b[0m | time: 0.159s\n",
      "| SGD | epoch: 027 | loss: 0.16325 - R2: 0.9948 -- iter: 1152/1168\n",
      "Training Step: 513  | total loss: \u001b[1m\u001b[32m0.16325\u001b[0m\u001b[0m | time: 1.168s\n",
      "| SGD | epoch: 027 | loss: 0.16325 - R2: 0.9948 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 514  | total loss: \u001b[1m\u001b[32m0.15947\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 028 | loss: 0.15947 - R2: 0.9958 -- iter: 0064/1168\n",
      "Training Step: 515  | total loss: \u001b[1m\u001b[32m0.15947\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 028 | loss: 0.15947 - R2: 0.9958 -- iter: 0128/1168\n",
      "Training Step: 516  | total loss: \u001b[1m\u001b[32m0.15598\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 028 | loss: 0.15598 - R2: 0.9963 -- iter: 0192/1168\n",
      "Training Step: 517  | total loss: \u001b[1m\u001b[32m0.15529\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 028 | loss: 0.15529 - R2: 0.9977 -- iter: 0256/1168\n",
      "Training Step: 518  | total loss: \u001b[1m\u001b[32m0.15242\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 028 | loss: 0.15242 - R2: 0.9982 -- iter: 0320/1168\n",
      "Training Step: 519  | total loss: \u001b[1m\u001b[32m0.15420\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 028 | loss: 0.15420 - R2: 0.9986 -- iter: 0384/1168\n",
      "Training Step: 520  | total loss: \u001b[1m\u001b[32m0.15420\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 028 | loss: 0.15420 - R2: 0.9986 -- iter: 0448/1168\n",
      "Training Step: 521  | total loss: \u001b[1m\u001b[32m0.15795\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 028 | loss: 0.15795 - R2: 0.9987 -- iter: 0512/1168\n",
      "Training Step: 522  | total loss: \u001b[1m\u001b[32m0.15795\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 028 | loss: 0.15795 - R2: 0.9988 -- iter: 0576/1168\n",
      "Training Step: 523  | total loss: \u001b[1m\u001b[32m0.15685\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 028 | loss: 0.15685 - R2: 0.9993 -- iter: 0640/1168\n",
      "Training Step: 524  | total loss: \u001b[1m\u001b[32m0.16076\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 028 | loss: 0.16076 - R2: 1.0006 -- iter: 0704/1168\n",
      "Training Step: 525  | total loss: \u001b[1m\u001b[32m0.16076\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 028 | loss: 0.16076 - R2: 1.0001 -- iter: 0768/1168\n",
      "Training Step: 526  | total loss: \u001b[1m\u001b[32m0.16041\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 028 | loss: 0.16041 - R2: 0.9982 -- iter: 0832/1168\n",
      "Training Step: 527  | total loss: \u001b[1m\u001b[32m0.16041\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 028 | loss: 0.16041 - R2: 0.9982 -- iter: 0896/1168\n",
      "Training Step: 528  | total loss: \u001b[1m\u001b[32m0.15870\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 028 | loss: 0.15870 - R2: 0.9971 -- iter: 0960/1168\n",
      "Training Step: 529  | total loss: \u001b[1m\u001b[32m0.15860\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 028 | loss: 0.15860 - R2: 0.9966 -- iter: 1024/1168\n",
      "Training Step: 530  | total loss: \u001b[1m\u001b[32m0.15620\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 028 | loss: 0.15620 - R2: 0.9973 -- iter: 1088/1168\n",
      "Training Step: 531  | total loss: \u001b[1m\u001b[32m0.15620\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 028 | loss: 0.15620 - R2: 0.9973 -- iter: 1152/1168\n",
      "Training Step: 532  | total loss: \u001b[1m\u001b[32m0.16566\u001b[0m\u001b[0m | time: 1.138s\n",
      "| SGD | epoch: 028 | loss: 0.16566 - R2: 0.9970 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 533  | total loss: \u001b[1m\u001b[32m0.16566\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 029 | loss: 0.16566 - R2: 0.9970 -- iter: 0064/1168\n",
      "Training Step: 534  | total loss: \u001b[1m\u001b[32m0.16174\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 029 | loss: 0.16174 - R2: 0.9977 -- iter: 0128/1168\n",
      "Training Step: 535  | total loss: \u001b[1m\u001b[32m0.16110\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 029 | loss: 0.16110 - R2: 0.9980 -- iter: 0192/1168\n",
      "Training Step: 536  | total loss: \u001b[1m\u001b[32m0.16165\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 029 | loss: 0.16165 - R2: 0.9986 -- iter: 0256/1168\n",
      "Training Step: 537  | total loss: \u001b[1m\u001b[32m0.16343\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 029 | loss: 0.16343 - R2: 0.9990 -- iter: 0320/1168\n",
      "Training Step: 538  | total loss: \u001b[1m\u001b[32m0.16343\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 029 | loss: 0.16343 - R2: 0.9990 -- iter: 0384/1168\n",
      "Training Step: 539  | total loss: \u001b[1m\u001b[32m0.16614\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 029 | loss: 0.16614 - R2: 0.9979 -- iter: 0448/1168\n",
      "Training Step: 540  | total loss: \u001b[1m\u001b[32m0.16770\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 029 | loss: 0.16770 - R2: 0.9970 -- iter: 0512/1168\n",
      "Training Step: 541  | total loss: \u001b[1m\u001b[32m0.16770\u001b[0m\u001b[0m | time: 0.130s\n",
      "| SGD | epoch: 029 | loss: 0.16770 - R2: 0.9970 -- iter: 0576/1168\n",
      "Training Step: 542  | total loss: \u001b[1m\u001b[32m0.16436\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 029 | loss: 0.16436 - R2: 0.9952 -- iter: 0640/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 543  | total loss: \u001b[1m\u001b[32m0.16436\u001b[0m\u001b[0m | time: 0.140s\n",
      "| SGD | epoch: 029 | loss: 0.16436 - R2: 0.9952 -- iter: 0704/1168\n",
      "Training Step: 544  | total loss: \u001b[1m\u001b[32m0.15840\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 029 | loss: 0.15840 - R2: 0.9967 -- iter: 0768/1168\n",
      "Training Step: 545  | total loss: \u001b[1m\u001b[32m0.15840\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 029 | loss: 0.15840 - R2: 0.9967 -- iter: 0832/1168\n",
      "Training Step: 546  | total loss: \u001b[1m\u001b[32m0.15615\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 029 | loss: 0.15615 - R2: 0.9975 -- iter: 0896/1168\n",
      "Training Step: 547  | total loss: \u001b[1m\u001b[32m0.15615\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 029 | loss: 0.15615 - R2: 0.9975 -- iter: 0960/1168\n",
      "Training Step: 548  | total loss: \u001b[1m\u001b[32m0.15433\u001b[0m\u001b[0m | time: 0.163s\n",
      "| SGD | epoch: 029 | loss: 0.15433 - R2: 0.9985 -- iter: 1024/1168\n",
      "Training Step: 549  | total loss: \u001b[1m\u001b[32m0.15317\u001b[0m\u001b[0m | time: 0.166s\n",
      "| SGD | epoch: 029 | loss: 0.15317 - R2: 0.9986 -- iter: 1088/1168\n",
      "Training Step: 550  | total loss: \u001b[1m\u001b[32m0.15637\u001b[0m\u001b[0m | time: 0.169s\n",
      "| SGD | epoch: 029 | loss: 0.15637 - R2: 0.9989 -- iter: 1152/1168\n",
      "Training Step: 551  | total loss: \u001b[1m\u001b[32m0.15967\u001b[0m\u001b[0m | time: 1.179s\n",
      "| SGD | epoch: 029 | loss: 0.15967 - R2: 0.9982 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 552  | total loss: \u001b[1m\u001b[32m0.15447\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 030 | loss: 0.15447 - R2: 0.9995 -- iter: 0064/1168\n",
      "Training Step: 553  | total loss: \u001b[1m\u001b[32m0.15462\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 030 | loss: 0.15462 - R2: 0.9999 -- iter: 0128/1168\n",
      "Training Step: 554  | total loss: \u001b[1m\u001b[32m0.15443\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 030 | loss: 0.15443 - R2: 0.9999 -- iter: 0192/1168\n",
      "Training Step: 555  | total loss: \u001b[1m\u001b[32m0.16073\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 030 | loss: 0.16073 - R2: 0.9999 -- iter: 0256/1168\n",
      "Training Step: 556  | total loss: \u001b[1m\u001b[32m0.16287\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 030 | loss: 0.16287 - R2: 1.0013 -- iter: 0320/1168\n",
      "Training Step: 557  | total loss: \u001b[1m\u001b[32m0.16491\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 030 | loss: 0.16491 - R2: 1.0013 -- iter: 0384/1168\n",
      "Training Step: 558  | total loss: \u001b[1m\u001b[32m0.16066\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 030 | loss: 0.16066 - R2: 1.0003 -- iter: 0448/1168\n",
      "Training Step: 559  | total loss: \u001b[1m\u001b[32m0.15859\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 030 | loss: 0.15859 - R2: 0.9995 -- iter: 0512/1168\n",
      "Training Step: 560  | total loss: \u001b[1m\u001b[32m0.15874\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 030 | loss: 0.15874 - R2: 0.9966 -- iter: 0576/1168\n",
      "Training Step: 561  | total loss: \u001b[1m\u001b[32m0.15887\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 030 | loss: 0.15887 - R2: 0.9939 -- iter: 0640/1168\n",
      "Training Step: 562  | total loss: \u001b[1m\u001b[32m0.15152\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 030 | loss: 0.15152 - R2: 0.9946 -- iter: 0704/1168\n",
      "Training Step: 563  | total loss: \u001b[1m\u001b[32m0.15592\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 030 | loss: 0.15592 - R2: 0.9943 -- iter: 0768/1168\n",
      "Training Step: 564  | total loss: \u001b[1m\u001b[32m0.16478\u001b[0m\u001b[0m | time: 0.136s\n",
      "| SGD | epoch: 030 | loss: 0.16478 - R2: 0.9937 -- iter: 0832/1168\n",
      "Training Step: 565  | total loss: \u001b[1m\u001b[32m0.16478\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 030 | loss: 0.16478 - R2: 0.9937 -- iter: 0896/1168\n",
      "Training Step: 566  | total loss: \u001b[1m\u001b[32m0.15941\u001b[0m\u001b[0m | time: 0.150s\n",
      "| SGD | epoch: 030 | loss: 0.15941 - R2: 0.9938 -- iter: 0960/1168\n",
      "Training Step: 567  | total loss: \u001b[1m\u001b[32m0.15941\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 030 | loss: 0.15941 - R2: 0.9938 -- iter: 1024/1168\n",
      "Training Step: 568  | total loss: \u001b[1m\u001b[32m0.15726\u001b[0m\u001b[0m | time: 0.155s\n",
      "| SGD | epoch: 030 | loss: 0.15726 - R2: 0.9952 -- iter: 1088/1168\n",
      "Training Step: 569  | total loss: \u001b[1m\u001b[32m0.15625\u001b[0m\u001b[0m | time: 0.160s\n",
      "| SGD | epoch: 030 | loss: 0.15625 - R2: 0.9967 -- iter: 1152/1168\n",
      "Training Step: 570  | total loss: \u001b[1m\u001b[32m0.15966\u001b[0m\u001b[0m | time: 1.165s\n",
      "| SGD | epoch: 030 | loss: 0.15966 - R2: 0.9977 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 571  | total loss: \u001b[1m\u001b[32m0.16134\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 031 | loss: 0.16134 - R2: 0.9980 -- iter: 0064/1168\n",
      "Training Step: 572  | total loss: \u001b[1m\u001b[32m0.15641\u001b[0m\u001b[0m | time: 0.051s\n",
      "| SGD | epoch: 031 | loss: 0.15641 - R2: 0.9973 -- iter: 0128/1168\n",
      "Training Step: 573  | total loss: \u001b[1m\u001b[32m0.15309\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 031 | loss: 0.15309 - R2: 0.9973 -- iter: 0192/1168\n",
      "Training Step: 574  | total loss: \u001b[1m\u001b[32m0.15528\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 031 | loss: 0.15528 - R2: 0.9965 -- iter: 0256/1168\n",
      "Training Step: 575  | total loss: \u001b[1m\u001b[32m0.15902\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 031 | loss: 0.15902 - R2: 0.9961 -- iter: 0320/1168\n",
      "Training Step: 576  | total loss: \u001b[1m\u001b[32m0.15990\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 031 | loss: 0.15990 - R2: 0.9958 -- iter: 0384/1168\n",
      "Training Step: 577  | total loss: \u001b[1m\u001b[32m0.15926\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 031 | loss: 0.15926 - R2: 0.9961 -- iter: 0448/1168\n",
      "Training Step: 578  | total loss: \u001b[1m\u001b[32m0.15786\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 031 | loss: 0.15786 - R2: 0.9963 -- iter: 0512/1168\n",
      "Training Step: 579  | total loss: \u001b[1m\u001b[32m0.15686\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 031 | loss: 0.15686 - R2: 0.9967 -- iter: 0576/1168\n",
      "Training Step: 580  | total loss: \u001b[1m\u001b[32m0.15422\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 031 | loss: 0.15422 - R2: 0.9974 -- iter: 0640/1168\n",
      "Training Step: 581  | total loss: \u001b[1m\u001b[32m0.15185\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 031 | loss: 0.15185 - R2: 0.9981 -- iter: 0704/1168\n",
      "Training Step: 582  | total loss: \u001b[1m\u001b[32m0.16093\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 031 | loss: 0.16093 - R2: 0.9979 -- iter: 0768/1168\n",
      "Training Step: 583  | total loss: \u001b[1m\u001b[32m0.15911\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 031 | loss: 0.15911 - R2: 0.9969 -- iter: 0832/1168\n",
      "Training Step: 584  | total loss: \u001b[1m\u001b[32m0.15856\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 031 | loss: 0.15856 - R2: 0.9973 -- iter: 0896/1168\n",
      "Training Step: 585  | total loss: \u001b[1m\u001b[32m0.15754\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 031 | loss: 0.15754 - R2: 0.9980 -- iter: 0960/1168\n",
      "Training Step: 586  | total loss: \u001b[1m\u001b[32m0.15754\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 031 | loss: 0.15754 - R2: 0.9980 -- iter: 1024/1168\n",
      "Training Step: 587  | total loss: \u001b[1m\u001b[32m0.15703\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 031 | loss: 0.15703 - R2: 0.9993 -- iter: 1088/1168\n",
      "Training Step: 588  | total loss: \u001b[1m\u001b[32m0.15703\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 031 | loss: 0.15703 - R2: 0.9993 -- iter: 1152/1168\n",
      "Training Step: 589  | total loss: \u001b[1m\u001b[32m0.17003\u001b[0m\u001b[0m | time: 1.139s\n",
      "| SGD | epoch: 031 | loss: 0.17003 - R2: 0.9981 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 590  | total loss: \u001b[1m\u001b[32m0.17003\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 032 | loss: 0.17003 - R2: 0.9981 -- iter: 0064/1168\n",
      "Training Step: 591  | total loss: \u001b[1m\u001b[32m0.16667\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 032 | loss: 0.16667 - R2: 0.9993 -- iter: 0128/1168\n",
      "Training Step: 592  | total loss: \u001b[1m\u001b[32m0.16667\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 032 | loss: 0.16667 - R2: 0.9993 -- iter: 0192/1168\n",
      "Training Step: 593  | total loss: \u001b[1m\u001b[32m0.16462\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 032 | loss: 0.16462 - R2: 0.9974 -- iter: 0256/1168\n",
      "Training Step: 594  | total loss: \u001b[1m\u001b[32m0.16239\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 032 | loss: 0.16239 - R2: 0.9979 -- iter: 0320/1168\n",
      "Training Step: 595  | total loss: \u001b[1m\u001b[32m0.15862\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 032 | loss: 0.15862 - R2: 0.9983 -- iter: 0384/1168\n",
      "Training Step: 596  | total loss: \u001b[1m\u001b[32m0.16186\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 032 | loss: 0.16186 - R2: 0.9988 -- iter: 0448/1168\n",
      "Training Step: 597  | total loss: \u001b[1m\u001b[32m0.16163\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 032 | loss: 0.16163 - R2: 0.9992 -- iter: 0512/1168\n",
      "Training Step: 598  | total loss: \u001b[1m\u001b[32m0.15932\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 032 | loss: 0.15932 - R2: 0.9967 -- iter: 0576/1168\n",
      "Training Step: 599  | total loss: \u001b[1m\u001b[32m0.15932\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 032 | loss: 0.15932 - R2: 0.9973 -- iter: 0640/1168\n",
      "Training Step: 600  | total loss: \u001b[1m\u001b[32m0.15126\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 032 | loss: 0.15126 - R2: 0.9978 -- iter: 0704/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 601  | total loss: \u001b[1m\u001b[32m0.14503\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 032 | loss: 0.14503 - R2: 0.9982 -- iter: 0768/1168\n",
      "Training Step: 602  | total loss: \u001b[1m\u001b[32m0.15114\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 032 | loss: 0.15114 - R2: 0.9981 -- iter: 0832/1168\n",
      "Training Step: 603  | total loss: \u001b[1m\u001b[32m0.15114\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 032 | loss: 0.15114 - R2: 0.9981 -- iter: 0896/1168\n",
      "Training Step: 604  | total loss: \u001b[1m\u001b[32m0.15302\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 032 | loss: 0.15302 - R2: 0.9981 -- iter: 0960/1168\n",
      "Training Step: 605  | total loss: \u001b[1m\u001b[32m0.15664\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 032 | loss: 0.15664 - R2: 0.9989 -- iter: 1024/1168\n",
      "Training Step: 606  | total loss: \u001b[1m\u001b[32m0.15128\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 032 | loss: 0.15128 - R2: 0.9991 -- iter: 1088/1168\n",
      "Training Step: 607  | total loss: \u001b[1m\u001b[32m0.15128\u001b[0m\u001b[0m | time: 0.162s\n",
      "| SGD | epoch: 032 | loss: 0.15128 - R2: 0.9991 -- iter: 1152/1168\n",
      "Training Step: 608  | total loss: \u001b[1m\u001b[32m0.14972\u001b[0m\u001b[0m | time: 1.170s\n",
      "| SGD | epoch: 032 | loss: 0.14972 - R2: 0.9991 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 609  | total loss: \u001b[1m\u001b[32m0.15153\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 033 | loss: 0.15153 - R2: 0.9992 -- iter: 0064/1168\n",
      "Training Step: 610  | total loss: \u001b[1m\u001b[32m0.15404\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 033 | loss: 0.15404 - R2: 0.9978 -- iter: 0128/1168\n",
      "Training Step: 611  | total loss: \u001b[1m\u001b[32m0.15404\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 033 | loss: 0.15404 - R2: 0.9978 -- iter: 0192/1168\n",
      "Training Step: 612  | total loss: \u001b[1m\u001b[32m0.15512\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 033 | loss: 0.15512 - R2: 0.9971 -- iter: 0256/1168\n",
      "Training Step: 613  | total loss: \u001b[1m\u001b[32m0.15799\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 033 | loss: 0.15799 - R2: 0.9977 -- iter: 0320/1168\n",
      "Training Step: 614  | total loss: \u001b[1m\u001b[32m0.15919\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 033 | loss: 0.15919 - R2: 0.9984 -- iter: 0384/1168\n",
      "Training Step: 615  | total loss: \u001b[1m\u001b[32m0.16259\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 033 | loss: 0.16259 - R2: 0.9990 -- iter: 0448/1168\n",
      "Training Step: 616  | total loss: \u001b[1m\u001b[32m0.16259\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 033 | loss: 0.16259 - R2: 0.9990 -- iter: 0512/1168\n",
      "Training Step: 617  | total loss: \u001b[1m\u001b[32m0.16052\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 033 | loss: 0.16052 - R2: 0.9988 -- iter: 0576/1168\n",
      "Training Step: 618  | total loss: \u001b[1m\u001b[32m0.15376\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 033 | loss: 0.15376 - R2: 0.9979 -- iter: 0640/1168\n",
      "Training Step: 619  | total loss: \u001b[1m\u001b[32m0.15376\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 033 | loss: 0.15376 - R2: 0.9979 -- iter: 0704/1168\n",
      "Training Step: 620  | total loss: \u001b[1m\u001b[32m0.17134\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 033 | loss: 0.17134 - R2: 0.9949 -- iter: 0768/1168\n",
      "Training Step: 621  | total loss: \u001b[1m\u001b[32m0.18715\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 033 | loss: 0.18715 - R2: 0.9949 -- iter: 0832/1168\n",
      "Training Step: 622  | total loss: \u001b[1m\u001b[32m0.18166\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 033 | loss: 0.18166 - R2: 0.9944 -- iter: 0896/1168\n",
      "Training Step: 623  | total loss: \u001b[1m\u001b[32m0.17847\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 033 | loss: 0.17847 - R2: 0.9971 -- iter: 0960/1168\n",
      "Training Step: 624  | total loss: \u001b[1m\u001b[32m0.17847\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 033 | loss: 0.17847 - R2: 0.9971 -- iter: 1024/1168\n",
      "Training Step: 625  | total loss: \u001b[1m\u001b[32m0.17399\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 033 | loss: 0.17399 - R2: 0.9982 -- iter: 1088/1168\n",
      "Training Step: 626  | total loss: \u001b[1m\u001b[32m0.17104\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 033 | loss: 0.17104 - R2: 0.9972 -- iter: 1152/1168\n",
      "Training Step: 627  | total loss: \u001b[1m\u001b[32m0.17104\u001b[0m\u001b[0m | time: 1.115s\n",
      "| SGD | epoch: 033 | loss: 0.17104 - R2: 0.9972 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 628  | total loss: \u001b[1m\u001b[32m0.17277\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 034 | loss: 0.17277 - R2: 0.9978 -- iter: 0064/1168\n",
      "Training Step: 629  | total loss: \u001b[1m\u001b[32m0.17512\u001b[0m\u001b[0m | time: 0.007s\n",
      "| SGD | epoch: 034 | loss: 0.17512 - R2: 0.9979 -- iter: 0128/1168\n",
      "Training Step: 630  | total loss: \u001b[1m\u001b[32m0.17238\u001b[0m\u001b[0m | time: 0.011s\n",
      "| SGD | epoch: 034 | loss: 0.17238 - R2: 0.9974 -- iter: 0192/1168\n",
      "Training Step: 631  | total loss: \u001b[1m\u001b[32m0.17326\u001b[0m\u001b[0m | time: 0.017s\n",
      "| SGD | epoch: 034 | loss: 0.17326 - R2: 0.9986 -- iter: 0256/1168\n",
      "Training Step: 632  | total loss: \u001b[1m\u001b[32m0.16770\u001b[0m\u001b[0m | time: 0.019s\n",
      "| SGD | epoch: 034 | loss: 0.16770 - R2: 0.9989 -- iter: 0320/1168\n",
      "Training Step: 633  | total loss: \u001b[1m\u001b[32m0.16276\u001b[0m\u001b[0m | time: 0.023s\n",
      "| SGD | epoch: 034 | loss: 0.16276 - R2: 0.9984 -- iter: 0384/1168\n",
      "Training Step: 634  | total loss: \u001b[1m\u001b[32m0.16276\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD | epoch: 034 | loss: 0.16276 - R2: 0.9984 -- iter: 0448/1168\n",
      "Training Step: 635  | total loss: \u001b[1m\u001b[32m0.16416\u001b[0m\u001b[0m | time: 0.034s\n",
      "| SGD | epoch: 034 | loss: 0.16416 - R2: 0.9977 -- iter: 0512/1168\n",
      "Training Step: 636  | total loss: \u001b[1m\u001b[32m0.16416\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 034 | loss: 0.16416 - R2: 0.9977 -- iter: 0576/1168\n",
      "Training Step: 637  | total loss: \u001b[1m\u001b[32m0.16177\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 034 | loss: 0.16177 - R2: 0.9975 -- iter: 0640/1168\n",
      "Training Step: 638  | total loss: \u001b[1m\u001b[32m0.15807\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 034 | loss: 0.15807 - R2: 0.9971 -- iter: 0704/1168\n",
      "Training Step: 639  | total loss: \u001b[1m\u001b[32m0.15416\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 034 | loss: 0.15416 - R2: 1.0006 -- iter: 0768/1168\n",
      "Training Step: 640  | total loss: \u001b[1m\u001b[32m0.15416\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 034 | loss: 0.15416 - R2: 1.0006 -- iter: 0832/1168\n",
      "Training Step: 641  | total loss: \u001b[1m\u001b[32m0.15065\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 034 | loss: 0.15065 - R2: 1.0038 -- iter: 0896/1168\n",
      "Training Step: 642  | total loss: \u001b[1m\u001b[32m0.15050\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 034 | loss: 0.15050 - R2: 1.0029 -- iter: 0960/1168\n",
      "Training Step: 643  | total loss: \u001b[1m\u001b[32m0.15343\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 034 | loss: 0.15343 - R2: 1.0022 -- iter: 1024/1168\n",
      "Training Step: 644  | total loss: \u001b[1m\u001b[32m0.15917\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 034 | loss: 0.15917 - R2: 1.0032 -- iter: 1088/1168\n",
      "Training Step: 645  | total loss: \u001b[1m\u001b[32m0.15574\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 034 | loss: 0.15574 - R2: 1.0030 -- iter: 1152/1168\n",
      "Training Step: 646  | total loss: \u001b[1m\u001b[32m0.15778\u001b[0m\u001b[0m | time: 1.077s\n",
      "| SGD | epoch: 034 | loss: 0.15778 - R2: 1.0010 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 647  | total loss: \u001b[1m\u001b[32m0.15819\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 035 | loss: 0.15819 - R2: 1.0008 -- iter: 0064/1168\n",
      "Training Step: 648  | total loss: \u001b[1m\u001b[32m0.16120\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 035 | loss: 0.16120 - R2: 1.0010 -- iter: 0128/1168\n",
      "Training Step: 649  | total loss: \u001b[1m\u001b[32m0.15952\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 035 | loss: 0.15952 - R2: 1.0007 -- iter: 0192/1168\n",
      "Training Step: 650  | total loss: \u001b[1m\u001b[32m0.15958\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 035 | loss: 0.15958 - R2: 1.0010 -- iter: 0256/1168\n",
      "Training Step: 651  | total loss: \u001b[1m\u001b[32m0.16362\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 035 | loss: 0.16362 - R2: 0.9993 -- iter: 0320/1168\n",
      "Training Step: 652  | total loss: \u001b[1m\u001b[32m0.16182\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 035 | loss: 0.16182 - R2: 0.9997 -- iter: 0384/1168\n",
      "Training Step: 653  | total loss: \u001b[1m\u001b[32m0.16121\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 035 | loss: 0.16121 - R2: 1.0004 -- iter: 0448/1168\n",
      "Training Step: 654  | total loss: \u001b[1m\u001b[32m0.17325\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 035 | loss: 0.17325 - R2: 1.0004 -- iter: 0512/1168\n",
      "Training Step: 655  | total loss: \u001b[1m\u001b[32m0.17088\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 035 | loss: 0.17088 - R2: 1.0001 -- iter: 0576/1168\n",
      "Training Step: 656  | total loss: \u001b[1m\u001b[32m0.17822\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 035 | loss: 0.17822 - R2: 0.9999 -- iter: 0640/1168\n",
      "Training Step: 657  | total loss: \u001b[1m\u001b[32m0.17397\u001b[0m\u001b[0m | time: 0.111s\n",
      "| SGD | epoch: 035 | loss: 0.17397 - R2: 1.0000 -- iter: 0704/1168\n",
      "Training Step: 658  | total loss: \u001b[1m\u001b[32m0.16751\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 035 | loss: 0.16751 - R2: 0.9992 -- iter: 0768/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 659  | total loss: \u001b[1m\u001b[32m0.16345\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 035 | loss: 0.16345 - R2: 0.9995 -- iter: 0832/1168\n",
      "Training Step: 660  | total loss: \u001b[1m\u001b[32m0.15541\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 035 | loss: 0.15541 - R2: 0.9997 -- iter: 0896/1168\n",
      "Training Step: 661  | total loss: \u001b[1m\u001b[32m0.14655\u001b[0m\u001b[0m | time: 0.152s\n",
      "| SGD | epoch: 035 | loss: 0.14655 - R2: 1.0000 -- iter: 0960/1168\n",
      "Training Step: 662  | total loss: \u001b[1m\u001b[32m0.14665\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 035 | loss: 0.14665 - R2: 1.0015 -- iter: 1024/1168\n",
      "Training Step: 663  | total loss: \u001b[1m\u001b[32m0.14745\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 035 | loss: 0.14745 - R2: 1.0017 -- iter: 1088/1168\n",
      "Training Step: 664  | total loss: \u001b[1m\u001b[32m0.14695\u001b[0m\u001b[0m | time: 0.169s\n",
      "| SGD | epoch: 035 | loss: 0.14695 - R2: 1.0011 -- iter: 1152/1168\n",
      "Training Step: 665  | total loss: \u001b[1m\u001b[32m0.14503\u001b[0m\u001b[0m | time: 1.175s\n",
      "| SGD | epoch: 035 | loss: 0.14503 - R2: 1.0003 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 666  | total loss: \u001b[1m\u001b[32m0.14503\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 036 | loss: 0.14503 - R2: 1.0003 -- iter: 0064/1168\n",
      "Training Step: 667  | total loss: \u001b[1m\u001b[32m0.14628\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 036 | loss: 0.14628 - R2: 0.9999 -- iter: 0128/1168\n",
      "Training Step: 668  | total loss: \u001b[1m\u001b[32m0.14341\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 036 | loss: 0.14341 - R2: 1.0005 -- iter: 0192/1168\n",
      "Training Step: 669  | total loss: \u001b[1m\u001b[32m0.15008\u001b[0m\u001b[0m | time: 0.061s\n",
      "| SGD | epoch: 036 | loss: 0.15008 - R2: 0.9997 -- iter: 0256/1168\n",
      "Training Step: 670  | total loss: \u001b[1m\u001b[32m0.15019\u001b[0m\u001b[0m | time: 0.065s\n",
      "| SGD | epoch: 036 | loss: 0.15019 - R2: 0.9997 -- iter: 0320/1168\n",
      "Training Step: 671  | total loss: \u001b[1m\u001b[32m0.15113\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 036 | loss: 0.15113 - R2: 0.9990 -- iter: 0384/1168\n",
      "Training Step: 672  | total loss: \u001b[1m\u001b[32m0.15055\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 036 | loss: 0.15055 - R2: 0.9993 -- iter: 0448/1168\n",
      "Training Step: 673  | total loss: \u001b[1m\u001b[32m0.15531\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 036 | loss: 0.15531 - R2: 0.9997 -- iter: 0512/1168\n",
      "Training Step: 674  | total loss: \u001b[1m\u001b[32m0.15941\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 036 | loss: 0.15941 - R2: 1.0003 -- iter: 0576/1168\n",
      "Training Step: 675  | total loss: \u001b[1m\u001b[32m0.15941\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 036 | loss: 0.15941 - R2: 1.0003 -- iter: 0640/1168\n",
      "Training Step: 676  | total loss: \u001b[1m\u001b[32m0.15572\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 036 | loss: 0.15572 - R2: 0.9999 -- iter: 0704/1168\n",
      "Training Step: 677  | total loss: \u001b[1m\u001b[32m0.15163\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 036 | loss: 0.15163 - R2: 0.9977 -- iter: 0768/1168\n",
      "Training Step: 678  | total loss: \u001b[1m\u001b[32m0.15163\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 036 | loss: 0.15163 - R2: 0.9977 -- iter: 0832/1168\n",
      "Training Step: 679  | total loss: \u001b[1m\u001b[32m0.16519\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 036 | loss: 0.16519 - R2: 1.0002 -- iter: 0896/1168\n",
      "Training Step: 680  | total loss: \u001b[1m\u001b[32m0.16519\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 036 | loss: 0.16519 - R2: 1.0023 -- iter: 0960/1168\n",
      "Training Step: 681  | total loss: \u001b[1m\u001b[32m0.17511\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 036 | loss: 0.17511 - R2: 1.0020 -- iter: 1024/1168\n",
      "Training Step: 682  | total loss: \u001b[1m\u001b[32m0.17511\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 036 | loss: 0.17511 - R2: 1.0020 -- iter: 1088/1168\n",
      "Training Step: 683  | total loss: \u001b[1m\u001b[32m0.17303\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 036 | loss: 0.17303 - R2: 1.0014 -- iter: 1152/1168\n",
      "Training Step: 684  | total loss: \u001b[1m\u001b[32m0.16825\u001b[0m\u001b[0m | time: 1.137s\n",
      "| SGD | epoch: 036 | loss: 0.16825 - R2: 1.0016 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 685  | total loss: \u001b[1m\u001b[32m0.16680\u001b[0m\u001b[0m | time: 0.062s\n",
      "| SGD | epoch: 037 | loss: 0.16680 - R2: 1.0017 -- iter: 0064/1168\n",
      "Training Step: 686  | total loss: \u001b[1m\u001b[32m0.16995\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 037 | loss: 0.16995 - R2: 1.0002 -- iter: 0128/1168\n",
      "Training Step: 687  | total loss: \u001b[1m\u001b[32m0.16032\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 037 | loss: 0.16032 - R2: 1.0006 -- iter: 0192/1168\n",
      "Training Step: 688  | total loss: \u001b[1m\u001b[32m0.16032\u001b[0m\u001b[0m | time: 0.077s\n",
      "| SGD | epoch: 037 | loss: 0.16032 - R2: 1.0006 -- iter: 0256/1168\n",
      "Training Step: 689  | total loss: \u001b[1m\u001b[32m0.15762\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 037 | loss: 0.15762 - R2: 1.0017 -- iter: 0320/1168\n",
      "Training Step: 690  | total loss: \u001b[1m\u001b[32m0.15890\u001b[0m\u001b[0m | time: 0.082s\n",
      "| SGD | epoch: 037 | loss: 0.15890 - R2: 1.0013 -- iter: 0384/1168\n",
      "Training Step: 691  | total loss: \u001b[1m\u001b[32m0.16044\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 037 | loss: 0.16044 - R2: 1.0002 -- iter: 0448/1168\n",
      "Training Step: 692  | total loss: \u001b[1m\u001b[32m0.16044\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 037 | loss: 0.16044 - R2: 1.0002 -- iter: 0512/1168\n",
      "Training Step: 693  | total loss: \u001b[1m\u001b[32m0.16384\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 037 | loss: 0.16384 - R2: 0.9991 -- iter: 0576/1168\n",
      "Training Step: 694  | total loss: \u001b[1m\u001b[32m0.15693\u001b[0m\u001b[0m | time: 0.091s\n",
      "| SGD | epoch: 037 | loss: 0.15693 - R2: 0.9993 -- iter: 0640/1168\n",
      "Training Step: 695  | total loss: \u001b[1m\u001b[32m0.15556\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 037 | loss: 0.15556 - R2: 0.9992 -- iter: 0704/1168\n",
      "Training Step: 696  | total loss: \u001b[1m\u001b[32m0.15375\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 037 | loss: 0.15375 - R2: 0.9986 -- iter: 0768/1168\n",
      "Training Step: 697  | total loss: \u001b[1m\u001b[32m0.15375\u001b[0m\u001b[0m | time: 0.101s\n",
      "| SGD | epoch: 037 | loss: 0.15375 - R2: 0.9986 -- iter: 0832/1168\n",
      "Training Step: 698  | total loss: \u001b[1m\u001b[32m0.15605\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 037 | loss: 0.15605 - R2: 0.9975 -- iter: 0896/1168\n",
      "Training Step: 699  | total loss: \u001b[1m\u001b[32m0.15862\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 037 | loss: 0.15862 - R2: 0.9977 -- iter: 0960/1168\n",
      "Training Step: 700  | total loss: \u001b[1m\u001b[32m0.15879\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 037 | loss: 0.15879 - R2: 0.9968 -- iter: 1024/1168\n",
      "Training Step: 701  | total loss: \u001b[1m\u001b[32m0.15894\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 037 | loss: 0.15894 - R2: 0.9960 -- iter: 1088/1168\n",
      "Training Step: 702  | total loss: \u001b[1m\u001b[32m0.15824\u001b[0m\u001b[0m | time: 0.128s\n",
      "| SGD | epoch: 037 | loss: 0.15824 - R2: 0.9969 -- iter: 1152/1168\n",
      "Training Step: 703  | total loss: \u001b[1m\u001b[32m0.15677\u001b[0m\u001b[0m | time: 1.133s\n",
      "| SGD | epoch: 037 | loss: 0.15677 - R2: 0.9970 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 704  | total loss: \u001b[1m\u001b[32m0.15762\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 038 | loss: 0.15762 - R2: 0.9974 -- iter: 0064/1168\n",
      "Training Step: 705  | total loss: \u001b[1m\u001b[32m0.15523\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 038 | loss: 0.15523 - R2: 0.9983 -- iter: 0128/1168\n",
      "Training Step: 706  | total loss: \u001b[1m\u001b[32m0.15836\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 038 | loss: 0.15836 - R2: 0.9981 -- iter: 0192/1168\n",
      "Training Step: 707  | total loss: \u001b[1m\u001b[32m0.15836\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 038 | loss: 0.15836 - R2: 0.9981 -- iter: 0256/1168\n",
      "Training Step: 708  | total loss: \u001b[1m\u001b[32m0.15883\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 038 | loss: 0.15883 - R2: 0.9976 -- iter: 0320/1168\n",
      "Training Step: 709  | total loss: \u001b[1m\u001b[32m0.16297\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 038 | loss: 0.16297 - R2: 0.9969 -- iter: 0384/1168\n",
      "Training Step: 710  | total loss: \u001b[1m\u001b[32m0.16208\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 038 | loss: 0.16208 - R2: 0.9969 -- iter: 0448/1168\n",
      "Training Step: 711  | total loss: \u001b[1m\u001b[32m0.16180\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 038 | loss: 0.16180 - R2: 0.9968 -- iter: 0512/1168\n",
      "Training Step: 712  | total loss: \u001b[1m\u001b[32m0.16180\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 038 | loss: 0.16180 - R2: 0.9968 -- iter: 0576/1168\n",
      "Training Step: 713  | total loss: \u001b[1m\u001b[32m0.15831\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 038 | loss: 0.15831 - R2: 0.9971 -- iter: 0640/1168\n",
      "Training Step: 714  | total loss: \u001b[1m\u001b[32m0.15592\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 038 | loss: 0.15592 - R2: 0.9981 -- iter: 0704/1168\n",
      "Training Step: 715  | total loss: \u001b[1m\u001b[32m0.15316\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 038 | loss: 0.15316 - R2: 0.9986 -- iter: 0768/1168\n",
      "Training Step: 716  | total loss: \u001b[1m\u001b[32m0.14901\u001b[0m\u001b[0m | time: 0.117s\n",
      "| SGD | epoch: 038 | loss: 0.14901 - R2: 0.9984 -- iter: 0832/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 717  | total loss: \u001b[1m\u001b[32m0.15614\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 038 | loss: 0.15614 - R2: 0.9982 -- iter: 0896/1168\n",
      "Training Step: 718  | total loss: \u001b[1m\u001b[32m0.15888\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 038 | loss: 0.15888 - R2: 0.9985 -- iter: 0960/1168\n",
      "Training Step: 719  | total loss: \u001b[1m\u001b[32m0.15909\u001b[0m\u001b[0m | time: 0.131s\n",
      "| SGD | epoch: 038 | loss: 0.15909 - R2: 0.9968 -- iter: 1024/1168\n",
      "Training Step: 720  | total loss: \u001b[1m\u001b[32m0.15909\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 038 | loss: 0.15909 - R2: 0.9968 -- iter: 1088/1168\n",
      "Training Step: 721  | total loss: \u001b[1m\u001b[32m0.15928\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 038 | loss: 0.15928 - R2: 0.9952 -- iter: 1152/1168\n",
      "Training Step: 722  | total loss: \u001b[1m\u001b[32m0.16082\u001b[0m\u001b[0m | time: 1.152s\n",
      "| SGD | epoch: 038 | loss: 0.16082 - R2: 0.9954 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 723  | total loss: \u001b[1m\u001b[32m0.16309\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 039 | loss: 0.16309 - R2: 0.9967 -- iter: 0064/1168\n",
      "Training Step: 724  | total loss: \u001b[1m\u001b[32m0.15854\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 039 | loss: 0.15854 - R2: 0.9971 -- iter: 0128/1168\n",
      "Training Step: 725  | total loss: \u001b[1m\u001b[32m0.16168\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 039 | loss: 0.16168 - R2: 0.9974 -- iter: 0192/1168\n",
      "Training Step: 726  | total loss: \u001b[1m\u001b[32m0.16168\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 039 | loss: 0.16168 - R2: 0.9974 -- iter: 0256/1168\n",
      "Training Step: 727  | total loss: \u001b[1m\u001b[32m0.15891\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 039 | loss: 0.15891 - R2: 0.9968 -- iter: 0320/1168\n",
      "Training Step: 728  | total loss: \u001b[1m\u001b[32m0.15933\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 039 | loss: 0.15933 - R2: 0.9965 -- iter: 0384/1168\n",
      "Training Step: 729  | total loss: \u001b[1m\u001b[32m0.15890\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 039 | loss: 0.15890 - R2: 0.9973 -- iter: 0448/1168\n",
      "Training Step: 730  | total loss: \u001b[1m\u001b[32m0.15890\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 039 | loss: 0.15890 - R2: 0.9973 -- iter: 0512/1168\n",
      "Training Step: 731  | total loss: \u001b[1m\u001b[32m0.15862\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 039 | loss: 0.15862 - R2: 0.9980 -- iter: 0576/1168\n",
      "Training Step: 732  | total loss: \u001b[1m\u001b[32m0.15764\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 039 | loss: 0.15764 - R2: 0.9971 -- iter: 0640/1168\n",
      "Training Step: 733  | total loss: \u001b[1m\u001b[32m0.15764\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 039 | loss: 0.15764 - R2: 0.9971 -- iter: 0704/1168\n",
      "Training Step: 734  | total loss: \u001b[1m\u001b[32m0.15828\u001b[0m\u001b[0m | time: 0.099s\n",
      "| SGD | epoch: 039 | loss: 0.15828 - R2: 0.9979 -- iter: 0768/1168\n",
      "Training Step: 735  | total loss: \u001b[1m\u001b[32m0.15828\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 039 | loss: 0.15828 - R2: 0.9979 -- iter: 0832/1168\n",
      "Training Step: 736  | total loss: \u001b[1m\u001b[32m0.16101\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 039 | loss: 0.16101 - R2: 0.9972 -- iter: 0896/1168\n",
      "Training Step: 737  | total loss: \u001b[1m\u001b[32m0.16101\u001b[0m\u001b[0m | time: 0.114s\n",
      "| SGD | epoch: 039 | loss: 0.16101 - R2: 0.9972 -- iter: 0960/1168\n",
      "Training Step: 738  | total loss: \u001b[1m\u001b[32m0.15897\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 039 | loss: 0.15897 - R2: 0.9960 -- iter: 1024/1168\n",
      "Training Step: 739  | total loss: \u001b[1m\u001b[32m0.15953\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 039 | loss: 0.15953 - R2: 0.9967 -- iter: 1088/1168\n",
      "Training Step: 740  | total loss: \u001b[1m\u001b[32m0.15978\u001b[0m\u001b[0m | time: 0.122s\n",
      "| SGD | epoch: 039 | loss: 0.15978 - R2: 0.9989 -- iter: 1152/1168\n",
      "Training Step: 741  | total loss: \u001b[1m\u001b[32m0.16001\u001b[0m\u001b[0m | time: 1.132s\n",
      "| SGD | epoch: 039 | loss: 0.16001 - R2: 1.0016 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 742  | total loss: \u001b[1m\u001b[32m0.15501\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 040 | loss: 0.15501 - R2: 1.0016 -- iter: 0064/1168\n",
      "Training Step: 743  | total loss: \u001b[1m\u001b[32m0.15693\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 040 | loss: 0.15693 - R2: 1.0027 -- iter: 0128/1168\n",
      "Training Step: 744  | total loss: \u001b[1m\u001b[32m0.15818\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 040 | loss: 0.15818 - R2: 1.0019 -- iter: 0192/1168\n",
      "Training Step: 745  | total loss: \u001b[1m\u001b[32m0.16634\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 040 | loss: 0.16634 - R2: 0.9999 -- iter: 0256/1168\n",
      "Training Step: 746  | total loss: \u001b[1m\u001b[32m0.16567\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 040 | loss: 0.16567 - R2: 0.9995 -- iter: 0320/1168\n",
      "Training Step: 747  | total loss: \u001b[1m\u001b[32m0.16729\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 040 | loss: 0.16729 - R2: 0.9996 -- iter: 0384/1168\n",
      "Training Step: 748  | total loss: \u001b[1m\u001b[32m0.16076\u001b[0m\u001b[0m | time: 0.104s\n",
      "| SGD | epoch: 040 | loss: 0.16076 - R2: 1.0000 -- iter: 0448/1168\n",
      "Training Step: 749  | total loss: \u001b[1m\u001b[32m0.15908\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 040 | loss: 0.15908 - R2: 0.9998 -- iter: 0512/1168\n",
      "Training Step: 750  | total loss: \u001b[1m\u001b[32m0.15862\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 040 | loss: 0.15862 - R2: 1.0002 -- iter: 0576/1168\n",
      "Training Step: 751  | total loss: \u001b[1m\u001b[32m0.15803\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 040 | loss: 0.15803 - R2: 0.9988 -- iter: 0640/1168\n",
      "Training Step: 752  | total loss: \u001b[1m\u001b[32m0.15803\u001b[0m\u001b[0m | time: 0.121s\n",
      "| SGD | epoch: 040 | loss: 0.15803 - R2: 0.9988 -- iter: 0704/1168\n",
      "Training Step: 753  | total loss: \u001b[1m\u001b[32m0.16128\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 040 | loss: 0.16128 - R2: 0.9977 -- iter: 0768/1168\n",
      "Training Step: 754  | total loss: \u001b[1m\u001b[32m0.16128\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 040 | loss: 0.16128 - R2: 0.9977 -- iter: 0832/1168\n",
      "Training Step: 755  | total loss: \u001b[1m\u001b[32m0.16883\u001b[0m\u001b[0m | time: 0.143s\n",
      "| SGD | epoch: 040 | loss: 0.16883 - R2: 0.9989 -- iter: 0896/1168\n",
      "Training Step: 756  | total loss: \u001b[1m\u001b[32m0.17341\u001b[0m\u001b[0m | time: 0.153s\n",
      "| SGD | epoch: 040 | loss: 0.17341 - R2: 0.9982 -- iter: 0960/1168\n",
      "Training Step: 757  | total loss: \u001b[1m\u001b[32m0.17118\u001b[0m\u001b[0m | time: 0.158s\n",
      "| SGD | epoch: 040 | loss: 0.17118 - R2: 0.9987 -- iter: 1024/1168\n",
      "Training Step: 758  | total loss: \u001b[1m\u001b[32m0.17118\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 040 | loss: 0.17118 - R2: 0.9987 -- iter: 1088/1168\n",
      "Training Step: 759  | total loss: \u001b[1m\u001b[32m0.16863\u001b[0m\u001b[0m | time: 0.168s\n",
      "| SGD | epoch: 040 | loss: 0.16863 - R2: 0.9990 -- iter: 1152/1168\n",
      "Training Step: 760  | total loss: \u001b[1m\u001b[32m0.16574\u001b[0m\u001b[0m | time: 1.173s\n",
      "| SGD | epoch: 040 | loss: 0.16574 - R2: 0.9992 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 761  | total loss: \u001b[1m\u001b[32m0.15890\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 041 | loss: 0.15890 - R2: 0.9990 -- iter: 0064/1168\n",
      "Training Step: 762  | total loss: \u001b[1m\u001b[32m0.15890\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 041 | loss: 0.15890 - R2: 0.9990 -- iter: 0128/1168\n",
      "Training Step: 763  | total loss: \u001b[1m\u001b[32m0.15991\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 041 | loss: 0.15991 - R2: 0.9973 -- iter: 0192/1168\n",
      "Training Step: 764  | total loss: \u001b[1m\u001b[32m0.15282\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 041 | loss: 0.15282 - R2: 0.9975 -- iter: 0256/1168\n",
      "Training Step: 765  | total loss: \u001b[1m\u001b[32m0.15260\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 041 | loss: 0.15260 - R2: 0.9974 -- iter: 0320/1168\n",
      "Training Step: 766  | total loss: \u001b[1m\u001b[32m0.15303\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 041 | loss: 0.15303 - R2: 0.9989 -- iter: 0384/1168\n",
      "Training Step: 767  | total loss: \u001b[1m\u001b[32m0.15303\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 041 | loss: 0.15303 - R2: 0.9989 -- iter: 0448/1168\n",
      "Training Step: 768  | total loss: \u001b[1m\u001b[32m0.15355\u001b[0m\u001b[0m | time: 0.090s\n",
      "| SGD | epoch: 041 | loss: 0.15355 - R2: 1.0007 -- iter: 0512/1168\n",
      "Training Step: 769  | total loss: \u001b[1m\u001b[32m0.14724\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 041 | loss: 0.14724 - R2: 1.0005 -- iter: 0576/1168\n",
      "Training Step: 770  | total loss: \u001b[1m\u001b[32m0.14859\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 041 | loss: 0.14859 - R2: 0.9995 -- iter: 0640/1168\n",
      "Training Step: 771  | total loss: \u001b[1m\u001b[32m0.14482\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 041 | loss: 0.14482 - R2: 0.9999 -- iter: 0704/1168\n",
      "Training Step: 772  | total loss: \u001b[1m\u001b[32m0.14163\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 041 | loss: 0.14163 - R2: 1.0002 -- iter: 0768/1168\n",
      "Training Step: 773  | total loss: \u001b[1m\u001b[32m0.14874\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 041 | loss: 0.14874 - R2: 1.0002 -- iter: 0832/1168\n",
      "Training Step: 774  | total loss: \u001b[1m\u001b[32m0.15263\u001b[0m\u001b[0m | time: 0.133s\n",
      "| SGD | epoch: 041 | loss: 0.15263 - R2: 1.0009 -- iter: 0896/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 775  | total loss: \u001b[1m\u001b[32m0.15057\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 041 | loss: 0.15057 - R2: 1.0007 -- iter: 0960/1168\n",
      "Training Step: 776  | total loss: \u001b[1m\u001b[32m0.14815\u001b[0m\u001b[0m | time: 0.142s\n",
      "| SGD | epoch: 041 | loss: 0.14815 - R2: 1.0002 -- iter: 1024/1168\n",
      "Training Step: 777  | total loss: \u001b[1m\u001b[32m0.14815\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 041 | loss: 0.14815 - R2: 1.0002 -- iter: 1088/1168\n",
      "Training Step: 778  | total loss: \u001b[1m\u001b[32m0.15309\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 041 | loss: 0.15309 - R2: 0.9997 -- iter: 1152/1168\n",
      "Training Step: 779  | total loss: \u001b[1m\u001b[32m0.14835\u001b[0m\u001b[0m | time: 1.158s\n",
      "| SGD | epoch: 041 | loss: 0.14835 - R2: 1.0014 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 780  | total loss: \u001b[1m\u001b[32m0.14835\u001b[0m\u001b[0m | time: 0.041s\n",
      "| SGD | epoch: 042 | loss: 0.14835 - R2: 1.0014 -- iter: 0064/1168\n",
      "Training Step: 781  | total loss: \u001b[1m\u001b[32m0.14394\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 042 | loss: 0.14394 - R2: 1.0033 -- iter: 0128/1168\n",
      "Training Step: 782  | total loss: \u001b[1m\u001b[32m0.14938\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 042 | loss: 0.14938 - R2: 1.0011 -- iter: 0192/1168\n",
      "Training Step: 783  | total loss: \u001b[1m\u001b[32m0.15276\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 042 | loss: 0.15276 - R2: 1.0003 -- iter: 0256/1168\n",
      "Training Step: 784  | total loss: \u001b[1m\u001b[32m0.15197\u001b[0m\u001b[0m | time: 0.059s\n",
      "| SGD | epoch: 042 | loss: 0.15197 - R2: 1.0002 -- iter: 0320/1168\n",
      "Training Step: 785  | total loss: \u001b[1m\u001b[32m0.15694\u001b[0m\u001b[0m | time: 0.066s\n",
      "| SGD | epoch: 042 | loss: 0.15694 - R2: 0.9982 -- iter: 0384/1168\n",
      "Training Step: 786  | total loss: \u001b[1m\u001b[32m0.16164\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 042 | loss: 0.16164 - R2: 0.9988 -- iter: 0448/1168\n",
      "Training Step: 787  | total loss: \u001b[1m\u001b[32m0.16129\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 042 | loss: 0.16129 - R2: 1.0007 -- iter: 0512/1168\n",
      "Training Step: 788  | total loss: \u001b[1m\u001b[32m0.15944\u001b[0m\u001b[0m | time: 0.081s\n",
      "| SGD | epoch: 042 | loss: 0.15944 - R2: 1.0004 -- iter: 0576/1168\n",
      "Training Step: 789  | total loss: \u001b[1m\u001b[32m0.15944\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 042 | loss: 0.15944 - R2: 1.0004 -- iter: 0640/1168\n",
      "Training Step: 790  | total loss: \u001b[1m\u001b[32m0.15358\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 042 | loss: 0.15358 - R2: 0.9998 -- iter: 0704/1168\n",
      "Training Step: 791  | total loss: \u001b[1m\u001b[32m0.15262\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 042 | loss: 0.15262 - R2: 1.0007 -- iter: 0768/1168\n",
      "Training Step: 792  | total loss: \u001b[1m\u001b[32m0.14962\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 042 | loss: 0.14962 - R2: 1.0012 -- iter: 0832/1168\n",
      "Training Step: 793  | total loss: \u001b[1m\u001b[32m0.15209\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 042 | loss: 0.15209 - R2: 0.9994 -- iter: 0896/1168\n",
      "Training Step: 794  | total loss: \u001b[1m\u001b[32m0.15088\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 042 | loss: 0.15088 - R2: 1.0001 -- iter: 0960/1168\n",
      "Training Step: 795  | total loss: \u001b[1m\u001b[32m0.15088\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 042 | loss: 0.15088 - R2: 1.0001 -- iter: 1024/1168\n",
      "Training Step: 796  | total loss: \u001b[1m\u001b[32m0.15018\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 042 | loss: 0.15018 - R2: 0.9980 -- iter: 1088/1168\n",
      "Training Step: 797  | total loss: \u001b[1m\u001b[32m0.15018\u001b[0m\u001b[0m | time: 0.119s\n",
      "| SGD | epoch: 042 | loss: 0.15018 - R2: 0.9980 -- iter: 1152/1168\n",
      "Training Step: 798  | total loss: \u001b[1m\u001b[32m0.15883\u001b[0m\u001b[0m | time: 1.128s\n",
      "| SGD | epoch: 042 | loss: 0.15883 - R2: 0.9985 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 799  | total loss: \u001b[1m\u001b[32m0.15459\u001b[0m\u001b[0m | time: 0.079s\n",
      "| SGD | epoch: 043 | loss: 0.15459 - R2: 0.9993 -- iter: 0064/1168\n",
      "Training Step: 800  | total loss: \u001b[1m\u001b[32m0.15459\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 043 | loss: 0.15459 - R2: 0.9993 -- iter: 0128/1168\n",
      "Training Step: 801  | total loss: \u001b[1m\u001b[32m0.15340\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 043 | loss: 0.15340 - R2: 0.9996 -- iter: 0192/1168\n",
      "Training Step: 802  | total loss: \u001b[1m\u001b[32m0.15340\u001b[0m\u001b[0m | time: 0.096s\n",
      "| SGD | epoch: 043 | loss: 0.15340 - R2: 0.9996 -- iter: 0256/1168\n",
      "Training Step: 803  | total loss: \u001b[1m\u001b[32m0.15377\u001b[0m\u001b[0m | time: 0.135s\n",
      "| SGD | epoch: 043 | loss: 0.15377 - R2: 0.9982 -- iter: 0320/1168\n",
      "Training Step: 804  | total loss: \u001b[1m\u001b[32m0.16246\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 043 | loss: 0.16246 - R2: 0.9991 -- iter: 0384/1168\n",
      "Training Step: 805  | total loss: \u001b[1m\u001b[32m0.15958\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 043 | loss: 0.15958 - R2: 0.9992 -- iter: 0448/1168\n",
      "Training Step: 806  | total loss: \u001b[1m\u001b[32m0.15971\u001b[0m\u001b[0m | time: 0.148s\n",
      "| SGD | epoch: 043 | loss: 0.15971 - R2: 0.9994 -- iter: 0512/1168\n",
      "Training Step: 807  | total loss: \u001b[1m\u001b[32m0.16267\u001b[0m\u001b[0m | time: 0.156s\n",
      "| SGD | epoch: 043 | loss: 0.16267 - R2: 0.9993 -- iter: 0576/1168\n",
      "Training Step: 808  | total loss: \u001b[1m\u001b[32m0.16267\u001b[0m\u001b[0m | time: 0.164s\n",
      "| SGD | epoch: 043 | loss: 0.16267 - R2: 0.9993 -- iter: 0640/1168\n",
      "Training Step: 809  | total loss: \u001b[1m\u001b[32m0.17065\u001b[0m\u001b[0m | time: 0.172s\n",
      "| SGD | epoch: 043 | loss: 0.17065 - R2: 0.9997 -- iter: 0704/1168\n",
      "Training Step: 810  | total loss: \u001b[1m\u001b[32m0.17479\u001b[0m\u001b[0m | time: 0.177s\n",
      "| SGD | epoch: 043 | loss: 0.17479 - R2: 0.9991 -- iter: 0768/1168\n",
      "Training Step: 811  | total loss: \u001b[1m\u001b[32m0.17278\u001b[0m\u001b[0m | time: 0.184s\n",
      "| SGD | epoch: 043 | loss: 0.17278 - R2: 0.9979 -- iter: 0832/1168\n",
      "Training Step: 812  | total loss: \u001b[1m\u001b[32m0.17421\u001b[0m\u001b[0m | time: 0.188s\n",
      "| SGD | epoch: 043 | loss: 0.17421 - R2: 1.0003 -- iter: 0896/1168\n",
      "Training Step: 813  | total loss: \u001b[1m\u001b[32m0.16767\u001b[0m\u001b[0m | time: 0.196s\n",
      "| SGD | epoch: 043 | loss: 0.16767 - R2: 1.0002 -- iter: 0960/1168\n",
      "Training Step: 814  | total loss: \u001b[1m\u001b[32m0.16767\u001b[0m\u001b[0m | time: 0.201s\n",
      "| SGD | epoch: 043 | loss: 0.16767 - R2: 1.0002 -- iter: 1024/1168\n",
      "Training Step: 815  | total loss: \u001b[1m\u001b[32m0.16571\u001b[0m\u001b[0m | time: 0.203s\n",
      "| SGD | epoch: 043 | loss: 0.16571 - R2: 1.0019 -- iter: 1088/1168\n",
      "Training Step: 816  | total loss: \u001b[1m\u001b[32m0.15879\u001b[0m\u001b[0m | time: 0.207s\n",
      "| SGD | epoch: 043 | loss: 0.15879 - R2: 1.0017 -- iter: 1152/1168\n",
      "Training Step: 817  | total loss: \u001b[1m\u001b[32m0.15465\u001b[0m\u001b[0m | time: 1.228s\n",
      "| SGD | epoch: 043 | loss: 0.15465 - R2: 1.0010 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 818  | total loss: \u001b[1m\u001b[32m0.14669\u001b[0m\u001b[0m | time: 0.086s\n",
      "| SGD | epoch: 044 | loss: 0.14669 - R2: 1.0001 -- iter: 0064/1168\n",
      "Training Step: 819  | total loss: \u001b[1m\u001b[32m0.14669\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 044 | loss: 0.14669 - R2: 1.0001 -- iter: 0128/1168\n",
      "Training Step: 820  | total loss: \u001b[1m\u001b[32m0.14169\u001b[0m\u001b[0m | time: 0.097s\n",
      "| SGD | epoch: 044 | loss: 0.14169 - R2: 0.9984 -- iter: 0192/1168\n",
      "Training Step: 821  | total loss: \u001b[1m\u001b[32m0.14498\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 044 | loss: 0.14498 - R2: 0.9965 -- iter: 0256/1168\n",
      "Training Step: 822  | total loss: \u001b[1m\u001b[32m0.14641\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 044 | loss: 0.14641 - R2: 0.9969 -- iter: 0320/1168\n",
      "Training Step: 823  | total loss: \u001b[1m\u001b[32m0.14641\u001b[0m\u001b[0m | time: 0.106s\n",
      "| SGD | epoch: 044 | loss: 0.14641 - R2: 0.9969 -- iter: 0384/1168\n",
      "Training Step: 824  | total loss: \u001b[1m\u001b[32m0.14908\u001b[0m\u001b[0m | time: 0.108s\n",
      "| SGD | epoch: 044 | loss: 0.14908 - R2: 0.9956 -- iter: 0448/1168\n",
      "Training Step: 825  | total loss: \u001b[1m\u001b[32m0.15841\u001b[0m\u001b[0m | time: 0.112s\n",
      "| SGD | epoch: 044 | loss: 0.15841 - R2: 0.9958 -- iter: 0512/1168\n",
      "Training Step: 826  | total loss: \u001b[1m\u001b[32m0.15614\u001b[0m\u001b[0m | time: 0.116s\n",
      "| SGD | epoch: 044 | loss: 0.15614 - R2: 0.9960 -- iter: 0576/1168\n",
      "Training Step: 827  | total loss: \u001b[1m\u001b[32m0.16293\u001b[0m\u001b[0m | time: 0.120s\n",
      "| SGD | epoch: 044 | loss: 0.16293 - R2: 0.9956 -- iter: 0640/1168\n",
      "Training Step: 828  | total loss: \u001b[1m\u001b[32m0.16293\u001b[0m\u001b[0m | time: 0.123s\n",
      "| SGD | epoch: 044 | loss: 0.16293 - R2: 0.9956 -- iter: 0704/1168\n",
      "Training Step: 829  | total loss: \u001b[1m\u001b[32m0.16123\u001b[0m\u001b[0m | time: 0.127s\n",
      "| SGD | epoch: 044 | loss: 0.16123 - R2: 0.9959 -- iter: 0768/1168\n",
      "Training Step: 830  | total loss: \u001b[1m\u001b[32m0.16111\u001b[0m\u001b[0m | time: 0.129s\n",
      "| SGD | epoch: 044 | loss: 0.16111 - R2: 0.9950 -- iter: 0832/1168\n",
      "Training Step: 831  | total loss: \u001b[1m\u001b[32m0.15601\u001b[0m\u001b[0m | time: 0.132s\n",
      "| SGD | epoch: 044 | loss: 0.15601 - R2: 0.9949 -- iter: 0896/1168\n",
      "Training Step: 832  | total loss: \u001b[1m\u001b[32m0.15557\u001b[0m\u001b[0m | time: 0.134s\n",
      "| SGD | epoch: 044 | loss: 0.15557 - R2: 0.9948 -- iter: 0960/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 833  | total loss: \u001b[1m\u001b[32m0.15571\u001b[0m\u001b[0m | time: 0.139s\n",
      "| SGD | epoch: 044 | loss: 0.15571 - R2: 0.9966 -- iter: 1024/1168\n",
      "Training Step: 834  | total loss: \u001b[1m\u001b[32m0.15870\u001b[0m\u001b[0m | time: 0.144s\n",
      "| SGD | epoch: 044 | loss: 0.15870 - R2: 0.9982 -- iter: 1088/1168\n",
      "Training Step: 835  | total loss: \u001b[1m\u001b[32m0.15817\u001b[0m\u001b[0m | time: 0.146s\n",
      "| SGD | epoch: 044 | loss: 0.15817 - R2: 0.9979 -- iter: 1152/1168\n",
      "Training Step: 836  | total loss: \u001b[1m\u001b[32m0.16059\u001b[0m\u001b[0m | time: 1.152s\n",
      "| SGD | epoch: 044 | loss: 0.16059 - R2: 0.9986 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 837  | total loss: \u001b[1m\u001b[32m0.15755\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 045 | loss: 0.15755 - R2: 0.9995 -- iter: 0064/1168\n",
      "Training Step: 838  | total loss: \u001b[1m\u001b[32m0.15985\u001b[0m\u001b[0m | time: 0.070s\n",
      "| SGD | epoch: 045 | loss: 0.15985 - R2: 0.9991 -- iter: 0128/1168\n",
      "Training Step: 839  | total loss: \u001b[1m\u001b[32m0.16316\u001b[0m\u001b[0m | time: 0.073s\n",
      "| SGD | epoch: 045 | loss: 0.16316 - R2: 0.9997 -- iter: 0192/1168\n",
      "Training Step: 840  | total loss: \u001b[1m\u001b[32m0.15898\u001b[0m\u001b[0m | time: 0.075s\n",
      "| SGD | epoch: 045 | loss: 0.15898 - R2: 1.0007 -- iter: 0256/1168\n",
      "Training Step: 841  | total loss: \u001b[1m\u001b[32m0.15522\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 045 | loss: 0.15522 - R2: 1.0016 -- iter: 0320/1168\n",
      "Training Step: 842  | total loss: \u001b[1m\u001b[32m0.15489\u001b[0m\u001b[0m | time: 0.080s\n",
      "| SGD | epoch: 045 | loss: 0.15489 - R2: 1.0013 -- iter: 0384/1168\n",
      "Training Step: 843  | total loss: \u001b[1m\u001b[32m0.15602\u001b[0m\u001b[0m | time: 0.083s\n",
      "| SGD | epoch: 045 | loss: 0.15602 - R2: 1.0010 -- iter: 0448/1168\n",
      "Training Step: 844  | total loss: \u001b[1m\u001b[32m0.15330\u001b[0m\u001b[0m | time: 0.085s\n",
      "| SGD | epoch: 045 | loss: 0.15330 - R2: 1.0000 -- iter: 0512/1168\n",
      "Training Step: 845  | total loss: \u001b[1m\u001b[32m0.15381\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 045 | loss: 0.15381 - R2: 0.9986 -- iter: 0576/1168\n",
      "Training Step: 846  | total loss: \u001b[1m\u001b[32m0.15288\u001b[0m\u001b[0m | time: 0.092s\n",
      "| SGD | epoch: 045 | loss: 0.15288 - R2: 0.9990 -- iter: 0640/1168\n",
      "Training Step: 847  | total loss: \u001b[1m\u001b[32m0.15284\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 045 | loss: 0.15284 - R2: 0.9980 -- iter: 0704/1168\n",
      "Training Step: 848  | total loss: \u001b[1m\u001b[32m0.15134\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 045 | loss: 0.15134 - R2: 0.9985 -- iter: 0768/1168\n",
      "Training Step: 849  | total loss: \u001b[1m\u001b[32m0.15134\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 045 | loss: 0.15134 - R2: 0.9985 -- iter: 0832/1168\n",
      "Training Step: 850  | total loss: \u001b[1m\u001b[32m0.14886\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 045 | loss: 0.14886 - R2: 0.9996 -- iter: 0896/1168\n",
      "Training Step: 851  | total loss: \u001b[1m\u001b[32m0.14966\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 045 | loss: 0.14966 - R2: 0.9992 -- iter: 0960/1168\n",
      "Training Step: 852  | total loss: \u001b[1m\u001b[32m0.14829\u001b[0m\u001b[0m | time: 0.107s\n",
      "| SGD | epoch: 045 | loss: 0.14829 - R2: 0.9984 -- iter: 1024/1168\n",
      "Training Step: 853  | total loss: \u001b[1m\u001b[32m0.14936\u001b[0m\u001b[0m | time: 0.110s\n",
      "| SGD | epoch: 045 | loss: 0.14936 - R2: 0.9988 -- iter: 1088/1168\n",
      "Training Step: 854  | total loss: \u001b[1m\u001b[32m0.15169\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 045 | loss: 0.15169 - R2: 0.9999 -- iter: 1152/1168\n",
      "Training Step: 855  | total loss: \u001b[1m\u001b[32m0.16103\u001b[0m\u001b[0m | time: 1.120s\n",
      "| SGD | epoch: 045 | loss: 0.16103 - R2: 0.9991 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 856  | total loss: \u001b[1m\u001b[32m0.15889\u001b[0m\u001b[0m | time: 0.017s\n",
      "| SGD | epoch: 046 | loss: 0.15889 - R2: 0.9989 -- iter: 0064/1168\n",
      "Training Step: 857  | total loss: \u001b[1m\u001b[32m0.15674\u001b[0m\u001b[0m | time: 0.019s\n",
      "| SGD | epoch: 046 | loss: 0.15674 - R2: 0.9991 -- iter: 0128/1168\n",
      "Training Step: 858  | total loss: \u001b[1m\u001b[32m0.15601\u001b[0m\u001b[0m | time: 0.025s\n",
      "| SGD | epoch: 046 | loss: 0.15601 - R2: 0.9982 -- iter: 0192/1168\n",
      "Training Step: 859  | total loss: \u001b[1m\u001b[32m0.15092\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 046 | loss: 0.15092 - R2: 1.0015 -- iter: 0256/1168\n",
      "Training Step: 860  | total loss: \u001b[1m\u001b[32m0.14633\u001b[0m\u001b[0m | time: 0.038s\n",
      "| SGD | epoch: 046 | loss: 0.14633 - R2: 1.0044 -- iter: 0320/1168\n",
      "Training Step: 861  | total loss: \u001b[1m\u001b[32m0.15253\u001b[0m\u001b[0m | time: 0.043s\n",
      "| SGD | epoch: 046 | loss: 0.15253 - R2: 1.0029 -- iter: 0384/1168\n",
      "Training Step: 862  | total loss: \u001b[1m\u001b[32m0.15532\u001b[0m\u001b[0m | time: 0.047s\n",
      "| SGD | epoch: 046 | loss: 0.15532 - R2: 1.0025 -- iter: 0448/1168\n",
      "Training Step: 863  | total loss: \u001b[1m\u001b[32m0.15390\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 046 | loss: 0.15390 - R2: 1.0026 -- iter: 0512/1168\n",
      "Training Step: 864  | total loss: \u001b[1m\u001b[32m0.15390\u001b[0m\u001b[0m | time: 0.053s\n",
      "| SGD | epoch: 046 | loss: 0.15390 - R2: 1.0026 -- iter: 0576/1168\n",
      "Training Step: 865  | total loss: \u001b[1m\u001b[32m0.15313\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 046 | loss: 0.15313 - R2: 1.0009 -- iter: 0640/1168\n",
      "Training Step: 866  | total loss: \u001b[1m\u001b[32m0.15313\u001b[0m\u001b[0m | time: 0.058s\n",
      "| SGD | epoch: 046 | loss: 0.15313 - R2: 1.0009 -- iter: 0704/1168\n",
      "Training Step: 867  | total loss: \u001b[1m\u001b[32m0.15511\u001b[0m\u001b[0m | time: 0.067s\n",
      "| SGD | epoch: 046 | loss: 0.15511 - R2: 1.0006 -- iter: 0768/1168\n",
      "Training Step: 868  | total loss: \u001b[1m\u001b[32m0.15511\u001b[0m\u001b[0m | time: 0.071s\n",
      "| SGD | epoch: 046 | loss: 0.15511 - R2: 1.0006 -- iter: 0832/1168\n",
      "Training Step: 869  | total loss: \u001b[1m\u001b[32m0.14934\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 046 | loss: 0.14934 - R2: 1.0000 -- iter: 0896/1168\n",
      "Training Step: 870  | total loss: \u001b[1m\u001b[32m0.14934\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 046 | loss: 0.14934 - R2: 1.0000 -- iter: 0960/1168\n",
      "Training Step: 871  | total loss: \u001b[1m\u001b[32m0.14679\u001b[0m\u001b[0m | time: 0.093s\n",
      "| SGD | epoch: 046 | loss: 0.14679 - R2: 1.0003 -- iter: 1024/1168\n",
      "Training Step: 872  | total loss: \u001b[1m\u001b[32m0.14824\u001b[0m\u001b[0m | time: 0.095s\n",
      "| SGD | epoch: 046 | loss: 0.14824 - R2: 1.0001 -- iter: 1088/1168\n",
      "Training Step: 873  | total loss: \u001b[1m\u001b[32m0.15021\u001b[0m\u001b[0m | time: 0.100s\n",
      "| SGD | epoch: 046 | loss: 0.15021 - R2: 0.9993 -- iter: 1152/1168\n",
      "Training Step: 874  | total loss: \u001b[1m\u001b[32m0.14555\u001b[0m\u001b[0m | time: 1.107s\n",
      "| SGD | epoch: 046 | loss: 0.14555 - R2: 1.0004 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 875  | total loss: \u001b[1m\u001b[32m0.14549\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 047 | loss: 0.14549 - R2: 0.9994 -- iter: 0064/1168\n",
      "Training Step: 876  | total loss: \u001b[1m\u001b[32m0.14431\u001b[0m\u001b[0m | time: 0.049s\n",
      "| SGD | epoch: 047 | loss: 0.14431 - R2: 0.9981 -- iter: 0128/1168\n",
      "Training Step: 877  | total loss: \u001b[1m\u001b[32m0.14431\u001b[0m\u001b[0m | time: 0.054s\n",
      "| SGD | epoch: 047 | loss: 0.14431 - R2: 0.9981 -- iter: 0192/1168\n",
      "Training Step: 878  | total loss: \u001b[1m\u001b[32m0.14557\u001b[0m\u001b[0m | time: 0.056s\n",
      "| SGD | epoch: 047 | loss: 0.14557 - R2: 0.9988 -- iter: 0256/1168\n",
      "Training Step: 879  | total loss: \u001b[1m\u001b[32m0.14652\u001b[0m\u001b[0m | time: 0.060s\n",
      "| SGD | epoch: 047 | loss: 0.14652 - R2: 0.9989 -- iter: 0320/1168\n",
      "Training Step: 880  | total loss: \u001b[1m\u001b[32m0.15235\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 047 | loss: 0.15235 - R2: 0.9986 -- iter: 0384/1168\n",
      "Training Step: 881  | total loss: \u001b[1m\u001b[32m0.15889\u001b[0m\u001b[0m | time: 0.068s\n",
      "| SGD | epoch: 047 | loss: 0.15889 - R2: 0.9975 -- iter: 0448/1168\n",
      "Training Step: 882  | total loss: \u001b[1m\u001b[32m0.16703\u001b[0m\u001b[0m | time: 0.072s\n",
      "| SGD | epoch: 047 | loss: 0.16703 - R2: 0.9977 -- iter: 0512/1168\n",
      "Training Step: 883  | total loss: \u001b[1m\u001b[32m0.16703\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 047 | loss: 0.16703 - R2: 0.9977 -- iter: 0576/1168\n",
      "Training Step: 884  | total loss: \u001b[1m\u001b[32m0.16536\u001b[0m\u001b[0m | time: 0.076s\n",
      "| SGD | epoch: 047 | loss: 0.16536 - R2: 0.9965 -- iter: 0640/1168\n",
      "Training Step: 885  | total loss: \u001b[1m\u001b[32m0.16764\u001b[0m\u001b[0m | time: 0.084s\n",
      "| SGD | epoch: 047 | loss: 0.16764 - R2: 0.9973 -- iter: 0704/1168\n",
      "Training Step: 886  | total loss: \u001b[1m\u001b[32m0.16869\u001b[0m\u001b[0m | time: 0.087s\n",
      "| SGD | epoch: 047 | loss: 0.16869 - R2: 0.9977 -- iter: 0768/1168\n",
      "Training Step: 887  | total loss: \u001b[1m\u001b[32m0.17248\u001b[0m\u001b[0m | time: 0.089s\n",
      "| SGD | epoch: 047 | loss: 0.17248 - R2: 0.9985 -- iter: 0832/1168\n",
      "Training Step: 888  | total loss: \u001b[1m\u001b[32m0.17319\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 047 | loss: 0.17319 - R2: 0.9988 -- iter: 0896/1168\n",
      "Training Step: 889  | total loss: \u001b[1m\u001b[32m0.17057\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 047 | loss: 0.17057 - R2: 0.9994 -- iter: 0960/1168\n",
      "Training Step: 890  | total loss: \u001b[1m\u001b[32m0.16434\u001b[0m\u001b[0m | time: 0.102s\n",
      "| SGD | epoch: 047 | loss: 0.16434 - R2: 0.9996 -- iter: 1024/1168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 891  | total loss: \u001b[1m\u001b[32m0.16434\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 047 | loss: 0.16434 - R2: 0.9996 -- iter: 1088/1168\n",
      "Training Step: 892  | total loss: \u001b[1m\u001b[32m0.16508\u001b[0m\u001b[0m | time: 0.115s\n",
      "| SGD | epoch: 047 | loss: 0.16508 - R2: 0.9992 -- iter: 1152/1168\n",
      "Training Step: 893  | total loss: \u001b[1m\u001b[32m0.16635\u001b[0m\u001b[0m | time: 1.122s\n",
      "| SGD | epoch: 047 | loss: 0.16635 - R2: 0.9988 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 894  | total loss: \u001b[1m\u001b[32m0.15601\u001b[0m\u001b[0m | time: 0.005s\n",
      "| SGD | epoch: 048 | loss: 0.15601 - R2: 1.0004 -- iter: 0064/1168\n",
      "Training Step: 895  | total loss: \u001b[1m\u001b[32m0.16165\u001b[0m\u001b[0m | time: 0.010s\n",
      "| SGD | epoch: 048 | loss: 0.16165 - R2: 0.9987 -- iter: 0128/1168\n",
      "Training Step: 896  | total loss: \u001b[1m\u001b[32m0.16165\u001b[0m\u001b[0m | time: 0.014s\n",
      "| SGD | epoch: 048 | loss: 0.16165 - R2: 0.9987 -- iter: 0192/1168\n",
      "Training Step: 897  | total loss: \u001b[1m\u001b[32m0.16729\u001b[0m\u001b[0m | time: 0.017s\n",
      "| SGD | epoch: 048 | loss: 0.16729 - R2: 0.9991 -- iter: 0256/1168\n",
      "Training Step: 898  | total loss: \u001b[1m\u001b[32m0.16823\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 048 | loss: 0.16823 - R2: 0.9976 -- iter: 0320/1168\n",
      "Training Step: 899  | total loss: \u001b[1m\u001b[32m0.16399\u001b[0m\u001b[0m | time: 0.029s\n",
      "| SGD | epoch: 048 | loss: 0.16399 - R2: 0.9972 -- iter: 0384/1168\n",
      "Training Step: 900  | total loss: \u001b[1m\u001b[32m0.16699\u001b[0m\u001b[0m | time: 0.033s\n",
      "| SGD | epoch: 048 | loss: 0.16699 - R2: 1.0001 -- iter: 0448/1168\n",
      "Training Step: 901  | total loss: \u001b[1m\u001b[32m0.17199\u001b[0m\u001b[0m | time: 0.035s\n",
      "| SGD | epoch: 048 | loss: 0.17199 - R2: 1.0020 -- iter: 0512/1168\n",
      "Training Step: 902  | total loss: \u001b[1m\u001b[32m0.17199\u001b[0m\u001b[0m | time: 0.039s\n",
      "| SGD | epoch: 048 | loss: 0.17199 - R2: 1.0020 -- iter: 0576/1168\n",
      "Training Step: 903  | total loss: \u001b[1m\u001b[32m0.16894\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 048 | loss: 0.16894 - R2: 1.0022 -- iter: 0640/1168\n",
      "Training Step: 904  | total loss: \u001b[1m\u001b[32m0.16792\u001b[0m\u001b[0m | time: 0.044s\n",
      "| SGD | epoch: 048 | loss: 0.16792 - R2: 1.0009 -- iter: 0704/1168\n",
      "Training Step: 905  | total loss: \u001b[1m\u001b[32m0.16825\u001b[0m\u001b[0m | time: 0.046s\n",
      "| SGD | epoch: 048 | loss: 0.16825 - R2: 1.0002 -- iter: 0768/1168\n",
      "Training Step: 906  | total loss: \u001b[1m\u001b[32m0.16815\u001b[0m\u001b[0m | time: 0.048s\n",
      "| SGD | epoch: 048 | loss: 0.16815 - R2: 1.0003 -- iter: 0832/1168\n",
      "Training Step: 907  | total loss: \u001b[1m\u001b[32m0.16550\u001b[0m\u001b[0m | time: 0.050s\n",
      "| SGD | epoch: 048 | loss: 0.16550 - R2: 1.0003 -- iter: 0896/1168\n",
      "Training Step: 908  | total loss: \u001b[1m\u001b[32m0.16366\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 048 | loss: 0.16366 - R2: 0.9992 -- iter: 0960/1168\n",
      "Training Step: 909  | total loss: \u001b[1m\u001b[32m0.16299\u001b[0m\u001b[0m | time: 0.055s\n",
      "| SGD | epoch: 048 | loss: 0.16299 - R2: 0.9994 -- iter: 1024/1168\n",
      "Training Step: 910  | total loss: \u001b[1m\u001b[32m0.16554\u001b[0m\u001b[0m | time: 0.057s\n",
      "| SGD | epoch: 048 | loss: 0.16554 - R2: 0.9999 -- iter: 1088/1168\n",
      "Training Step: 911  | total loss: \u001b[1m\u001b[32m0.16654\u001b[0m\u001b[0m | time: 0.064s\n",
      "| SGD | epoch: 048 | loss: 0.16654 - R2: 1.0008 -- iter: 1152/1168\n",
      "Training Step: 912  | total loss: \u001b[1m\u001b[32m0.16381\u001b[0m\u001b[0m | time: 1.073s\n",
      "| SGD | epoch: 048 | loss: 0.16381 - R2: 1.0001 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 913  | total loss: \u001b[1m\u001b[32m0.16087\u001b[0m\u001b[0m | time: 0.074s\n",
      "| SGD | epoch: 049 | loss: 0.16087 - R2: 1.0001 -- iter: 0064/1168\n",
      "Training Step: 914  | total loss: \u001b[1m\u001b[32m0.16148\u001b[0m\u001b[0m | time: 0.078s\n",
      "| SGD | epoch: 049 | loss: 0.16148 - R2: 0.9996 -- iter: 0128/1168\n",
      "Training Step: 915  | total loss: \u001b[1m\u001b[32m0.15651\u001b[0m\u001b[0m | time: 0.088s\n",
      "| SGD | epoch: 049 | loss: 0.15651 - R2: 1.0011 -- iter: 0192/1168\n",
      "Training Step: 916  | total loss: \u001b[1m\u001b[32m0.15836\u001b[0m\u001b[0m | time: 0.094s\n",
      "| SGD | epoch: 049 | loss: 0.15836 - R2: 1.0008 -- iter: 0256/1168\n",
      "Training Step: 917  | total loss: \u001b[1m\u001b[32m0.15937\u001b[0m\u001b[0m | time: 0.098s\n",
      "| SGD | epoch: 049 | loss: 0.15937 - R2: 1.0012 -- iter: 0320/1168\n",
      "Training Step: 918  | total loss: \u001b[1m\u001b[32m0.15948\u001b[0m\u001b[0m | time: 0.103s\n",
      "| SGD | epoch: 049 | loss: 0.15948 - R2: 1.0012 -- iter: 0384/1168\n",
      "Training Step: 919  | total loss: \u001b[1m\u001b[32m0.16787\u001b[0m\u001b[0m | time: 0.105s\n",
      "| SGD | epoch: 049 | loss: 0.16787 - R2: 1.0012 -- iter: 0448/1168\n",
      "Training Step: 920  | total loss: \u001b[1m\u001b[32m0.16205\u001b[0m\u001b[0m | time: 0.109s\n",
      "| SGD | epoch: 049 | loss: 0.16205 - R2: 0.9989 -- iter: 0512/1168\n",
      "Training Step: 921  | total loss: \u001b[1m\u001b[32m0.15681\u001b[0m\u001b[0m | time: 0.113s\n",
      "| SGD | epoch: 049 | loss: 0.15681 - R2: 0.9969 -- iter: 0576/1168\n",
      "Training Step: 922  | total loss: \u001b[1m\u001b[32m0.15384\u001b[0m\u001b[0m | time: 0.118s\n",
      "| SGD | epoch: 049 | loss: 0.15384 - R2: 0.9970 -- iter: 0640/1168\n",
      "Training Step: 923  | total loss: \u001b[1m\u001b[32m0.15384\u001b[0m\u001b[0m | time: 0.124s\n",
      "| SGD | epoch: 049 | loss: 0.15384 - R2: 0.9970 -- iter: 0704/1168\n",
      "Training Step: 924  | total loss: \u001b[1m\u001b[32m0.15345\u001b[0m\u001b[0m | time: 0.126s\n",
      "| SGD | epoch: 049 | loss: 0.15345 - R2: 0.9963 -- iter: 0768/1168\n",
      "Training Step: 925  | total loss: \u001b[1m\u001b[32m0.15505\u001b[0m\u001b[0m | time: 0.138s\n",
      "| SGD | epoch: 049 | loss: 0.15505 - R2: 0.9946 -- iter: 0832/1168\n",
      "Training Step: 926  | total loss: \u001b[1m\u001b[32m0.15729\u001b[0m\u001b[0m | time: 0.145s\n",
      "| SGD | epoch: 049 | loss: 0.15729 - R2: 0.9949 -- iter: 0896/1168\n",
      "Training Step: 927  | total loss: \u001b[1m\u001b[32m0.15486\u001b[0m\u001b[0m | time: 0.147s\n",
      "| SGD | epoch: 049 | loss: 0.15486 - R2: 0.9952 -- iter: 0960/1168\n",
      "Training Step: 928  | total loss: \u001b[1m\u001b[32m0.15087\u001b[0m\u001b[0m | time: 0.149s\n",
      "| SGD | epoch: 049 | loss: 0.15087 - R2: 0.9961 -- iter: 1024/1168\n",
      "Training Step: 929  | total loss: \u001b[1m\u001b[32m0.15116\u001b[0m\u001b[0m | time: 0.151s\n",
      "| SGD | epoch: 049 | loss: 0.15116 - R2: 0.9962 -- iter: 1088/1168\n",
      "Training Step: 930  | total loss: \u001b[1m\u001b[32m0.14715\u001b[0m\u001b[0m | time: 0.154s\n",
      "| SGD | epoch: 049 | loss: 0.14715 - R2: 0.9966 -- iter: 1152/1168\n",
      "Training Step: 931  | total loss: \u001b[1m\u001b[32m0.14696\u001b[0m\u001b[0m | time: 1.158s\n",
      "| SGD | epoch: 049 | loss: 0.14696 - R2: 0.9971 | val_loss: 0.16063 - val_acc: 0.9945 -- iter: 1168/1168\n",
      "--\n",
      "Training Step: 932  | total loss: \u001b[1m\u001b[32m0.15123\u001b[0m\u001b[0m | time: 0.052s\n",
      "| SGD | epoch: 050 | loss: 0.15123 - R2: 0.9965 -- iter: 0064/1168\n",
      "Training Step: 933  | total loss: \u001b[1m\u001b[32m0.15358\u001b[0m\u001b[0m | time: 0.063s\n",
      "| SGD | epoch: 050 | loss: 0.15358 - R2: 0.9961 -- iter: 0128/1168\n",
      "Training Step: 934  | total loss: \u001b[1m\u001b[32m0.15148\u001b[0m\u001b[0m | time: 0.069s\n",
      "| SGD | epoch: 050 | loss: 0.15148 - R2: 0.9970 -- iter: 0192/1168\n"
     ]
    }
   ],
   "source": [
    "model.fit(train, labels_nl,show_metric=True,validation_set=0.2,shuffle=True,n_epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3c100e90-e9a9-c028-22a7-4c74ee31ab0c"
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "\n",
    "predictions_huber = best_clf.predict(test)\n",
    "predictions_DNN = model.predict(test)\n",
    "predictions_huber = np.exp(predictions_huber)\n",
    "predictions_DNN = np.exp(predictions_DNN)\n",
    "predictions_DNN = predictions_DNN.reshape(-1,)\n",
    "\n",
    "sub = pd.DataFrame({\n",
    "        \"Id\": ids,\n",
    "        \"SalePrice\": predictions_DNN\n",
    "    })\n",
    "\n",
    "sub.to_csv(\"prices_submission.csv\", index=False)\n",
    "#print(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b3730440-457e-7629-9eec-2faaef1608eb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_change_revision": 14399,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
