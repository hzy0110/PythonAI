{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_files(file_dir):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        file_dir: file directory\n",
    "    Returns:\n",
    "        list of images and labels\n",
    "    \"\"\"\n",
    "    cats = []\n",
    "    label_cats = []\n",
    "    dogs = []\n",
    "    label_dogs = []\n",
    "    for file in os.listdir(file_dir):\n",
    "        name = file.split(sep='.')\n",
    "        if name[0] == 'cat':\n",
    "            cats.append(file_dir + file)\n",
    "            label_cats.append(0)\n",
    "        else:\n",
    "            dogs.append(file_dir + file)\n",
    "            label_dogs.append(1)\n",
    "    print('There are %d cats\\nThere are %d dogs' % (len(cats), len(dogs)))\n",
    "\n",
    "    image_list = np.hstack((cats, dogs))\n",
    "    label_list = np.hstack((label_cats, label_dogs))\n",
    "\n",
    "    temp = np.array([image_list, label_list])\n",
    "    temp = temp.transpose()\n",
    "    np.random.shuffle(temp)\n",
    "\n",
    "    image_list = list(temp[:, 0])\n",
    "    label_list = list(temp[:, 1])\n",
    "    label_list = [int(i) for i in label_list]\n",
    "\n",
    "    return image_list, label_list\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "def get_batch(image, label, image_W, image_H, batch_size, capacity):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image: list type\n",
    "        label: list type\n",
    "        image_W: image width\n",
    "        image_H: image height\n",
    "        batch_size: batch size\n",
    "        capacity: the maximum elements in queue\n",
    "    Returns:\n",
    "        image_batch: 4D tensor [batch_size, width, height, 3], dtype=tf.float32\n",
    "        label_batch: 1D tensor [batch_size], dtype=tf.int32\n",
    "    \"\"\"\n",
    "\n",
    "    image = tf.cast(image, tf.string)\n",
    "    label = tf.cast(label, tf.int32)\n",
    "\n",
    "    # make an input queue\n",
    "    input_queue = tf.train.slice_input_producer([image, label])\n",
    "\n",
    "    label = input_queue[1]\n",
    "    image_contents = tf.read_file(input_queue[0])\n",
    "    image = tf.image.decode_jpeg(image_contents, channels=3)\n",
    "\n",
    "    ######################################\n",
    "    # data argumentation should go to here\n",
    "    ######################################\n",
    "\n",
    "    image = tf.image.resize_image_with_crop_or_pad(image, image_W, image_H)\n",
    "\n",
    "    # if you want to test the generated batches of images, you might want to comment the following line.\n",
    "    # 如果想看到正常的图片，请注释掉111行（标准化）和 126行（image_batch = tf.cast(image_batch, tf.float32)）\n",
    "    # 训练时不要注释掉！\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "\n",
    "    image_batch, label_batch = tf.train.batch([image, label],\n",
    "                                              batch_size=batch_size,\n",
    "                                              num_threads=64,\n",
    "                                              capacity=capacity)\n",
    "\n",
    "    # you can also use shuffle_batch\n",
    "    #    image_batch, label_batch = tf.train.shuffle_batch([image,label],\n",
    "    #                                                      batch_size=BATCH_SIZE,\n",
    "    #                                                      num_threads=64,\n",
    "    #                                                      capacity=CAPACITY,\n",
    "    #                                                      min_after_dequeue=CAPACITY-1)\n",
    "\n",
    "    label_batch = tf.reshape(label_batch, [batch_size])\n",
    "    image_batch = tf.cast(image_batch, tf.float32)\n",
    "\n",
    "    return image_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def VGG16N(x, n_classes, is_pretrain=True):\n",
    "    with tf.name_scope('VGG16'):\n",
    "        x = conv('conv1_1', x, 64, kernel_size=[3, 3], stride=[1, 1, 1, 1], is_pretrain=is_pretrain)\n",
    "        x = conv('conv1_2', x, 64, kernel_size=[3, 3], stride=[1, 1, 1, 1], is_pretrain=is_pretrain)\n",
    "        with tf.name_scope('pool1'):\n",
    "            x = pool('pool1', x, kernel=[1, 2, 2, 1], stride=[1, 2, 2, 1], is_max_pool=True)\n",
    "\n",
    "        x = conv('conv2_1', x, 128, kernel_size=[3, 3], stride=[1, 1, 1, 1], is_pretrain=is_pretrain)\n",
    "        x = conv('conv2_2', x, 128, kernel_size=[3, 3], stride=[1, 1, 1, 1], is_pretrain=is_pretrain)\n",
    "        with tf.name_scope('pool2'):\n",
    "            x = pool('pool2', x, kernel=[1, 2, 2, 1], stride=[1, 2, 2, 1], is_max_pool=True)\n",
    "\n",
    "        x = conv('conv3_1', x, 256, kernel_size=[3, 3], stride=[1, 1, 1, 1], is_pretrain=is_pretrain)\n",
    "        x = conv('conv3_2', x, 256, kernel_size=[3, 3], stride=[1, 1, 1, 1], is_pretrain=is_pretrain)\n",
    "        x = conv('conv3_3', x, 256, kernel_size=[3, 3], stride=[1, 1, 1, 1], is_pretrain=is_pretrain)\n",
    "        with tf.name_scope('pool3'):\n",
    "            x = pool('pool3', x, kernel=[1, 2, 2, 1], stride=[1, 2, 2, 1], is_max_pool=True)\n",
    "\n",
    "        x = conv('conv4_1', x, 512, kernel_size=[3, 3], stride=[1, 1, 1, 1], is_pretrain=is_pretrain)\n",
    "        x = conv('conv4_2', x, 512, kernel_size=[3, 3], stride=[1, 1, 1, 1], is_pretrain=is_pretrain)\n",
    "        x = conv('conv4_3', x, 512, kernel_size=[3, 3], stride=[1, 1, 1, 1], is_pretrain=is_pretrain)\n",
    "        with tf.name_scope('pool4'):\n",
    "            x = pool('pool4', x, kernel=[1, 2, 2, 1], stride=[1, 2, 2, 1], is_max_pool=True)\n",
    "\n",
    "        x = conv('conv5_1', x, 512, kernel_size=[3, 3], stride=[1, 1, 1, 1], is_pretrain=is_pretrain)\n",
    "        x = conv('conv5_2', x, 512, kernel_size=[3, 3], stride=[1, 1, 1, 1], is_pretrain=is_pretrain)\n",
    "        x = conv('conv5_3', x, 512, kernel_size=[3, 3], stride=[1, 1, 1, 1], is_pretrain=is_pretrain)\n",
    "        with tf.name_scope('pool5'):\n",
    "            x = pool('pool5', x, kernel=[1, 2, 2, 1], stride=[1, 2, 2, 1], is_max_pool=True)\n",
    "\n",
    "        x = FC_layer('fc6', x, out_nodes=4096)\n",
    "        with tf.name_scope('batch_norm1'):\n",
    "            x = batch_norm(x)\n",
    "        x = FC_layer('fc7', x, out_nodes=4096)\n",
    "        with tf.name_scope('batch_norm2'):\n",
    "            x = batch_norm(x)\n",
    "        x = FC_layer('fc8', x, out_nodes=n_classes)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv(layer_name, x, out_channels, kernel_size=[3, 3], stride=[1, 1, 1, 1], is_pretrain=True):\n",
    "    in_channels = x.get_shape()[-1]\n",
    "    with tf.variable_scope(layer_name):\n",
    "        w = tf.get_variable(name='weights',\n",
    "                            trainable=is_pretrain,\n",
    "                            shape=[kernel_size[0], kernel_size[1], in_channels, out_channels],\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())  # default is uniform distribution initialization\n",
    "        b = tf.get_variable(name='biases',\n",
    "                            trainable=is_pretrain,\n",
    "                            shape=[out_channels],\n",
    "                            initializer=tf.constant_initializer(0.0))\n",
    "        x = tf.nn.conv2d(x, w, stride, padding='SAME', name='conv')\n",
    "        x = tf.nn.bias_add(x, b, name='bias_add')\n",
    "        x = tf.nn.relu(x, name='relu')\n",
    "        return x\n",
    "    \n",
    "def pool(layer_name, x, kernel=[1, 2, 2, 1], stride=[1, 2, 2, 1], is_max_pool=True, padding='SAME'):\n",
    "    if is_max_pool:\n",
    "        x = tf.nn.max_pool(x, kernel, strides=stride, padding=padding, name=layer_name)\n",
    "    else:\n",
    "        x = tf.nn.avg_pool(x, kernel, strides=stride, padding=padding, name=layer_name)\n",
    "    return x\n",
    "\n",
    "def loss(logits, labels):\n",
    "    \"\"\"Compute loss\n",
    "    Args:\n",
    "        logits: logits tensor, [batch_size, n_classes]\n",
    "        labels: one-hot labels\n",
    "    \"\"\"\n",
    "    with tf.name_scope('loss') as scope:\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels, name='cross-entropy')\n",
    "        loss = tf.reduce_mean(cross_entropy, name='loss')\n",
    "        tf.summary.scalar(scope + '/loss', loss)\n",
    "        return loss\n",
    "    \n",
    "def accuracy(logits, labels):\n",
    "    \"\"\"Evaluate the quality of the logits at predicting the label.\n",
    "    Args:\n",
    "    logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "    labels: Labels tensor, \n",
    "    \"\"\"\n",
    "    with tf.name_scope('accuracy') as scope:\n",
    "        correct = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "        correct = tf.cast(correct, tf.float32)\n",
    "        accuracy = tf.reduce_mean(correct) * 100.0\n",
    "        tf.summary.scalar(scope + '/accuracy', accuracy)\n",
    "    return accuracy\n",
    "\n",
    "def FC_layer(layer_name, x, out_nodes):\n",
    "    \"\"\"Wrapper for fully connected layers with RELU activation as default\n",
    "    Args:\n",
    "        layer_name: e.g. 'FC1', 'FC2'\n",
    "        x: input feature map\n",
    "        out_nodes: number of neurons for current FC layer\n",
    "    \"\"\"\n",
    "    shape = x.get_shape()\n",
    "    if len(shape) == 4:\n",
    "        size = shape[1].value * shape[2].value * shape[3].value\n",
    "    else:\n",
    "        size = shape[-1].value\n",
    "\n",
    "    with tf.variable_scope(layer_name):\n",
    "        w = tf.get_variable('weights',\n",
    "                            shape=[size, out_nodes],\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.get_variable('biases',\n",
    "                            shape=[out_nodes],\n",
    "                            initializer=tf.constant_initializer(0.0))\n",
    "        flat_x = tf.reshape(x, [-1, size])  # flatten into 1D\n",
    "\n",
    "        x = tf.nn.bias_add(tf.matmul(flat_x, w), b)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x\n",
    "    \n",
    "def batch_norm(x):\n",
    "    \"\"\"Batch normlization(I didn't include the offset and scale)\n",
    "    \"\"\"\n",
    "    epsilon = 1e-3\n",
    "    batch_mean, batch_var = tf.nn.moments(x, [0])\n",
    "    x = tf.nn.batch_normalization(x,\n",
    "                                  mean=batch_mean,\n",
    "                                  variance=batch_var,\n",
    "                                  offset=None,\n",
    "                                  scale=None,\n",
    "                                  variance_epsilon=epsilon)\n",
    "    return x\n",
    "\n",
    "def optimize(loss, learning_rate, global_step):\n",
    "    \"\"\"optimization, use Gradient Descent as default\n",
    "    \"\"\"\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        # optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "        return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义参数\n",
    "BATCH_SIZE = 64\n",
    "CAPACITY = 256\n",
    "IMG_W = 208\n",
    "IMG_H = 208\n",
    "N_CLASSES = 2\n",
    "learning_rate = 0.01\n",
    "MAX_STEP = 150\n",
    "IS_PRETRAIN = True\n",
    "train_dir = \"./data/train\"\n",
    "test_dir = \"./data/test\"\n",
    "train_log_dir = './logs/train/'\n",
    "val_log_dir = './logs/val/'\n",
    "pre_trained_weights = './VGG16/vgg16.npy'\n",
    "graphCNN = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_with_skip(data_path, session, skip_layer):\n",
    "    \"\"\"\n",
    "    控制哪几层的参数不加载\n",
    "    :param data_path:\n",
    "    :param session:\n",
    "    :param skip_layer:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data_dict = np.load(data_path, encoding='latin1').item()\n",
    "    for key in data_dict:\n",
    "        if key not in skip_layer:\n",
    "            with tf.variable_scope(key, reuse=True):\n",
    "                for subkey, data in zip(('weights', 'biases'), data_dict[key]):\n",
    "                    session.run(tf.get_variable(subkey).assign(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12500 cats\n",
      "There are 12500 dogs\n"
     ]
    }
   ],
   "source": [
    "with graphCNN.as_default():\n",
    "    # 读取文件\n",
    "    image_list, label_list = get_files(train_dir)\n",
    "    # 转换数据文件\n",
    "    image_batch, label_batch = get_batch(image_list, label_list, IMG_W, IMG_H, len(image_list), CAPACITY)\n",
    "    label_batch_reshape = tf.reshape(label_batch,[-1,1])\n",
    "    \n",
    "    # 占位符定义输入数据形状和标签形状\n",
    "    x = tf.placeholder(tf.float32, shape=[BATCH_SIZE, IMG_W, IMG_H, 3])\n",
    "    y = tf.placeholder(tf.int16, shape=[BATCH_SIZE, N_CLASSES])\n",
    "\n",
    "    logits = VGG16N(x, N_CLASSES, IS_PRETRAIN)\n",
    "    loss = loss(logits, y)\n",
    "    accuracy = accuracy(logits, y)\n",
    "\n",
    "    my_global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    train_op = optimize(loss, learning_rate, my_global_step)\n",
    "    # tf.global_variables() 获取程序中的变量\n",
    "    saver = tf.train.Saver(tf.global_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 208, 208, 3)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graphCNN) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    image_list_len = len(image_list)\n",
    "    # 读取训练好的VGG16\n",
    "#     load_with_skip(pre_trained_weights, sess, ['fc8'])\n",
    "\n",
    "    # 多线程\n",
    "#     coord = tf.train.Coordinator()\n",
    "#     threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "    # 记录图形和文件位置\n",
    "#     tra_summary_writer = tf.summary.FileWriter(train_log_dir, sess.graph)\n",
    "#     val_summary_writer = tf.summary.FileWriter(val_log_dir, sess.graph)\n",
    "    print(image_batch.shape)\n",
    "    print(label_batch.shape)\n",
    "    \n",
    "\n",
    "    for step in np.arange(MAX_STEP):\n",
    "        # 数据分批\n",
    "        offset = (step * BATCH_SIZE) % (image_list_len - BATCH_SIZE)\n",
    "        batch_data_train = image_batch[offset:(offset + BATCH_SIZE), :, :, :]\n",
    "        batch_labels_train = label_batch_reshape[offset:(offset + BATCH_SIZE), :]\n",
    "        sess.run([batch_data_train, batch_labels_train])\n",
    "#         summary_t, _, tra_loss, tra_acc = sess.run([summary_op, train_op, loss, accuracy],\n",
    "#                                                    feed_dict={x: batch_data_train, y: batch_labels_train})\n",
    "#         # 输出loss值\n",
    "#         if step % 50 == 0:\n",
    "#             # 每一百步保存\n",
    "\n",
    "#             # summary_v, _, val_loss, val_acc = sess.run([summary_op, train_op, loss, accuracy],\n",
    "#             #                                            feed_dict={x: valid_dataset, y: valid_labels})\n",
    "#             print('Step: %d, loss: %.4f, tra_accuracy: %.4f%%' % (i, tra_loss, tra_acc))\n",
    "#             # print('Step: %d, loss: %.4f, val_accuracy: %.4f%%' % (i, val_loss, val_acc))\n",
    "# #             tra_summary_writer.add_summary(summary_t, i)\n",
    "#             # val_summary_writer.add_summary(summary_v, i)\n",
    "\n",
    "#     print(\"Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
