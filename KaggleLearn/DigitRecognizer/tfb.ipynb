{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# settings\n",
    "LEARNING_RATE = 1e-4\n",
    "# set to 20000 on local environment to get 0.99 accuracy\n",
    "TRAINING_ITERATIONS = 25\n",
    "\n",
    "DROPOUT = 0.5\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "# set to 0 to train on all available data\n",
    "VALIDATION_SIZE = 2000\n",
    "\n",
    "# image number to output\n",
    "IMAGE_TO_DISPLAY = 10\n",
    "\n",
    "# read training data from CSV file\n",
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "# print('data({0[0]},{0[1]})'.format(data.shape))\n",
    "# print(data.head())\n",
    "\n",
    "images = data.iloc[:, 1:].values\n",
    "images = images.astype(np.float)\n",
    "\n",
    "# 数据乘以1/255，0-255->0-1\n",
    "# convert from [0:255] => [0.0:1.0]\n",
    "images = np.multiply(images, 1.0 / 255.0)\n",
    "\n",
    "# print('images({0[0]},{0[1]})'.format(images.shape))\n",
    "\n",
    "# print(data)\n",
    "\n",
    "# labels_flat = data[[0]].values.ravel()\n",
    "# iiitttttrr\n",
    "labels_flat = data.iloc[:, 0].values\n",
    "# 打印标签列长度\n",
    "# print('labels_flat({0})'.format(len(labels_flat)))\n",
    "# 打印第X行的图片对应的标签\n",
    "# print('labels_flat[{0}] => {1}'.format(IMAGE_TO_DISPLAY, labels_flat[IMAGE_TO_DISPLAY]))\n",
    "\n",
    "# unique是返回数组的唯一元素，然后获取形状位置0\n",
    "labels_count = np.unique(labels_flat).shape[0]\n",
    "# 打印总数\n",
    "# print('labels_count = {0}'.format(labels_count))\n",
    "\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1  # grayscale\n",
    "batch_size = 32\n",
    "patch_size = 5\n",
    "depth = 32\n",
    "num_hidden = 1024\n",
    "graph = tf.Graph()\n",
    "regularation_param = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 转换类标签标量到独热编码\n",
    "# convert class labels from scalars to one-hot vectors\n",
    "# 0 => [1 0 0 0 0 0 0 0 0 0]\n",
    "# 1 => [0 1 0 0 0 0 0 0 0 0]\n",
    "# ...\n",
    "# 9 => [0 0 0 0 0 0 0 0 0 1]\n",
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "    #    labels_dense = [1 0 1 ..., 7 6 9]\n",
    "    #    num_classes = 10\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    #    num_labels = 42000\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    #    index_offset = [     0     10     20 ..., 419970 419980 419990]\n",
    "    #    index_offset.shape = (42000,)\n",
    "    #    创建一个全0数组\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    #     print(labels_one_hot)\n",
    "    #    labels_one_hot.shape = (42000,10)\n",
    "    #    flat是一个循环迭代器,通过乘以10，来给每一行的数字打上one hot的1\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    #     print(labels_one_hot.flat[22])\n",
    "    return labels_one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# print(labels_one_hot.flat[21])\n",
    "labels = dense_to_one_hot(labels_flat, labels_count)\n",
    "labels = labels.astype(np.uint8)\n",
    "\n",
    "# print('labels({0[0]},{0[1]})'.format(labels.shape))\n",
    "# print('labels[{0}] => {1}'.format(IMAGE_TO_DISPLAY, labels[IMAGE_TO_DISPLAY]))\n",
    "\n",
    "# 分割数据，分成验证集和训练集\n",
    "validation_images = images[:VALIDATION_SIZE]\n",
    "validation_labels = labels[:VALIDATION_SIZE]\n",
    "\n",
    "train_images = images[VALIDATION_SIZE:]\n",
    "train_labels = labels[VALIDATION_SIZE:]\n",
    "# print(\"train_labels.shape\", train_labels.shape)\n",
    "# print('train_images({0[0]},{0[1]})'.format(train_images.shape))\n",
    "#\n",
    "# print('validation_images({0[0]},{0[1]})'.format(validation_images.shape))\n",
    "# print('validation_labels({0[0]},{0[1]})'.format(validation_images.shape))\n",
    "\n",
    "train_images, validation_images = model_selection.train_test_split(images, test_size=0.3, random_state=0)\n",
    "train_labels, validation_labels = model_selection.train_test_split(labels, test_size=0.3, random_state=0)\n",
    "# print(\"train_labels.shape\", train_labels.shape)\n",
    "# print('train_images({0[0]},{0[1]})'.format(train_images.shape))\n",
    "#\n",
    "# print('validation_images({0[0]},{0[1]})'.format(validation_images.shape))\n",
    "# print('validation_labels({0[0]},{0[1]})'.format(validation_images.shape))\n",
    "\n",
    "# read test data from CSV file\n",
    "test_images = pd.read_csv('test.csv').values\n",
    "test_images = test_images.astype(np.float)\n",
    "\n",
    "# convert from [0:255] => [0.0:1.0]\n",
    "test_images = np.multiply(test_images, 1.0 / 255.0)\n",
    "\n",
    "# print('test_images({0[0]},{0[1]})'.format(test_images.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算精准度\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / predictions.shape[0])\n",
    "\n",
    "\n",
    "# 保存结果\n",
    "def savecsv(test_prediction_np, savename='submission_uda_dr.csv'):\n",
    "    np.savetxt(savename,\n",
    "               np.c_[range(1, len(test_images) + 1), test_prediction_np],\n",
    "               delimiter=',',\n",
    "               header='ImageId,Label',\n",
    "               comments='',\n",
    "               fmt='%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print('train set', train_images.shape, train_labels.shape)\n",
    "# print('validation set', validation_images.shape, validation_labels.shape)\n",
    "# print('test set', test_images.shape)\n",
    "valid_labels = validation_labels\n",
    "\n",
    "\n",
    "def reformat2(dataset, labels):\n",
    "    dataset = dataset.reshape(\n",
    "        (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "    labels = (np.arange(num_labels) == labels[:, None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "\n",
    "def reformat1(dataset):\n",
    "    dataset = dataset.reshape((-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_dataset = reformat1(train_images)\n",
    "valid_dataset = reformat1(validation_images)\n",
    "test_dataset = reformat1(test_images)\n",
    "# print('train set', train_dataset.shape, train_labels.shape)\n",
    "# print('validation set', valid_dataset.shape, valid_labels.shape)\n",
    "# print('test set', test_dataset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 创建权重\n",
    "def weight_varible(shape, name=None):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1, name=name)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "# 创建偏好\n",
    "def bias_variable(shape, name=None):\n",
    "    initial = tf.constant(0.1, shape=shape, name=name)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "# 设置卷积\n",
    "def conv2d(x, W, step=1):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, step, step, 1], padding='SAME')\n",
    "\n",
    "\n",
    "# 设置池化\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_ConvLayer(inputs, patch, in_deep, out_deep, n_layer, step=1, activation_function=tf.nn.relu):\n",
    "    # add one more layer and return the output of this layer\n",
    "    layer_name = 'layer%s' % n_layer\n",
    "    with tf.name_scope(layer_name):\n",
    "        with tf.name_scope('weights'):\n",
    "            Weights = weight_varible([patch, patch, in_deep, out_deep], name='W')\n",
    "            # variable_summaries(Weights)\n",
    "            # tf.summary.histogram(layer_name + '/weights', Weights)\n",
    "        with tf.name_scope('biases'):\n",
    "            # biases = tf.Variable(tf.zeros([1, depth]) + 0.1, name='b')\n",
    "            biases = bias_variable([out_deep], name='b')\n",
    "            # variable_summaries(biases)\n",
    "            # tf.summary.histogram(layer_name + '/biases', biases)\n",
    "        if step == 2:\n",
    "            conv = conv2d(inputs, Weights, step)\n",
    "            linear = tf.add(conv, biases)\n",
    "            # tf.summary.histogram('linear', linear)\n",
    "        elif step == 1:\n",
    "            conv = conv2d(inputs, Weights, step)\n",
    "            maxPool = max_pool_2x2(conv)\n",
    "            linear = tf.add(maxPool, biases)\n",
    "            # tf.summary.histogram('linear', linear)\n",
    "\n",
    "        # 激活函数\n",
    "        activations = activation_function(linear)\n",
    "        # tf.summary.histogram('activations', activations)\n",
    "        # tf.summary.histogram(layer_name + '/outputs', outputs)\n",
    "    return activations\n",
    "\n",
    "\n",
    "def add_FullLayer(inputs, in_size, out_size, n_layer, activation_function=tf.nn.relu):\n",
    "    # add one more layer and return the output of this layer\n",
    "    layer_name = 'layer%s' % n_layer\n",
    "    with tf.name_scope(layer_name):\n",
    "        with tf.name_scope('weights'):\n",
    "            Weights = weight_varible([in_size, out_size], name='W')\n",
    "            # tf.summary.histogram(layer_name + '/weights', Weights)\n",
    "        with tf.name_scope('biases'):\n",
    "            # biases = tf.Variable(tf.zeros([1, depth]) + 0.1, name='b')\n",
    "            biases = bias_variable([out_size], name='b')\n",
    "            # tf.summary.histogram(layer_name + '/biases', biases)\n",
    "\n",
    "        n = tf.add(tf.matmul(inputs, Weights), biases)\n",
    "\n",
    "        # 激活函数\n",
    "        outputs = activation_function(n)\n",
    "        # tf.summary.histogram(layer_name + '/outputs', outputs)\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 卷积模型\n",
    "def model_conv(data):\n",
    "    l1 = add_ConvLayer(data, patch_size, num_channels, depth, step=2, n_layer=1)\n",
    "    l2 = add_ConvLayer(l1, patch_size, depth, depth, step=2, n_layer=2)\n",
    "    l3 = add_ConvLayer(l2, patch_size, depth, depth, step=2, n_layer=3)\n",
    "    shape = l3.get_shape().as_list()\n",
    "    reshape = tf.reshape(l3, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    l4 = add_FullLayer(reshape, shape[1] * shape[2] * shape[3], num_hidden, n_layer=4)\n",
    "    l5 = add_FullLayer(l4, num_hidden, num_labels, n_layer=5)\n",
    "    return l5\n",
    "\n",
    "\n",
    "def model_maxpool(data):\n",
    "    l1 = add_ConvLayer(data, patch_size, num_channels, depth, n_layer=1)\n",
    "    l2 = add_ConvLayer(l1, patch_size, depth, depth, step=1, n_layer=2)\n",
    "    shape = l2.get_shape().as_list()\n",
    "    reshape = tf.reshape(l2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    l3 = add_FullLayer(reshape, shape[1] * shape[2] * shape[3], num_hidden, n_layer=3)\n",
    "    # DropOut\n",
    "    # with tf.name_scope('dropout'):\n",
    "    keep_prob = 0.7\n",
    "    # tf.summary.scalar('dropout_keep_probability', keep_prob)\n",
    "    l3 = tf.nn.dropout(l3, keep_prob)\n",
    "    l4 = add_FullLayer(l3, num_hidden, num_labels, n_layer=4)\n",
    "    return l4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        # 计算参数的均值，并使用tf.summary.scaler记录\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "\n",
    "        # 计算参数的标准差\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            # 使用tf.summary.scaler记录记录下标准差，最大值，最小值\n",
    "            tf.summary.scalar('stddev', stddev)\n",
    "            tf.summary.scalar('max', tf.reduce_max(var))\n",
    "            tf.summary.scalar('min', tf.reduce_min(var))\n",
    "            # 用直方图记录参数的分布\n",
    "            tf.summary.histogram('histogram', var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    # 输入数据\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # 转数据类型，f64->f32\n",
    "    tf_train_dataset = tf.to_float(tf_train_dataset)\n",
    "    tf_valid_dataset = tf.to_float(tf_valid_dataset)\n",
    "    tf_test_dataset = tf.to_float(tf_test_dataset)\n",
    "\n",
    "    # 变量，在这里是过滤器用\n",
    "    # truncated_normal按照正态分布初始化权重\n",
    "    # mean是正态分布的平均值\n",
    "    # stddev是正态分布的标准差（standard deviation）\n",
    "    # seed是作为分布的random seed（随机种子，我百度了一下，跟什么伪随机数发生器还有关，就是产生随机数的）\n",
    "\n",
    "    layer1_weights = weight_varible([patch_size, patch_size, num_channels, depth])\n",
    "    layer21_weights = weight_varible([patch_size, patch_size, depth, depth])\n",
    "    layer2_weights = weight_varible([patch_size, patch_size, depth, depth])\n",
    "    # 全连接层\n",
    "    layer3_weights = weight_varible([512, num_hidden])\n",
    "    layer4_weights = weight_varible([num_hidden, num_labels])\n",
    "\n",
    "\n",
    "    # 训练计算\n",
    "    # 损失函数（loss function）是用来估量你模型的预测值f(x)与真实值Y的不一致程度\n",
    "    # 它是一个非负实值函数，通常使用L(Y, f(x))来表示，损失函数越小，模型的可能指就越好。\n",
    "    logits = model_maxpool(tf_train_dataset)\n",
    "\n",
    "    #     print(logits.get_shape())# (16, 10)\n",
    "    #     print(tf_train_labels.get_shape()) # (16, 10)\n",
    "    with tf.name_scope('loss'):\n",
    "        hpl2 = regularation_param * (tf.nn.l2_loss(layer1_weights)\n",
    "                                     + tf.nn.l2_loss(layer21_weights)\n",
    "                                     + tf.nn.l2_loss(layer2_weights)\n",
    "                                     + tf.nn.l2_loss(layer3_weights)\n",
    "                                     + tf.nn.l2_loss(layer4_weights))\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "        loss = tf.add(loss, hpl2)\n",
    "        tf.summary.scalar('loss', loss)\n",
    "        # 计算参数的标准差\n",
    "    # 计算预测值prediction和真实值的误差\n",
    "    # loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "\n",
    "    # 初始的学习速率\n",
    "    starter_learning_rate = 0.1\n",
    "    # 全局的step，与 decay_step 和 decay_rate一起决定了 learning rate的变化\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    # 衰减速度\n",
    "    decay_steps = 50\n",
    "    # 衰减系数\n",
    "    decay_rate = 0.9\n",
    "    # 如果staircase=True，那就表明每decay_steps次计算学习速率变化，更新原始学习速率.\n",
    "    # 如果是False，那就是每一步都更新学习速率\n",
    "    staircase = False\n",
    "    # 指数衰减:法通过这个函数，可以先使用较大的学习率来快速得到一个比较优的解，然后随着迭代的继续逐步减小学习率，使得模型在训练后期更加稳定\n",
    "    # 87.7% 仅仅指数衰减\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, decay_steps, decay_rate, staircase)\n",
    "\n",
    "    # 优化器\n",
    "    # optimizer = tf.train.GradientDescentOptimizer(0.03).minimize(loss)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.03).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "    # 对训练，验证和测试数据集进行预测\n",
    "\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model_maxpool(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model_maxpool(tf_test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Step 0 (epoch 0.00), 25.8 ms\n",
      "Minibatch loss at step 0: 3.735353\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-d7046fedae5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mva\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_prediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Minibatch accuracy: %.1f%%'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Validation accuracy: %.1f%%'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mva\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m     \"\"\"\n\u001b[0;32m--> 541\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   4083\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4084\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 4085\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    num_steps = 201\n",
    "    EVAL_FREQUENCY = 100\n",
    "    train_size = train_labels.shape[0]\n",
    "    start_time = time.time()\n",
    "\n",
    "    merged = tf.summary.merge_all()\n",
    "    summary_writer = tf.summary.FileWriter('logs', session.graph)\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    start = time.time()\n",
    "    for s in range(num_steps):\n",
    "        offset = (s * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        # _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        summary, l, predictions = session.run([merged, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "        if s % EVAL_FREQUENCY == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            start_time = time.time()\n",
    "\n",
    "            print('Step %d (epoch %.2f), %.1f ms' %\n",
    "                  (s, float(s) * batch_size / train_size,\n",
    "                   1000 * elapsed_time / EVAL_FREQUENCY))\n",
    "            print('Minibatch loss at step %d: %f' % (s, l))\n",
    "\n",
    "            ma = accuracy(predictions, batch_labels)\n",
    "            va = accuracy(valid_prediction.eval(), valid_labels)\n",
    "            print('Minibatch accuracy: %.1f%%' % ma)\n",
    "            print('Validation accuracy: %.1f%%' % va)\n",
    "            # tf.summary.scalar('accuracy', ma)\n",
    "            # tf.summary.scalar('accuracy', va)\n",
    "            summary_writer.add_summary(summary, s)\n",
    "\n",
    "    # 获取结果，用于保存\n",
    "    test_prediction_np = test_prediction.eval()\n",
    "    test_prediction_np = np.argmax(test_prediction_np, 1)\n",
    "    savecsv(test_prediction_np, \"submission_uda_conv.csv\")\n",
    "    end = time.time()\n",
    "    print(end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
