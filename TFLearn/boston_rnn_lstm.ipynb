{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T01:20:37.546322Z",
     "start_time": "2018-12-17T01:19:53.281679Z"
    }
   },
   "outputs": [],
   "source": [
    "# 参考https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/5-09-RNN3/\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T01:20:38.523885Z",
     "start_time": "2018-12-17T01:20:37.549232Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "波士顿数据X: (506, 13)\n",
      "波士顿房价Y: (506,)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "tf.reset_default_graph()\n",
    "# 波士顿房价数据\n",
    "boston = load_boston()\n",
    "x = boston.data\n",
    "y = boston.target\n",
    "\n",
    "print('波士顿数据X:', x.shape)  # (506, 13)\n",
    "# print(x[::100])\n",
    "print('波士顿房价Y:', y.shape)\n",
    "# print(y[::100])\n",
    "# 数据标准化\n",
    "ss_x = preprocessing.StandardScaler()\n",
    "train_x = ss_x.fit_transform(x)\n",
    "# print(train_x)\n",
    "ss_y = preprocessing.StandardScaler()\n",
    "train_y = ss_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "BATCH_START = 0  # 建立 batch data 时候的 index\n",
    "TIME_STEPS = 10  # backpropagation through time 的 time_steps\n",
    "BATCH_SIZE = 30\n",
    "INPUT_SIZE = 13  \n",
    "OUTPUT_SIZE = 1  \n",
    "CELL_SIZE = 10  # RNN 的 hidden unit size\n",
    "LR = 0.006  # learning rate\n",
    "# 保存session状态的位置\n",
    "save_dir = './bst_save/ckpt/bst'\n",
    "# logs_path = './bst_save/log/'\n",
    "\n",
    "def get_batch_boston():\n",
    "    global train_x, train_y, BATCH_START, TIME_STEPS\n",
    "    # 每次都是拿300行，13列的数据，但是起始点根据 BATCH_START\n",
    "    x_part1 = train_x[BATCH_START:BATCH_START + TIME_STEPS * BATCH_SIZE]\n",
    "    y_part1 = train_y[BATCH_START:BATCH_START + TIME_STEPS * BATCH_SIZE]\n",
    "    # 时间段= 0 300\n",
    "    # x_part1= (300, 13)\n",
    "    # 时间段= 10 310\n",
    "    # x_part1= (300, 13)\n",
    "#     print('时间段=', BATCH_START, BATCH_START + TIME_STEPS * BATCH_SIZE)\n",
    "#     print('x_part1=', x_part1.shape)\n",
    "#     print(BATCH_SIZE, TIME_STEPS, INPUT_SIZE)\n",
    "    # 转成 30 10 13的形状\n",
    "    seq = x_part1.reshape((BATCH_SIZE, TIME_STEPS, INPUT_SIZE))\n",
    "    res = y_part1.reshape((BATCH_SIZE, TIME_STEPS, 1))\n",
    "\n",
    "    BATCH_START += TIME_STEPS\n",
    "\n",
    "    # returned seq, res and xs: shape (batch, step, input)\n",
    "    #np.newaxis 用来增加一个维度 变为三个维度，第三个维度将用来存上一批样本的状态\n",
    "    return [seq, res]\n",
    "\n",
    "\n",
    "# def get_batch():\n",
    "#     global BATCH_START, TIME_STEPS\n",
    "#     # xs shape (50batch, 20steps)\n",
    "#     xs = np.arange(BATCH_START, BATCH_START + TIME_STEPS * BATCH_SIZE).reshape(\n",
    "#         (BATCH_SIZE, TIME_STEPS)) / (10 * np.pi)\n",
    "#     print('xs.shape=', xs.shape)\n",
    "#     seq = np.sin(xs)\n",
    "#     res = np.cos(xs)\n",
    "#     BATCH_START += TIME_STEPS\n",
    "#     # import matplotlib.pyplot as plt\n",
    "#     # plt.plot(xs[0, :], res[0, :], 'r', xs[0, :], seq[0, :], 'b--')\n",
    "#     # plt.show()\n",
    "#     print('增加维度前:', seq.shape)\n",
    "#     print(seq[:2])\n",
    "#     print('增加维度后:', seq[:, :, np.newaxis].shape)\n",
    "#     print(seq[:2])\n",
    "#     # returned seq, res and xs: shape (batch, step, input)\n",
    "#     #np.newaxis 用来增加一个维度 变为三个维度，第三个维度将用来存上一批样本的状态\n",
    "#     return [seq[:, :, np.newaxis], res[:, :, np.newaxis], xs]\n",
    "\n",
    "\n",
    "class LSTMRNN(object):\n",
    "    def __init__(self, n_steps, input_size, output_size, cell_size,\n",
    "                 batch_size):\n",
    "        '''\n",
    "        :param n_steps: 每批数据总包含多少时间刻度\n",
    "        :param input_size: 输入数据的维度\n",
    "        :param output_size: 输出数据的维度 如果是类似价格曲线的话，应该为1\n",
    "        :param cell_size: cell的大小\n",
    "        :param batch_size: 每批次训练数据的数量\n",
    "        '''\n",
    "        self.n_steps = n_steps\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.cell_size = cell_size\n",
    "        self.batch_size = batch_size\n",
    "        with tf.name_scope('inputs'):\n",
    "            self.xs = tf.placeholder(\n",
    "                tf.float32, [None, n_steps, input_size], name='xs')  #xs 有三个维度\n",
    "            self.ys = tf.placeholder(\n",
    "                tf.float32, [None, n_steps, output_size], name='ys')  #ys 有三个维度\n",
    "        with tf.variable_scope('in_hidden'):\n",
    "            self.add_input_layer()\n",
    "        with tf.variable_scope('LSTM_cell'):\n",
    "            self.add_cell()\n",
    "        with tf.variable_scope('out_hidden'):\n",
    "            self.add_output_layer()\n",
    "        with tf.name_scope('cost'):\n",
    "            self.compute_cost()\n",
    "        with tf.name_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(LR).minimize(self.cost)\n",
    "\n",
    "    #增加一个输入层\n",
    "    def add_input_layer(self, ):\n",
    "        # l_in_x:(batch*n_step, in_size),相当于把这个批次的样本串到一个长度1000的时间线上，每批次50个样本，每个样本20个时刻\n",
    "        l_in_x = tf.reshape(\n",
    "            self.xs, [-1, self.input_size], name='2_2D')  #-1 表示任意行数\n",
    "        # Ws (in_size, cell_size)\n",
    "        Ws_in = self._weight_variable([self.input_size, self.cell_size])\n",
    "        # bs (cell_size, )\n",
    "        bs_in = self._bias_variable([\n",
    "            self.cell_size,\n",
    "        ])\n",
    "        # l_in_y = (batch * n_steps, cell_size)\n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            l_in_y = tf.matmul(l_in_x, Ws_in) + bs_in\n",
    "        # reshape l_in_y ==> (batch, n_steps, cell_size)\n",
    "        self.l_in_y = tf.reshape(\n",
    "            l_in_y, [-1, self.n_steps, self.cell_size], name='2_3D')\n",
    "\n",
    "    #多时刻的状态叠加层 CELL\n",
    "    def add_cell(self):\n",
    "        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(\n",
    "            self.cell_size, forget_bias=1.0, state_is_tuple=True)\n",
    "        with tf.name_scope('initial_state'):\n",
    "            self.cell_init_state = lstm_cell.zero_state(\n",
    "                self.batch_size, dtype=tf.float32)\n",
    "        #time_major=False 表示时间主线不是第一列batch\n",
    "        self.cell_outputs, self.cell_final_state = tf.nn.dynamic_rnn(\n",
    "            lstm_cell,\n",
    "            self.l_in_y,\n",
    "            initial_state=self.cell_init_state,\n",
    "            time_major=False)\n",
    "\n",
    "    # 增加一个输出层\n",
    "    def add_output_layer(self):\n",
    "        # shape = (batch * steps, cell_size)\n",
    "        l_out_x = tf.reshape(\n",
    "            self.cell_outputs, [-1, self.cell_size], name='2_2D')\n",
    "        Ws_out = self._weight_variable([self.cell_size, self.output_size])\n",
    "        bs_out = self._bias_variable([\n",
    "            self.output_size,\n",
    "        ])\n",
    "        # shape = (batch * steps, output_size)\n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            self.pred = tf.matmul(l_out_x, Ws_out) + bs_out  #预测结果\n",
    "    \n",
    "    # 计算损失\n",
    "    def compute_cost(self):\n",
    "        losses = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "            [tf.reshape(self.pred, [-1], name='reshape_pred')],\n",
    "            [tf.reshape(self.ys, [-1], name='reshape_target')],\n",
    "            [tf.ones([self.batch_size * self.n_steps], dtype=tf.float32)],\n",
    "            average_across_timesteps=True,\n",
    "            softmax_loss_function=self.ms_error,\n",
    "            name='losses')\n",
    "        with tf.name_scope('average_cost'):\n",
    "            self.cost = tf.div(\n",
    "                tf.reduce_sum(losses, name='losses_sum'),\n",
    "                self.batch_size,\n",
    "                name='average_cost')\n",
    "            tf.summary.scalar('cost', self.cost)\n",
    "\n",
    "    def ms_error(self, labels, logits):\n",
    "        return tf.square(tf.subtract(labels, logits))\n",
    "\n",
    "    def _weight_variable(self, shape, name='weights'):\n",
    "        initializer = tf.random_normal_initializer(\n",
    "            mean=0.,\n",
    "            stddev=1.,\n",
    "        )\n",
    "        return tf.get_variable(shape=shape, initializer=initializer, name=name)\n",
    "\n",
    "    def _bias_variable(self, shape, name='biases'):\n",
    "        initializer = tf.constant_initializer(0.1)\n",
    "        return tf.get_variable(name=name, shape=shape, initializer=initializer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T01:21:11.310858Z",
     "start_time": "2018-12-17T01:20:38.526801Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cost:  4.9066\n",
      "1 cost:  2.7821\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "tf.reset_default_graph()\n",
    "if __name__ == '__main__':\n",
    "    # 设置画布初始属性和内容\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 3))  # dpi参数指定绘图对象的分辨率，即每英寸多少个像素，缺省值为80\n",
    "    axes = fig.add_subplot(1, 1, 1)\n",
    "    train_y1 = train_y[190:490]\n",
    "    # 限制100个显示值\n",
    "#     line3, = axes.plot(range(100), train_y1.flatten()[-100:], 'r', label='实际')\n",
    "    line3, = axes.plot(train_y1.flatten(), 'r', label='实际')\n",
    "    plt.ion()\n",
    "    plt.show()\n",
    "    \n",
    "    seq, res = get_batch_boston()\n",
    "    model = LSTMRNN(TIME_STEPS, INPUT_SIZE, OUTPUT_SIZE, CELL_SIZE, BATCH_SIZE)\n",
    "    sess = tf.Session()\n",
    "    merged = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"logs\", sess.graph)\n",
    "    # tf.initialize_all_variables() no long valid from\n",
    "    # 2017-03-02 if using tensorflow >= 0.12\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # relocate to the local dir and run this line to view it on Chrome (http://0.0.0.0:6006/):\n",
    "    # $ tensorboard --logdir='logs'\n",
    "    for j in range(2):  #训练200次\n",
    "        pred_res = None\n",
    "        for i in range(20):  #把整个数据分为20个时间段\n",
    "            seq, res = get_batch_boston()\n",
    "#             print('seq',seq.shape)\n",
    "            if i == 0:\n",
    "                feed_dict = {\n",
    "                    model.xs: seq,\n",
    "                    model.ys: res,\n",
    "                    # create initial state\n",
    "                }\n",
    "            else:\n",
    "                feed_dict = {\n",
    "                    model.xs: seq,\n",
    "                    model.ys: res,\n",
    "                    model.cell_init_state:\n",
    "                    state  # use last state as the initial state for this run\n",
    "                }\n",
    "#             print('seq',seq.shape)\n",
    "            _, cost, state, pred = sess.run([\n",
    "                model.train_op, model.cost, model.cell_final_state, model.pred\n",
    "            ],feed_dict=feed_dict)\n",
    "            pred_res = pred\n",
    "\n",
    "            result = sess.run(merged, feed_dict)\n",
    "            writer.add_summary(result, i)\n",
    "            \n",
    "        print('{0} cost: '.format(j), round(cost, 4))\n",
    "        BATCH_START = 0  #从头再来一遍\n",
    "\n",
    "        # 画图\n",
    "#         print(\"结果:\", pred_res.shape)\n",
    "        #与最后一次训练所用的数据保持一致\n",
    "       \n",
    "#         print('实际', train_y1.flatten().shape)\n",
    "\n",
    "    #     r_size = BATCH_SIZE * TIME_STEPS\n",
    "        ###画图###########################################################################\n",
    "        \n",
    "        #为了方便看，只显示了后100行数据\n",
    "        try:\n",
    "            axes.lines.remove(line1)\n",
    "        except Exception:\n",
    "            pass\n",
    "#         handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        # 返回是, 是一个Line2D的对象，无逗号是一个包含了Line2D的数组\n",
    "#         line1, = axes.plot(\n",
    "#             range(100), pred.flatten()[-100:], 'b--', label='rnn计算结果 '+str(j))\n",
    "        line1, = axes.plot(pred.flatten(), 'b--', label='rnn计算结果 '+str(j))\n",
    "    #     line2,=axes.plot(range(len(gbr_pridict)), gbr_pridict, 'r--',label='优选参数')\n",
    "#         plt.draw()\n",
    "        axes.grid()\n",
    "        fig.tight_layout()\n",
    "        #plt.legend(handles=[line1, line2,line3])\n",
    "#         plt.legend(handles=handles)\n",
    "        plt.legend(handles=[line1,line3])\n",
    "        plt.title('递归神经网络')\n",
    "    #     plt.show()\n",
    "        plt.pause(0.5)\n",
    "    plt.ioff()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # 保存模型\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir,)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T01:14:05.393739Z",
     "start_time": "2018-12-06T01:14:04.400177Z"
    },
    "code_folding": [
     19
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "时间段= 10 310\n",
      "x_part1= (300, 1)\n",
      "30 10 1\n",
      "seq (30, 10, 1)\n",
      "INFO:tensorflow:Restoring parameters from ./bst_save/ckpt/bst\n",
      "else\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'next_seq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e1b67131a5cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m         state, pred = sess.run([model.cell_final_state, model.pred],\n\u001b[1;32m     49\u001b[0m                                feed_dict=feed_dict)\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;31m#         print('next_seq', next_seq.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m#         print('next_seq',state.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'next_seq' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "gen_length = 500\n",
    "prime_word = '1'\n",
    "loaded_graph = tf.Graph()\n",
    "tf.reset_default_graph()\n",
    "seq, res = get_batch_boston()\n",
    "model = LSTMRNN(TIME_STEPS, INPUT_SIZE, OUTPUT_SIZE, CELL_SIZE, BATCH_SIZE)\n",
    "saver = tf.train.Saver()\n",
    "fig = plt.figure(figsize=(20, 3))  # dpi参数指定绘图对象的分辨率，即每英寸多少个像素，缺省值为80\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "train_y1 = train_y[190:490]\n",
    "train_y2 = train_y[0:300]\n",
    "train_y3 = train_y[0:506]\n",
    "line5, = axes.plot(train_y3.flatten(), 'y', label='实际3')\n",
    "# line3, = axes.plot(train_y1.flatten(), 'r', label='实际1')\n",
    "# line4, = axes.plot(train_y2.flatten(), 'g', label='实际2')\n",
    "plt.ion()\n",
    "plt.show()\n",
    "print('seq', seq.shape)\n",
    "with tf.Session() as sess:\n",
    "    # 加载保存过的session\n",
    "    #     loader = tf.train.import_meta_graph(save_dir + '.meta')\n",
    "    saver.restore(sess, './bst_save/ckpt/bst')\n",
    "    #取训练集的最后一行，作为开始\n",
    "    prev_seq = seq\n",
    "    #     print(type(seq))\n",
    "    #     print(seq.shape)\n",
    "    #     print(prev_seq.shape)\n",
    "    #     print(seq)\n",
    "    #     print(prev_seq)\n",
    "    #     Dimensions of inputs should match: shape[0] = [1,10] vs. shape[1] = [30,10]\n",
    "    #     predict=[]\n",
    "    #     得到之后100个预测结果\n",
    "    for i in range(20):\n",
    "        if i == 1:\n",
    "            print('if')\n",
    "            feed_dict = {\n",
    "                # create initial state\n",
    "                model.xs: seq  \n",
    "            }\n",
    "        else:\n",
    "            print('else')\n",
    "            feed_dict = {\n",
    "                model.xs: seq,\n",
    "                # use last state as the initial state for this run\n",
    "                model.cell_init_state: state \n",
    "            }\n",
    "        state, pred = sess.run([model.cell_final_state, model.pred],\n",
    "                               feed_dict=feed_dict)\n",
    "#         print(type(next_seq))\n",
    "#         print('next_seq', next_seq.shape)\n",
    "        #         print('next_seq',state.shape)\n",
    "#         next_seq_re = tf.reshape(next_seq, [30, 10])\n",
    "        #         predict.append(next_seq[-1])\n",
    "        #         print(prev_seq.shape)\n",
    "        #         print(next_seq.shape)\n",
    "        #         print(next_seq)\n",
    "        #         print(next_seq[-1])\n",
    "        #每次得到最后一个时间步的预测结果，与之前的数据加在一起，形成新的测试样本\n",
    "        # 需要把新的预测值加入，然后重新预测，如此循环\n",
    "        #         prev_seq=np.vstack((prev_seq[1:],next_seq[-1]))\n",
    "\n",
    "        try:\n",
    "            axes.lines.remove(line1)\n",
    "        except Exception:\n",
    "            pass\n",
    "        line1, = axes.plot(next_seq.flatten(), 'b--', label='rnn计算结果' + str(i))\n",
    "        axes.grid()\n",
    "        fig.tight_layout()\n",
    "        #plt.legend(handles=[line1, line2,line3])\n",
    "        #         plt.legend(handles=handles)\n",
    "        plt.legend(handles=[line1, line3])\n",
    "        plt.title('递归神经网络')\n",
    "        #     plt.show()\n",
    "        plt.pause(0.5)\n",
    "\n",
    "    #     plt.plot(list(range(len(normalize_data), len(normalize_data) + len(predict))), predict, color='r')\n",
    "    BATCH_START = 0\n",
    "    plt.ioff()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T03:26:15.855897Z",
     "start_time": "2018-12-03T03:26:15.071890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "时间段= 170 470\n",
      "x_part1= (300, 13)\n",
      "<class 'numpy.ndarray'>\n",
      "(300, 1)\n"
     ]
    }
   ],
   "source": [
    "# %matplotlib qt\n",
    "gen_length = 500\n",
    "prime_word = '1'\n",
    "loaded_graph = tf.Graph()\n",
    "tf.reset_default_graph()\n",
    "seq, res = get_batch_boston()\n",
    "model = LSTMRNN(TIME_STEPS, INPUT_SIZE, OUTPUT_SIZE, CELL_SIZE, BATCH_SIZE)\n",
    "saver = tf.train.Saver()\n",
    "fig = plt.figure(figsize=(20, 3))  # dpi参数指定绘图对象的分辨率，即每英寸多少个像素，缺省值为80\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "train_y1 = train_y[190:490]\n",
    "train_y2 = train_y[0:300]\n",
    "train_y3 = train_y\n",
    "# train_y1 = train_y[190:300]\n",
    "# train_y2 = train_y[190:300]\n",
    "\n",
    "print(type(train_y1))\n",
    "print(train_y1.shape)\n",
    "# train_y1.flatten()[-100:]\n",
    "# line3, = axes.plot(train_y1.flatten(), 'r', label='实际1>190')\n",
    "\n",
    "line5, = axes.plot(train_y3.flatten(), 'y', label='实际3')\n",
    "line4, = axes.plot(train_y2.flatten(), 'g', label='实际2>0')\n",
    "plt.legend(handles=[line4,line3])\n",
    "plt.ion()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T02:37:20.185309Z",
     "start_time": "2018-12-03T02:37:20.177864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a [[0 1]\n",
      " [2 3]]\n",
      "a1 [2 3]\n",
      "a (2, 2)\n",
      "a1 (2,)\n"
     ]
    }
   ],
   "source": [
    "a=np.arange(4).reshape(2,2)\n",
    "print('a',a)\n",
    "\n",
    "print('a1',a[1])\n",
    "\n",
    "print('a',a.shape)\n",
    "print('a1',a[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T02:35:58.191782Z",
     "start_time": "2018-12-03T02:35:58.184888Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]]\n",
      "[0 1 2 3 4 5]\n",
      "[4 5]\n"
     ]
    }
   ],
   "source": [
    "a=np.arange(6).reshape(3,2)\n",
    "print(a)\n",
    "print(a.flatten())\n",
    "print(a.flatten()[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T03:34:24.975787Z",
     "start_time": "2018-12-03T03:34:24.477551Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.subplot(2,1,1)      # 分成两行一列，起始点为1\n",
    "plt.plot([0,1],[0,1])   # 设置xy轴范围\n",
    "\n",
    "plt.subplot(2,3,4)      # 分成两行三列，起始点位4\n",
    "plt.plot([0,1],[0,2])\n",
    "\n",
    "plt.subplot(2,3,5)\n",
    "plt.plot([0,1],[0,3])\n",
    "\n",
    "plt.subplot(2,3,6)\n",
    "plt.plot([0,1],[0,4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T01:25:33.073469Z",
     "start_time": "2018-12-06T01:25:33.049645Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3  4  5  6]\n",
      " [ 7  8  9 10 11 12 13]\n",
      " [14 15 16 17 18 19 20]\n",
      " [21 22 23 24 25 26 27]\n",
      " [28 29 30 31 32 33 34]\n",
      " [35 36 37 38 39 40 41]\n",
      " [42 43 44 45 46 47 48]\n",
      " [49 50 51 52 53 54 55]]\n",
      "[ 0  7 14 21 28 35 42 49]\n",
      "[-15.          80.66666667 176.33333333 272.         367.66666667\n",
      " 463.33333333 559.         654.66666667]\n",
      "a [[ 7  8  9 10 11 12 13]\n",
      " [14 15 16 17 18 19 20]\n",
      " [21 22 23 24 25 26 27]\n",
      " [28 29 30 31 32 33 34]\n",
      " [35 36 37 38 39 40 41]\n",
      " [42 43 44 45 46 47 48]\n",
      " [49 50 51 52 53 54 55]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a=np.arange(56).reshape(8,7)\n",
    "#X取前5行的前三列（从1计数）\n",
    "x=a[0:0+5,:3]\n",
    "#Y取前5行的第4列（从0计数），并行转列\n",
    "y=a[0:0+5,0,np.newaxis]\n",
    "z=a[:,0]\n",
    "print(a)\n",
    "print(z)\n",
    "zz =  (a[:,0] * 1000 / 60) - (a[:,4] + a[:,5] + a[:,6])\n",
    "print(zz)\n",
    "print('a',a[1:])\n",
    "# print(x.shape)\n",
    "# print(x)\n",
    "# print(y.shape)\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T07:03:07.784201Z",
     "start_time": "2018-12-06T07:03:07.776953Z"
    }
   },
   "outputs": [],
   "source": [
    "train_x,train_y=[],[]\n",
    "for i in range(len(y)-TIME_STEPS-1):\n",
    "    x1=y[i:i+TIME_STEPS]\n",
    "    y1=y[i+1:i+TIME_STEPS+1]\n",
    "    train_x.append(x1.tolist())\n",
    "    train_y.append(y1.tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T07:09:00.746644Z",
     "start_time": "2018-12-06T07:09:00.741467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "[[24.0, 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9], [21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15.0], [34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15.0, 18.9], [33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15.0, 18.9, 21.7], [36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15.0, 18.9, 21.7, 20.4], [28.7, 22.9, 27.1, 16.5, 18.9, 15.0, 18.9, 21.7, 20.4, 18.2], [22.9, 27.1, 16.5, 18.9, 15.0, 18.9, 21.7, 20.4, 18.2, 19.9], [27.1, 16.5, 18.9, 15.0, 18.9, 21.7, 20.4, 18.2, 19.9, 23.1], [16.5, 18.9, 15.0, 18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5], [18.9, 15.0, 18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2]]\n"
     ]
    }
   ],
   "source": [
    "print(len(train_x[-1]))\n",
    "print(len(train_x[0:10]))\n",
    "print(train_x[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
