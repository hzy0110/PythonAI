{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T03:49:56.484284Z",
     "start_time": "2018-10-10T03:49:41.485263Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T03:52:00.603019Z",
     "start_time": "2018-10-10T03:52:00.565272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, [[4.]]\n"
     ]
    }
   ],
   "source": [
    "tf.executing_eagerly()\n",
    "x = [[2.]]\n",
    "m = tf.matmul(x, x)\n",
    "print(\"hello, {}\".format(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T03:52:01.301345Z",
     "start_time": "2018-10-10T03:52:01.286963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[2 3]\n",
      " [4 5]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[1, 2], [3, 4]])\n",
    "print(a)\n",
    "\n",
    "b = tf.add(a, 1)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T03:52:01.899091Z",
     "start_time": "2018-10-10T03:52:01.887173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  6]\n",
      " [12 20]]\n",
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "c = np.multiply(a, b)\n",
    "print(c)\n",
    "print(a.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T03:52:02.411668Z",
     "start_time": "2018-10-10T03:52:02.396248Z"
    }
   },
   "outputs": [],
   "source": [
    "def fizzbuzz(max_num):\n",
    "    counter = tf.constant(0)\n",
    "    for num in range(max_num):\n",
    "        num = tf.constant(num)\n",
    "        if int(num % 3) == 0 and int(num % 5) == 0:\n",
    "            print('FizzBuzz')\n",
    "        elif int(num % 3) == 0:\n",
    "            print('Fizz')\n",
    "        elif int(num % 5) == 0:\n",
    "            print('Buzz')\n",
    "        else:\n",
    "            print(num)\n",
    "        counter += 1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T03:52:02.577891Z",
     "start_time": "2018-10-10T03:52:02.561355Z"
    }
   },
   "outputs": [],
   "source": [
    "class MySimpleLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, output_units):\n",
    "    self.output_units = output_units\n",
    "\n",
    "  def build(self, input):\n",
    "    # The build method gets called the first time your layer is used.\n",
    "    # Creating variables on build() allows you to make their shape depend\n",
    "    # on the input shape and hence remove the need for the user to specify\n",
    "    # full shapes. It is possible to create variables during __init__() if\n",
    "    # you already know their full shapes.\n",
    "    self.kernel = self.add_variable(\n",
    "      \"kernel\", [input.shape[-1], self.output_units])\n",
    "\n",
    "  def call(self, input):\n",
    "    # Override call() instead of __call__ so we can perform some bookkeeping.\n",
    "    return tf.matmul(input, self.kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T03:52:02.745145Z",
     "start_time": "2018-10-10T03:52:02.706019Z"
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10, input_shape=(784,)),  # must declare input shape\n",
    "  tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T03:52:03.327325Z",
     "start_time": "2018-10-10T03:52:03.312463Z"
    }
   },
   "outputs": [],
   "source": [
    "class MNISTModel(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(MNISTModel, self).__init__()\n",
    "    self.dense1 = tf.keras.layers.Dense(units=10)\n",
    "    self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "  def call(self, input):\n",
    "    \"\"\"Run the model.\"\"\"\n",
    "    result = self.dense1(input)\n",
    "    result = self.dense2(result)\n",
    "    result = self.dense2(result)  # reuse variables from dense2 layer\n",
    "    return result\n",
    "\n",
    "model = MNISTModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T03:52:16.287034Z",
     "start_time": "2018-10-10T03:52:03.903747Z"
    }
   },
   "outputs": [],
   "source": [
    "tfe = tf.contrib.eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T03:52:16.304656Z",
     "start_time": "2018-10-10T03:52:16.289257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=76, shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "w = tfe.Variable([[1.0]])\n",
    "with tf.GradientTape() as tape:\n",
    "  loss = w * w\n",
    "\n",
    "grad = tape.gradient(loss, [w])\n",
    "print(grad)  # => [tf.Tensor([[ 2.]], shape=(1, 1), dtype=float32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T03:52:18.678562Z",
     "start_time": "2018-10-10T03:52:18.610834Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 1968509.88it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(range(10000)):\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T03:52:20.216662Z",
     "start_time": "2018-10-10T03:52:19.841550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 69.164\n",
      "Loss at step 000: 66.462\n",
      "Loss at step 020: 30.154\n",
      "Loss at step 040: 13.981\n",
      "Loss at step 060: 6.778\n",
      "Loss at step 080: 3.569\n",
      "Loss at step 100: 2.140\n",
      "Loss at step 120: 1.503\n",
      "Loss at step 140: 1.219\n",
      "Loss at step 160: 1.093\n",
      "Loss at step 180: 1.037\n",
      "Final loss: 1.013\n",
      "W = 3.0266048908233643, B = 2.135002374649048\n"
     ]
    }
   ],
   "source": [
    "# A toy dataset of points around 3 * x + 2\n",
    "NUM_EXAMPLES = 1000\n",
    "training_inputs = tf.random_normal([NUM_EXAMPLES])\n",
    "noise = tf.random_normal([NUM_EXAMPLES])\n",
    "training_outputs = training_inputs * 3 + 2 + noise\n",
    "\n",
    "\n",
    "def prediction(input, weight, bias):\n",
    "    return input * weight + bias\n",
    "\n",
    "\n",
    "# A loss function using mean-squared error\n",
    "def loss(weights, biases):\n",
    "    error = prediction(training_inputs, weights, biases) - training_outputs\n",
    "    return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "\n",
    "# Return the derivative of loss with respect to weight and bias\n",
    "def grad(weights, biases):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(weights, biases)\n",
    "    return tape.gradient(loss_value, [weights, biases])\n",
    "\n",
    "\n",
    "train_steps = 200\n",
    "learning_rate = 0.01\n",
    "# Start with arbitrary values for W and B on the same batch of data\n",
    "W = tfe.Variable(5.)\n",
    "B = tfe.Variable(10.)\n",
    "\n",
    "print(\"Initial loss: {:.3f}\".format(loss(W, B)))\n",
    "\n",
    "for i in range(train_steps):\n",
    "    dW, dB = grad(W, B)\n",
    "    W.assign_sub(dW * learning_rate)\n",
    "    B.assign_sub(dB * learning_rate)\n",
    "    if i % 20 == 0:\n",
    "        print(\"Loss at step {:03d}: {:.3f}\".format(i, loss(W, B)))\n",
    "\n",
    "print(\"Final loss: {:.3f}\".format(loss(W, B)))\n",
    "print(\"W = {}, B = {}\".format(W.numpy(), B.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T03:52:22.699480Z",
     "start_time": "2018-10-10T03:52:22.577522Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-860102f6813d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m dataset = tf.data.Dataset.from_tensor_slices((data.train.images,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                               data.train.labels))\n\u001b[1;32m      3\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((data.train.images,\n",
    "                                              data.train.labels))\n",
    "...\n",
    "for (batch, (images, labels)) in enumerate(dataset):\n",
    "    ...\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(images, training=True)\n",
    "        loss_value = loss(logits, labels)\n",
    "    ...\n",
    "    grads = tape.gradient(loss_value, model.variables)\n",
    "    optimizer.apply_gradients(\n",
    "        zip(grads, model.variables),\n",
    "        global_step=tf.train.get_or_create_global_step())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T03:52:29.625628Z",
     "start_time": "2018-10-10T03:52:29.598531Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 784)\n"
     ]
    }
   ],
   "source": [
    "batch = tf.zeros([1, 1, 784])\n",
    "print(batch.shape)  # => (1, 1, 784)\n",
    "\n",
    "result = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T03:52:32.882694Z",
     "start_time": "2018-10-10T03:52:30.146745Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://storage.googleapis.com/cvdf-datasets/mnist/train-labels-idx1-ubyte.gz to /var/folders/5p/sww1_hjj4dx_bx7v5m0h08vh0000gn/T/tmp2dkm1vg5.gz\n"
     ]
    }
   ],
   "source": [
    "import dataset  # download dataset.py file\n",
    "dataset_train = dataset.train('./datasets').shuffle(60000).repeat(4).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T03:53:41.844815Z",
     "start_time": "2018-10-10T03:52:32.885655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 2.323\n",
      "Loss at step 0000: 2.334\n",
      "Loss at step 0200: 2.125\n",
      "Loss at step 0400: 2.046\n",
      "Loss at step 0600: 1.969\n",
      "Loss at step 0800: 1.930\n",
      "Loss at step 1000: 1.826\n",
      "Loss at step 1200: 1.879\n",
      "Loss at step 1400: 1.770\n",
      "Loss at step 1600: 1.593\n",
      "Loss at step 1800: 1.629\n",
      "Loss at step 2000: 1.306\n",
      "Loss at step 2200: 1.324\n",
      "Loss at step 2400: 1.300\n",
      "Loss at step 2600: 1.262\n",
      "Loss at step 2800: 1.315\n",
      "Loss at step 3000: 1.058\n",
      "Loss at step 3200: 1.025\n",
      "Loss at step 3400: 1.096\n",
      "Loss at step 3600: 0.831\n",
      "Loss at step 3800: 0.948\n",
      "Loss at step 4000: 0.826\n",
      "Loss at step 4200: 1.062\n",
      "Loss at step 4400: 0.963\n",
      "Loss at step 4600: 0.960\n",
      "Loss at step 4800: 0.768\n",
      "Loss at step 5000: 0.712\n",
      "Loss at step 5200: 0.629\n",
      "Loss at step 5400: 0.645\n",
      "Loss at step 5600: 0.651\n",
      "Loss at step 5800: 0.749\n",
      "Loss at step 6000: 0.658\n",
      "Loss at step 6200: 0.770\n",
      "Loss at step 6400: 0.959\n",
      "Loss at step 6600: 0.530\n",
      "Loss at step 6800: 0.485\n",
      "Loss at step 7000: 0.622\n",
      "Loss at step 7200: 0.474\n",
      "Loss at step 7400: 0.507\n",
      "Final loss: 0.649\n"
     ]
    }
   ],
   "source": [
    "def loss(model, x, y):\n",
    "  prediction = model(x)\n",
    "  return tf.losses.sparse_softmax_cross_entropy(labels=y, logits=prediction)\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss_value = loss(model, inputs, targets)\n",
    "  return tape.gradient(loss_value, model.variables)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "\n",
    "x, y = iter(dataset_train).next()\n",
    "print(\"Initial loss: {:.3f}\".format(loss(model, x, y)))\n",
    "\n",
    "# Training loop\n",
    "for (i, (x, y)) in enumerate(dataset_train):\n",
    "  # Calculate derivatives of the input function with respect to its parameters.\n",
    "  grads = grad(model, x, y)\n",
    "  # Apply the gradient to the model\n",
    "  optimizer.apply_gradients(zip(grads, model.variables),\n",
    "                            global_step=tf.train.get_or_create_global_step())\n",
    "  if i % 200 == 0:\n",
    "    print(\"Loss at step {:04d}: {:.3f}\".format(i, loss(model, x, y)))\n",
    "\n",
    "print(\"Final loss: {:.3f}\".format(loss(model, x, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T06:28:26.159986Z",
     "start_time": "2018-10-10T06:28:26.097406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.0>\n"
     ]
    }
   ],
   "source": [
    "x = tfe.Variable(10.)\n",
    "\n",
    "checkpoint = tfe.Checkpoint(x=x)  # save as \"x\"\n",
    "\n",
    "x.assign(2.)  # Assign a new value to the variables and save.\n",
    "save_path = checkpoint.save('./ckpt/')\n",
    "\n",
    "x.assign(11.)  # Change the variable after saving.\n",
    "\n",
    "# Restore values from the checkpoint\n",
    "checkpoint.restore(save_path)\n",
    "\n",
    "print(x)  # => 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T06:29:03.130707Z",
     "start_time": "2018-10-10T06:29:03.103797Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MyModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-2b8539e3c031>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcheckpoint_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/path/to/model_dir'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcheckpoint_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m root = tfe.Checkpoint(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MyModel' is not defined"
     ]
    }
   ],
   "source": [
    "model = MyModel()\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "checkpoint_dir = '/path/to/model_dir'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "root = tfe.Checkpoint(\n",
    "    optimizer=optimizer,\n",
    "    model=model,\n",
    "    optimizer_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "root.save(file_prefix=checkpoint_prefix)\n",
    "# or\n",
    "root.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
